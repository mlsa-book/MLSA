<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Raphael Sonabend and Andreas Bender">

<title>13&nbsp; Boosting Methods – Machine Learning in Survival Analysis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./P3C17_neural.html" rel="next">
<link href="./P3C15_svm.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-be0bf478834dcd5fa1d207f31fcfa0a3.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-1008a5a8f5e3c2ed0fd245401abc5d00.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./P3C13_classical.html">Models</a></li><li class="breadcrumb-item"><a href="./P3C16_boosting.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Boosting Methods</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning in Survival Analysis</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/mlsa-book/MLSA/tree/main/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Machine-Learning-in-Survival-Analysis.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P0C0_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Symbols and Notation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P0C1_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Survival Analysis and Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P1C3_machinelearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Machine Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P1C4_survival.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Survival Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P1C5_eha.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Event-history Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P1C6_survtsk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Survival Task</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Evaluation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P2C8_rank.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Discrimination</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P2C9_calib.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Calibration</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P2C10_rules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Scoring Rules</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P2C11_time.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Survival Time Measures</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P3C13_classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Traditional Survival Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P3C14_forests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Random Forests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P3C15_svm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P3C16_boosting.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Boosting Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P3C17_neural.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Reduction Techniques</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P4C19_reductions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Reductions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P4C20_competing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Competing Risks Pipelines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P4C21_discrete.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Discrete Time Survival Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P4C22_poisson.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Connections to Poisson Regression and Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P4C23_pseudo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Connections to Regression and Imputation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P5C24_conclusions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">FAQs and Outlook</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P5C26_references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-surv-ml-models-boost-regr" id="toc-sec-surv-ml-models-boost-regr" class="nav-link active" data-scroll-target="#sec-surv-ml-models-boost-regr"><span class="header-section-number">13.1</span> GBMs for Regression</a></li>
  <li><a href="#sec-surv-ml-models-boost-surv" id="toc-sec-surv-ml-models-boost-surv" class="nav-link" data-scroll-target="#sec-surv-ml-models-boost-surv"><span class="header-section-number">13.2</span> GBMs for Survival Analysis</a>
  <ul class="collapse">
  <li><a href="#ph-and-aft-gbms" id="toc-ph-and-aft-gbms" class="nav-link" data-scroll-target="#ph-and-aft-gbms"><span class="header-section-number">13.2.1</span> PH and AFT GBMs</a></li>
  <li><a href="#discrimination-boosting" id="toc-discrimination-boosting" class="nav-link" data-scroll-target="#discrimination-boosting"><span class="header-section-number">13.2.2</span> Discrimination Boosting</a></li>
  <li><a href="#coxboost" id="toc-coxboost" class="nav-link" data-scroll-target="#coxboost"><span class="header-section-number">13.2.3</span> CoxBoost</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">13.3</span> Conclusion</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/mlsa-book/MLSA/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/mlsa-book/MLSA/edit/main/book/P3C16_boosting.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/mlsa-book/MLSA/blob/main/book/P3C16_boosting.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./P3C13_classical.html">Models</a></li><li class="breadcrumb-item"><a href="./P3C16_boosting.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Boosting Methods</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-boost" class="quarto-section-identifier"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Boosting Methods</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    TODO (150-200 WORDS)
  </div>
</div>


</header>


<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Major changes expected!
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>This page is a work in progress and major changes will be made over time.</strong></p>
</div>
</div>
<p>Boosting is a machine learning strategy that can be applied to any model class. Similarly to random forests, boosting is an ensemble method that creates a model from a ‘committee’ of learners. The committee is formed of <em>weak</em> learners that make poor predictions individually, which creates a <em>slow learning</em> approach (as opposed to ‘greedy’) that requires many iterations for a model to be a good fit to the data. Boosting models are similar to random forests in that both make predictions from a large committee of learners. However the two differ in how the members of the committee are correlated and in how they are combined to make a prediction. In random forests, each decision tree is grown independently and their predictions are combined by a simple mean calculation. In contrast, weak learners in a boosting model are fit sequentially with errors from one learner used to train the next, predictions are then made by a linear combination of predictions from each learner (<a href="#fig-boosting" class="quarto-xref">Figure&nbsp;<span>13.1</span></a>).</p>
<section id="sec-surv-ml-models-boost-regr" class="level2" data-number="13.1">
<h2 data-number="13.1" class="anchored" data-anchor-id="sec-surv-ml-models-boost-regr"><span class="header-section-number">13.1</span> GBMs for Regression</h2>
<p>One of the earliest boosting algorithms is AdaBoost <span class="citation" data-cites="Freund1996">(<a href="P5C26_references.html#ref-Freund1996" role="doc-biblioref">Freund and Schapire 1996</a>)</span>, which is more generally a Forward Stagewise Additive Model (FSAM) with an exponential loss <span class="citation" data-cites="Hastie2001">(<a href="P5C26_references.html#ref-Hastie2001" role="doc-biblioref">Hastie, Tibshirani, and Friedman 2001</a>)</span>. Today, the most widely used boosting model is the Gradient Boosting Machine (GBM) <span class="citation" data-cites="Friedman2001">(<a href="P5C26_references.html#ref-Friedman2001" role="doc-biblioref">J. H. Friedman 2001</a>)</span> or extensions thereof.</p>
<p><a href="#fig-boosting" class="quarto-xref">Figure&nbsp;<span>13.1</span></a> illustrates the process of training a GBM in a least-squares regression setting:</p>
<ol type="1">
<li>A weak learner, <span class="math inline">\(f_1\)</span>, often a decision tree of shallow depth is fit on the training data <span class="math inline">\((\mathbf{X}, \mathbf{y})\)</span>.</li>
<li>Predictions from the learner, <span class="math inline">\(f_1(\mathbf{X})\)</span>, are compared to the ground truth, <span class="math inline">\(\mathbf{y}\)</span>, and the residuals are calculated as <span class="math inline">\(\mathbf{r}_1 = f_1(\mathbf{X}) - \mathbf{y}\)</span>.</li>
<li>The next weak learner, <span class="math inline">\(f_2\)</span>, uses the previous residuals for the target prediction, <span class="math inline">\((\mathbf{X}, \mathbf{r}_1)\)</span></li>
<li>This is repeated to train <span class="math inline">\(M\)</span> learners, <span class="math inline">\(f_1,...,f_M\)</span></li>
</ol>
<p>Predictions are then made as <span class="math inline">\(\hat{\mathbf{y}} = f_1(\mathbf{X}) + f_2(\mathbf{X}) + ... + f_M(\mathbf{X})\)</span>.</p>
<div id="fig-boosting" class="quarto-float quarto-figure quarto-figure-center anchored" alt="TODO">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-boosting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/boosting/boosting.png" class="img-fluid figure-img" alt="TODO">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-boosting-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13.1: Least squares regression Boosting algorithm where the gradient is calculated as the difference between ground truth and predictions.
</figcaption>
</figure>
</div>
<p>This is a simplification of the general gradient boosting algorithm, where the residuals are used to train the next model. More generally, a suitable, differentiable loss function relating to the problem of interest is chosen and the negative gradient is computed by comparing the predictions in each iteration with the ground truth. Residuals can be used in the regression case as these are proportional to the negative gradient of the mean squared error.</p>
<p>The algorithm above is also a simplification as no hyper-parameters other than <span class="math inline">\(M\)</span> were included for controlling the algorithm. In order to reduce overfitting, three common hyper-parameters are utilised:</p>
<p><strong>Number of iterations</strong>, <span class="math inline">\(M\)</span>: The number of iterations is often claimed to be the most important hyper-parameter in GBMs and it has been demonstrated that as the number of iterations increases, so too does the model performance (with respect to a given loss on test data) up to a certain point of overfitting <span class="citation" data-cites="Buhlmann2006 Hastie2001 Schmid2008a">(<a href="P5C26_references.html#ref-Buhlmann2006" role="doc-biblioref">Buhlmann 2006</a>; <a href="P5C26_references.html#ref-Hastie2001" role="doc-biblioref">Hastie, Tibshirani, and Friedman 2001</a>; <a href="P5C26_references.html#ref-Schmid2008a" role="doc-biblioref">Schmid and Hothorn 2008a</a>)</span>. This makes sense as the foundation of boosting rests on the idea that weak learners can slowly be combined to form a single powerful model. Finding the optimal value of <span class="math inline">\(M\)</span> is critical as a value too small will result in poor predictions, whilst a value too large will result in model overfitting.</p>
<p><strong>Subsampling proportion</strong>, <span class="math inline">\(\phi\)</span>: Sampling a fraction, <span class="math inline">\(\phi\)</span>, of the training data at each iteration can improve performance and reduce runtime <span class="citation" data-cites="Hastie2001">(<a href="P5C26_references.html#ref-Hastie2001" role="doc-biblioref">Hastie, Tibshirani, and Friedman 2001</a>)</span>, with <span class="math inline">\(\phi = 0.5\)</span> often used. Motivated by the success of bagging in random forests, stochastic gradient boosting <span class="citation" data-cites="Friedman1999">(<a href="P5C26_references.html#ref-Friedman1999" role="doc-biblioref">J. Friedman 1999</a>)</span> randomly samples the data in each iteration. It appears that subsampling performs best when also combined with shrinkage <span class="citation" data-cites="Hastie2001">(<a href="P5C26_references.html#ref-Hastie2001" role="doc-biblioref">Hastie, Tibshirani, and Friedman 2001</a>)</span> and as with the other hyper-parameters, selection of <span class="math inline">\(\phi\)</span> is usually performed by nested cross-validation.</p>
<p><strong>Step-size</strong>, <span class="math inline">\(\nu\)</span>: The step-size parameter is a shrinkage parameter that controls the contribution of each weak learner at each iteration. Several studies have demonstrated that GBMs perform better when shrinkage is applied and a value of <span class="math inline">\(\nu = 0.1\)</span> is often suggested <span class="citation" data-cites="Buhlmann2007 Hastie2001 Friedman2001 Lee2018 Schmid2008a">(<a href="P5C26_references.html#ref-Buhlmann2007" role="doc-biblioref">Buhlmann and Hothorn 2007</a>; <a href="P5C26_references.html#ref-Hastie2001" role="doc-biblioref">Hastie, Tibshirani, and Friedman 2001</a>; <a href="P5C26_references.html#ref-Friedman2001" role="doc-biblioref">J. H. Friedman 2001</a>; <a href="P5C26_references.html#ref-Lee2018" role="doc-biblioref">Lee, Chen, and Ishwaran 2019</a>; <a href="P5C26_references.html#ref-Schmid2008a" role="doc-biblioref">Schmid and Hothorn 2008a</a>)</span>. The optimal values of <span class="math inline">\(\nu\)</span> and <span class="math inline">\(M\)</span> depend on each other, such that smaller values of <span class="math inline">\(\nu\)</span> require larger values of <span class="math inline">\(M\)</span>, and vice versa. This is intuitive as smaller <span class="math inline">\(\nu\)</span> results in a slower learning algorithm and therefore more iterations are required to fit the model. Accurately selecting the <span class="math inline">\(M\)</span> parameter is generally considered to be of more importance, and therefore a value of <span class="math inline">\(\nu\)</span> is often chosen heuristically (e.g.&nbsp;the common value of <span class="math inline">\(0.1\)</span>) and then <span class="math inline">\(M\)</span> is tuned by cross-validation and/or early-stopping, which is the process of monitoring the model’s training performance and stopping when a set performance is reached or when performance stagnates (i.e., no improvement over a set number of rounds).</p>
<p>As well as these parameters, the underlying weak learner hyper-parameters are also commonly tuned. If using a decision tree, then it is usual to restrict the number of terminal nodes in the tree to be between <span class="math inline">\(4\)</span> and <span class="math inline">\(8\)</span>, which corresponds to two or three splits in the tree. Including these hyper-parameters, the general gradient boosting machine algorithm is as follows:</p>
<ol type="1">
<li><span class="math inline">\(g_0 \gets \text{ Initial guess}\)</span></li>
<li><strong>For</strong> <span class="math inline">\(m = 1,...,M\)</span>:</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(\mathcal{D}_{train}^* \gets \text{ Randomly sample } \mathcal{D}_{train}\text{ with probability } \phi\)</span></li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(r_{im} \gets -[\frac{\partial L(y_i, g_{m-1}(X_i))}{\partial g_{m-1}(X_i)}], \forall i \in \{i: X_i \in \mathcal{D}_{train}^*\}\)</span></li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Fit a weak learner, <span class="math inline">\(h_m\)</span>, to <span class="math inline">\((\mathbf{X}, \mathbf{r}_m)\)</span></li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="math inline">\(g_m \gets g_{m-1} + \nu h_m\)</span></li>
<li><strong>end For</strong></li>
<li><strong>return</strong> <span class="math inline">\(\hat{g}= g_M\)</span></li>
</ol>
<p>Note:</p>
<ol type="1">
<li>The initial guess, <span class="math inline">\(g_0\)</span>, is often the mean of <span class="math inline">\(y\)</span> for regression problems but can also simply be <span class="math inline">\(0\)</span>.</li>
<li>Line 4 is the calculation of the negative gradient, which is equivalent to calculating the residuals in a regression problem with the mean squared error loss.</li>
<li>Lines 5-6 differ between implementations, with some fitting multiple weak learners and selecting the one that minimizes a simple optimization problem. The version above is simplest to implement and quickest to run, whilst still providing good model performance.</li>
</ol>
<p>Once the model is trained, predictions are made for new data, <span class="math inline">\(\mathbf{X}_{test}\)</span> with</p>
<p><span class="math display">\[
\hat{Y}= \hat{g}(\mathbf{X}_{test}) = g_0(\mathbf{X}_{test}) + \nu \sum^M_{i=1} g_i(\mathbf{X}_{test})
\]</span></p>
<p>GBMs provide a flexible, modular algorithm, primarily comprised of a differentiable loss to minimise, <span class="math inline">\(L\)</span>, and the selection of weak learners. This chapter focuses on tree-based weak learners, though other weak learners are possible. Perhaps the most common alternatives are linear least squares <span class="citation" data-cites="Friedman2001">(<a href="P5C26_references.html#ref-Friedman2001" role="doc-biblioref">J. H. Friedman 2001</a>)</span> and smoothing splines <span class="citation" data-cites="Buhlmann2003">(<a href="P5C26_references.html#ref-Buhlmann2003" role="doc-biblioref">Bühlmann and Yu 2003</a>)</span>, we will not discuss these further here as decision trees are primarily used for survival analysis, due the flexibility demonstrated in <a href="P3C14_forests.html" class="quarto-xref"><span>Chapter 11</span></a>. See references at the end of the chapter for other weak learners. Extension to survival analysis therefore follows by considering alternative losses.</p>
</section>
<section id="sec-surv-ml-models-boost-surv" class="level2" data-number="13.2">
<h2 data-number="13.2" class="anchored" data-anchor-id="sec-surv-ml-models-boost-surv"><span class="header-section-number">13.2</span> GBMs for Survival Analysis</h2>
<p>Unlike other machine learning algorithms that historically ignored survival analysis, early GBM papers considered boosting in a survival context <span class="citation" data-cites="Ridgeway1999">(<a href="P5C26_references.html#ref-Ridgeway1999" role="doc-biblioref">Ridgeway 1999</a>)</span>; though there appears to be a decade gap before further considerations were made in the survival setting. After that period, developments, discussed in this chapter, by Binder, Schmid, and Hothorn, adapted GBMs to a framework suitable for survival analysis.</p>
<p>All survival GBMs make ranking predictions and none are able to directly predict survival distributions. However, depending on the underlying model, the predictions may be indirectly composed into a survival distribution, for example algorithms that assume a proportional hazards (PH) or accelerated failure time (AFT) form. This section starts with those models with simpler underlying forms, then explores more complex alternatives.</p>
<section id="ph-and-aft-gbms" class="level3" data-number="13.2.1">
<h3 data-number="13.2.1" class="anchored" data-anchor-id="ph-and-aft-gbms"><span class="header-section-number">13.2.1</span> PH and AFT GBMs</h3>
<p>The negative log-likelihood of the semi-parametric PH and fully-parametric AFT models can be derived from the (partial) likelihoods presented in <a href="P1C4_survival.html#sec-surv-estimation-param" class="quarto-xref"><span>Section 3.5.1</span></a>. Given the likelihoods measure the goodness of fit of model parameters, algorithms that use these losses use boosting to train the model coefficients, <span class="math inline">\(\boldsymbol{\beta}\)</span>, hence at each iteration in the algorithm, <span class="math inline">\(g_m(\mathbf{x}_i) = \mathbf{x}_i\boldsymbol{\beta}^{(m)}\)</span>, where <span class="math inline">\(\boldsymbol{\beta}^{(m)}\)</span> are the updated coefficients in iteration <span class="math inline">\(m\)</span>.</p>
<p>The Cox partial likelihood <span class="citation" data-cites="Cox1972 Cox1975">(<a href="P5C26_references.html#ref-Cox1972" role="doc-biblioref">Cox 1972</a>, <a href="P5C26_references.html#ref-Cox1975" role="doc-biblioref">1975</a>)</span> is given by</p>
<p><span class="math display">\[
L^{PH}(\boldsymbol{\beta}) = \prod^n_{i:\delta_i=1} \frac{\exp(\eta_i)}{\sum^n_{j \in \mathcal{R}_{t_i}} \exp(\eta_j)}
\]</span></p>
<p>with corresponding negative log-likelihood</p>
<p><span id="eq-surv-logpartial"><span class="math display">\[
-l^{PH}(\boldsymbol{\beta}) = -\sum^n_{i=1} \delta_i \Big[\eta_i \ - \ \log\Big(\sum^n_{j \in \mathcal{R}_{t_i}} \exp(\eta_i)\Big)\Big]
\tag{13.1}\]</span></span> where <span class="math inline">\(\mathcal{R}_{t_i}\)</span> is the set of patients at risk at time <span class="math inline">\(t_i\)</span> and <span class="math inline">\(\eta_i = \mathbf{x}_i\boldsymbol{\beta}\)</span>.</p>
<p>The gradient of <span class="math inline">\(-l^{PH}\)</span> at iteration <span class="math inline">\(m\)</span> is then <span id="eq-surv-partialgrad"><span class="math display">\[
r_{im} := \delta_i - \sum^n_{j=1} \delta_j \frac{\mathbb{I}(t_i \geq t_j) \exp(g_{m-1}(\mathbf{x}_i))}{\sum_{k \in \mathcal{R}_{t_j}} \exp(g_{m-1}(\mathbf{x}_k))}
\tag{13.2}\]</span></span> where <span class="math inline">\(g_{m-1}(\mathbf{x}_i) = \mathbf{x}_i\boldsymbol{\beta}^{(m-1)}\)</span>.</p>
<p>For non-PH data, boosting an AFT model can outperform boosted PH models <span class="citation" data-cites="Schmid2008b">(<a href="P5C26_references.html#ref-Schmid2008b" role="doc-biblioref">Schmid and Hothorn 2008b</a>)</span>. The AFT is defined by <span class="math display">\[
\log \mathbf{y}= \boldsymbol{\eta}+ \sigma W
\]</span> where <span class="math inline">\(W\)</span> is a random noise variable independent of <span class="math inline">\(X\)</span>, and <span class="math inline">\(\sigma\)</span> is a scale parameter controlling the amount of noise; again <span class="math inline">\(\boldsymbol{\eta}= \mathbf{X}\boldsymbol{\beta}\)</span>. By assuming a distribution on <span class="math inline">\(W\)</span>, a distribution is assumed for the full parametric model. The model is boosted by simultaneously estimating <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\boldsymbol{\beta}\)</span>. Assuming a location-scale distribution with location <span class="math inline">\(g(\mathbf{x}_i)\)</span> and scale <span class="math inline">\(\sigma\)</span>, one can derive the negative log-likelihood in the <span class="math inline">\(m\)</span>th iteration as <span class="citation" data-cites="Klein2003">(<a href="P5C26_references.html#ref-Klein2003" role="doc-biblioref">Klein and Moeschberger 2003</a>)</span></p>
<p><span class="math display">\[
\begin{split}
-l^{AFT}_m(\boldsymbol{\beta}) = -\sum^n_{i=1} \delta_i\Big[- \log\sigma + \log f_W\Big(\frac{\log(t_i) - \hat{g}_{m-1}(\mathbf{x}_i)}{\hat{\sigma}_{m-1}}\Big)\Big] + \\
(1-\delta_i)\Big[\log S_W\Big(\frac{\log(t_i) - \hat{g}_{m-1}(\mathbf{x}_i)}{\hat{\sigma}_{m-1}}\Big)\Big]
\end{split}
\]</span></p>
<p>where <span class="math inline">\(\hat{g}_{m-1}, \hat{\sigma}_{m-1}\)</span> are the location-scale parameters estimated in the previous iteration. Note this key difference to other GBM methods in which two estimates are made in each iteration step. After updating <span class="math inline">\(\hat{g}_m\)</span>, the scale parameter, <span class="math inline">\(\hat{\sigma}_m\)</span>, is updated as <span class="math display">\[
\hat{\sigma}_m := \mathop{\mathrm{arg\,min}}_\sigma -l^{AFT}_m(\boldsymbol{\beta})
\]</span> <span class="math inline">\(\sigma_0\)</span> is commonly initialized as <span class="math inline">\(1\)</span> <span class="citation" data-cites="Schmid2008b">(<a href="P5C26_references.html#ref-Schmid2008b" role="doc-biblioref">Schmid and Hothorn 2008b</a>)</span>.</p>
<p>As well as boosting fully-parametric AFTs, one could also consider boosting semi-parametric AFTs, for example using the Gehan loss <span class="citation" data-cites="Johnson2011">(<a href="P5C26_references.html#ref-Johnson2011" role="doc-biblioref">Johnson and Long 2011</a>)</span> or using Buckley-James imputation <span class="citation" data-cites="Wang2010">(<a href="P5C26_references.html#ref-Wang2010" role="doc-biblioref">Wang and Wang 2010</a>)</span>. However, known problems with semi-parametric AFT models and the Buckey-James procedure <span class="citation" data-cites="Wei1992">(<a href="P5C26_references.html#ref-Wei1992" role="doc-biblioref">Wei 1992</a>)</span>, as well as a lack of off-shelf implementation, mean that these methods are rarely used in practice.</p>
</section>
<section id="discrimination-boosting" class="level3" data-number="13.2.2">
<h3 data-number="13.2.2" class="anchored" data-anchor-id="discrimination-boosting"><span class="header-section-number">13.2.2</span> Discrimination Boosting</h3>
<p>Instead of optimising models based on a given model form, one could instead estimate <span class="math inline">\(\hat{\eta}\)</span> by optimizing a concordance index, such as Uno’s or Harrell’s C <span class="citation" data-cites="Chen2013 Mayr2014">(<a href="P5C26_references.html#ref-Chen2013" role="doc-biblioref">Y. Chen et al. 2013</a>; <a href="P5C26_references.html#ref-Mayr2014" role="doc-biblioref">Mayr and Schmid 2014</a>)</span>. Consider Uno’s C (<a href="P2C8_rank.html#sec-eval-crank-disc-conc" class="quarto-xref"><span>Section 6.1</span></a>): <span class="math display">\[
C_U(\hat{g}, \mathcal{D}_{train}) = \frac{\sum_{i \neq j}\delta_i\{\hat{G}_{KM}(t_i)\}^{-2}\mathbb{I}(t_i &lt; t_j)\mathbb{I}(\hat{g}(\mathbf{x}_i) &gt;\hat{g}(\mathbf{x}_j))}{\sum_{i \neq j}\delta_i\{\hat{G}_{KM}(t_i)\}^{-2}\mathbb{I}(t_i &lt; t_j)}
\]</span></p>
<p>The GBM algorithm requires that the chosen loss, here <span class="math inline">\(C_U\)</span>, be differentiable with respect to <span class="math inline">\(\hat{g}(X)\)</span>, which is not the case here due to the indicator term, <span class="math inline">\(\mathbb{I}(\hat{g}(X_i) &gt; \hat{g}(X_j))\)</span>, however this term can be replaced with a sigmoid function to create a differentiable loss <span class="citation" data-cites="Ma2006">(<a href="P5C26_references.html#ref-Ma2006" role="doc-biblioref">Ma and Huang 2006</a>)</span></p>
<p><span class="math display">\[
K(u|\omega) = \frac{1}{1 + \exp(-u/\omega)}
\]</span></p>
<p>where <span class="math inline">\(\omega\)</span> is a tunable hyper-parameter controlling the smoothness of the approximation. The measure to optimise is then,</p>
<p><span id="eq-surv-gbm-cus"><span class="math display">\[
C_{USmooth}(\boldsymbol{\beta}|\omega) = \sum_{i \neq j} \frac{k_{ij}}{1 + \exp\big[(\hat{g}(X_j) - \hat{g}(X_i))/\omega)\big]}
\tag{13.3}\]</span></span></p>
<p>with</p>
<p><span class="math display">\[
k_{ij} = \frac{\Delta_i (\hat{G}_{KM}(T_i))^{-2}\mathbb{I}(T_i &lt; T_j)}{\sum_{i \neq j} \Delta_i(\hat{G}_{KM}(T_i))^{-2}\mathbb{I}(T_i &lt; T_j)}
\]</span></p>
<p>The negative gradient at iteration <span class="math inline">\(m\)</span> for observation <span class="math inline">\(i\)</span> is then calculated as,</p>
<p><span id="eq-surv-gbm-cus-grad"><span class="math display">\[
r_{im} := - \sum^n_{j = 1} k_{ij} \frac{-\exp(\frac{\hat{g}_{m-1}(\mathbf{x}_j) - \hat{g}_{m-1}(\mathbf{x}_i)}{\omega})}{\omega(1 + \exp(\frac{\hat{g}_{m-1}(\mathbf{x}_j) - \hat{g}_{m-1}(\mathbf{x}_i)}{\omega}))}
\tag{13.4}\]</span></span></p>
<p>The GBM algorithm is then followed as normal with the above loss and gradient. This algorithm may be more insensitive to overfitting than others <span class="citation" data-cites="Mayr2016">(<a href="P5C26_references.html#ref-Mayr2016" role="doc-biblioref">Mayr, Hofner, and Schmid 2016</a>)</span>, however stability selection <span class="citation" data-cites="Meinshausen2010">(<a href="P5C26_references.html#ref-Meinshausen2010" role="doc-biblioref">Meinshausen and Bühlmann 2010</a>)</span>, which is implemented in off-shelf software packages <span class="citation" data-cites="pkgmboost">(<a href="P5C26_references.html#ref-pkgmboost" role="doc-biblioref">Hothorn et al. 2020</a>)</span>, can be considered for variable selection.</p>
</section>
<section id="coxboost" class="level3" data-number="13.2.3">
<h3 data-number="13.2.3" class="anchored" data-anchor-id="coxboost"><span class="header-section-number">13.2.3</span> CoxBoost</h3>
<p>Finally, ‘CoxBoost’ is an alternative method to boost Cox models and has been demonstrated to perform well in experiments. This algorithm boosts the Cox PH by optimising the penalized partial-log likelihood; additionally the algorithm allows for mandatory (or ‘forced’) covariates <span class="citation" data-cites="Binder2008">(<a href="P5C26_references.html#ref-Binder2008" role="doc-biblioref">Binder and Schumacher 2008</a>)</span>. In medical domains the inclusion of mandatory covariates may be essential, either for model interpretability, or due to prior expert knowledge. CoxBoost deviates from the algorithm presented above by instead using an offset-based approach for generalized linear models <span class="citation" data-cites="Tutz2007">(<a href="P5C26_references.html#ref-Tutz2007" role="doc-biblioref">Tutz and Binder 2007</a>)</span>.</p>
<p>Let <span class="math inline">\(\mathcal{I}= \{1,...,p\}\)</span> be the indices of the covariates, let <span class="math inline">\(\mathcal{I}_{mand}\)</span> be the indices of the mandatory covariates that must be included in all iterations, and let <span class="math inline">\(\mathcal{I}_{opt} = \mathcal{I}\setminus \mathcal{I}_{mand}\)</span> be the indices of the optional covariates that may be included in any iteration. In the <span class="math inline">\(m\)</span>th iteration, the algorithm fits a weak learner on all mandatory covariates and <em>one</em> optional covariate: <span class="math display">\[
\mathcal{I}_m = \mathcal{I}_{mand} \cup \{x | x \in \mathcal{I}_{opt}\}
\]</span></p>
<p>In addition, a penalty matrix <span class="math inline">\(\mathbf{P} \in \mathbb{R}^{p \times p}\)</span> is considered such that <span class="math inline">\(P_{ii} &gt; 0\)</span> implies that covariate <span class="math inline">\(i\)</span> is penalized and <span class="math inline">\(P_{ii} = 0\)</span> means no penalization. In practice, this is usually a diagonal matrix <span class="citation" data-cites="Binder2008">(<a href="P5C26_references.html#ref-Binder2008" role="doc-biblioref">Binder and Schumacher 2008</a>)</span> and by setting <span class="math inline">\(P_{ii} = 0, i \in I_{mand}\)</span> and <span class="math inline">\(P_{ii} &gt; 0, i \not\in I_{mand}\)</span>, only optional (non-mandatory) covariates are penalized. The penalty matrix can be allowed to vary with each iteration, which allows for a highly flexible approach, however in implementation a simpler approach is to either select a single penalty to be applied in each iteration step or to have a single penalty matrix <span class="citation" data-cites="pkgcoxboost">(<a href="P5C26_references.html#ref-pkgcoxboost" role="doc-biblioref">Binder 2013</a>)</span>.</p>
<p>At the <span class="math inline">\(m\)</span>th iteration and the <span class="math inline">\(k\)</span>th set of indices to consider (<span class="math inline">\(k = 1,...,p\)</span>), the loss to optimize is the penalized partial-log likelihood given by <span class="math display">\[
\begin{split}
&amp;l_{pen}(\gamma_{mk}) = \sum^n_{i=1} \delta_i \Big[\eta_{i,m-1} + \mathbf{x}_{i,\mathcal{I}_{mk}}\gamma^\top_{mk}\Big] - \\
&amp;\quad\delta_i\log\Big(\sum^n_{j = 1} \mathbb{I}(t_j \leq t_i) \exp(\eta_{i,{m-1}} + \mathbf{x}_{i, \mathcal{I}_{mk}}\gamma^\top_{mk}\Big) - \lambda\gamma_{mk}\mathbf{P}_{mk}\gamma^\top_{mk}
\end{split}
\]</span></p>
<p>where <span class="math inline">\(\eta_{i,m} = \mathbf{x}_i\beta_m\)</span>, <span class="math inline">\(\gamma_{mk}\)</span> are the coefficients corresponding to the covariates in <span class="math inline">\(\mathcal{I}_{mk}\)</span> which is the possible set of candidates for a subset of total candidates <span class="math inline">\(k = 1,...,p\)</span>; <span class="math inline">\(\mathbf{P}_{mk}\)</span> is the penalty matrix; and <span class="math inline">\(\lambda\)</span> is a penalty hyper-parameter to be tuned or selected.</p>
<p>In each iteration, all potential candidate sets (the union of mandatory covariates and one other covariate) are updated by <span class="math display">\[
\hat{\gamma}_{mk} = \mathbf{I}^{-1}_{pen}(\hat{\gamma}_{(m-1)k})U(\hat{\gamma}_{(m-1)k})
\]</span> where <span class="math inline">\(U(\gamma) = \partial l / \partial \gamma (\gamma)\)</span> and <span class="math inline">\(\mathbf{I}^{-1}_{pen} = \partial^2 l/\partial\gamma\partial\gamma^T (\gamma + \lambda\mathbf{P}_{(m-1)k})\)</span> are the first and second derivatives of the unpenalized partial-log-likelihood. The optimal set is then found as <span class="math display">\[
k^* := \mathop{\mathrm{arg\,max}}_k l_{pen}(\hat{\gamma}_{mk})
\]</span> and the estimated coefficients are updated with <span class="math display">\[
\hat{\beta}_m = \hat{\beta}_{m-1} + \hat{\gamma}_{mk^*}, \quad k^* \in \mathcal{I}_{mk}
\]</span></p>
<p>This deviates from the standard GBM algorithm by directly optimizing <span class="math inline">\(l_{pen}\)</span> and not its gradient, additionally model coefficients are iteratively updated instead of a more general model form.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="13.3">
<h2 data-number="13.3" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">13.3</span> Conclusion</h2>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>Key takeaways
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>GBMs are a highly flexible and powerful machine learning tool. They have proven particularly useful in survival analysis as minimal adjustments are required to make use of off-shelf software.</li>
<li>The flexibility of the algorithm allows all the models above to be implemented in relatively few open-source packages.</li>
<li>There is evidence that boosting models can outperform the Cox PH even in low-dimensional settings <span class="citation" data-cites="Schmid2008b">(<a href="P5C26_references.html#ref-Schmid2008b" role="doc-biblioref">Schmid and Hothorn 2008b</a>)</span>, which is not not something all ML models can claim.</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Limitations
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Boosting, especially with tree learners, is viewed as a black-box model that is increasingly difficult to interpret as the number of iterations increase. However, there are several methods for increasing interpretability, such as variable importance and SHAPs <span class="citation" data-cites="Lundberg2017">(<a href="P5C26_references.html#ref-Lundberg2017" role="doc-biblioref">Lundberg and Lee 2017</a>)</span>.</li>
<li>Boosting often relies on intensive computing power, however, dedicated packages such as <span class="math inline">\(\textbf{xgboost}\)</span> <span class="citation" data-cites="pkgxgboost">(<a href="P5C26_references.html#ref-pkgxgboost" role="doc-biblioref">T. Chen et al. 2020</a>)</span>, exist to push CPU/GPUs to their limits in order to optimise predictive performance.</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>Further reading
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><span class="citation" data-cites="Buhlmann2003">Bühlmann and Yu (<a href="P5C26_references.html#ref-Buhlmann2003" role="doc-biblioref">2003</a>)</span>; <span class="citation" data-cites="pkgmboost">Hothorn et al. (<a href="P5C26_references.html#ref-pkgmboost" role="doc-biblioref">2020</a>)</span>; <span class="citation" data-cites="Wang2010">Wang and Wang (<a href="P5C26_references.html#ref-Wang2010" role="doc-biblioref">2010</a>)</span> for more general information and background on componentwise GBMs</li>
<li><span class="citation" data-cites="Friedman2001">J. H. Friedman (<a href="P5C26_references.html#ref-Friedman2001" role="doc-biblioref">2001</a>)</span>; <span class="citation" data-cites="Wang2010">Wang and Wang (<a href="P5C26_references.html#ref-Wang2010" role="doc-biblioref">2010</a>)</span> for linear least squares weak learners</li>
<li><span class="citation" data-cites="Buhlmann2003">Bühlmann and Yu (<a href="P5C26_references.html#ref-Buhlmann2003" role="doc-biblioref">2003</a>)</span>; <span class="citation" data-cites="Friedman2001">J. H. Friedman (<a href="P5C26_references.html#ref-Friedman2001" role="doc-biblioref">2001</a>)</span> for decision tree weak learners</li>
<li><span class="citation" data-cites="Ridgeway1999">Ridgeway (<a href="P5C26_references.html#ref-Ridgeway1999" role="doc-biblioref">1999</a>)</span> for early research into GBMs for survival analysis</li>
<li><span class="citation" data-cites="Johnson2011">Johnson and Long (<a href="P5C26_references.html#ref-Johnson2011" role="doc-biblioref">2011</a>)</span> and <span class="citation" data-cites="Wang2010">Wang and Wang (<a href="P5C26_references.html#ref-Wang2010" role="doc-biblioref">2010</a>)</span> for semi-parametric AFT boosting</li>
</ul>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-pkgcoxboost" class="csl-entry" role="listitem">
Binder, Harald. 2013. <span>“<span class="nocase">CoxBoost: Cox models by likelihood based boosting for a single survival endpoint or competing risks</span>.”</span> CRAN.
</div>
<div id="ref-Binder2008" class="csl-entry" role="listitem">
Binder, Harald, and Martin Schumacher. 2008. <span>“<span class="nocase">Allowing for mandatory covariates in boosting estimation of sparse high-dimensional survival models</span>.”</span> <em>BMC Bioinformatics</em> 9 (1): 14. <a href="https://doi.org/10.1186/1471-2105-9-14">https://doi.org/10.1186/1471-2105-9-14</a>.
</div>
<div id="ref-Buhlmann2006" class="csl-entry" role="listitem">
Buhlmann, Peter. 2006. <span>“<span class="nocase">Boosting for high-dimensional linear models</span>.”</span> <em>Ann. Statist.</em> 34 (2): 559–83. <a href="https://doi.org/10.1214/009053606000000092">https://doi.org/10.1214/009053606000000092</a>.
</div>
<div id="ref-Buhlmann2007" class="csl-entry" role="listitem">
Buhlmann, Peter, and Torsten Hothorn. 2007. <span>“<span class="nocase">Boosting Algorithms: Regularization, Prediction and Model Fitting</span>.”</span> <em>Statist. Sci.</em> 22 (4): 477–505. <a href="https://doi.org/10.1214/07-STS242">https://doi.org/10.1214/07-STS242</a>.
</div>
<div id="ref-Buhlmann2003" class="csl-entry" role="listitem">
Bühlmann, Peter, and Bin Yu. 2003. <span>“<span class="nocase">Boosting With the L2 Loss</span>.”</span> <em>Journal of the American Statistical Association</em> 98 (462): 324–39. <a href="https://doi.org/10.1198/016214503000125">https://doi.org/10.1198/016214503000125</a>.
</div>
<div id="ref-pkgxgboost" class="csl-entry" role="listitem">
Chen, Tianqi, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen, et al. 2020. <span>“<span class="nocase">xgboost: Extreme Gradient Boosting</span>.”</span> CRAN. <a href="https://cran.r-project.org/package=xgboost">https://cran.r-project.org/package=xgboost</a>.
</div>
<div id="ref-Chen2013" class="csl-entry" role="listitem">
Chen, Yifei, Zhenyu Jia, Dan Mercola, and Xiaohui Xie. 2013. <span>“<span class="nocase">A Gradient Boosting Algorithm for Survival Analysis via Direct Optimization of Concordance Index</span>.”</span> Edited by Lev Klebanov. <em>Computational and Mathematical Methods in Medicine</em> 2013: 873595. <a href="https://doi.org/10.1155/2013/873595">https://doi.org/10.1155/2013/873595</a>.
</div>
<div id="ref-Cox1972" class="csl-entry" role="listitem">
Cox, D. R. 1972. <span>“<span class="nocase">Regression Models and Life-Tables</span>.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 34 (2): 187–220.
</div>
<div id="ref-Cox1975" class="csl-entry" role="listitem">
———. 1975. <span>“<span>Partial Likelihood</span>.”</span> <em>Biometrika</em> 62 (2): 269–76. <a href="https://doi.org/10.1080/03610910701884021">https://doi.org/10.1080/03610910701884021</a>.
</div>
<div id="ref-Freund1996" class="csl-entry" role="listitem">
Freund, Yoav, and Robert E Schapire. 1996. <span>“<span class="nocase">Experiments with a new boosting algorithm</span>.”</span> In. Citeseer.
</div>
<div id="ref-Friedman1999" class="csl-entry" role="listitem">
Friedman, Jerome. 1999. <span>“<span>Stochastic Gradient Boosting</span>.”</span> <em>Computational Statistics &amp; Data Analysis</em> 38 (March): 367–78. <a href="https://doi.org/10.1016/S0167-9473(01)00065-2">https://doi.org/10.1016/S0167-9473(01)00065-2</a>.
</div>
<div id="ref-Friedman2001" class="csl-entry" role="listitem">
Friedman, Jerome H. 2001. <span>“<span>Greedy Function Approximation: A Gradient Boosting Machine</span>.”</span> <em>The Annals of Statistics</em> 29 (5): 1189–1232. <a href="http://www.jstor.org/stable/2699986">http://www.jstor.org/stable/2699986</a>.
</div>
<div id="ref-Hastie2001" class="csl-entry" role="listitem">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2001. <em><span class="nocase">The Elements of Statistical Learning</span></em>. Springer New York Inc.
</div>
<div id="ref-pkgmboost" class="csl-entry" role="listitem">
Hothorn, Torsten, Peter Buehlmann, Thomas Kneib, Matthias Schmid, and Benjamin Hofner. 2020. <span>“<span class="nocase">mboost: Model-Based Boosting</span>.”</span> CRAN. <a href="https://cran.r-project.org/package=mboost">https://cran.r-project.org/package=mboost</a>.
</div>
<div id="ref-Johnson2011" class="csl-entry" role="listitem">
Johnson, Brent A, and Qi Long. 2011. <span>“<span class="nocase">Survival ensembles by the sum of pairwise differences with application to lung cancer microarray studies</span>.”</span> <em>Ann. Appl. Stat.</em> 5 (2A): 1081–101. <a href="https://doi.org/10.1214/10-AOAS426">https://doi.org/10.1214/10-AOAS426</a>.
</div>
<div id="ref-Klein2003" class="csl-entry" role="listitem">
Klein, John P, and Melvin L Moeschberger. 2003. <em><span class="nocase">Survival analysis: techniques for censored and truncated data</span></em>. 2nd ed. Springer Science &amp; Business Media.
</div>
<div id="ref-Lee2018" class="csl-entry" role="listitem">
Lee, Donald K K, Ningyuan Chen, and Hemant Ishwaran. 2019. <span>“<span class="nocase">Boosted nonparametric hazards with time-dependent covariates</span>.”</span> <a href="https://arxiv.org/abs/arXiv:1701.07926v6">https://arxiv.org/abs/arXiv:1701.07926v6</a>.
</div>
<div id="ref-Lundberg2017" class="csl-entry" role="listitem">
Lundberg, Scott M, and Su-In Lee. 2017. <span>“<span class="nocase">A Unified Approach to Interpreting Model Predictions</span>.”</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div>
<div id="ref-Ma2006" class="csl-entry" role="listitem">
Ma, Shuangge, and Jian Huang. 2006. <span>“<span class="nocase">Regularized ROC method for disease classification and biomarker selection with microarray data</span>.”</span> <em>Bioinformatics (Oxford, England)</em> 21 (January): 4356–62. <a href="https://doi.org/10.1093/bioinformatics/bti724">https://doi.org/10.1093/bioinformatics/bti724</a>.
</div>
<div id="ref-Mayr2016" class="csl-entry" role="listitem">
Mayr, Andreas, Benjamin Hofner, and Matthias Schmid. 2016. <span>“<span class="nocase">Boosting the discriminatory power of sparse survival models via optimization of the concordance index and stability selection</span>.”</span> <em>BMC Bioinformatics</em> 17 (1): 288. <a href="https://doi.org/10.1186/s12859-016-1149-8">https://doi.org/10.1186/s12859-016-1149-8</a>.
</div>
<div id="ref-Mayr2014" class="csl-entry" role="listitem">
Mayr, Andreas, and Matthias Schmid. 2014. <span>“<span class="nocase">Boosting the concordance index for survival data–a unified framework to derive and evaluate biomarker combinations</span>.”</span> <em>PloS One</em> 9 (1): e84483–83. <a href="https://doi.org/10.1371/journal.pone.0084483">https://doi.org/10.1371/journal.pone.0084483</a>.
</div>
<div id="ref-Meinshausen2010" class="csl-entry" role="listitem">
Meinshausen, Nicolai, and Peter Bühlmann. 2010. <span>“<span class="nocase">Stability selection</span>.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 72 (4): 417–73. <a href="https://doi.org/10.1111/j.1467-9868.2010.00740.x">https://doi.org/10.1111/j.1467-9868.2010.00740.x</a>.
</div>
<div id="ref-Ridgeway1999" class="csl-entry" role="listitem">
Ridgeway, Greg. 1999. <span>“<span class="nocase">The state of boosting</span>.”</span> <em>Computing Science and Statistics</em> 31: 172–81.
</div>
<div id="ref-Schmid2008a" class="csl-entry" role="listitem">
Schmid, Matthias, and Torsten Hothorn. 2008a. <span>“<span class="nocase">Boosting additive models using component-wise P-splines</span>.”</span> <em>Computational Statistics &amp; Data Analysis</em> 53 (2): 298–311.
</div>
<div id="ref-Schmid2008b" class="csl-entry" role="listitem">
———. 2008b. <span>“<span class="nocase">Flexible boosting of accelerated failure time models</span>.”</span> <em>BMC Bioinformatics</em> 9 (February): 269. <a href="https://doi.org/10.1186/1471-2105-9-269">https://doi.org/10.1186/1471-2105-9-269</a>.
</div>
<div id="ref-Tutz2007" class="csl-entry" role="listitem">
Tutz, Gerhard, and Harald Binder. 2007. <span>“<span>Boosting Ridge Regression</span>.”</span> <em>Computational Statistics &amp; Data Analysis</em> 51 (February): 6044–59. <a href="https://doi.org/10.1016/j.csda.2006.11.041">https://doi.org/10.1016/j.csda.2006.11.041</a>.
</div>
<div id="ref-Wang2010" class="csl-entry" role="listitem">
Wang, Zhu, and C Y Wang. 2010. <span>“<span class="nocase">Buckley-James Boosting for Survival Analysis with High-Dimensional Biomarker Data</span>.”</span> <em>Statistical Applications in Genetics and Molecular Biology</em> 9 (1). https://doi.org/<a href="https://doi.org/10.2202/1544-6115.1550">https://doi.org/10.2202/1544-6115.1550</a>.
</div>
<div id="ref-Wei1992" class="csl-entry" role="listitem">
Wei, L J. 1992. <span>“<span class="nocase">The Accelerated Failure Time Model: A Useful Alternative to the Cox Regression Model in Survival Analysis</span>.”</span> <em>Statistics in Medicine</em> 11: 1871–79.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./P3C15_svm.html" class="pagination-link" aria-label="Support Vector Machines">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./P3C17_neural.html" class="pagination-link" aria-label="Neural Networks">
        <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Neural Networks</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>All content licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> <br> © Raphael Sonabend, Andreas Bender.</p>
</div>   
    <div class="nav-footer-center">
<p><a href="https://www.mlsabook.com">Website</a> | <a href="https://github.com/mlsa-book/MLSA">GitHub</a></p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mlsa-book/MLSA/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/mlsa-book/MLSA/edit/main/book/P3C16_boosting.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/mlsa-book/MLSA/blob/main/book/P3C16_boosting.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>