<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Raphael Sonabend and Andreas Bender">
<title>Machine Learning in Survival Analysis - 14&nbsp; Support Vector Machines</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./boosting.html" rel="next">
<link href="./forests.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="styles.css">
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./classical.html">Models</a></li><li class="breadcrumb-item"><a href="./svm.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning in Survival Analysis</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/mlsa-book/MLSA/tree/main/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Machine-Learning-in-Survival-Analysis.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Symbols and Notation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
 <span class="menu-text">Survival Analysis and Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">MLSA From Start to Finish</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./machinelearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Statistical Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./survival.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Survival Analysis</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">Evaluation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_what.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">What are Survival Measures?</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_rank.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Discrimination Measures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_calib.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Calibration Measures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_rules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Evaluating Distributions by Scoring Rules</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_time.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Evaluating Survival Time</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_choosing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Choosing Measures</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Classical Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mlmodels.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Machine Learning Survival Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./forests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Tree-Based Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./svm.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Boosting Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./neuralnetworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./alternatives.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Alternative Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./models_choosing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Choosing Models</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
 <span class="menu-text">Reduction Techniques</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reductions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Reductions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./competing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Competing Risks Pipelines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./discretetime.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Discrete Time Survival Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./poisson.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Connections to Poisson Regression and Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pseudo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Connections to Regression and Imputation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./advanced.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Advanced Methods</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
 <span class="menu-text">Extensions and Outlook</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./common.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Common problems in survival analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Survival Software</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./next.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">What’s next for MLSA?</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#svms-for-regression" id="toc-svms-for-regression" class="nav-link active" data-scroll-target="#svms-for-regression"><span class="header-section-number">14.0.1</span> SVMs for Regression</a></li>
  <li><a href="#sec-surv-ml-models-svm-surv" id="toc-sec-surv-ml-models-svm-surv" class="nav-link" data-scroll-target="#sec-surv-ml-models-svm-surv"><span class="header-section-number">14.0.2</span> SVMs for Survival Analysis</a></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions"><span class="header-section-number">14.0.3</span> Conclusions</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/mlsa-book/MLSA/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/mlsa-book/MLSA/edit/main/book/svm.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/mlsa-book/MLSA/blob/main/book/svm.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./classical.html">Models</a></li><li class="breadcrumb-item"><a href="./svm.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></a></li></ol></nav><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-surv-ml-models-svm" class="quarto-section-identifier"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    TODO (150-200 WORDS)
  </div>
</div>


</header><div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Major changes expected!
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>This page is a work in progress and major changes will be made over time.</strong></p>
</div>
</div>
<section id="svms-for-regression" class="level3" data-number="14.0.1"><h3 data-number="14.0.1" class="anchored" data-anchor-id="svms-for-regression">
<span class="header-section-number">14.0.1</span> SVMs for Regression</h3>
<p>In the simplest explanation, support vector machines (SVMs) <span class="citation" data-cites="CortesVapnik1995">(<a href="references.html#ref-CortesVapnik1995" role="doc-biblioref">Cortes and Vapnik 1995</a>)</span> fit a hyperplane, <span class="math inline">\(g\)</span>, on given training data and make predictions for new values as <span class="math inline">\(\hat{g}(X^*)\)</span> for some testing covariate <span class="math inline">\(X^*\)</span>. One may expect the hyperplane to be fit so that all training covariates would map perfectly to the observed labels (a ‘hard-boundary’) however this would result in overfitting and instead an acceptable (‘soft’-)boundary of error, the `<span class="math inline">\(\epsilon\)</span>-tube’, dictates how ‘incorrect’ predictions may be, i.e.&nbsp;how large an underestimate or overestimate. (<a href="#fig-surv-svm" class="quarto-xref">Figure&nbsp;<span>14.1</span></a>) visualises support vector machines for regression with a linear hyperplane <span class="math inline">\(g\)</span>, and an acceptable boundary of error within the dashed lines (the <span class="math inline">\(\epsilon\)</span>-tube). SVMs are not limited to linear boundaries and <em>kernel</em> functions are utilised to specify more complex hyperplanes. Exact details of the optimization/separating procedure are not discussed here but many off-shelf ‘solvers’ exist in different programming languages for fitting SVMs.</p>
<p>In the regression setting, the goal of SVMs is to estimate the function <span id="eq-svm"><span class="math display">\[
g: \mathbb{R}^p \rightarrow \mathbb{R}; \quad (x) \mapsto x\beta + \beta_0
\tag{14.1}\]</span></span> by estimation of the weights <span class="math inline">\(\beta \in \mathbb{R}^p, \beta_0 \in \mathbb{R}\)</span> via the optimisation problem <span id="eq-svm-opt"><span class="math display">\[
\begin{aligned}
&amp; \min_{\beta,\beta_0, \xi, \xi^*} \frac{1}{2} \|\beta\|^2 + C \sum^n_{i=1}(\xi_i + \xi_i^*) \\
&amp; \textrm{subject to}
\begin{dcases}
Y_i - g(X_i) &amp; \leq \epsilon + \xi_i \\
g(X_i) - Y_i &amp; \leq \epsilon + \xi_i^* \\
\xi_i, \xi_i^* &amp; \geq 0, \ i = 1,...,n
\end{dcases}
\end{aligned}
\tag{14.2}\]</span></span> where <span class="math inline">\(C \in \mathbb{R}\)</span> is the regularization/cost parameter, <span class="math inline">\(\xi_i,\xi_i^*\)</span> are slack parameters and <span class="math inline">\(\epsilon\)</span> is a margin of error for observations on the wrong side of the hyperplane, and <span class="math inline">\(g\)</span> is defined in (<a href="#eq-svm" class="quarto-xref">Equation&nbsp;<span>14.1</span></a>). The effect of the slack parameters is seen in (<a href="#fig-surv-svm" class="quarto-xref">Figure&nbsp;<span>14.1</span></a>) in which a maximal distance from the <span class="math inline">\(\epsilon\)</span>-tube is dictated by the slack variables.</p>
<p>In fitting, the dual of the optimisation is instead solved and substituting the optimised parameters into (<a href="#eq-svm" class="quarto-xref">Equation&nbsp;<span>14.1</span></a>) gives the prediction function, <span class="math display">\[
\hat{g}(X^*) = \sum^n_{i=1} (\alpha_i - \alpha_i^*)K(X^*,X_i) + \beta_0
\]</span> where <span class="math inline">\(\alpha_i, \alpha_i^*\)</span> are Lagrangrian multipliers and <span class="math inline">\(K\)</span> is some kernel function. The Karush-Kuhn-Tucker conditions required to solve the optimisation for <span class="math inline">\(\alpha\)</span> result in the key property of SVMs, which is that values <span class="math inline">\(\alpha_i = \alpha_i^* = 0\)</span> indicate that observation <span class="math inline">\(i\)</span> is ‘inside’ the <span class="math inline">\(\epsilon\)</span>-tube and if <span class="math inline">\(\alpha_i \neq 0\)</span> or <span class="math inline">\(\alpha^*_i \neq 0\)</span> then <span class="math inline">\(i\)</span> is outside the tube and termed a <em>support vector</em>. It is these ‘support vectors’ that influence the shape of the separating boundary.</p>
<p>The choice of kernel and its parameters, the regularization parameter <span class="math inline">\(C\)</span>, and the acceptable error <span class="math inline">\(\epsilon\)</span>, are all tunable hyper-parameters, which makes the support vector machine a highly adaptable and often well-performing machine learning method. However the parameters <span class="math inline">\(C\)</span> and <span class="math inline">\(\epsilon\)</span> often have no clear apriori meaning (especially true when predicting abstract rankings) and thus require extensive tuning over a great range of values; no tuning will result in a very poor model fit.</p>
<div id="fig-surv-svm" class="quarto-figure quarto-figure-center quarto-float anchored" alt="TODO">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-surv-svm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/svm/svm.png" class="img-fluid figure-img" alt="TODO">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-surv-svm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.1: Visualising a support vector machine with an <span class="math inline">\(\epsilon\)</span>-tube and slack parameters <span class="math inline">\(\xi\)</span> and <span class="math inline">\(\xi^*\)</span>. Red circles are values within the <span class="math inline">\(\epsilon\)</span>-tube and blue diamonds are values outside the tube. x-axis is single covariate, <span class="math inline">\(x\)</span>, and y-axis is <span class="math inline">\(g(x) = x\beta + \beta_0\)</span>.
</figcaption></figure>
</div>
</section><section id="sec-surv-ml-models-svm-surv" class="level3" data-number="14.0.2"><h3 data-number="14.0.2" class="anchored" data-anchor-id="sec-surv-ml-models-svm-surv">
<span class="header-section-number">14.0.2</span> SVMs for Survival Analysis</h3>
<p>Similarly to random forests, all research for Survival Support Vector Machines (SSVMs) can be reduced to very few algorithms, in fact only one unique off-shelf algorithm is identified in this survey. No SSVM for distribution predictions exist, instead they either predict survival time, rankings, or a hybrid of the two.</p>
<p>Other reviews and surveys of SSVMs include a short review by Wang <span class="math inline">\(\textit{et al.}\)</span> (2017) <span class="citation" data-cites="Wang2017">(<a href="references.html#ref-Wang2017" role="doc-biblioref">Wang, Li, and Reddy 2019</a>)</span> and some benchmark experiments and short surveys from Van Belle <span class="math inline">\(\textit{et al.}\)</span> (2011) <span class="citation" data-cites="VanBelle2011b">(<a href="references.html#ref-VanBelle2011b" role="doc-biblioref">Vanya Van Belle, Pelckmans, Van Huffel, et al. 2011</a>)</span>, Goli <span class="math inline">\(\textit{et al.}\)</span> (2016) <span class="citation" data-cites="Goli2016a">(<a href="references.html#ref-Goli2016a" role="doc-biblioref">Goli, Mahjub, Faradmal, and Soltanian 2016</a>)</span> and Fouodo <span class="math inline">\(\textit{et al.}\)</span> (2018) <span class="citation" data-cites="pkgsurvivalsvm">(<a href="references.html#ref-pkgsurvivalsvm" role="doc-biblioref">Fouodo et al. 2018</a>)</span>. All the benchmark experiments in these papers indicate that the Cox PH performs as well as, if not better than, the SSVMs.</p>
<p>Initial attempts at developing SSVMs by Shivaswamy <span class="math inline">\(\textit{et al.}\)</span> (2007) <span class="citation" data-cites="Shivaswamy2007">(<a href="references.html#ref-Shivaswamy2007" role="doc-biblioref">Shivaswamy, Chu, and Jansche 2007</a>)</span> took the most ‘natural’ course and attempt to treat the problem as a regression one with adjustments in the optimisation for censoring. These methods have a natural interpretation and are intuitive in their construction. Further development of these by Khan and Zubek (2008) <span class="citation" data-cites="Khan2008">(<a href="references.html#ref-Khan2008" role="doc-biblioref">Khan and Bayer Zubek 2008</a>)</span> and Land <span class="math inline">\(\textit{et al.}\)</span> (2011) <span class="citation" data-cites="Land2011">(<a href="references.html#ref-Land2011" role="doc-biblioref">Land et al. 2011</a>)</span> focused on different adjustments for censoring in order to best reflect a realistic survival data set-up. Simultaneously, ranking models were developed in order to directly optimise a model’s discriminatory power. Developments started with the work of Evers and Messow (2008) <span class="citation" data-cites="Evers2008">(<a href="references.html#ref-Evers2008" role="doc-biblioref">Evers and Messow 2008</a>)</span> but were primarily made by Van Belle <span class="math inline">\(\textit{et al.}\)</span> (2007)-(2011) <span class="citation" data-cites="VanBelle2010 VanBelle2007 VanBelle2008 VanBelle2011a">(<a href="references.html#ref-VanBelle2010" role="doc-biblioref">V. Van Belle et al. 2010</a>; <a href="references.html#ref-VanBelle2007" role="doc-biblioref">Vanya Van Belle et al. 2007</a>, <a href="references.html#ref-VanBelle2008" role="doc-biblioref">2008</a>; <a href="references.html#ref-VanBelle2011a" role="doc-biblioref">Vanya Van Belle, Pelckmans, Suykens, et al. 2011</a>)</span>. These lack the survival time interpretation but are less restrictive in the optimisation constraints. Finally a hybrid of the two followed naturally from Van Belle <span class="math inline">\(\textit{et al.}\)</span> (2011) <span class="citation" data-cites="VanBelle2011b">(<a href="references.html#ref-VanBelle2011b" role="doc-biblioref">Vanya Van Belle, Pelckmans, Van Huffel, et al. 2011</a>)</span> by combining the constraints from both the regression and ranking tasks. This hybrid method allows a survival time interpretation whilst still optimising discrimination. These hybrid models have become increasingly popular in not only SSVMs, but also neural networks (<a href="neuralnetworks.html#sec-surv-ml-models-nn" class="quarto-xref"><span>Section 16.1</span></a>). Instead of presenting these models chronologically, the final hybrid model is defined and then other developments can be more simply presented as components of this hybrid. One model with an entirely different formulation is considered after the hybrid.</p>
<p>For all SSVMs defined in this section let: <span class="math inline">\(\xi_i,\xi_i^*,\xi_i'\)</span> be slack variables; <span class="math inline">\(\beta,\beta_0\)</span> be model weights in <span class="math inline">\(\mathbb{R}\)</span>; <span class="math inline">\(C, \mu\)</span> be regularisation hyper-parameters in <span class="math inline">\(\mathbb{R}\)</span>; <span class="math inline">\((X_i, T_i, \Delta_i) \stackrel{i.i.d.}\sim(X,T,\Delta)\)</span> be the usual training data; and <span class="math inline">\(g(x) = x\beta + \beta_0\)</span>.</p>
<section id="mod-ssvmhybrid" class="level4" data-number="14.0.2.1"><h4 data-number="14.0.2.1" class="anchored" data-anchor-id="mod-ssvmhybrid">
<span class="header-section-number">14.0.2.1</span> SSVM-Hybrid {.unnumbered .unlisted}</h4>
<p>Van Belle <span class="math inline">\(\textit{et al.}\)</span> published several papers developing SSVMs, which culminate in the hybrid model here termed ‘SSVM-Hybrid’ <span class="citation" data-cites="VanBelle2011b">(<a href="references.html#ref-VanBelle2011b" role="doc-biblioref">Vanya Van Belle, Pelckmans, Van Huffel, et al. 2011</a>)</span>. The model is defined by the optimisation problem,</p>
<p><strong>SSVM-Hybrid</strong>\ <span class="math display">\[
\begin{aligned}
&amp; \min_{\beta, \beta_0, \xi, \xi', \xi^*} \frac{1}{2}\|\beta\|^2 + C\sum_{i =1}^n \xi_i + \mu \sum^n_{i=1}(\xi_i' + \xi_i^*) \\
&amp; \textrm{subject to}
\begin{dcases}
&amp; g(X_i) - g(X_{j(i)}) \geq T_i - T_{j(i)} - \xi_i, \\
&amp; \Delta_i(g(X_i) - T_i) \leq \xi^*_i \\
&amp; T_i - g(X_i) \leq \xi'_i \\
&amp; \xi_i, \xi_i', \xi_i^* \geq 0, \quad \forall i = 1,...,n \\
\end{dcases}
\end{aligned}
\label{eq:surv_ssvmvb2}
\]</span></p>
<p>where <span class="math inline">\(j(i) := \operatornamewithlimits{argmax}_{j \in 1,...n} \{T_j : T_j &lt; T_i\}\)</span> is an index discussed further below. A prediction for test data is given by,</p>
<p><span class="math display">\[
\hat{g}(X^*) = \sum^n_{i=1} \alpha_i(K(X_i, X^*) - K(X_{j(i)}, X^*)) + \alpha^*_i K(X_i, X^*) - \Delta_i\alpha_i'K(X_i, X^*) + \beta_0
\]</span></p>
<p>where <span class="math inline">\(\alpha_i, \alpha_i^*, \alpha_i'\)</span> are Lagrange multipliers and <span class="math inline">\(K\)</span> is a chosen kernel function, which may have hyper-parameters to select or tune.</p>
</section><section id="svcr-regression" class="level4 unnumbered unlisted"><h4 class="unnumbered unlisted anchored" data-anchor-id="svcr-regression">SVCR (Regression)</h4>
<p>Examining the components of the SSVM-Hybrid model will help identify its relation to previously published SSVMs. First note the model’s connection to the regression setting when on setting <span class="math inline">\(C = 0\)</span>, removing the associated first constraint and ignoring <span class="math inline">\(\Delta\)</span> in the second constraint, the regression setting is exactly recovered:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \min_{\beta, \beta_0, \xi, \xi'} \frac{1}{2}\|\beta\|^2 + \mu \sum^n_{i=1}(\xi_i + \xi_i') \\
&amp; \textrm{subject to}
\begin{dcases}
&amp; g(X_i) - T_i \leq \xi_i \\
&amp; T_i - g(X_i) \leq \xi'_i \\
&amp; \xi_i, \xi_i' \geq 0, \quad \forall i = 1,...,n \\
\end{dcases}
\end{aligned}
\]</span></p>
<p>Note a slight difference in the formulation of this optimisation to the original regression problem, here no error component <span class="math inline">\(\epsilon\)</span> is directly included, instead this is part of the optimisation and considered as part of the slack parameters <span class="math inline">\(\xi_i, \xi'_i\)</span>; effectively this is the same as setting <span class="math inline">\(\epsilon = 0\)</span>. This formulation removes the <span class="math inline">\(\epsilon\)</span>-tube symmetry seen previously and therefore distinguishes more clearly between overestimates and underestimates, with each being penalised differently. Removing the <span class="math inline">\(\epsilon\)</span> parameter can lead to model overfitting as all points become support vectors, however careful tuning of other hyper-parameters can effectively control for this.</p>
<p>This formulation allows for clearer control over left-, right-, and un-censored observations. Clearly if an observation is uncensored then the true value is known and should be predicted exactly, hence under- and over-estimates are equally problematic and should be penalised the same. If an observation is right-censored then the true death time is greater than the observed time and therefore overestimates should not be heavily penalised but underestimates should be; conversely for left-censored observations.</p>
<p>This leads to the first SSVM for regression from Shivaswamy <span class="math inline">\(\textit{et al.}\)</span> (2007) <span class="citation" data-cites="Shivaswamy2007">(<a href="references.html#ref-Shivaswamy2007" role="doc-biblioref">Shivaswamy, Chu, and Jansche 2007</a>)</span>.</p>
<p><strong>SVCR</strong> <span class="math display">\[
\begin{aligned}
&amp; \min_{\beta, \beta_0, \xi, \xi^*} \frac{1}{2}\|\beta\|^2 + \mu\Big(\sum_{i \in R} \xi_i + \sum_{i \in L} \xi_i^*\Big) \\
&amp; \textrm{subject to}
\begin{dcases}
&amp; g(X_i) - T_i \leq \xi^*_i, \quad \forall i \in R \\
&amp; T_i - g(X_i) \leq \xi_i, \quad \forall i \in L \\
&amp; \xi_i \geq 0, \forall i\in R; \xi^*_i \geq 0, \forall i \in L
\end{dcases}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(L\)</span> is the set of observations who are either left- or un-censored, and <span class="math inline">\(R\)</span> is the set of observations who are either right- or un-censored. Hence an uncensored observation is constrained on both sides as their true survival time is known, whereas a left-censored observation is constrained in the amount of ‘over-prediction’ and a right-censored observation is constrained by ‘under-prediction’. This is intuitive as the only known for these censoring types are the lower and upper bounds of the actual survival time respectively.</p>
<p>Reducing this to the book scope of right-censoring only results in the optimisation:</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \min_{\beta, \beta_0, \xi, \xi^*} \frac{1}{2}\|\beta\|^2 + \mu\Big(\sum_{i = 1}^n \xi_i + \xi_i^*\Big) \\
&amp; \textrm{subject to}
\begin{dcases}
&amp; \Delta_i(g(X_i) - T_i) \leq \xi_i \\
&amp; T_i - g(X_i) \leq \xi^*_i \\
&amp; \xi_i, \xi_i^* \geq 0 \\
&amp; \forall i\in 1,...,n
\end{dcases}
\end{aligned}
\]</span> which can be seen to be identical to SSVM-Hybrid when <span class="math inline">\(C=0\)</span> and the first constraint is removed. Predictions are found by,</p>
<p><span class="math display">\[
\hat{g}(X^*) = \sum^n_{i=1} \alpha^*_i K(X_i, X^*) - \Delta_i\alpha_i'K(X_i, X^*) + \beta_0
\]</span></p>
<p>The advantage of this algorithm is its simplicity. Clearly if no-one is censored then the optimisation is identical to the regression optimisation in (<a href="#eq-svm-opt" class="quarto-xref">Equation&nbsp;<span>14.2</span></a>). As there is no <span class="math inline">\(\epsilon\)</span> hyper-parameter, the run-time complexity is the same as, if not quicker than, a regression SVM. Both left- and right-censoring are handled and no assumptions are made about independent censoring. With respect to performance, benchmark experiments <span class="citation" data-cites="pkgsurvivalsvm">(<a href="references.html#ref-pkgsurvivalsvm" role="doc-biblioref">Fouodo et al. 2018</a>)</span> indicate that the SVCR does not outperform a na"ive SVR (i.e.&nbsp;censoring ignored). The SVCR is implemented in the <span class="math inline">\(\textsf{R}\)</span> package <span class="math inline">\(\textbf{survivalsvm}\)</span> <span class="citation" data-cites="pkgsurvivalsvm">(<a href="references.html#ref-pkgsurvivalsvm" role="doc-biblioref">Fouodo et al. 2018</a>)</span> and is referred to as ‘regression’.</p>
<p>As discussed, the error margin for left- and right- censoring should not necessarily be equal and the penalty for each should not necessarily be equal either. Hence a natural extension to SVCR is to add further parameters to better separate the different censoring types, which gives rise to the SVRc <span class="citation" data-cites="Khan2008">(<a href="references.html#ref-Khan2008" role="doc-biblioref">Khan and Bayer Zubek 2008</a>)</span>. However this model is only briefly discussed as left-censoring is out of scope of this book and also the model is patented and therefore not easily accessible. The model is given by the optimisation,</p>
<p><strong>SVRc</strong> <span class="math display">\[
\begin{aligned}
&amp; \min_{\beta, \beta_0, \xi, \xi^*} \frac{1}{2}\|\beta\|^2 + \sum^n_{i=1} C_i\xi_i + C^*_i\xi'_i \\
&amp; \textrm{subject to}
\begin{dcases}
&amp; g(X_i) - T_i \leq \epsilon'_i + \xi'_i \\
&amp; T_i - g(X_i) \leq \epsilon_i + \xi_i \\
&amp; \xi_i, \xi_i' \geq 0, \quad \forall i = 1,...,n \\
\end{dcases}
\end{aligned}
\]</span></p>
<p>Where <span class="math inline">\(C_i = \Delta_iC_c + (1-\Delta_i)C_n, \epsilon_i = \Delta_i\epsilon_c + (1-\Delta_i)\epsilon_n\)</span> and analogously for <span class="math inline">\(C^*_i, C_C^*, \epsilon^*,...\)</span>. The new hyper-parameters <span class="math inline">\(C_c, C_n, \epsilon_c, \epsilon_n\)</span> are the penalty for errors in censored predictions (c) and uncensored predictions (n) for left and right (*) censoring, and the acceptable margin of errors respectively. The rationale behind this algorithm is clear, by having asymmetric error margins the algorithm can penalise predictions that are clearly wrong whilst allowing predictions that may be correct (but ultimately unknown due to censoring). Experiments indicate the model may have superior discrimination than the Cox PH <span class="citation" data-cites="Khan2008">(<a href="references.html#ref-Khan2008" role="doc-biblioref">Khan and Bayer Zubek 2008</a>)</span> and SVCR <span class="citation" data-cites="Du2011">(<a href="references.html#ref-Du2011" role="doc-biblioref">Du and Dua 2011</a>)</span>. However these conclusions are weak as independent experiments do not have access to the patented model.</p>
<p>The largest drawback of the algorithm is a need to tune eight parameters. As the number of hyper-parameters to tune increases, so too does model fitting time as well as the risk of overfitting. The problem of extra hyper-parameters is the most common disadvantage of the model given in the literature <span class="citation" data-cites="pkgsurvivalsvm Land2011">(<a href="references.html#ref-pkgsurvivalsvm" role="doc-biblioref">Fouodo et al. 2018</a>; <a href="references.html#ref-Land2011" role="doc-biblioref">Land et al. 2011</a>)</span>. Land <span class="math inline">\(\textit{et al.}\)</span> (2011) <span class="citation" data-cites="Land2011">(<a href="references.html#ref-Land2011" role="doc-biblioref">Land et al. 2011</a>)</span> present an adaptation to the SVRc to improve model fitting time, termed the EP-SVRc, which uses Evolutionary Programming to determine the optimal values for the parameters. No specific model or algorithm is described, nor any quantitative results presented. No evidence can be found for this method being used since publication. The number of hyper-parameters in the SVRc, coupled with its lack of accessibility, outweigh the benefits of the claimed predictive performance and is therefore clearly not accessible.</p>
</section><section id="mod-svmem" class="level4" data-number="14.0.2.2"><h4 data-number="14.0.2.2" class="anchored" data-anchor-id="mod-svmem">
<span class="header-section-number">14.0.2.2</span> SSVM-Rank {.unnumbered .unlisted}</h4>
<p>The regression components of SSVM-Hybrid (<span class="math inline">\(\ref{eq:surv_ssvmvb2}\)</span>) have been fully examined, now turning to the ranking components and setting <span class="math inline">\(\mu = 0\)</span>. In this case the model reduces to</p>
<p><strong>SSVM-Rank</strong> <span class="math display">\[
\begin{aligned}
&amp; \min_{\beta, \beta_0, \xi} \frac{1}{2}\|\beta\|^2 + C\sum_{i =1}^n \xi_i \\
&amp; \textrm{subject to}
\begin{dcases}
&amp; g(X_i) - g(X_{j(i)}) \geq T_i - T_{j(i)} - \xi_i, \\
&amp; \xi_i \geq 0, \quad \forall i = 1,...,n \\
\end{dcases}
\end{aligned}
\]</span></p>
<p>with predictions</p>
<p><span class="math display">\[
\hat{g}(X^*) = \sum^n_{i=1} \alpha_i(K(X_i, X^*) - K(X_{j(i)}, X^*))
\]</span></p>
<p>This formulation, termed here ‘SSVM-Rank’, has been considered by numerous authors in different forms, including Evers and Messow <span class="citation" data-cites="Evers2008">(<a href="references.html#ref-Evers2008" role="doc-biblioref">Evers and Messow 2008</a>)</span> and Van Belle <span class="math inline">\(\textit{et al.}\)</span> <span class="citation" data-cites="VanBelle2007 VanBelle2008 VanBelle2011b">(<a href="references.html#ref-VanBelle2007" role="doc-biblioref">Vanya Van Belle et al. 2007</a>, <a href="references.html#ref-VanBelle2008" role="doc-biblioref">2008</a>; <a href="references.html#ref-VanBelle2011b" role="doc-biblioref">Vanya Van Belle, Pelckmans, Van Huffel, et al. 2011</a>)</span>. The primary differences between the various models are in which observations are compared in order to optimise discrimination; to motivate why this matters, first observe the intuitive nature of the optimisation constraints. By example, define <span class="math inline">\(k := T_i - T_{j(i)}\)</span> and say <span class="math inline">\(T_i &gt; T_{j(i)}\)</span>. Then, in the first constraint, <span class="math inline">\(g(X_i) - g(X_{j(i)}) \geq k - \xi_i\)</span>. As <span class="math inline">\(k &gt; 0\)</span> and <span class="math inline">\(\xi_i \geq 0\)</span>, it follows that <span class="math inline">\(g(X_i) &gt; g(X_{j(i)})\)</span>, hence creating a concordant ranking which is the opposite to the between observations <span class="math inline">\(i\)</span> (ranked higher) and <span class="math inline">\(j(i)\)</span>; illustrating why this optimisation results in a ranking model.</p>
<p>This choice of comparing observations <span class="math inline">\(i\)</span> and <span class="math inline">\(j(i)\)</span> (defined below) stems from a few years of research in an attempt to optimise the algorithm with respect to both speed and predictive performance. In the original formulation, RANKSVMC <span class="citation" data-cites="VanBelle2007">(<a href="references.html#ref-VanBelle2007" role="doc-biblioref">Vanya Van Belle et al. 2007</a>)</span>, the model ranks all possible pairs of observations. This is clearly infeasible as it increases the problem to a <span class="math inline">\(\mathcal{O}(qn^2/2)\)</span> runtime where <span class="math inline">\(q\)</span> is the proportion of non-censored observations out of a total sample size <span class="math inline">\(n\)</span> <span class="citation" data-cites="VanBelle2008">(<a href="references.html#ref-VanBelle2008" role="doc-biblioref">Vanya Van Belle et al. 2008</a>)</span>. The problem was reduced by taking a nearest neighbours approach and only considering the <span class="math inline">\(k\)</span>th closest observations <span class="citation" data-cites="VanBelle2008">(<a href="references.html#ref-VanBelle2008" role="doc-biblioref">Vanya Van Belle et al. 2008</a>)</span>. Simulation experiments determined that the single nearest neighbour was sufficient, thus arriving at <span class="math inline">\(j(i)\)</span>, the observation with the largest observed survival time smaller than <span class="math inline">\(T_i\)</span>, <span class="math display">\[
j(i) := \operatornamewithlimits{argmax}_{j \in 1,...n} \{T_j : T_j &lt; T_i\}
\]</span></p>
<p>This requires that the first observation is taken to be an event, even if it is actually censored. In practice, sorting observations by survival time then greatly speeds up the model run-time <span class="citation" data-cites="pkgsurvivalsvm">(<a href="references.html#ref-pkgsurvivalsvm" role="doc-biblioref">Fouodo et al. 2018</a>)</span>. The RANKSVMC and SSVM-RANK are implemented in <span class="math inline">\(\textbf{survivalsvm}\)</span> <span class="citation" data-cites="pkgsurvivalsvm">(<a href="references.html#ref-pkgsurvivalsvm" role="doc-biblioref">Fouodo et al. 2018</a>)</span> and referred to as ‘vanbelle1’ and ‘vanbelle2’ respectively.</p>
<p>The hybrid model is repeated below with the ranking components in blue, the regression components in red, and the common components in black, clearly highlighting the composite nature of the model.</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \min_{\beta, \beta_0, \xi, \xi', \xi^*} \frac{1}{2}\|\beta\|^2 + \textcolor{blue}{C\sum_{i =1}^n \xi_i} + \textcolor{red}{\mu \sum^n_{i=1}(\xi_i' + \xi_i^*)} \\
&amp; \textrm{subject to}
\begin{dcases}
&amp; \textcolor{blue}{g(X_i) - g(X_{j(i)}) \geq T_i - T_{j(i)} - \xi_i} \\
&amp; \textcolor{red}{\Delta_i(g(X_i) - T_i) \leq \xi^*_i} \\
&amp; \textcolor{red}{T_i - g(X_i) \leq \xi'_i} \\
&amp; \textcolor{blue}{\xi_i}, \textcolor{red}{\xi_i', \xi_i^*} \geq 0, \quad \forall i = 1,...,n \\
\end{dcases}
\end{aligned}
\]</span></p>
<p>and predictions are made with,</p>
<p><span class="math display">\[
\hat{g}(X^*) = \sum^n_{i=1} \textcolor{blue}{\alpha_i(K(X_i, X^*) - K(X_{j(i)}, X^*))} + \textcolor{red}{\alpha^*_i K(X_i, X^*) - \Delta_i\alpha_i'K(X_i, X^*)} + \beta_0
\]</span></p>
<p>The regularizer hyper-parameters <span class="math inline">\(C\)</span> and <span class="math inline">\(\mu\)</span> now have a clear interpretation. <span class="math inline">\(C\)</span> is the penalty associated with the regression method and <span class="math inline">\(\mu\)</span> is the penalty associated with the ranking method. By always fitting the hybrid models and tuning these two parameters, there is never a requirement to separately fit the regression or ranking methods as these would be automatically identified as superior in the tuning procedure. Moreover, the hybrid model retains the interpretability of the regression method and predictions can be interpreted as survival times. The hybrid method is implemented in <span class="math inline">\(\textbf{survivalsvm}\)</span> as ‘hybrid’. By Van Belle’s own simulation studies, these models do not outperform the Cox PH with respect to Harrell’s C.</p>
</section><section id="ssvr-mrl" class="level4 unnumbered unlisted"><h4 class="unnumbered unlisted anchored" data-anchor-id="ssvr-mrl">SSVR-MRL</h4>
<p>Not all SSVMs can be considered a variant of the SSVM-Hybrid, though all prominent and commonly utilised suggestions do seem to have this formulation. One other algorithm of note is termed here the ‘SSVM-MRL’ <span class="citation" data-cites="Goli2016a Goli2016b">(<a href="references.html#ref-Goli2016a" role="doc-biblioref">Goli, Mahjub, Faradmal, and Soltanian 2016</a>; <a href="references.html#ref-Goli2016b" role="doc-biblioref">Goli, Mahjub, Faradmal, Mashayekhi, et al. 2016</a>)</span>, which is a regression SSVM. The algorithm is identical to SVCR with one additional constraint.</p>
<p><strong>SSVR-MRL</strong>\ <span class="math display">\[
\begin{aligned}
&amp; \min_{\beta, \beta_0, \xi, \xi^*,\xi'} \frac{1}{2}\|\beta\|^2 + C\sum^n_{i=1} (\xi_i + \xi_i^*) + C^*\sum^n_{i=1} \xi_i' \\
&amp; \textrm{subject to}
\begin{dcases}
&amp; T_i - g(X_i) \leq \xi_i \\
&amp; \Delta_i(g(X_i) - T_i) \leq \xi_i^* \\
&amp; (1 - \Delta_i)(g(X_i) - T_i - MRL(T_i|\hat{S})) \leq \xi_i' \\
&amp; \xi_i, \xi_i^*, \xi_i' \geq 0 \\
&amp; \forall i = 1,...,n
\end{dcases}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(MRL(T_i|\hat{S})\)</span> is the ‘mean residual lifetime’ function <span class="citation" data-cites="Klein2003">(<a href="references.html#ref-Klein2003" role="doc-biblioref">Klein and Moeschberger 2003</a>)</span> <span class="math display">\[
MRL(\tau|\hat{S}) = \frac{\int^\infty_\tau \hat{S}(u) du}{\hat{S}(\tau)}
\]</span> which is the area under the estimated survival curve (say by Kaplan Meier), <span class="math inline">\(\hat{S}\)</span>, from point <span class="math inline">\(\tau\)</span>, weighted by the probability of being alive at point <span class="math inline">\(\tau\)</span>. This is interpreted as the expected remaining lifetime from point <span class="math inline">\(\tau\)</span>. On setting <span class="math inline">\(C^* = 0\)</span> and removing associated constraint three, this reduces exactly to the SVCR and similarly if there’s no censoring then the standard regression setting is recovered. Unlike other strategies, no new hyper-parameters are introduced and Kaplan-Meier estimation should not noticeably impact run-time. There is no evidence of this model being used in practice, nor of any off-shelf implementation. Theoretically, the hybrid model could be expanded to include this extra penalty term and constraint (discussed below).</p>
</section></section><section id="conclusions" class="level3" data-number="14.0.3"><h3 data-number="14.0.3" class="anchored" data-anchor-id="conclusions">
<span class="header-section-number">14.0.3</span> Conclusions</h3>
<p>Several SSVMs have been proposed for survival analysis. These can generally be categorised into ‘regression’ models that adapt SVMs to account for censoring and predict a survival time, ‘ranking’ models that predict a relative ranking in order to optimise measures of discrimination, and ‘hybrid’ models that optimise measures of discrimination but make survival time predictions. Other SSVMs that lie outside of these groupings are not able to solve the survival task (e.g. <span class="citation" data-cites="Shiao2013">(<a href="references.html#ref-Shiao2013" role="doc-biblioref">Shiao and Cherkassky 2013</a>)</span>). Other SVM-type approaches could be considered, including relevance vector machines and import vector machines, however less work has been developed in these areas and further consideration is beyond the scope of this book.</p>
<p>The models that have received the most attention are SVCR, SSVM-Rank, and SSVM-Hybrid; the first two are special cases of SSVM-Hybrid. Judging if SSVM-Hybrid (and by extension SVCR and SSVM-Rank) is accessible and transparent is not straightforward. On the one hand it could be considered transparent as SVMs have been studied for decades and the literature for SSVMs, especially from Van Belle, is extensive. On the other hand, the predictions from SSVM-Hybrid should be interpretable as survival times but first hand experience indicates that this is not the case (though this may be due to implementation), which calls into question whether the interpretation they claim to have is actually correct. For accessibility, there appears to be only one implementation of SSVMs in <span class="math inline">\(\textsf{R}\)</span> <span class="citation" data-cites="pkgsurvivalsvm">(<a href="references.html#ref-pkgsurvivalsvm" role="doc-biblioref">Fouodo et al. 2018</a>)</span>, and also only one in Python <span class="citation" data-cites="pkgsksurvival">(<a href="references.html#ref-pkgsksurvival" role="doc-biblioref">Pölsterl 2020</a>)</span>, which may be due to SSVMs being difficult to implement, even when several optimisation solvers exist off-shelf. Finally, there is no evidence that SSVMs outperform the Cox PH or baseline models and moreover they often perform worse <span class="citation" data-cites="pkgsurvivalsvm VanBelle2011b">(<a href="references.html#ref-pkgsurvivalsvm" role="doc-biblioref">Fouodo et al. 2018</a>; <a href="references.html#ref-VanBelle2011b" role="doc-biblioref">Vanya Van Belle, Pelckmans, Van Huffel, et al. 2011</a>)</span>, which is also seen in <span class="citation" data-cites="Sonabend2021b">(<a href="references.html#ref-Sonabend2021b" role="doc-biblioref">Sonabend 2021</a>)</span>. Yet one cannot dismiss SSVMs outright as they often require extensive tuning to perform well, even in classification settings, and no benchmark experiment has yet to emerge for testing SSVMs with the required set-up.</p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-CortesVapnik1995" class="csl-entry" role="listitem">
Cortes, Corinna, and Vladimir Vapnik. 1995. <span>“<span>Support-Vector Networks</span>.”</span> <em>Machine Learning</em> 20: 273–97. <a href="https://doi.org/10.1007/BF00994018">https://doi.org/10.1007/BF00994018</a>.
</div>
<div id="ref-Du2011" class="csl-entry" role="listitem">
Du, Xian, and Sumeet Dua. 2011. <span>“<span class="nocase">Cancer prognosis using support vector regression in imaging modality</span>.”</span> <em>World Journal of Clinical Oncology</em> 2 (1): 44–49. <a href="https://doi.org/10.5306/wjco.v2.i1.44">https://doi.org/10.5306/wjco.v2.i1.44</a>.
</div>
<div id="ref-Evers2008" class="csl-entry" role="listitem">
Evers, Ludger, and Claudia-Martina Messow. 2008. <span>“<span class="nocase">Sparse kernel methods for high-dimensional survival data</span>.”</span> <em>Bioinformatics</em> 24 (14): 1632–38.
</div>
<div id="ref-pkgsurvivalsvm" class="csl-entry" role="listitem">
Fouodo, Cesaire J K, I Konig, C Weihs, A Ziegler, and M Wright. 2018. <span>“<span class="nocase">Support vector machines for survival analysis with R</span>.”</span> <em>The R Journal</em> 10 (July): 412–23.
</div>
<div id="ref-Goli2016b" class="csl-entry" role="listitem">
Goli, Shahrbanoo, Hossein Mahjub, Javad Faradmal, Hoda Mashayekhi, and Ali-Reza Soltanian. 2016. <span>“<span class="nocase">Survival Prediction and Feature Selection in Patients with Breast Cancer Using Support Vector Regression</span>.”</span> Edited by Francesco Pappalardo. <em>Computational and Mathematical Methods in Medicine</em> 2016: 2157984. <a href="https://doi.org/10.1155/2016/2157984">https://doi.org/10.1155/2016/2157984</a>.
</div>
<div id="ref-Goli2016a" class="csl-entry" role="listitem">
Goli, Shahrbanoo, Hossein Mahjub, Javad Faradmal, and Ali-Reza Soltanian. 2016. <span>“<span class="nocase">Performance Evaluation of Support Vector Regression Models for Survival Analysis: A Simulation Study</span>.”</span> <em>International Journal of Advanced Computer Science and Applications</em> 7 (June). <a href="https://doi.org/10.14569/IJACSA.2016.070650">https://doi.org/10.14569/IJACSA.2016.070650</a>.
</div>
<div id="ref-Khan2008" class="csl-entry" role="listitem">
Khan, Faisal M., and Valentina Bayer Zubek. 2008. <span>“<span class="nocase">Support vector regression for censored data (SVRc): A novel tool for survival analysis</span>.”</span> <em>Proceedings - IEEE International Conference on Data Mining, ICDM</em>, 863–68. <a href="https://doi.org/10.1109/ICDM.2008.50">https://doi.org/10.1109/ICDM.2008.50</a>.
</div>
<div id="ref-Klein2003" class="csl-entry" role="listitem">
Klein, John P, and Melvin L Moeschberger. 2003. <em><span class="nocase">Survival analysis: techniques for censored and truncated data</span></em>. 2nd ed. Springer Science &amp; Business Media.
</div>
<div id="ref-Land2011" class="csl-entry" role="listitem">
Land, Walker H, Xingye Qiao, Dan Margolis, and Ron Gottlieb. 2011. <span>“<span class="nocase">A new tool for survival analysis: evolutionary programming/evolutionary strategies (EP/ES) support vector regression hybrid using both censored / non-censored (event) data</span>.”</span> <em>Procedia Computer Science</em> 6: 267–72. https://doi.org/<a href="https://doi.org/10.1016/j.procs.2011.08.050">https://doi.org/10.1016/j.procs.2011.08.050</a>.
</div>
<div id="ref-pkgsksurvival" class="csl-entry" role="listitem">
Pölsterl, Sebastian. 2020. <span>“<span class="nocase">scikit-survival: A Library for Time-to-Event Analysis Built on Top of scikit-learn</span>.”</span> <em>Journal of Machine Learning Research</em> 21 (212): 1—–6. <a href="http://jmlr.org/papers/v21/20-729.html">http://jmlr.org/papers/v21/20-729.html</a>.
</div>
<div id="ref-Shiao2013" class="csl-entry" role="listitem">
Shiao, Han-Tai, and Vladimir Cherkassky. 2013. <span>“<span class="nocase">SVM-based approaches for predictive modeling of survival data</span>.”</span> In <em>Proceedings of the International Conference on Data Mining (DMIN)</em>, 1. The Steering Committee of The World Congress in Computer Science, Computer <span>…</span>.
</div>
<div id="ref-Shivaswamy2007" class="csl-entry" role="listitem">
Shivaswamy, Pannagadatta K., Wei Chu, and Martin Jansche. 2007. <span>“<span class="nocase">A support vector approach to censored targets</span>.”</span> In <em>Proceedings - IEEE International Conference on Data Mining, ICDM</em>, 655–60. <a href="https://doi.org/10.1109/ICDM.2007.93">https://doi.org/10.1109/ICDM.2007.93</a>.
</div>
<div id="ref-Sonabend2021b" class="csl-entry" role="listitem">
Sonabend, Raphael Edward Benjamin. 2021. <span>“<span class="nocase">A Theoretical and Methodological Framework for Machine Learning in Survival Analysis: Enabling Transparent and Accessible Predictive Modelling on Right-Censored Time-to-Event Data</span>.”</span> PhD, University College London (UCL). <a href="https://discovery.ucl.ac.uk/id/eprint/10129352/">https://discovery.ucl.ac.uk/id/eprint/10129352/</a>.
</div>
<div id="ref-VanBelle2008" class="csl-entry" role="listitem">
Van Belle, Vanya, Kristiaan Pelckmans, Johan A K Suykens, and Sabine Van Huffel. 2008. <span>“<span class="nocase">Survival SVM: a practical scalable algorithm</span>.”</span> In <em>Proceedings of the 16th European Symposium on Artificial Neural Networks (ESANN)</em>, 89–94.
</div>
<div id="ref-VanBelle2007" class="csl-entry" role="listitem">
Van Belle, Vanya, Kristiaan Pelckmans, Johan A. K. Suykens, and Sabine Van Huffel. 2007. <span>“<span class="nocase">Support Vector Machines for Survival Analysis</span>.”</span> In <em>In Proceedings of the Third International Conference on Computational Intelligence in Medicine and Healthcare</em>. 1.
</div>
<div id="ref-VanBelle2011b" class="csl-entry" role="listitem">
Van Belle, Vanya, Kristiaan Pelckmans, Sabine Van Huffel, and Johan A. K. Suykens. 2011. <span>“<span class="nocase">Support vector methods for survival analysis: A comparison between ranking and regression approaches</span>.”</span> <em>Artificial Intelligence in Medicine</em> 53 (2): 107–18. <a href="https://doi.org/10.1016/j.artmed.2011.06.006">https://doi.org/10.1016/j.artmed.2011.06.006</a>.
</div>
<div id="ref-VanBelle2011a" class="csl-entry" role="listitem">
Van Belle, Vanya, K Pelckmans, Johan A. K. Suykens, and Sabine Van Huffel. 2011. <span>“<span class="nocase">Learning Transformation Models for Ranking and Survival Analysis</span>.”</span> <em>Journal of Machine Learning Research</em> 12: 819–62.
</div>
<div id="ref-VanBelle2010" class="csl-entry" role="listitem">
Van Belle, V, K Pelckmans, J A K Suykens, and S Van Huffel. 2010. <span>“<span class="nocase">Additive survival least-squares support vector machines</span>.”</span> <em>Statistics in Medicine</em> 29 (2): 296–308. <a href="https://doi.org/10.1002/sim.3743">https://doi.org/10.1002/sim.3743</a>.
</div>
<div id="ref-Wang2017" class="csl-entry" role="listitem">
Wang, Ping, Yan Li, and Chandan K. Reddy. 2019. <span>“<span class="nocase">Machine Learning for Survival Analysis</span>.”</span> <em>ACM Computing Surveys</em> 51 (6): 1–36. <a href="https://doi.org/10.1145/3214306">https://doi.org/10.1145/3214306</a>.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./forests.html" class="pagination-link  aria-label=" methods="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Tree-Based Methods</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./boosting.html" class="pagination-link" aria-label="<span class='chapter-number'>15</span>&nbsp; <span class='chapter-title'>Boosting Methods</span>">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Boosting Methods</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">abstract:</span><span class="co"> TODO (150-200 WORDS)</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>{{&lt; include _setup.qmd &gt;}}</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu"># Support Vector Machines {#sec-surv-ml-models-svm}</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>{{&lt; include _wip.qmd &gt;}}</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">### SVMs for Regression</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>In the simplest explanation, support vector machines (SVMs)  <span class="co">[</span><span class="ot">@CortesVapnik1995</span><span class="co">]</span> fit a hyperplane, $g$, on given training data and make predictions for new values as $\hatg(X^*)$ for some testing covariate $X^*$. One may expect the hyperplane to be fit so that all training covariates would map perfectly to the observed labels (a 'hard-boundary') however this would result in overfitting and instead an acceptable ('soft'-)boundary of error, the `$\epsilon$-tube', dictates how 'incorrect' predictions may be, i.e. how large an underestimate or overestimate. (@fig-surv-svm) visualises support vector machines for regression with a linear hyperplane $g$, and an acceptable boundary of error within the dashed lines (the $\epsilon$-tube). SVMs are not limited to linear boundaries and *kernel* functions are utilised to specify more complex hyperplanes. Exact details of the optimization/separating procedure are not discussed here but many off-shelf 'solvers' exist in different programming languages for fitting SVMs.</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>In the regression setting, the goal of SVMs is to estimate the function</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>g: \Reals^p \rightarrow \Reals; \quad (x) \mapsto x\beta + \beta_0</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>$$ {#eq-svm}</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>by estimation of the weights $\beta \in \Reals^p, \beta_0 \in \Reals$ via the optimisation problem</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>&amp; \min_{\beta,\beta_0, \xi, \xi^*} \frac{1}{2} \|\beta\|^2 + C \sum^n_{i=1}(\xi_i + \xi_i^*) <span class="sc">\\</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>&amp; \textrm{subject to}</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>\begin{dcases}</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>Y_i - g(X_i) &amp; \leq \epsilon + \xi_i <span class="sc">\\</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>g(X_i) - Y_i &amp; \leq \epsilon + \xi_i^* <span class="sc">\\</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>\xi_i, \xi_i^* &amp; \geq 0, \ i = 1,...,n</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>\end{dcases}</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>$$ {#eq-svm-opt}</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>where $C \in \Reals$ is the regularization/cost parameter, $\xi_i,\xi_i^*$ are slack parameters and $\epsilon$ is a margin of error for observations on the wrong side of the hyperplane, and $g$ is defined in (@eq-svm). The effect of the slack parameters is seen in (@fig-surv-svm) in which a maximal distance from the $\epsilon$-tube is dictated by the slack variables.</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>In fitting, the dual of the optimisation is instead solved and substituting the optimised parameters into (@eq-svm) gives the prediction function,</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>\hatg(X^*) = \sum^n_{i=1} (\alpha_i - \alpha_i^*)K(X^*,X_i) + \beta_0</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>where $\alpha_i, \alpha_i^*$ are Lagrangrian multipliers and $K$ is some kernel function.\footnote{Discussion about the purpose of kernels and sensible choices can be found in  [@pkgsurvivalsvm; @Hastie2013; @Vapnik1998].} The Karush-Kuhn-Tucker conditions required to solve the optimisation for $\alpha$ result in the key property of SVMs, which is that values $\alpha_i = \alpha_i^* = 0$ indicate that observation $i$ is 'inside' the $\epsilon$-tube and if $\alpha_i \neq 0$ or $\alpha^*_i \neq 0$ then $i$ is outside the tube and termed a *support vector*. It is these 'support vectors' that influence the shape of the separating boundary.</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>The choice of kernel and its parameters, the regularization parameter $C$, and the acceptable error $\epsilon$, are all tunable hyper-parameters, which makes the support vector machine a highly adaptable and often well-performing machine learning method. However the parameters $C$ and $\epsilon$ often have no clear apriori meaning (especially true when predicting abstract rankings) and thus require extensive tuning over a great range of values; no tuning will result in a very poor model fit.</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="al">![Visualising a support vector machine with an $\epsilon$-tube and slack parameters $\xi$ and $\xi^*$. Red circles are values within the $\epsilon$-tube and blue diamonds are values outside the tube. x-axis is single covariate, $x$, and y-axis is $g(x) = x\beta + \beta_0$.](Figures/svm/svm.png)</span>{#fig-surv-svm fig-alt="TODO"}</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="fu">### SVMs for Survival Analysis {#sec-surv-ml-models-svm-surv}</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>Similarly to random forests, all research for Survival Support Vector Machines (SSVMs) can be reduced to very few algorithms, in fact only one unique off-shelf algorithm is identified in this survey. No SSVM for distribution predictions exist, instead they either predict survival time, rankings, or a hybrid of the two.</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>Other reviews and surveys of SSVMs include a short review by Wang $\etal$ (2017)  <span class="co">[</span><span class="ot">@Wang2017</span><span class="co">]</span> and some benchmark experiments and short surveys from Van Belle $\etal$ (2011)  <span class="co">[</span><span class="ot">@VanBelle2011b</span><span class="co">]</span>, Goli $\etal$ (2016)  <span class="co">[</span><span class="ot">@Goli2016a</span><span class="co">]</span> and Fouodo $\etal$ (2018)  <span class="co">[</span><span class="ot">@pkgsurvivalsvm</span><span class="co">]</span>. All the benchmark experiments in these papers indicate that the Cox PH performs as well as, if not better than, the SSVMs.</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>Initial attempts at developing SSVMs by Shivaswamy $\etal$ (2007)  <span class="co">[</span><span class="ot">@Shivaswamy2007</span><span class="co">]</span> took the most 'natural' course and attempt to treat the problem as a regression one with adjustments in the optimisation for censoring. These methods have a natural interpretation and are intuitive in their construction. Further development of these by Khan and Zubek (2008)  <span class="co">[</span><span class="ot">@Khan2008</span><span class="co">]</span> and Land $\etal$ (2011)  <span class="co">[</span><span class="ot">@Land2011</span><span class="co">]</span>  focused on different adjustments for censoring in order to best reflect a realistic survival data set-up. Simultaneously, ranking models were developed in order to directly optimise a model's discriminatory power. Developments started with the work of Evers and Messow (2008)  <span class="co">[</span><span class="ot">@Evers2008</span><span class="co">]</span> but were primarily made by Van Belle $\etal$ (2007)-(2011)  <span class="co">[</span><span class="ot">@VanBelle2010; @VanBelle2007; @VanBelle2008; @VanBelle2011a</span><span class="co">]</span>. These lack the survival time interpretation but are less restrictive in the optimisation constraints. Finally a hybrid of the two followed naturally from Van Belle $\etal$ (2011)  <span class="co">[</span><span class="ot">@VanBelle2011b</span><span class="co">]</span> by combining the constraints from both the regression and ranking tasks. This hybrid method allows a survival time interpretation whilst still optimising discrimination. These hybrid models have become increasingly popular in not only SSVMs, but also neural networks (@sec-surv-ml-models-nn). Instead of presenting these models chronologically, the final hybrid model is defined and then other developments can be more simply presented as components of this hybrid. One model with an entirely different formulation is considered after the hybrid.</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>For all SSVMs defined in this section let: $\xi_i,\xi_i^*,\xi_i'$ be slack variables; $\beta,\beta_0$ be model weights in $\Reals$; $C, \mu$ be regularisation hyper-parameters in $\Reals$; $(X_i, T_i, \Delta_i) \iid (X,T,\Delta)$ be the usual training data; and $g(x) = x\beta + \beta_0$.</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="fu">#### SSVM-Hybrid {.unnumbered .unlisted} {#mod-ssvmhybrid}</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>Van Belle $\etal$ published several papers developing SSVMs, which culminate in the hybrid model here termed 'SSVM-Hybrid'  <span class="co">[</span><span class="ot">@VanBelle2011b</span><span class="co">]</span>. The model is defined by the optimisation problem,</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>**SSVM-Hybrid**<span class="sc">\\</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>&amp; \min_{\beta, \beta_0, \xi, \xi', \xi^*} \frac{1}{2}\|\beta\|^2 + C\sum_{i =1}^n \xi_i + \mu \sum^n_{i=1}(\xi_i' + \xi_i^*) <span class="sc">\\</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>&amp; \textrm{subject to}</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>\begin{dcases}</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>&amp; g(X_i) - g(X_{j(i)}) \geq T_i - T_{j(i)} - \xi_i, <span class="sc">\\</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>&amp; \Delta_i(g(X_i) - T_i) \leq \xi^*_i <span class="sc">\\</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>&amp; T_i - g(X_i) \leq \xi'_i <span class="sc">\\</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>&amp; \xi_i, \xi_i', \xi_i^* \geq 0, \quad \forall i = 1,...,n <span class="sc">\\</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>\end{dcases}</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>\label{eq:surv_ssvmvb2}</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>where $j(i) := \argmax_{j \in 1,...n} <span class="sc">\{</span>T_j : T_j &lt; T_i<span class="sc">\}</span>$ is an index discussed further below. A prediction for test data is given by,</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>\hatg(X^*) = \sum^n_{i=1} \alpha_i(K(X_i, X^*) - K(X_{j(i)}, X^*)) + \alpha^*_i K(X_i, X^*) - \Delta_i\alpha_i'K(X_i, X^*) + \beta_0</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>where $\alpha_i, \alpha_i^*, \alpha_i'$ are Lagrange multipliers and $K$ is a chosen kernel function, which may have hyper-parameters to select or tune.</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a><span class="fu">#### SVCR (Regression) {.unnumbered .unlisted}</span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>Examining the components of the SSVM-Hybrid model will help identify its relation to previously published SSVMs. First note the model's connection to the regression setting when on setting $C = 0$, removing the associated first constraint and ignoring $\Delta$ in the second constraint, the regression setting is exactly recovered:</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>&amp; \min_{\beta, \beta_0, \xi, \xi'} \frac{1}{2}\|\beta\|^2 + \mu \sum^n_{i=1}(\xi_i + \xi_i') <span class="sc">\\</span></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>&amp; \textrm{subject to}</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>\begin{dcases}</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>&amp; g(X_i) - T_i \leq \xi_i <span class="sc">\\</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>&amp; T_i - g(X_i) \leq \xi'_i <span class="sc">\\</span></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>&amp; \xi_i, \xi_i' \geq 0, \quad \forall i = 1,...,n <span class="sc">\\</span></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>\end{dcases}</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>Note a slight difference in the formulation of this optimisation to the original regression problem, here no error component $\epsilon$ is directly included, instead this is part of the optimisation and considered as part of the slack parameters $\xi_i, \xi'_i$; effectively this is the same as setting $\epsilon = 0$. This formulation removes the $\epsilon$-tube symmetry seen previously and therefore distinguishes more clearly between overestimates and underestimates, with each being penalised differently. Removing the $\epsilon$ parameter can lead to model overfitting as all points become support vectors, however careful tuning of other hyper-parameters can effectively control for this.</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>This formulation allows for clearer control over left-, right-, and un-censored observations. Clearly if an observation is uncensored then the true value is known and should be predicted exactly, hence under- and over-estimates are equally problematic and should be penalised the same. If an observation is right-censored then the true death time is greater than the observed time and therefore overestimates should not be heavily penalised but underestimates should be; conversely for left-censored observations.</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>This leads to the first SSVM for regression from Shivaswamy $\etal$ (2007)  <span class="co">[</span><span class="ot">@Shivaswamy2007</span><span class="co">]</span>.</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>**SVCR**\label{mod-svcr}</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>&amp; \min_{\beta, \beta_0, \xi, \xi^*} \frac{1}{2}\|\beta\|^2 + \mu\Big(\sum_{i \in R} \xi_i + \sum_{i \in L} \xi_i^*\Big) <span class="sc">\\</span></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>&amp; \textrm{subject to}</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>\begin{dcases}</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>&amp; g(X_i) - T_i \leq \xi^*_i, \quad \forall i \in R <span class="sc">\\</span></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>&amp; T_i - g(X_i) \leq \xi_i, \quad \forall i \in L <span class="sc">\\</span></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>&amp; \xi_i \geq 0, \forall i\in R; \xi^*_i \geq 0, \forall i \in L</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>\end{dcases}</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>where $L$ is the set of observations who are either left- or un-censored, and $R$ is the set of observations who are either right- or un-censored. Hence an uncensored observation is constrained on both sides as their true survival time is known, whereas a left-censored observation is constrained in the amount of 'over-prediction' and a right-censored observation is constrained by 'under-prediction'. This is intuitive as the only known for these censoring types are the lower and upper bounds of the actual survival time respectively.</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>Reducing this to the book scope of right-censoring only results in the optimisation:</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>&amp; \min_{\beta, \beta_0, \xi, \xi^*} \frac{1}{2}\|\beta\|^2 + \mu\Big(\sum_{i = 1}^n \xi_i + \xi_i^*\Big) <span class="sc">\\</span></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>&amp; \textrm{subject to}</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>\begin{dcases}</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>&amp; \Delta_i(g(X_i) - T_i) \leq \xi_i <span class="sc">\\</span></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>&amp; T_i - g(X_i) \leq \xi^*_i <span class="sc">\\</span></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>&amp; \xi_i, \xi_i^* \geq 0 <span class="sc">\\</span></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>&amp; \forall i\in 1,...,n</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>\end{dcases}</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>which can be seen to be identical to SSVM-Hybrid when $C=0$ and the first constraint is removed. Predictions are found by,</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>\hatg(X^*) = \sum^n_{i=1} \alpha^*_i K(X_i, X^*) - \Delta_i\alpha_i'K(X_i, X^*) + \beta_0</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>The advantage of this algorithm is its simplicity. Clearly if no-one is censored then the optimisation is identical to the regression optimisation in (@eq-svm-opt). As there is no $\epsilon$ hyper-parameter, the run-time complexity is the same as, if not quicker than, a regression SVM. Both left- and right-censoring are handled and no assumptions are made about independent censoring. With respect to performance, benchmark experiments  <span class="co">[</span><span class="ot">@pkgsurvivalsvm</span><span class="co">]</span> indicate that the SVCR does not outperform a na\"ive SVR (i.e. censoring ignored). The SVCR is implemented in the $\Rstats$ package $\pkg{survivalsvm}$  <span class="co">[</span><span class="ot">@pkgsurvivalsvm</span><span class="co">]</span> and is referred to as 'regression'.</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>As discussed, the error margin for left- and right- censoring should not necessarily be equal and the penalty for each should not necessarily be equal either. Hence a natural extension to SVCR is to add further parameters to better separate the different censoring types, which gives rise to the SVRc  <span class="co">[</span><span class="ot">@Khan2008</span><span class="co">]</span>. However this model is only briefly discussed as left-censoring is out of scope of this book and also the model is patented and therefore not easily accessible. The model is given by the optimisation,</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>**SVRc**\label{mod-svrc}</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>&amp; \min_{\beta, \beta_0, \xi, \xi^*} \frac{1}{2}\|\beta\|^2 + \sum^n_{i=1} C_i\xi_i + C^*_i\xi'_i <span class="sc">\\</span></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>&amp; \textrm{subject to}</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>\begin{dcases}</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>&amp; g(X_i) - T_i \leq \epsilon'_i + \xi'_i <span class="sc">\\</span></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>&amp; T_i - g(X_i) \leq \epsilon_i + \xi_i <span class="sc">\\</span></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>&amp; \xi_i, \xi_i' \geq 0, \quad \forall i = 1,...,n <span class="sc">\\</span></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>\end{dcases}</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>Where $C_i = \Delta_iC_c + (1-\Delta_i)C_n, \epsilon_i = \Delta_i\epsilon_c + (1-\Delta_i)\epsilon_n$ and analogously for $C^*_i, C_C^*, \epsilon^*,...$. The new hyper-parameters $C_c, C_n, \epsilon_c, \epsilon_n$ are the penalty for errors in censored predictions (c) and uncensored predictions (n) for left and right (*) censoring, and the acceptable margin of errors respectively. The rationale behind this algorithm is clear, by having asymmetric error margins the algorithm can penalise predictions that are clearly wrong whilst allowing predictions that may be correct (but ultimately unknown due to censoring). Experiments indicate the model may have superior discrimination than the Cox PH  <span class="co">[</span><span class="ot">@Khan2008</span><span class="co">]</span> and SVCR  <span class="co">[</span><span class="ot">@Du2011</span><span class="co">]</span>. However these conclusions are weak as independent experiments do not have access to the patented model.</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>The largest drawback of the algorithm is a need to tune eight parameters. As the number of hyper-parameters to tune increases, so too does model fitting time as well as the risk of overfitting. The problem of extra hyper-parameters is the most common disadvantage of the model given in the literature  <span class="co">[</span><span class="ot">@pkgsurvivalsvm; @Land2011</span><span class="co">]</span>. Land $\etal$ (2011)  <span class="co">[</span><span class="ot">@Land2011</span><span class="co">]</span> present an adaptation to the SVRc to improve model fitting time, termed the EP-SVRc, which uses Evolutionary Programming to determine the optimal values for the parameters. No specific model or algorithm is described, nor any quantitative results presented. No evidence can be found for this method being used since publication. The number of hyper-parameters in the SVRc, coupled with its lack of accessibility, outweigh the benefits of the claimed predictive performance and is therefore clearly not accessible.</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a><span class="fu">#### SSVM-Rank {.unnumbered .unlisted} {#mod-svmem}</span></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>The regression components of SSVM-Hybrid (\ref{eq:surv_ssvmvb2}) have been fully examined, now turning to the ranking components and setting $\mu = 0$. In this case the model reduces to</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>**SSVM-Rank**\label{mod-ranksvmc}\label{mod:ssvmvb1}</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>&amp; \min_{\beta, \beta_0, \xi} \frac{1}{2}\|\beta\|^2 + C\sum_{i =1}^n \xi_i <span class="sc">\\</span></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>&amp; \textrm{subject to}</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>\begin{dcases}</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>&amp; g(X_i) - g(X_{j(i)}) \geq T_i - T_{j(i)} - \xi_i, <span class="sc">\\</span></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>&amp; \xi_i \geq 0, \quad \forall i = 1,...,n <span class="sc">\\</span></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>\end{dcases}</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>with predictions</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>\hatg(X^*) = \sum^n_{i=1} \alpha_i(K(X_i, X^*) - K(X_{j(i)}, X^*))</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>This formulation, termed here 'SSVM-Rank', has been considered by numerous authors in different forms, including Evers and Messow  <span class="co">[</span><span class="ot">@Evers2008</span><span class="co">]</span> and Van Belle $\etal$  <span class="co">[</span><span class="ot">@VanBelle2007; @VanBelle2008; @VanBelle2011b</span><span class="co">]</span>. The primary differences between the various models are in which observations are compared in order to optimise discrimination; to motivate why this matters, first observe the intuitive nature of the optimisation constraints. By example, define $k := T_i - T_{j(i)}$ and say $T_i &gt; T_{j(i)}$. Then, in the first constraint, $g(X_i) - g(X_{j(i)}) \geq k - \xi_i$. As $k &gt; 0$ and $\xi_i \geq 0$, it follows that $g(X_i) &gt; g(X_{j(i)})$, hence creating a concordant ranking\footnote{Note this ranking has the interpretation 'higher rank equals lower risk'.} which is the opposite to the  between observations $i$ (ranked higher) and $j(i)$; illustrating why this optimisation results in a ranking model.</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>This choice of comparing observations $i$ and $j(i)$ (defined below) stems from a few years of research in an attempt to optimise the algorithm with respect to both speed and predictive performance. In the original formulation, RANKSVMC  <span class="co">[</span><span class="ot">@VanBelle2007</span><span class="co">]</span>, the model ranks all possible pairs of observations. This is clearly infeasible as it increases the problem to a $\calO(qn^2/2)$ runtime where $q$ is the proportion of non-censored observations out of a total sample size $n$  <span class="co">[</span><span class="ot">@VanBelle2008</span><span class="co">]</span>. The problem was reduced by taking a nearest neighbours approach and only considering the $k$th closest observations  <span class="co">[</span><span class="ot">@VanBelle2008</span><span class="co">]</span>. Simulation experiments determined that the single nearest neighbour was sufficient, thus arriving at $j(i)$, the observation with the largest observed survival time smaller than $T_i$,</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>j(i) := \argmax_{j \in 1,...n} <span class="sc">\{</span>T_j : T_j &lt; T_i<span class="sc">\}</span></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>This requires that the first observation is taken to be an event, even if it is actually censored. In practice, sorting observations by survival time then greatly speeds up the model run-time  <span class="co">[</span><span class="ot">@pkgsurvivalsvm</span><span class="co">]</span>. The RANKSVMC and SSVM-RANK are implemented in $\pkg{survivalsvm}$  <span class="co">[</span><span class="ot">@pkgsurvivalsvm</span><span class="co">]</span> and referred to as 'vanbelle1' and 'vanbelle2' respectively.</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>The hybrid model is repeated below with the ranking components in blue, the regression components in red, and the common components in black, clearly highlighting the composite nature of the model.</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>&amp; \min_{\beta, \beta_0, \xi, \xi', \xi^*} \frac{1}{2}\|\beta\|^2 + \textcolor{blue}{C\sum_{i =1}^n \xi_i} + \textcolor{red}{\mu \sum^n_{i=1}(\xi_i' + \xi_i^*)} <span class="sc">\\</span></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>&amp; \textrm{subject to}</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>\begin{dcases}</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>&amp; \textcolor{blue}{g(X_i) - g(X_{j(i)}) \geq T_i - T_{j(i)} - \xi_i} <span class="sc">\\</span></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>&amp; \textcolor{red}{\Delta_i(g(X_i) - T_i) \leq \xi^*_i} <span class="sc">\\</span></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>&amp; \textcolor{red}{T_i - g(X_i) \leq \xi'_i} <span class="sc">\\</span></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>&amp; \textcolor{blue}{\xi_i}, \textcolor{red}{\xi_i', \xi_i^*} \geq 0, \quad \forall i = 1,...,n <span class="sc">\\</span></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>\end{dcases}</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>and predictions are made with,</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>\hatg(X^*) = \sum^n_{i=1} \textcolor{blue}{\alpha_i(K(X_i, X^*) - K(X_{j(i)}, X^*))} + \textcolor{red}{\alpha^*_i K(X_i, X^*) - \Delta_i\alpha_i'K(X_i, X^*)} + \beta_0</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>The regularizer hyper-parameters $C$ and $\mu$ now have a clear interpretation. $C$ is the penalty associated with the regression method and $\mu$ is the penalty associated with the ranking method. By always fitting the hybrid models and tuning these two parameters, there is never a requirement to separately fit the regression or ranking methods as these would be automatically identified as superior in the tuning procedure. Moreover, the hybrid model retains the interpretability of the regression method and predictions can be interpreted as survival times. The hybrid method is implemented in $\pkg{survivalsvm}$ as 'hybrid'. By Van Belle's own simulation studies, these models do not outperform the Cox PH with respect to Harrell's C.</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a><span class="fu">#### SSVR-MRL {.unnumbered .unlisted}</span></span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>Not all SSVMs can be considered a variant of the SSVM-Hybrid, though all prominent and commonly utilised suggestions do seem to have this formulation. One other algorithm of note is termed here the 'SSVM-MRL'  <span class="co">[</span><span class="ot">@Goli2016a; @Goli2016b</span><span class="co">]</span>, which is a regression SSVM. The algorithm is identical to SVCR with one additional constraint.</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>**SSVR-MRL**\label{mod-ssvrmrl}<span class="sc">\\</span></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>&amp; \min_{\beta, \beta_0, \xi, \xi^*,\xi'} \frac{1}{2}\|\beta\|^2 + C\sum^n_{i=1} (\xi_i + \xi_i^*) + C^*\sum^n_{i=1} \xi_i' <span class="sc">\\</span></span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a>&amp; \textrm{subject to}</span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>\begin{dcases}</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a>&amp; T_i - g(X_i) \leq \xi_i <span class="sc">\\</span></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>&amp; \Delta_i(g(X_i) - T_i) \leq \xi_i^* <span class="sc">\\</span></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>&amp; (1 - \Delta_i)(g(X_i) - T_i - MRL(T_i|\hatS)) \leq \xi_i' <span class="sc">\\</span></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>&amp; \xi_i, \xi_i^*, \xi_i' \geq 0 <span class="sc">\\</span></span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>&amp; \forall i = 1,...,n</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>\end{dcases}</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>where $MRL(T_i|\hatS)$ is the 'mean residual lifetime' function  <span class="co">[</span><span class="ot">@Klein2003</span><span class="co">]</span></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>MRL(\tau|\hatS) = \frac{\int^\infty_\tau \hat{S}(u) du}{\hat{S}(\tau)}</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a>which is the area under the estimated survival curve (say by Kaplan Meier), $\hatS$, from point $\tau$, weighted by the probability of being alive at point $\tau$. This is interpreted as the expected remaining lifetime from point $\tau$. On setting $C^* = 0$ and removing associated constraint three, this reduces exactly to the SVCR and similarly if there's no censoring then the standard regression setting is recovered. Unlike other strategies, no new hyper-parameters are introduced and Kaplan-Meier estimation should not noticeably impact run-time. There is no evidence of this model being used in practice, nor of any off-shelf implementation. Theoretically, the hybrid model could be expanded to include this extra penalty term and constraint (discussed below).</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conclusions</span></span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>Several SSVMs have been proposed for survival analysis. These can generally be categorised into 'regression' models that adapt SVMs to account for censoring and predict a survival time, 'ranking' models that predict a relative ranking in order to optimise measures of discrimination, and 'hybrid' models that optimise measures of discrimination but make survival time predictions. Other SSVMs that lie outside of these groupings are not able to solve the survival task (e.g.  <span class="co">[</span><span class="ot">@Shiao2013</span><span class="co">]</span>).  Other SVM-type approaches could be considered, including relevance vector machines and import vector machines, however less work has been developed in these areas and further consideration is beyond the scope of this book.</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>The models that have received the most attention are SVCR, SSVM-Rank, and SSVM-Hybrid; the first two are special cases of SSVM-Hybrid. Judging if SSVM-Hybrid (and by extension SVCR and SSVM-Rank) is accessible and transparent is not straightforward. On the one hand it could be considered transparent as SVMs have been studied for decades and the literature for SSVMs, especially from Van Belle, is extensive. On the other hand, the predictions from SSVM-Hybrid should be interpretable as survival times but first hand experience indicates that this is not the case (though this may be due to implementation), which calls into question whether the interpretation they claim to have is actually correct. For accessibility, there appears to be only one implementation of SSVMs in $\Rstats$  <span class="co">[</span><span class="ot">@pkgsurvivalsvm</span><span class="co">]</span>, and also only one in Python  <span class="co">[</span><span class="ot">@pkgsksurvival</span><span class="co">]</span>, which may be due to SSVMs being difficult to implement, even when several optimisation solvers exist off-shelf. Finally, there is no evidence that SSVMs outperform the Cox PH or baseline models and moreover they often perform worse  <span class="co">[</span><span class="ot">@pkgsurvivalsvm; @VanBelle2011b</span><span class="co">]</span>, which is also seen in <span class="co">[</span><span class="ot">@Sonabend2021b</span><span class="co">]</span>. Yet one cannot dismiss SSVMs outright as they often require extensive tuning to perform well, even in classification settings, and no benchmark experiment has yet to emerge for testing SSVMs with the required set-up.\footnote{Though one is in progress as a result of the work in <span class="co">[</span><span class="ot">@Sonabend2021b</span><span class="co">]</span>.}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>All content licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> <br> © Raphael Sonabend, Andreas Bender.</p>
</div>   
    <div class="nav-footer-center">
<p><a href="https://www.mlsabook.com">Website</a> | <a href="https://github.com/mlsa-book/MLSA">GitHub</a></p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mlsa-book/MLSA/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/mlsa-book/MLSA/edit/main/book/svm.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/mlsa-book/MLSA/blob/main/book/svm.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


</body></html>