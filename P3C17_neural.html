<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Raphael Sonabend and Andreas Bender">

<title>15&nbsp; Neural Networks – Machine Learning in Survival Analysis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./P4C19_reductions.html" rel="next">
<link href="./P3C16_boosting.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-0d45b1ff1595a53868627e64e30aef28.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-b6db0a1bf8162d09ed5006fd111a6427.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./P3C13_classical.html">Models</a></li><li class="breadcrumb-item"><a href="./P3C17_neural.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Neural Networks</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning in Survival Analysis</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/mlsa-book/MLSA/tree/main/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Machine-Learning-in-Survival-Analysis.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P0C0_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Symbols and Notation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P0C1_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Survival Analysis and Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P1C2_preview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">MLSA From Start to Finish</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P1C3_machinelearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Machine Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P1C4_survival.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Survival Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P1C5_eha.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Event-history Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P1C6_survtsk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Survival Task</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Evaluation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P2C8_rank.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Discrimination</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P2C9_calib.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Calibration</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P2C10_rules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Scoring Rules</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P2C11_time.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Survival Time Measures</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P3C13_classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Classical Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P3C14_forests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Random Forests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P3C15_svm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P3C16_boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Boosting Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P3C17_neural.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Reduction Techniques</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P4C19_reductions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Reductions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P4C20_competing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Competing Risks Pipelines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P4C21_discrete.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Discrete Time Survival Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P4C22_poisson.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Connections to Poisson Regression and Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P4C23_pseudo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Connections to Regression and Imputation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P5C24_conclusions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">FAQs and Outlook</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P5C25_exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P5C26_references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-surv-ml-models-nn" id="toc-sec-surv-ml-models-nn" class="nav-link active" data-scroll-target="#sec-surv-ml-models-nn"><span class="header-section-number">15.1</span> Neural Networks</a>
  <ul class="collapse">
  <li><a href="#neural-networks-for-regression" id="toc-neural-networks-for-regression" class="nav-link" data-scroll-target="#neural-networks-for-regression"><span class="header-section-number">15.1.1</span> Neural Networks for Regression</a></li>
  <li><a href="#neural-networks-for-survival-analysis" id="toc-neural-networks-for-survival-analysis" class="nav-link" data-scroll-target="#neural-networks-for-survival-analysis"><span class="header-section-number">15.1.2</span> Neural Networks for Survival Analysis</a></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions"><span class="header-section-number">15.1.3</span> Conclusions</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/mlsa-book/MLSA/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/mlsa-book/MLSA/edit/main/book/P3C17_neural.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/mlsa-book/MLSA/blob/main/book/P3C17_neural.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./P3C13_classical.html">Models</a></li><li class="breadcrumb-item"><a href="./P3C17_neural.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Neural Networks</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Neural Networks</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    TODO (150-200 WORDS)
  </div>
</div>


</header>


<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Major changes expected!
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>This page is a work in progress and major changes will be made over time.</strong></p>
</div>
</div>
<section id="sec-surv-ml-models-nn" class="level2" data-number="15.1">
<h2 data-number="15.1" class="anchored" data-anchor-id="sec-surv-ml-models-nn"><span class="header-section-number">15.1</span> Neural Networks</h2>
<p>Before starting the survey on neural networks, first a comment about their transparency and accessibility. Neural networks are infamously difficult to interpret and train, with some calling building and training neural networks an ‘art’ <span class="citation" data-cites="Hastie2001">(<a href="P5C26_references.html#ref-Hastie2001" role="doc-biblioref">Hastie, Tibshirani, and Friedman 2001</a>)</span>. As discussed in the introduction of this book, whilst neural networks are not transparent with respect to their predictions, they are transparent with respect to implementation. In fact the simplest form of neural network, as seen below, is no more complex than a simple linear model. With regard to accessibility, whilst it is true that defining a custom neural network architecture is complex and highly subjective, established models are implemented with a default architecture and are therefore accessible ‘off-shelf’.</p>
<section id="neural-networks-for-regression" class="level3" data-number="15.1.1">
<h3 data-number="15.1.1" class="anchored" data-anchor-id="neural-networks-for-regression"><span class="header-section-number">15.1.1</span> Neural Networks for Regression</h3>
<p>(Artificial) Neural networks (ANNs) are a class of model that fall within the greater paradigm of <em>deep learning</em>. The simplest form of ANN, a feed-forward single-hidden-layer network, is a relatively simple algorithm that relies on linear models, basic activation functions, and simple derivatives. A short introduction to feed-forward regression ANNs is provided to motivate the survival models. This focuses on single-hidden-layer models and increasing this to multiple hidden layers follows relatively simply.</p>
<p>The single hidden-layer network is defined through three equations</p>
<p><span class="math display">\[
\begin{aligned}
&amp; Z_m = \sigma(\alpha_{0m} + \alpha^T_mX_i), \quad m = 1,...,M \\
&amp; T = \beta_{0k} + \beta_k^TZ, \quad k = 1,..,K \\
&amp; g_k(X_i) = \phi_k(T)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\((X_1,...,X_n) \stackrel{i.i.d.}\sim X\)</span> are the usual training data, <span class="math inline">\(\alpha_{0m}, \beta_0\)</span> are bias parameters, and <span class="math inline">\(\theta = \{\alpha_m, \beta\}\)</span> (<span class="math inline">\(m = 1,..,,M\)</span>) are model weights where <span class="math inline">\(M\)</span> is the number of hidden units. <span class="math inline">\(K\)</span> is the number of classes in the output, which for regression is usually <span class="math inline">\(K = 1\)</span>. The function <span class="math inline">\(\phi\)</span> is a ‘link’ or ‘activation function’, which transforms the predictions in order to provide an outcome of the correct return type; usually in regression, <span class="math inline">\(\phi(x) = x\)</span>. <span class="math inline">\(\sigma\)</span> is the ‘activation function’, which transforms outputs from each layer. The <span class="math inline">\(\alpha_m\)</span> parameters are often referred to as ‘activations’. Different activation functions may be used in each layer or the same used throughout, the choice is down to expert knowledge. Common activation functions seen in this section include the sigmoid function, <span class="math display">\[
\sigma(v) = (1 + \exp(-v))^{-1}
\]</span> tanh function, <span id="eq-surv-tanh"><span class="math display">\[
\sigma(v) = \frac{\exp(v) - \exp(-v)}{\exp(v) + \exp(-v)}
\tag{15.1}\]</span></span> and ReLU <span class="citation" data-cites="Nair2010">(<a href="P5C26_references.html#ref-Nair2010" role="doc-biblioref">Nair and Hinton 2010</a>)</span> <span id="eq-surv-relu"><span class="math display">\[
\sigma(v) = \max(0, v)
\tag{15.2}\]</span></span></p>
<p>A single-hidden-layer model can also be expressed in a single equation, which highlights the relative simplicity of what may appear a complex algorithm.</p>
<p><span id="eq-surv-nnet"><span class="math display">\[
g_k(X_i) = \sigma_0(\beta_{k0} + \sum_{h=1}^H (\beta_{kh}\sigma_h (\beta_{h0} + \sum^M_{m=1} \beta_{hm}X_{i;m}))
\tag{15.3}\]</span></span> where <span class="math inline">\(H\)</span> are the number of hidden units, <span class="math inline">\(\beta\)</span> are the model weights, <span class="math inline">\(\sigma_h\)</span> is the activation function in unit <span class="math inline">\(h\)</span>, also <span class="math inline">\(\sigma_0\)</span> is the output unit activation, and <span class="math inline">\(X_{i;m}\)</span> is the <span class="math inline">\(i\)</span>th observation features in the <span class="math inline">\(m\)</span>th hidden unit.</p>
<p>An example feed-forward single-hidden-layer regression ANN is displayed in (<a href="#fig-surv-ann" class="quarto-xref">Figure&nbsp;<span>15.1</span></a>). This model has 10 input units, 13 hidden units, and one output unit; two bias parameters are fit. The model is described as ‘feed-forward’ as there are no cycles in the node and information is passed forward from the input nodes (left) to the output node (right).</p>
<div id="fig-surv-ann" class="quarto-float quarto-figure quarto-figure-center anchored" alt="TODO">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-surv-ann-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/neuralnetworks/ann.png" class="img-fluid figure-img" alt="TODO">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-surv-ann-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15.1: Single-hidden-layer artificial neural network with 13 hidden units fit on the <code>mtcars</code> <span class="citation" data-cites="datamtcars">(<a href="P5C26_references.html#ref-datamtcars" role="doc-biblioref">Henderson and Velleman 1981</a>)</span> dataset using the <span class="math inline">\(\textbf{nnet}\)</span> <span class="citation" data-cites="pkgnnet">(<a href="P5C26_references.html#ref-pkgnnet" role="doc-biblioref">N. Venables and D. Ripley 2002</a>)</span> package, and <span class="citation" data-cites="pkggamlssadd">(<a href="P5C26_references.html#ref-pkggamlssadd" role="doc-biblioref">Stasinopoulos et al. 2020</a>)</span> for plotting. Left column are input variables, I1-I10, second column are 13 hidden units, H1-H13, right column is single output variable, O1. B1 and B2 are bias parameters.
</figcaption>
</figure>
</div>
<section id="back-propagation" class="level4 unnumbered unlisted">
<h4 class="unnumbered unlisted anchored" data-anchor-id="back-propagation">Back-Propagation</h4>
<p>The model weights, <span class="math inline">\(\theta\)</span>, in this section are commonly fit by ‘back-propagation’ although this method is often considered inefficient compared to more recent advances. A brief pseudo-algorithm for the process is provided below.</p>
<p>Let <span class="math inline">\(L\)</span> be a chosen loss function for model fitting, let <span class="math inline">\(\theta = (\alpha, \beta)\)</span> be model weights, and let <span class="math inline">\(J \in \mathbb{N}_{&gt; 0}\)</span> be the number of iterations to train the model over. Then the back-propagation method is given by,</p>
<ul>
<li><strong>For</strong> <span class="math inline">\(j = 1,...,J\)</span>: <em>[] </em>Forward Pass<em> </em>[i.] Fix weights <span class="math inline">\(\theta^{(j-1)}\)</span>. <em>[ii.] Compute predictions <span class="math inline">\(\hat{Y} := \hat{g}^{(j)}_k(X_i|\theta^{(j-1)})\)</span> with (<a href="#eq-surv-nnet" class="quarto-xref">Equation&nbsp;<span>15.3</span></a>). </em>[] <em>Backward Pass</em> <em>[iii.] Calculate the gradients of the loss <span class="math inline">\(L(\hat{Y}|\mathcal{D}_{train})\)</span>. </em>[] <em>Update</em> *[iv.] Update <span class="math inline">\(\alpha^{(r)}, \beta^{(r)}\)</span> with gradient descent.</li>
<li><strong>End For</strong></li>
</ul>
<p>In regression, a common choice for <span class="math inline">\(L\)</span> is the squared loss, <span class="math display">\[
L(\hat{g}, \theta|\mathcal{D}_{train}) = \sum_{i=1}^n (Y_i - \hat{g}(X_i|\theta))^2
\]</span> which may help illustrate how the training outcome, <span class="math inline">\((Y_1,...,Y_n) \stackrel{i.i.d.}\sim Y\)</span>, is utilised for model fitting.</p>
</section>
<section id="making-predictions" class="level4 unnumbered unlisted">
<h4 class="unnumbered unlisted anchored" data-anchor-id="making-predictions">Making Predictions</h4>
<p>Once the model is fitted, predictions for new data follow by passing the testing data as inputs to the model with fitted weights,</p>
<p><span class="math display">\[
g_k(X^*) = \sigma_0(\hat{\beta}_{k0} + \sum_{h=1}^H (\hat{\beta}_{kh}\sigma_h (\hat{\beta}_{h0} + \sum^M_{m=1} \hat{\beta}_{hm}X^*_m))
\]</span></p>
</section>
<section id="hyper-parameters" class="level4 unnumbered unlisted">
<h4 class="unnumbered unlisted anchored" data-anchor-id="hyper-parameters">Hyper-Parameters</h4>
<p>In practice, a regularization parameter, <span class="math inline">\(\lambda\)</span>, is usually added to the loss function in order to help avoid overfitting. This parameter has the effect of shrinking model weights towards zero and hence in the context of ANNs regularization is usually referred to as ‘weight decay’. The value of <span class="math inline">\(\lambda\)</span> is one of three important hyper-parameters in all ANNs, the other two are: the range of values to simulate initial weights from, and the number of hidden units, <span class="math inline">\(M\)</span>.</p>
<p>The range of values for initial weights is usually not tuned but instead a consistent range is specified and the neural network is trained multiple times to account for randomness in initialization.</p>
<p>The regularization parameter and number of hidden units, <span class="math inline">\(M\)</span>, depend on each other and have a similar relationship to the learning rate and number of iterations in the GBMs (<span class="quarto-unresolved-ref">?sec-surv-ml-models-boost</span>). Like the GBMs, it is simplest to set a high number of hidden units and then tune the regularization parameter <span class="citation" data-cites="Bishop2006 Hastie2001">(<a href="P5C26_references.html#ref-Bishop2006" role="doc-biblioref">Bishop 2006</a>; <a href="P5C26_references.html#ref-Hastie2001" role="doc-biblioref">Hastie, Tibshirani, and Friedman 2001</a>)</span>. Determining how many hidden layers to include, and how to connect them, is informed by expert knowledge and well beyond the scope of this book; decades of research has been required to derive sensible new configurations.</p>
</section>
<section id="training-batches" class="level4 unnumbered unlisted">
<h4 class="unnumbered unlisted anchored" data-anchor-id="training-batches">Training Batches</h4>
<p>ANNs can either be trained using complete data, in batches, or online. This decision is usually data-driven and will affect the maximum number of iterations used to train the algorithm; as such this will also often be chosen by expert-knowledge and not empirical methods such as cross-validation.</p>
</section>
<section id="neural-terminology" class="level4 unnumbered unlisted">
<h4 class="unnumbered unlisted anchored" data-anchor-id="neural-terminology">Neural Terminology</h4>
<p>Neural network terminology often reflects the structures of the brain. Therefore ANN units are referred to as nodes or neurons and sometimes the connections between neurons are referred to as synapses. Neurons are said to be ‘fired’ if they are ‘activated’. The simplest example of activating a neuron is with the Heaviside activation function with a threshold of <span class="math inline">\(0\)</span>: <span class="math inline">\(\sigma(v) = \mathbb{I}(v \geq 0)\)</span>. Then a node is activated and passes its output to the next layer if its value is positive, otherwise it contributes no value to the next layer.</p>
</section>
</section>
<section id="neural-networks-for-survival-analysis" class="level3" data-number="15.1.2">
<h3 data-number="15.1.2" class="anchored" data-anchor-id="neural-networks-for-survival-analysis"><span class="header-section-number">15.1.2</span> Neural Networks for Survival Analysis</h3>
<p>Surveying neural networks is a non-trivial task as there has been a long history in machine learning of publishing very specific data-driven neural networks with limited applications; this is also true in survival analysis. This does mean however that where limited developments for survival were made in other machine learning classes, ANN survival adaptations have been around for several decades. A review in 2000 by Schwarzer <span class="math inline">\(\textit{et al.}\)</span> surveyed 43 ANNs for diagnosis and prognosis published in the first half of the 90s, however only up to ten of these are specifically for survival data. Of those, Schwarzer <span class="math inline">\(\textit{et al.}\)</span> deemed three to be ‘na"ive applications to survival data’, and recommended for future research models developed by Liestl <span class="math inline">\(\textit{et al.}\)</span> (1994) <span class="citation" data-cites="Liestol1994">(<a href="P5C26_references.html#ref-Liestol1994" role="doc-biblioref">Liestol, Andersen, and Andersen 1994</a>)</span>, Faraggi and Simon (1995) <span class="citation" data-cites="Faraggi1995">(<a href="P5C26_references.html#ref-Faraggi1995" role="doc-biblioref">Faraggi and Simon 1995</a>)</span>, and Biganzoli <span class="math inline">\(\textit{et al.}\)</span> (1998) <span class="citation" data-cites="Biganzoli1998">(<a href="P5C26_references.html#ref-Biganzoli1998" role="doc-biblioref">E. Biganzoli et al. 1998</a>)</span>.</p>
<p>This survey will not be as comprehensive as the 2000 survey, and nor has any survey since, although there have been several ANN reviews <span class="citation" data-cites="Ripley2001 Huang2020a Ohno-Machado1996 Yang2010 Zhu2020">(<a href="P5C26_references.html#ref-Ripley2001" role="doc-biblioref">B. D. Ripley and Ripley 2001</a>; <a href="P5C26_references.html#ref-Huang2020a" role="doc-biblioref">Huang et al. 2020a</a>; <a href="P5C26_references.html#ref-Ohno-Machado1996" role="doc-biblioref">Ohno-Machado 1996</a>; <a href="P5C26_references.html#ref-Yang2010" role="doc-biblioref">Yang 2010</a>; <a href="P5C26_references.html#ref-Zhu2020" role="doc-biblioref">W. Zhu et al. 2020</a>)</span>. ANNs are considered to be a black-box model, with interpretability decreasing steeply as the number of hidden layers and nodes increases. In terms of accessibility there have been relatively few open-source packages developed for survival ANNs; where these are available the focus has historically been in Python, with no <span class="math inline">\(\textsf{R}\)</span> implementations. The new <span class="math inline">\(\textbf{survivalmodels}\)</span> <span class="citation" data-cites="pkgsurvivalmodels">(<a href="P5C26_references.html#ref-pkgsurvivalmodels" role="doc-biblioref">R. Sonabend 2020</a>)</span> package, implements these Python models via <span class="math inline">\(\textbf{reticulate}\)</span> <span class="citation" data-cites="pkgreticulate">(<a href="P5C26_references.html#ref-pkgreticulate" role="doc-biblioref">Ushey, Allaire, and Tang 2020</a>)</span>. No recurrent neural netwoks are included in this survey though the survival models SRN <span class="citation" data-cites="Oh2018">(<a href="P5C26_references.html#ref-Oh2018" role="doc-biblioref">Oh et al. 2018</a>)</span> and RNN-Surv <span class="citation" data-cites="Giunchiglia2018">(<a href="P5C26_references.html#ref-Giunchiglia2018" role="doc-biblioref">Giunchiglia, Nemchenko, and Schaar 2018</a>)</span> are acknowledged.</p>
<p>This survey is made slightly more difficult as neural networks are often proposed for many different tasks, which are not necessarily clearly advertised in a paper’s title or abstract. For example, many papers claim to use neural networks for survival analysis and make comparisons to Cox models, whereas the task tends to be death at a particular (usually 5-year) time-point (classification) <span class="citation" data-cites="Han2018 Lundin1999 Ripley2001 Ripley1998 Seker2002">(<a href="P5C26_references.html#ref-Han2018" role="doc-biblioref">Han et al. 2018</a>; <a href="P5C26_references.html#ref-Lundin1999" role="doc-biblioref">Lundin et al. 1999</a>; <a href="P5C26_references.html#ref-Ripley2001" role="doc-biblioref">B. D. Ripley and Ripley 2001</a>; <a href="P5C26_references.html#ref-Ripley1998" role="doc-biblioref">R. M. Ripley, Harris, and Tarassenko 1998</a>; <a href="P5C26_references.html#ref-Seker2002" role="doc-biblioref">Huseyin Seker et al. 2002</a>)</span>, which is often not made clear until mid-way through the paper. Reviews and surveys have also conflated these different tasks, for example a very recent review concluded superior performance of ANNs over Cox models, when in fact this is only in classification <span class="citation" data-cites="Huang2020">(<a href="P5C26_references.html#ref-Huang2020" role="doc-biblioref">Huang et al. 2020b</a>)</span> (RM2) {sec:car_reduxstrats_mistakes}. To clarify, this form of classification task does fall into the general <em>field</em> of survival analysis, but not the survival <em>task</em> (<span class="citation" data-cites="box-task-surv">(<a href="P5C26_references.html#ref-box-task-surv" role="doc-biblioref"><strong>box-task-surv?</strong></a>)</span>). Therefore this is not a comment on the classification task but a reason for omitting these models from this survey.</p>
<p>Using ANNs for feature selection (often in gene expression data) and computer vision is also very common in survival analysis, and indeed it is in this area that most success has been seen <span class="citation" data-cites="Bello2019 Chen2014 Cui2020 Lao2017 McKinney2020 Rietschel2018 Seker2002a Zhang2020 Zhu2016">(<a href="P5C26_references.html#ref-Bello2019" role="doc-biblioref">Bello et al. 2019</a>; <a href="P5C26_references.html#ref-Chen2014" role="doc-biblioref">Chen, Ke, and Chiu 2014</a>; <a href="P5C26_references.html#ref-Cui2020" role="doc-biblioref">Cui et al. 2020</a>; <a href="P5C26_references.html#ref-Lao2017" role="doc-biblioref">Lao et al. 2017</a>; <a href="P5C26_references.html#ref-McKinney2020" role="doc-biblioref">McKinney et al. 2020</a>; <a href="P5C26_references.html#ref-Rietschel2018" role="doc-biblioref">Rietschel, Yoon, and Schaar 2018</a>; <a href="P5C26_references.html#ref-Seker2002a" role="doc-biblioref">H. Seker et al. 2002</a>; <a href="P5C26_references.html#ref-Zhang2020" role="doc-biblioref">Zhang et al. 2020</a>; <a href="P5C26_references.html#ref-Zhu2016" role="doc-biblioref">X. Zhu, Yao, and Huang 2016</a>)</span>, but these are again beyond the scope of this survey.</p>
<p>The key difference between neural networks is in their output layer, required data transformations, the model prediction, and the loss function used to fit the model. Therefore the following are discussed for each of the surveyed models: the loss function for training, <span class="math inline">\(L\)</span>, the model prediction type, <span class="math inline">\(\hat{g}\)</span>, and any required data transformation. Notation is continued from the previous surveys with the addition of <span class="math inline">\(\theta\)</span> denoting model weights (which will be different for each model).</p>
<section id="probabilistic-survival-models" class="level4" data-number="15.1.2.1">
<h4 data-number="15.1.2.1" class="anchored" data-anchor-id="probabilistic-survival-models"><span class="header-section-number">15.1.2.1</span> Probabilistic Survival Models</h4>
<p>Unlike other classes of machine learning models, the focus in ANNs has been on probabilistic models. The vast majority make these predictions via reduction to binary classification <span class="math inline">\(\ref{sec:car_reduxes_r7}\)</span>. Whilst almost all of these networks implicitly reduce the problem to classification, most are not transparent in exactly how they do so and none provide clear or detailed interface points in implementation allowing for control over this reduction. Most importantly, the majority of these models do not detail how valid survival predictions are derived from the binary setting, which is not just a theoretical problem as some implementations, such as the Logistic-Hazard model in <span class="math inline">\(\textbf{pycox}\)</span> <span class="citation" data-cites="pkgpycox">(<a href="P5C26_references.html#ref-pkgpycox" role="doc-biblioref">Kvamme 2018</a>)</span>, have been observed to make survival predictions outside the range <span class="math inline">\([0,1]\)</span>. This is not a statement about the performance of models in this section but a remark about the lack of transparency across all probabilistic ANNs.</p>
<p>Many of these algorithms use an approach that formulate the Cox PH as a non-linear model and minimise the partial likelihood. These are referred to as ‘neural-Cox’ models and the earliest appears to have been developed by Faraggi and Simon <span class="citation" data-cites="Faraggi1995">(<a href="P5C26_references.html#ref-Faraggi1995" role="doc-biblioref">Faraggi and Simon 1995</a>)</span>. All these models are technically composites that first predict a ranking, however they assume a PH form and in implementation they all appear to return a probabilistic prediction.</p>
<p><strong>ANN-COX</strong> {#mod-anncox}\ Faraggi and Simon <span class="citation" data-cites="Faraggi1995">(<a href="P5C26_references.html#ref-Faraggi1995" role="doc-biblioref">Faraggi and Simon 1995</a>)</span> proposed a non-linear PH model <span id="eq-surv-farsim"><span class="math display">\[
h(\tau|X_i,\theta) = h_0(\tau)\exp(\phi(X_i\beta))
\tag{15.4}\]</span></span> where <span class="math inline">\(\phi\)</span> is the sigmoid function and <span class="math inline">\(\theta = \{\beta\}\)</span> are model weights. This model, ‘ANN-COX’, estimates the prediction functional, <span class="math inline">\(\hat{g}(X^*) = \phi(X^*\hat{\beta})\)</span>. The model is trained with the partial-likelihood function <span class="math display">\[
L(\hat{g}, \theta|\mathcal{D}_{train}) = \prod_{i = 1}^n \frac{\exp(\sum^M_{m=1} \alpha_m\hat{g}_m(X^*))}{\sum_{j \in \mathcal{R}_{t_i}} \exp(\sum^M_{m=1} \alpha_m\hat{g}_m(X^*))}
\]</span> where <span class="math inline">\(\mathcal{R}_{t_i}\)</span> is the risk group alive at <span class="math inline">\(t_i\)</span>; <span class="math inline">\(M\)</span> is the number of hidden units; <span class="math inline">\(\hat{g}_m(X^*) = (1 + \exp(-X^*\hat{\beta}_m))^{-1}\)</span>; and <span class="math inline">\(\theta = \{\beta, \alpha\}\)</span> are model weights.</p>
<p>The authors proposed a single hidden layer network, trained using back-propagation and weight optimisation with Newton-Raphson. This architecture did not outerperform a Cox PH <span class="citation" data-cites="Faraggi1995">(<a href="P5C26_references.html#ref-Faraggi1995" role="doc-biblioref">Faraggi and Simon 1995</a>)</span>. Further adjustments including (now standard) pre-processing and hyper-parameter tuning did not improve the model performance <span class="citation" data-cites="Mariani1997">(<a href="P5C26_references.html#ref-Mariani1997" role="doc-biblioref">Mariani et al. 1997</a>)</span>. Further independent studies demonstrated worse performance than the Cox model <span class="citation" data-cites="Faraggi1995 Xiang2000">(<a href="P5C26_references.html#ref-Faraggi1995" role="doc-biblioref">Faraggi and Simon 1995</a>; <a href="P5C26_references.html#ref-Xiang2000" role="doc-biblioref">Xiang et al. 2000</a>)</span>.</p>
<p><strong>COX-NNET</strong> {#mod-coxnnet}\ COX-NNET <span class="citation" data-cites="Ching2018a">(<a href="P5C26_references.html#ref-Ching2018a" role="doc-biblioref">Ching, Zhu, and Garmire 2018</a>)</span> updates the ANN-COX by instead maximising the regularized partial log-likelihood <span class="math display">\[
L(\hat{g}, \theta|\mathcal{D}_{train}, \lambda) = \sum^n_{i=1} \Delta_i \Big[\hat{g}(X_i) \ - \ \log\Big(\sum_{j \in \mathcal{R}_{t_i}} \exp(\hat{g}(X_j))\Big)\Big] + \lambda(\|\beta\|_2 + \|w\|_2)
\]</span> with weights <span class="math inline">\(\theta = (\beta, w)\)</span> and where <span class="math inline">\(\hat{g}(X_i) = \sigma(wX_i + b)^T\beta\)</span> for bias term <span class="math inline">\(b\)</span>, and activation function <span class="math inline">\(\sigma\)</span>; <span class="math inline">\(\sigma\)</span> is chosen to be the tanh function ((<a href="#eq-surv-tanh" class="quarto-xref">Equation&nbsp;<span>15.1</span></a>)). In addition to weight decay, dropout <span class="citation" data-cites="Srivastava2014">(<a href="P5C26_references.html#ref-Srivastava2014" role="doc-biblioref">Srivastava et al. 2014</a>)</span> is employed to prevent overfitting. Dropout can be thought of as a similar concept to the variable selection in random forests, as each node is randomly deactivated with probability <span class="math inline">\(p\)</span>, where <span class="math inline">\(p\)</span> is a hyper-parameter to be tuned.</p>
<p>Independent simulation studies suggest that COX-NNET does not outperform the Cox PH <span class="citation" data-cites="Gensheimer2019">(<a href="P5C26_references.html#ref-Gensheimer2019" role="doc-biblioref">Michael F. Gensheimer and Narasimhan 2019</a>)</span>.</p>
<p><strong>DeepSurv</strong> {#mod-deepsurv}\ DeepSurv <span class="citation" data-cites="Katzman2018">(<a href="P5C26_references.html#ref-Katzman2018" role="doc-biblioref">J. L. Katzman et al. 2018</a>)</span> extends these models to deep learning with multiple hidden layers. The chosen error function is the average negative log-partial-likelihood with weight decay <span class="math display">\[
L(\hat{g}, \theta|\mathcal{D}_{train}, \lambda) = -\frac{1}{n^*} \sum_{i = 1}^n \Delta_i \Big[ \Big(\hat{g}(X_i) - \log \sum_{j \in \mathcal{R}_{t_i})} \exp(\hat{g}(X_j)\Big)\Big] + \lambda\|\theta\|^2_2
\]</span> where <span class="math inline">\(n^* := \sum^n_{i=1} \mathbb{I}(\Delta_i = 1)\)</span> is the number of uncensored observations and <span class="math inline">\(\hat{g}(X_i) = \phi(X_i|\theta)\)</span> is the same prediction object as the ANN-COX. State-of-the-art methods are used for data pre-processing and model training. The model architecture uses a combination of fully-connected and dropout layers. Benchmark experiments by the authors indicate that DeepSurv can outperform the Cox PH in ranking tasks <span class="citation" data-cites="Katzman2016 Katzman2018">(<a href="P5C26_references.html#ref-Katzman2016" role="doc-biblioref">J. Katzman et al. 2016</a>; <a href="P5C26_references.html#ref-Katzman2018" role="doc-biblioref">J. L. Katzman et al. 2018</a>)</span> although independent experiments do not confirm this <span class="citation" data-cites="Zhao2020">(<a href="P5C26_references.html#ref-Zhao2020" role="doc-biblioref">Zhao and Feng 2020</a>)</span>.</p>
<p>*Cox-Time** {#mod-coxtime}\ Kvamme <span class="math inline">\(\textit{et al.}\)</span> <span class="citation" data-cites="Kvamme2019a">(<a href="P5C26_references.html#ref-Kvamme2019a" role="doc-biblioref">Kvamme, Borgan, and Scheel 2019</a>)</span> build on these models by allowing time-varying effects. The loss function to minimise, with regularization, is given by <span class="math display">\[
L(\hat{g}, \theta|\mathcal{D}_{train}, \lambda) = \frac{1}{n} \sum_{i:\Delta_i = 1} \log\Big(\sum_{j \in \mathcal{R}_{t_i}} \exp[\hat{g}(X_j,T_i) - \hat{g}(X_i, T_i)]\Big) + \lambda \sum_{i:\Delta_i=1}\sum_{j \in \mathcal{R}_{t_i}} |\hat{g}(X_j,T_i)|
\]</span> where <span class="math inline">\(\hat{g}= \hat{g}_1,...,\hat{g}_n\)</span> is the same non-linear predictor but with a time interaction and <span class="math inline">\(\lambda\)</span> is the regularization parameter. The model is trained with stochastic gradient descent and the risk set, <span class="math inline">\(\mathcal{R}_{t_i}\)</span>, in the equation above is instead reduced to batches, as opposed to the complete dataset. ReLU activations <span class="citation" data-cites="Nair2010">(<a href="P5C26_references.html#ref-Nair2010" role="doc-biblioref">Nair and Hinton 2010</a>)</span> and dropout are employed in training. Benchmark experiments indicate good performance of Cox-Time, though no formal statistical comparisons are provided and hence no comment about general performance can be made.</p>
<p><strong>ANN-CDP</strong> {#mod-anncdp}\ One of the earliest ANNs that was noted by Schwarzer <span class="math inline">\(\textit{et al.}\)</span> <span class="citation" data-cites="Schwarzer2000">(<a href="P5C26_references.html#ref-Schwarzer2000" role="doc-biblioref">Schwarzer, Vach, and Schumacher 2000</a>)</span> was developed by Liestl <span class="math inline">\(\textit{et al.}\)</span> <span class="citation" data-cites="Liestol1994">(<a href="P5C26_references.html#ref-Liestol1994" role="doc-biblioref">Liestol, Andersen, and Andersen 1994</a>)</span> and predicts conditional death probabilities (hence ‘ANN-CDP’). The model first partitions the continuous survival times into disjoint intervals <span class="math inline">\(\mathcal{I}_k, k = 1,...,m\)</span> such that <span class="math inline">\(\mathcal{I}_k\)</span> is the interval <span class="math inline">\((t_{k-1}, t_k]\)</span>. The model then studies the logistic Cox model (proportional odds) <span class="citation" data-cites="Cox1972">(<a href="P5C26_references.html#ref-Cox1972" role="doc-biblioref">Cox 1972</a>)</span> given by <span class="math display">\[
\frac{p_k(\mathbf{x})}{q_k(\mathbf{x})} = \exp(\eta + \theta_k)
\]</span> where <span class="math inline">\(p_k = 1-q_k\)</span>, <span class="math inline">\(\theta_k = \log(p_k(0)/q_k(0))\)</span> for some baseline probability of survival, <span class="math inline">\(q_k(0)\)</span>, to be estimated; <span class="math inline">\(\eta\)</span> is the usual linear predictor, and <span class="math inline">\(q_k = P(T \geq T_k | T \geq T_{k-1})\)</span> is the conditional survival probability at time <span class="math inline">\(T_k\)</span> given survival at time <span class="math inline">\(T_{k-1}\)</span> for <span class="math inline">\(k = 1,...,K\)</span> total time intervals. A logistic activation function is used to predict <span class="math inline">\(\hat{g}(X^*) = \phi(\eta + \theta_k)\)</span>, which provides an estimate for <span class="math inline">\(\hat{p}_k\)</span>.</p>
<p>The model is trained on discrete censoring indicators <span class="math inline">\(D_{ki}\)</span> such that <span class="math inline">\(D_{ki} = 1\)</span> if individual <span class="math inline">\(i\)</span> dies in interval <span class="math inline">\(\mathcal{I}_k\)</span> and <span class="math inline">\(0\)</span> otherwise. Then with <span class="math inline">\(K\)</span> output nodes and maximum likelihood estimation to find the model parameters, <span class="math inline">\(\hat{\eta}\)</span>, the final prediction provides an estimate for the conditional death probabilities <span class="math inline">\(\hat{p}_k\)</span>. The negative log-likelihood to optimise is given by <span class="math display">\[
L(\hat{g}, \theta|\mathcal{D}_{train}) = \sum^n_{i=1}\sum^{m_i}_{k=1} [D_{ki}\log(\hat{p}_k(X_i)) + (1-D_{ki})\log(\hat{q}_k(X_i))]
\]</span> where <span class="math inline">\(m_i\)</span> is the number of intervals in which observation <span class="math inline">\(i\)</span> is not censored.</p>
<p>Liestl <span class="math inline">\(\textit{et al.}\)</span>{} discuss different weighting options and how they correspond to the PH assumption. In the most generalised case, a weight-decay type regularization is applied to the model weights given by <span class="math display">\[
\alpha \sum_l \sum_k (w_{kl} - w_{k-1,l})^2
\]</span> where <span class="math inline">\(w\)</span> are weights, and <span class="math inline">\(\alpha\)</span> is a hyper-parameter to be tuned, which can be used alongside standard weight decay. This corresponds to penalizing deviations from proportionality thus creating a model with approximate proportionality. The authors also suggest the possibility of fixing the weights to be equal in some nodes and different in others; equal weights strictly enforces the proportionality assumption. Their simulations found that removing the proportionality assumption completely, or strictly enforcing it, gave inferior results. Comparing their model to a standard Cox PH resulted in a ‘better’ negative log-likelihood, however this is not a precise evaluation metric and an independent simulation would be preferred. Finally Listl <span class="math inline">\(\textit{et al.}\)</span> included a warning `‘The flexibility is, however, obtained at unquestionable costs: many parameters, difficult interpretation of the parameters and a slow numerical procedure’’ <span class="citation" data-cites="Liestol1994">(<a href="P5C26_references.html#ref-Liestol1994" role="doc-biblioref">Liestol, Andersen, and Andersen 1994</a>)</span>.</p>
<p><strong>PLANN</strong> {#mod-plann}\ Biganzoli <span class="math inline">\(\textit{et al.}\)</span> (1998) <span class="citation" data-cites="Biganzoli1998">(<a href="P5C26_references.html#ref-Biganzoli1998" role="doc-biblioref">E. Biganzoli et al. 1998</a>)</span> studied the same proportional-odds model as the ANN-CDP <span class="citation" data-cites="Liestol1994">(<a href="P5C26_references.html#ref-Liestol1994" role="doc-biblioref">Liestol, Andersen, and Andersen 1994</a>)</span>. Their model utilises partial logistic regression <span class="citation" data-cites="Efron1988">(<a href="P5C26_references.html#ref-Efron1988" role="doc-biblioref">Efron 1988</a>)</span> with added hidden nodes, hence ‘PLANN’. Unlike ANN-CDP, PLANN predicts a smoothed hazard function by using smoothing splines. The continuous time outcome is again discretised into disjoint intervals <span class="math inline">\(t_m, m = 1,...,M\)</span>. At each time-interval, <span class="math inline">\(t_m\)</span>, the number of events, <span class="math inline">\(d_m\)</span>, and number of subjects at risk, <span class="math inline">\(n_m\)</span>, can be used to calculate the discrete hazard function, <span id="eq-surv-dischaz"><span class="math display">\[
\hat{h}_m = \frac{d_m}{n_m}, m = 1,...,M
\tag{15.5}\]</span></span> This quantity is used as the target to train the neural network. The survival function is then estimated by the Kaplan-Meier type estimator, <span id="eq-surv-discreteKM"><span class="math display">\[
\hat{S}(\tau) = \prod_{m:t_m \leq \tau} (1 - \hat{h}_m)
\tag{15.6}\]</span></span></p>
<p>The model is fit by employing one of the more ‘usual’ survival reduction strategies in which an observation’s survival time is treated as a covariate in the model <span class="citation" data-cites="Tutz2016">(<a href="P5C26_references.html#ref-Tutz2016" role="doc-biblioref">Tutz and Schmid 2016</a>)</span>. As this model uses discrete time, the survival time is discretised into one of the <span class="math inline">\(M\)</span> intervals. This approach removes the proportional odds constraint as interaction effects between time and covariates can be modelled (as time-updated covariates). Again the model makes predictions at a given time <span class="math inline">\(m\)</span>, <span class="math inline">\(\phi(\theta_m + \eta)\)</span>, where <span class="math inline">\(\eta\)</span> is the usual linear predictor, <span class="math inline">\(\theta\)</span> is the baseline proportional odds hazard <span class="math inline">\(\theta_m = \log(h_m(0)/(1-h_m(0))\)</span>. The logistic activation provides estimates for the discrete hazard, <span class="math display">\[
h_m(X_i) = \frac{\exp(\theta_m + \hat{\eta})}{1 + \exp(\theta_m + \hat{\eta})}
\]</span> which is smoothed with cubic splines <span class="citation" data-cites="Efron1988">(<a href="P5C26_references.html#ref-Efron1988" role="doc-biblioref">Efron 1988</a>)</span> that require tuning.</p>
<p>A cross-entropy error function is used for training <span class="math display">\[
L(\hat{h}, \theta|\mathcal{D}_{train}, a) = - \sum^M_{m = 1} \Big[\hat{h}_m \log \Big(\frac{h_l(X_i, a_l)}{\hat{h}_m}\Big) + (1 - \hat{h}_m) \log \Big(\frac{1 - h_l(X_i, a_l)}{1 - \hat{h}_m}\Big)\Big]n_m
\]</span> where <span class="math inline">\(h_l(X_i, a_l)\)</span> is the discrete hazard <span class="math inline">\(h_l\)</span> with smoothing at mid-points <span class="math inline">\(a_l\)</span>. Weight decay can be applied and the authors suggest <span class="math inline">\(\lambda \approx 0.01-0.1\)</span> <span class="citation" data-cites="Biganzoli1998">(<a href="P5C26_references.html#ref-Biganzoli1998" role="doc-biblioref">E. Biganzoli et al. 1998</a>)</span>, though they make use of an AIC type criterion instead of cross-validation.</p>
<p>This model makes smoothed hazard predictions at a given time-point, <span class="math inline">\(\tau\)</span>, by including <span class="math inline">\(\tau\)</span> in the input covariates <span class="math inline">\(X_i\)</span>. Therefore the model first requires transformation of the input data by replicating all observations and replacing the single survival indicator <span class="math inline">\(\Delta_i\)</span>, with a time-dependent indicator <span class="math inline">\(D_{ik}\)</span>, the same approach as in ANN-CDP. Further developments have extended the PLANN to Bayesian modelling, and for competing risks <span class="citation" data-cites="Biganzoli2009">(<a href="P5C26_references.html#ref-Biganzoli2009" role="doc-biblioref">E. M. Biganzoli, Ambrogi, and Boracchi 2009</a>)</span>.</p>
<p>No formal comparison is made to simpler model classes. The authors recommend ANNs primarily for exploration, feature selection, and understanding underlying patterns in the data <span class="citation" data-cites="Biganzoli2009">(<a href="P5C26_references.html#ref-Biganzoli2009" role="doc-biblioref">E. M. Biganzoli, Ambrogi, and Boracchi 2009</a>)</span>.</p>
<p><strong>Nnet-survival</strong> {#mod-nnetsurvival}\ Aspects of the PLANN algorithm have been generalised into discrete-time survival algorithms in several papers <span class="citation" data-cites="Gensheimer2019 Kvamme2019 Mani1999 Street1998">(<a href="P5C26_references.html#ref-Gensheimer2019" role="doc-biblioref">Michael F. Gensheimer and Narasimhan 2019</a>; <a href="P5C26_references.html#ref-Kvamme2019" role="doc-biblioref"><strong>Kvamme2019?</strong></a>; <a href="P5C26_references.html#ref-Mani1999" role="doc-biblioref">Mani et al. 1999</a>; <a href="P5C26_references.html#ref-Street1998" role="doc-biblioref">Street 1998</a>)</span>. Various estimates have been derived for transforming the input data to a discrete hazard or survival function. Though only one is considered here as it is the most modern and has a natural interpretation as the ‘usual’ Kaplan-Meier estimator for the survival function. Others by Street (1998) <span class="citation" data-cites="Street1998">(<a href="P5C26_references.html#ref-Street1998" role="doc-biblioref">Street 1998</a>)</span> and Mani (1999) <span class="citation" data-cites="Mani1999">(<a href="P5C26_references.html#ref-Mani1999" role="doc-biblioref">Mani et al. 1999</a>)</span> are acknowledged. The discrete hazard estimator (<a href="#eq-surv-dischaz" class="quarto-xref">Equation&nbsp;<span>15.5</span></a>), <span class="math inline">\(\hat{h}\)</span>, is estimated and these values are used as the targets for the ANN. For the error function, the mean negative log-likelihood for discrete time <span class="citation" data-cites="Kvamme2019">(<a href="P5C26_references.html#ref-Kvamme2019" role="doc-biblioref"><strong>Kvamme2019?</strong></a>)</span> is minimised to estimate <span class="math inline">\(\hat{h}\)</span>, <span class="math display">\[
\begin{split}
L(\hat{h}, \theta|\mathcal{D}_{train}) = -\frac{1}{n} \sum^n_{i=1}\sum^{k(T_i)}_{j=1} (\mathbb{I}(T_i = \tau_j, \Delta_i = 1) \log[\hat{h}_i(\tau_j)] \ + \\
(1-\mathbb{I}(T_i = \tau_j, \Delta_i = 1))\log(1 - \hat{h}_i(\tau_j)))
\end{split}
\]</span> where <span class="math inline">\(k(T_i)\)</span> is the time-interval index in which observation <span class="math inline">\(i\)</span> dies/is censored, <span class="math inline">\(\tau_j\)</span> is the <span class="math inline">\(j\)</span>th discrete time-interval, and the prediction of <span class="math inline">\(\hat{h}\)</span> is obtained via <span class="math display">\[
\hat{h}(\tau_j|\mathcal{D}_{train}) = [1 + \exp(-\hat{g}_j(\mathcal{D}_{train}))]^{-1}
\]</span> where <span class="math inline">\(\hat{g}_j\)</span> is the <span class="math inline">\(j\)</span>th output for <span class="math inline">\(j = 1,...,m\)</span> discrete time intervals. The number of units in the output layer for these models corresponds to the number of discrete-time intervals. Deciding the width of the time-intervals is an additional hyper-parameter to consider.</p>
<p>Gensheimer and Narasimhan’s ‘Nnet-survival’ <span class="citation" data-cites="Gensheimer2019">(<a href="P5C26_references.html#ref-Gensheimer2019" role="doc-biblioref">Michael F. Gensheimer and Narasimhan 2019</a>)</span> has two different implementations. The first assumes a PH form and predicts the linear predictor in the final layer, which can then be composed to a distribution. Their second ‘flexible’ approach instead predicts the log-odds of survival in each node, which are then converted to a conditional probability of survival, <span class="math inline">\(1 - h_j\)</span>, in a given interval using the sigmoid activation function. The full survival function can be derived with (<a href="#eq-surv-discreteKM" class="quarto-xref">Equation&nbsp;<span>15.6</span></a>). The model has been demonstrated not to outperform the Cox PH with respect to Harrell’s C or the Graf (Brier) score <span class="citation" data-cites="Gensheimer2019">(<a href="P5C26_references.html#ref-Gensheimer2019" role="doc-biblioref">Michael F. Gensheimer and Narasimhan 2019</a>)</span>.</p>
<p><strong>PC-Hazard</strong> {#mod-pchazard}\ Kvamme and Borgan deviate from nnet-survival in their ‘PC-Hazard’ <span class="citation" data-cites="Kvamme2019">(<a href="P5C26_references.html#ref-Kvamme2019" role="doc-biblioref"><strong>Kvamme2019?</strong></a>)</span> by first considering a discrete-time approach with a softmax activation function influenced by multi-class classification. They expand upon this by studying a piecewise constant hazard function in continuous time and defining the mean negative log-likelihood as <span class="math display">\[
L(\hat{g}, \theta|\mathcal{D}_{train}) = -\frac{1}{n} \sum^n_{i=1} \Big(\Delta_i X_i\log\tilde{\eta}_{k(T_i)} - X_i\tilde{\eta}_{k(T_i)}\rho(T_i) - \sum^{k(T_i)-1}_{j=1} \tilde{\eta}_jX_i\Big)
\]</span> where <span class="math inline">\(k(T_i)\)</span> and <span class="math inline">\(\tau_i\)</span> is the same as defined above, <span class="math inline">\(\rho(t) = \frac{t - \tau_{k(t)-1}}{\Delta\tau_{k(t)}}\)</span>, <span class="math inline">\(\Delta\tau_j = \tau_j - \tau_{j-1}\)</span>, and <span class="math inline">\(\tilde{\eta}_j := \log(1 + \exp(\hat{g}_j(X_i))\)</span> where again <span class="math inline">\(\hat{g}_j\)</span> is the <span class="math inline">\(j\)</span>th output for <span class="math inline">\(j = 1,...,m\)</span> discrete time intervals. Once the weights have been estimated, the predicted survival function is given by <span class="math display">\[
\hat{S}(\tau, X^*|\mathcal{D}_{train}) = \exp(-X^*\tilde{\eta}_{k(\tau)}\rho(\tau)) \prod^{k(\tau)-1}_{j=1} \exp(-\tilde{\eta}_j(X^*))
\]</span> Benchmark experiments indicate similar performance to nnet-survival <span class="citation" data-cites="Kvamme2019">(<a href="P5C26_references.html#ref-Kvamme2019" role="doc-biblioref"><strong>Kvamme2019?</strong></a>)</span>, an unsurprising result given their implementations are identical with the exception of the loss function <span class="citation" data-cites="Kvamme2019">(<a href="P5C26_references.html#ref-Kvamme2019" role="doc-biblioref"><strong>Kvamme2019?</strong></a>)</span>, which is also similar for both models. A key result found that varying values for interval width lead to significant differences and therefore should be carefully tuned.</p>
<p><strong>DNNSurv</strong> {#mod-dnnsurv}\ A very recent (pre-print) approach <span class="citation" data-cites="Zhao2020">(<a href="P5C26_references.html#ref-Zhao2020" role="doc-biblioref">Zhao and Feng 2020</a>)</span> instead first computes ‘pseudo-survival probabilities’ and uses these to train a regression ANN with sigmoid activation and squared error loss. These pseudo-probabilities are computed using a jackknife-style estimator given by <span class="math display">\[
\tilde{S}_{ij}(T_{j+1}, \mathcal{R}_{t_j}) = n_j\hat{S}(T_{j+1}|\mathcal{R}_{t_j}) - (n_j - 1)\hat{S}^{-i}(T_{j+1}|\mathcal{R}_{t_j})
\]</span> where <span class="math inline">\(\hat{S}\)</span> is the IPCW weighted Kaplan-Meier estimator (defined below) for risk set <span class="math inline">\(\mathcal{R}_{t_j}\)</span>, <span class="math inline">\(\hat{S}^{-i}\)</span> is the Kaplan-Meier estimator for all observations in <span class="math inline">\(\mathcal{R}_{t_j}\)</span> excluding observation <span class="math inline">\(i\)</span>, and <span class="math inline">\(n_j := |\mathcal{R}_{t_j}|\)</span>. The IPCW weighted Kaplan-Meier estimate is found via the IPCW Nelson-Aalen estimator, <span class="math display">\[
\hat{H}(\tau|\mathcal{D}_{train}) = \sum^n_{i=1} \int^\tau_0 \frac{\mathbb{I}(T_i \leq u, \Delta_i = 1)\hat{W}_i(u)}{\sum^n_{j=1} \mathbb{I}(T_j \geq u) \hat{W}_j(u)} \ du
\]</span> where <span class="math inline">\(\hat{W}_i,\hat{W}_j\)</span> are subject specific IPC weights.</p>
<p>In their simulation studies, they found no improvement over other proposed neural networks. Arguably the most interesting outcome of their paper are comparisons of multiple survival ANNs at specific time-points, evaluated with C-index and Brier score. Their results indicate identical performance from all models. They also provide further evidence of neural networks not outperforming a Cox PH when the PH assumption is valid. However, in their non-PH dataset, DNNSurv appears to outperform the Cox model (no formal tests are provided). Data is replicated similarly to previous models except that no special indicator separates censoring and death, this is assumed to be handled by the IPCW pseudo probabilities.</p>
<!-- %**RNN-SURV** {#mod-rnnsurv}\\
%\hl{consider deleting this model}
%RNN-SURV  [@Giunchiglia2018] again uses a reduction to binary classification approach though makes use of recurrent layers to incorporate the sequential nature of survival data and time-variant features. The final layer in the model consists of $K$ nodes, which correspond with individual survival probability predictions for interval $k$, $\hatS(\tau_k|X)$, where $\tau_k, k = 1,...,K,$ are $K$ discrete time-intervals $\{(\tau_0, \tau_1],...,(\tau_{K-1}, \tau_K)]\}$. The model assumes constant hazards within each time interval. These estimates can be linearly combined to predict a single risk score for each observation, $\hat{r}_i = \sum^K_{k=1} \theta_k\hatS_i(\tau_k)$ where $\theta_k$ are the weights from the final model layer. In order to accommodate these predictions the data is again first transformed to consist of vectors of covariates with their corresponding time interval. The model uses a composite loss built from a linear combination of two individual losses with weight decay,
%$$
%L(\hatS, t, \delta|\theta, \alpha, \gamma) = \alpha L_1(\hatS, t, \delta|\theta) + \gamma L_2(\hatS, t,\delta|\theta) + \lambda\|\theta\|^2_2
%$$
%where $\alpha, \beta, \gamma$ are hyper-parameters to tune, $\theta$ are the model weights, and
%$$
%L_1(\hatS, t,\delta| \theta) = -\sum^K_{k=1}\sum_{i \in U_k} [\II(t_i > \tau_k)\log(\hatS_i(\tau_k)) + (1-\II(t_i > \tau_k))\log(1-\hatS_i(\tau_k)]
%$$ {#eq-surv-rnnsurv-cross}
%where $U_i = \{i : \delta_i = 1 \cup \delta_i = 0 \cap T_i > \tau_k\}$ is the set of observations who are either alive, dead, or not-yet-censored. $L_1$ can be recognised as a cross-entropy loss  [@Graf1999]. $L_2$ is an upper bound of the negative C-index  [@Steck2008],
%$$
%L_2(\theta) = -\frac{1}{|\calC|}\sum_{(i,j)\in\calC} \Big[1 + \Big(\frac{\log \ \sigma(\hat{r}_j-\hat{r}_i)}{\log \ 2}\Big)\Big]
%$$
%where $\calC = \{(i,j) : \delta_i = 1 \cap t_i \leq t_j\}$. The ReLU activation function is used for the feed-forward layers, long short-term memory (LSTM)  [@Hochreiter1997] cells in the recurrent layers, and the sigmoid function in the final layer. Dropout is again employed. The model demonstrates good sepration ability with respect to Harrell's C and is shown to significantly outperform Cox PH, RSFs  [@Ishwaran2008], and DeepSurv  [@Katzman2018]. No experiments are performed to assess the predictive performance of the predicted survival distributions. No explicit methodology is provided for enforcing a decreasing monotonic prediction for the discrete survival distribution, which is a general problem that is discussed further in (@sec-car). No off-shelf implementation is available.
% -->
<p><strong>DeepHit</strong> {#mod-deephit}\ DeepHit <span class="citation" data-cites="Lee2018a">(<a href="P5C26_references.html#ref-Lee2018a" role="doc-biblioref">Lee et al. 2018</a>)</span> was originally built to accommodate competing risks, but only the non-competing case is discussed here <span class="citation" data-cites="Kvamme2019a">(<a href="P5C26_references.html#ref-Kvamme2019a" role="doc-biblioref">Kvamme, Borgan, and Scheel 2019</a>)</span>. The model builds on previous approaches by discretising the continuous time outcome, and makes use of a composite loss. It has the advantage of making no parametric assumptions and directly predicts the probability of failure in each time-interval (which again correspond to different terminal nodes), i.e.&nbsp;<span class="math inline">\(\hat{g}(\tau_k|\mathcal{D}_{test}) = \hat{P}(T^* = \tau_k|X^*)\)</span> where again <span class="math inline">\(\tau_k, k = 1,...,K\)</span> are the distinct time intervals. The estimated survival function is found with <span class="math inline">\(\hat{S}(\tau_K|X^*) = 1 - \sum^K_{k = 1} \hat{g}_i(\tau_k|X^*)\)</span>. ReLU activations were used in all fully connected layers and a softmax activation in the final layer. The losses in the composite error function are given by <span class="math display">\[
L_1(\hat{g}, \theta|\mathcal{D}_{train}) = -\sum^N_{i=1} [\Delta_i \log(\hat{g}_i(T_i)) + (1-\Delta_i)\log(\hat{S}_i(T_i))]
\]</span> and <span class="math display">\[
L_2(\hat{g}, \theta|\mathcal{D}_{train}, \sigma) = \sum_{i \neq j} \Delta_i \mathbb{I}(T_i &lt; T_j) \sigma(\hat{S}_i(T_i), \hat{S}_j(T_i))
\]</span> for some convex loss function <span class="math inline">\(\sigma\)</span> and where <span class="math inline">\(\hat{g}_i(t) = \hat{g}(t|X_i)\)</span>. Again these can be seen to be a cross-entropy loss and a ranking loss. Benchmark experiments demonstrate the model outperforming the Cox PH and RSFs <span class="citation" data-cites="Lee2018a">(<a href="P5C26_references.html#ref-Lee2018a" role="doc-biblioref">Lee et al. 2018</a>)</span> with respect to separation, and an independent experiment supports these findings <span class="citation" data-cites="Kvamme2019a">(<a href="P5C26_references.html#ref-Kvamme2019a" role="doc-biblioref">Kvamme, Borgan, and Scheel 2019</a>)</span>. However, the same independent study demonstrated worse performance than a Cox PH with respect to the integrated Brier score <span class="citation" data-cites="Graf1999">(<a href="P5C26_references.html#ref-Graf1999" role="doc-biblioref">Graf et al. 1999</a>)</span>.</p>
</section>
<section id="deterministic-survival-models" class="level4" data-number="15.1.2.2">
<h4 data-number="15.1.2.2" class="anchored" data-anchor-id="deterministic-survival-models"><span class="header-section-number">15.1.2.2</span> Deterministic Survival Models</h4>
<p>Whilst the vast majority of survival ANNs have focused on probabilistic predictions (often via ranking), a few have also tackled the deterministic or ‘hybrid’ problem.</p>
<p><strong>RankDeepSurv</strong> {#mod-rankdeepsurv}\ Jing <span class="math inline">\(\textit{et al.}\)</span> <span class="citation" data-cites="Jing2019">(<a href="P5C26_references.html#ref-Jing2019" role="doc-biblioref">Jing et al. 2019</a>)</span> observed the past two decades of research in survival ANNs and then published a completely novel solution, RankDeepSurv, which makes predictions for the survival time <span class="math inline">\(\hat{T} = (\hat{T}_1,...,\hat{T}_n)\)</span>. They proposed a composite loss function <span class="math display">\[
L(\hat{T}, \theta|\mathcal{D}_{train}, \alpha,\gamma,\lambda) = \alpha L_1(\hat{T},T,\Delta) + \gamma L_2(\hat{T},T,\Delta) + \lambda\|\theta\|^2_2
\]</span> where <span class="math inline">\(\theta\)</span> are the model weights, <span class="math inline">\(\alpha,\gamma \in \mathbb{R}_{&gt;0}\)</span>, <span class="math inline">\(\lambda\)</span> is the shrinkage parameter, by a slight abuse of notation <span class="math inline">\(T = (T_1,...,T_n)\)</span> and <span class="math inline">\(\Delta = (\Delta_1,...,\Delta_n)\)</span>, and <span class="math display">\[
L_1(\hat{T}, \theta|\mathcal{D}_{train}) = \frac{1}{n} \sum_{\{i: I(i) = 1\}} (\hat{T}_i - T_i)^2;
\quad I(i) =
\begin{cases}
1, &amp; \Delta_i = 1 \cup (\Delta_i = 0 \cap \hat{T}_i \leq T_i) \\
0, &amp; \text{otherwise}
\end{cases}
\]</span> <span class="math display">\[
L_2(\hat{T}, \theta|\mathcal{D}_{train}) = \frac{1}{n}\sum^n_{\{i,j : I(i,j) = 1\}} [(T_j - T_i) - (\hat{T}_j - \hat{T}_i)]^2;
\quad
I(i,j) =
\begin{cases}
1, &amp; T_j - T_i &gt; \hat{T}_j - \hat{T}_i \\
0, &amp; \text{otherwise}
\end{cases}
\]</span> where <span class="math inline">\(\hat{T}_i\)</span> is the predicted survival time for observation <span class="math inline">\(i\)</span>. A clear contrast can be made between these loss functions and the constraints used in SSVM-Hybrid <span class="citation" data-cites="VanBelle2011b">(<a href="P5C26_references.html#ref-VanBelle2011b" role="doc-biblioref">Van Belle et al. 2011</a>)</span> (<a href="P3C15_svm.html#sec-surv-ml-models-svm-surv" class="quarto-xref"><span>Section 13.2</span></a>). <span class="math inline">\(L_1\)</span> is the squared second constraint in <span class="math inline">\(\ref{eq:surv_ssvmvb2}\)</span> and <span class="math inline">\(L_2\)</span> is the squared first constraint in <span class="math inline">\(\ref{eq:surv_ssvmvb2}\)</span>. However <span class="math inline">\(L_1\)</span> in RankDeepSurv discards the squared error difference for all censored observations when the prediction is lower than the observed survival time; which is problematic as if someone is censored at time <span class="math inline">\(T_i\)</span> then it is guaranteed that their true survival time is greater than <span class="math inline">\(T_i\)</span> (this constraint may be more sensible if the inequality were reversed). An advantage to this loss is, like the SSVM-Hybrid, it enables a survival time interpretation for a ranking optimised model; however these ‘survival times’ should be interpreted with care.</p>
<p>The authors propose a model architecture with several fully connected layers with the ELU <span class="citation" data-cites="Clevert2015">(<a href="P5C26_references.html#ref-Clevert2015" role="doc-biblioref">Clevert, Unterthiner, and Hochreiter 2015</a>)</span> activation function and a single dropout layer. Determining the success of this model is not straightforward. The authors claim superiority of RankDeepSurv over Cox PH, DeepSurv, and RSFs however this is an unclear comparison (RM2) {sec:car_reduxstrats_mistakes} that requires independent study.</p>
</section>
</section>
<section id="conclusions" class="level3" data-number="15.1.3">
<h3 data-number="15.1.3" class="anchored" data-anchor-id="conclusions"><span class="header-section-number">15.1.3</span> Conclusions</h3>
<p>There have been many advances in neural networks for survival analysis. It is not possible to review all proposed survival neural networks without diverting too far from the book scope. This survey of ANNs should demonstrate two points: firstly that the vast majority (if not all) of survival ANNs are reduction models that either find a way around censoring via imputation or discretisation of time-intervals, or by focusing on partial likelihoods only; secondly that no survival ANN is fully accessible or transparent.</p>
<p>Despite ANNs being highly performant in other areas of supervised learning, there is strong evidence that the survival ANNs above are inferior to a Cox PH when the data follows the PH assumption or when variables are linearly related <span class="citation" data-cites="Gensheimer2018 Luxhoj1997 Ohno-Machado1997 Puddu2012 Xiang2000 Yang2010 Yasodhara2018 Zhao2020">(<a href="P5C26_references.html#ref-Gensheimer2018" role="doc-biblioref">Michael F. Gensheimer and Narasimhan 2018</a>; <a href="P5C26_references.html#ref-Luxhoj1997" role="doc-biblioref">Luxhoj and Shyur 1997</a>; <a href="P5C26_references.html#ref-Ohno-Machado1997" role="doc-biblioref">Ohno-Machado 1997</a>; <a href="P5C26_references.html#ref-Puddu2012" role="doc-biblioref">Puddu and Menotti 2012</a>; <a href="P5C26_references.html#ref-Xiang2000" role="doc-biblioref">Xiang et al. 2000</a>; <a href="P5C26_references.html#ref-Yang2010" role="doc-biblioref">Yang 2010</a>; <a href="P5C26_references.html#ref-Yasodhara2018" role="doc-biblioref">Yasodhara, Bhat, and Goldenberg 2018</a>; <a href="P5C26_references.html#ref-Zhao2020" role="doc-biblioref">Zhao and Feng 2020</a>)</span>. There are not enough experiments to make conclusions in the case when the data is non-PH. Experiments in <span class="citation" data-cites="Sonabend2021b">(<a href="P5C26_references.html#ref-Sonabend2021b" role="doc-biblioref">R. E. B. Sonabend 2021</a>)</span> support the finding that survival ANNs are not performant.</p>
<p>There is evidence that many papers introducing neural networks do not utilise proper methods of comparison or evaluation <span class="citation" data-cites="Kiraly2018d">(<a href="P5C26_references.html#ref-Kiraly2018d" role="doc-biblioref">Király, Mateen, and Sonabend 2018</a>)</span> and in conducting this survey, these findings are further supported. Many papers made claims of being ‘superior’ to the Cox model based on unfair comparisons (RM2){sec:car_reduxstrats_mistakes} or miscommunicating (or misinterpreting) results (e.g. <span class="citation" data-cites="Fotso2018">(<a href="P5C26_references.html#ref-Fotso2018" role="doc-biblioref">Fotso 2018</a>)</span>). At this stage, it does not seem possible to make any conclusions about the effectiveness of neural networks in survival analysis. Moreover, even the authors of these models have pointed out problems with transparency <span class="citation" data-cites="Biganzoli2009 Liestol1994">(<a href="P5C26_references.html#ref-Biganzoli2009" role="doc-biblioref">E. M. Biganzoli, Ambrogi, and Boracchi 2009</a>; <a href="P5C26_references.html#ref-Liestol1994" role="doc-biblioref">Liestol, Andersen, and Andersen 1994</a>)</span>, which was further highlighted by Schwarzer <span class="math inline">\(\textit{et al.}\)</span> <span class="citation" data-cites="Schwarzer2000">(<a href="P5C26_references.html#ref-Schwarzer2000" role="doc-biblioref">Schwarzer, Vach, and Schumacher 2000</a>)</span>.</p>
<p>Finally, accessibility of neural networks is also problematic. Many papers do not release their code and instead just state their networks architecture and available packages. In theory, this is enough to build the models however this does not guarantee the reproducibility that is usually expected. For users with a technical background and good coding ability, many of the models above could be implemented in one of the neural network packages in <span class="math inline">\(\textsf{R}\)</span>, such as <span class="math inline">\(\textbf{nnet}\)</span> <span class="citation" data-cites="pkgnnet">(<a href="P5C26_references.html#ref-pkgnnet" role="doc-biblioref">N. Venables and D. Ripley 2002</a>)</span> and <span class="math inline">\(\textbf{neuralnet}\)</span> <span class="citation" data-cites="pkgneuralnet">(<a href="P5C26_references.html#ref-pkgneuralnet" role="doc-biblioref">Fritsch, Guenther, and N. Wright 2019</a>)</span>; though in practice the only package that does contain these models, <span class="math inline">\(\textbf{survivalmodels}\)</span>, does not directly implement the models in <span class="math inline">\(\textsf{R}\)</span> (which is much slower than Python) but provides a method for interfacing the Python implementations in <span class="math inline">\(\textbf{pycox}\)</span> <span class="citation" data-cites="pkgpycox">(<a href="P5C26_references.html#ref-pkgpycox" role="doc-biblioref">Kvamme 2018</a>)</span>.</p>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Further reading
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><span class="citation" data-cites="Schwarzer2000">Schwarzer, Vach, and Schumacher (<a href="P5C26_references.html#ref-Schwarzer2000" role="doc-biblioref">2000</a>)</span> provided an early survey of neural networks, focusing on ways in which neural networks have been ‘misused’ in the context of survival analysis. Whilst neural networks have moved on substantially since, their early observations remain valid today.</li>
</ul>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Bello2019" class="csl-entry" role="listitem">
Bello, Ghalib A, Timothy J W Dawes, Jinming Duan, Carlo Biffi, Antonio de Marvao, Luke S G E Howard, J Simon R Gibbs, et al. 2019. <span>“<span class="nocase">Deep-learning cardiac motion analysis for human survival prediction</span>.”</span> <em>Nature Machine Intelligence</em> 1 (2): 95–104. <a href="https://doi.org/10.1038/s42256-019-0019-2">https://doi.org/10.1038/s42256-019-0019-2</a>.
</div>
<div id="ref-Biganzoli2009" class="csl-entry" role="listitem">
Biganzoli, E M, F Ambrogi, and P Boracchi. 2009. <span>“<span class="nocase">Partial logistic artificial neural networks (PLANN) for flexible modeling of censored survival data</span>.”</span> In <em>2009 International Joint Conference on Neural Networks</em>, 340–46. <a href="https://doi.org/10.1109/IJCNN.2009.5178824">https://doi.org/10.1109/IJCNN.2009.5178824</a>.
</div>
<div id="ref-Biganzoli1998" class="csl-entry" role="listitem">
Biganzoli, Elia, Patrizia Boracchi, Luigi Mariani, and Ettore Marubini. 1998. <span>“<span class="nocase">Feed forward neural networks for the analysis of censored survival data: a partial logistic regression approach</span>.”</span> <em>Statistics in Medicine</em> 17 (10): 1169–86. <a href="https://doi.org/10.1002/(SICI)1097-0258(19980530)17:10<1169::AID-SIM796>3.0.CO;2-D">https://doi.org/10.1002/(SICI)1097-0258(19980530)17:10&lt;1169::AID-SIM796&gt;3.0.CO;2-D</a>.
</div>
<div id="ref-Bishop2006" class="csl-entry" role="listitem">
Bishop, Christopher M. 2006. <em><span class="nocase">Pattern recognition and machine learning</span></em>. springer.
</div>
<div id="ref-Chen2014" class="csl-entry" role="listitem">
Chen, Yen-Chen, Wan-Chi Ke, and Hung-Wen Chiu. 2014. <span>“<span class="nocase">Risk classification of cancer survival using ANN with gene expression data from multiple laboratories</span>.”</span> <em>Computers in Biology and Medicine</em> 48: 1–7. https://doi.org/<a href="https://doi.org/10.1016/j.compbiomed.2014.02.006">https://doi.org/10.1016/j.compbiomed.2014.02.006</a>.
</div>
<div id="ref-Ching2018a" class="csl-entry" role="listitem">
Ching, Travers, Xun Zhu, and Lana X Garmire. 2018. <span>“<span class="nocase">Cox-nnet: An artificial neural network method for prognosis prediction of high-throughput omics data</span>.”</span> <em>PLOS Computational Biology</em> 14 (4): e1006076. <a href="https://doi.org/10.1371/journal.pcbi.1006076">https://doi.org/10.1371/journal.pcbi.1006076</a>.
</div>
<div id="ref-Clevert2015" class="csl-entry" role="listitem">
Clevert, Djork-Arné, Thomas Unterthiner, and Sepp Hochreiter. 2015. <span>“<span class="nocase">Fast and accurate deep network learning by exponential linear units (elus)</span>.”</span> <em>arXiv Preprint arXiv:1511.07289</em>.
</div>
<div id="ref-Cox1972" class="csl-entry" role="listitem">
Cox, D. R. 1972. <span>“<span class="nocase">Regression Models and Life-Tables</span>.”</span> <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 34 (2): 187–220.
</div>
<div id="ref-Cui2020" class="csl-entry" role="listitem">
Cui, Lei, Hansheng Li, Wenli Hui, Sitong Chen, Lin Yang, Yuxin Kang, Qirong Bo, and Jun Feng. 2020. <span>“<span class="nocase">A deep learning-based framework for lung cancer survival analysis with biomarker interpretation</span>.”</span> <em>BMC Bioinformatics</em> 21 (1): 112. <a href="https://doi.org/10.1186/s12859-020-3431-z">https://doi.org/10.1186/s12859-020-3431-z</a>.
</div>
<div id="ref-Efron1988" class="csl-entry" role="listitem">
Efron, Bradley. 1988. <span>“<span class="nocase">Logistic Regression, Survival Analysis, and the Kaplan-Meier Curve</span>.”</span> <em>Journal of the American Statistical Association</em> 83 (402): 414–25. <a href="https://doi.org/10.2307/2288857">https://doi.org/10.2307/2288857</a>.
</div>
<div id="ref-Faraggi1995" class="csl-entry" role="listitem">
Faraggi, David, and Richard Simon. 1995. <span>“<span class="nocase">A neural network model for survival data</span>.”</span> <em>Statistics in Medicine</em> 14 (1): 73–82. <a href="https://doi.org/10.1002/sim.4780140108">https://doi.org/10.1002/sim.4780140108</a>.
</div>
<div id="ref-Fotso2018" class="csl-entry" role="listitem">
Fotso, Stephane. 2018. <span>“<span class="nocase">Deep Neural Networks for Survival Analysis Based on a Multi-Task Framework</span>.”</span> <em>arXiv Preprint arXiv:1801.05512</em>, January. <a href="http://arxiv.org/abs/1801.05512">http://arxiv.org/abs/1801.05512</a>.
</div>
<div id="ref-pkgneuralnet" class="csl-entry" role="listitem">
Fritsch, Stefan, Frauke Guenther, and Marvin N. Wright. 2019. <span>“<span class="nocase">neuralnet: Training of Neural Networks</span>.”</span> CRAN. <a href="https://cran.r-project.org/package=neuralnet">https://cran.r-project.org/package=neuralnet</a>.
</div>
<div id="ref-Gensheimer2018" class="csl-entry" role="listitem">
Gensheimer, Michael F., and Balasubramanian Narasimhan. 2018. <span>“<span class="nocase">A Simple Discrete-Time Survival Model for Neural Networks</span>,”</span> 1–17. <a href="https://doi.org/arXiv:1805.00917v3">https://doi.org/arXiv:1805.00917v3</a>.
</div>
<div id="ref-Gensheimer2019" class="csl-entry" role="listitem">
Gensheimer, Michael F, and Balasubramanian Narasimhan. 2019. <span>“<span class="nocase">A scalable discrete-time survival model for neural networks</span>.”</span> <em>PeerJ</em> 7: e6257.
</div>
<div id="ref-Giunchiglia2018" class="csl-entry" role="listitem">
Giunchiglia, Eleonora, Anton Nemchenko, and Mihaela van der Schaar. 2018. <span>“<span class="nocase">Rnn-surv: A deep recurrent model for survival analysis</span>.”</span> In <em>International Conference on Artificial Neural Networks</em>, 23–32. Springer.
</div>
<div id="ref-Graf1999" class="csl-entry" role="listitem">
Graf, Erika, Claudia Schmoor, Willi Sauerbrei, and Martin Schumacher. 1999. <span>“<span class="nocase">Assessment and comparison of prognostic classification schemes for survival data</span>.”</span> <em>Statistics in Medicine</em> 18 (17-18): 2529–45. <a href="https://doi.org/10.1002/(SICI)1097-0258(19990915/30)18:17/18<2529::AID-SIM274>3.0.CO;2-5">https://doi.org/10.1002/(SICI)1097-0258(19990915/30)18:17/18&lt;2529::AID-SIM274&gt;3.0.CO;2-5</a>.
</div>
<div id="ref-Han2018" class="csl-entry" role="listitem">
Han, Ilkyu, June Hyuk Kim, Heeseol Park, Han-Soo Kim, and Sung Wook Seo. 2018. <span>“<span class="nocase">Deep learning approach for survival prediction for patients with synovial sarcoma</span>.”</span> <em>Tumor Biology</em> 40 (9): 1010428318799264. <a href="https://doi.org/10.1177/1010428318799264">https://doi.org/10.1177/1010428318799264</a>.
</div>
<div id="ref-Hastie2001" class="csl-entry" role="listitem">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2001. <em><span class="nocase">The Elements of Statistical Learning</span></em>. Springer New York Inc.
</div>
<div id="ref-datamtcars" class="csl-entry" role="listitem">
Henderson, and Velleman. 1981. <span>“<span class="nocase">Building multiple regression models interactively</span>.”</span> <em>Biometrics</em> 37: 391–411.
</div>
<div id="ref-Huang2020" class="csl-entry" role="listitem">
Huang, Shigao, Jie Yang, Simon Fong, and Qi Zhao. 2020b. <span>“<span class="nocase">Artificial intelligence in cancer diagnosis and prognosis: Opportunities and challenges</span>.”</span> <em>Cancer Letters</em> 471: 61–71. https://doi.org/<a href="https://doi.org/10.1016/j.canlet.2019.12.007">https://doi.org/10.1016/j.canlet.2019.12.007</a>.
</div>
<div id="ref-Huang2020a" class="csl-entry" role="listitem">
———. 2020a. <span>“<span class="nocase">Artificial intelligence in cancer diagnosis and prognosis: Opportunities and challenges</span>.”</span> <em>Cancer Letters</em> 471: 61–71. https://doi.org/<a href="https://doi.org/10.1016/j.canlet.2019.12.007">https://doi.org/10.1016/j.canlet.2019.12.007</a>.
</div>
<div id="ref-Jing2019" class="csl-entry" role="listitem">
Jing, Bingzhong, Tao Zhang, Zixian Wang, Ying Jin, Kuiyuan Liu, Wenze Qiu, Liangru Ke, et al. 2019. <span>“<span class="nocase">A deep survival analysis method based on ranking</span>.”</span> <em>Artificial Intelligence in Medicine</em> 98: 1–9. https://doi.org/<a href="https://doi.org/10.1016/j.artmed.2019.06.001">https://doi.org/10.1016/j.artmed.2019.06.001</a>.
</div>
<div id="ref-Katzman2018" class="csl-entry" role="listitem">
Katzman, Jared L, Uri Shaham, Alexander Cloninger, Jonathan Bates, Tingting Jiang, and Yuval Kluger. 2018. <span>“<span class="nocase">DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network</span>.”</span> <em>BMC Medical Research Methodology</em> 18 (1): 24. <a href="https://doi.org/10.1186/s12874-018-0482-1">https://doi.org/10.1186/s12874-018-0482-1</a>.
</div>
<div id="ref-Katzman2016" class="csl-entry" role="listitem">
Katzman, Jared, Uri Shaham, Alexander Cloninger, Jonathan Bates, Tingting Jiang, and Yuval Kluger. 2016. <span>“<span>Deep Survival: A Deep Cox Proportional Hazards Network</span>,”</span> June.
</div>
<div id="ref-Kiraly2018d" class="csl-entry" role="listitem">
Király, Franz J, Bilal Mateen, and Raphael Sonabend. 2018. <span>“<span class="nocase">NIPS - Not Even Wrong? A Systematic Review of Empirically Complete Demonstrations of Algorithmic Effectiveness in the Machine Learning and Artificial Intelligence Literature</span>.”</span> <em>arXiv</em>, December. <a href="http://arxiv.org/abs/1812.07519">http://arxiv.org/abs/1812.07519</a>.
</div>
<div id="ref-pkgpycox" class="csl-entry" role="listitem">
Kvamme, Håvard. 2018. <span>“Pycox.”</span> <a href="https://pypi.org/project/pycox/">https://pypi.org/project/pycox/</a>.
</div>
<div id="ref-Kvamme2019a" class="csl-entry" role="listitem">
Kvamme, Håvard, Ørnulf Borgan, and Ida Scheel. 2019. <span>“<span class="nocase">Time-to-event prediction with neural networks and Cox regression</span>.”</span> <em>Journal of Machine Learning Research</em> 20 (129): 1–30.
</div>
<div id="ref-Lao2017" class="csl-entry" role="listitem">
Lao, Jiangwei, Yinsheng Chen, Zhi-Cheng Li, Qihua Li, Ji Zhang, Jing Liu, and Guangtao Zhai. 2017. <span>“<span class="nocase">A Deep Learning-Based Radiomics Model for Prediction of Survival in Glioblastoma Multiforme</span>.”</span> <em>Scientific Reports</em> 7 (1): 10353. <a href="https://doi.org/10.1038/s41598-017-10649-8">https://doi.org/10.1038/s41598-017-10649-8</a>.
</div>
<div id="ref-Lee2018a" class="csl-entry" role="listitem">
Lee, Changhee, William Zame, Jinsung Yoon, and Mihaela Van der Schaar. 2018. <span>“<span class="nocase">DeepHit: A Deep Learning Approach to Survival Analysis With Competing Risks</span>.”</span> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> 32 (1). <a href="https://doi.org/10.1609/aaai.v32i1.11842">https://doi.org/10.1609/aaai.v32i1.11842</a>.
</div>
<div id="ref-Liestol1994" class="csl-entry" role="listitem">
Liestol, Knut, Per Kragh Andersen, and Ulrich Andersen. 1994. <span>“<span class="nocase">Survival analysis and neural nets</span>.”</span> <em>Statistics in Medicine</em> 13 (12): 1189–1200. <a href="https://doi.org/10.1002/sim.4780131202">https://doi.org/10.1002/sim.4780131202</a>.
</div>
<div id="ref-Lundin1999" class="csl-entry" role="listitem">
Lundin, M, J Lundin, H B Burke, S Toikkanen, L Pylkkänen, and H Joensuu. 1999. <span>“<span class="nocase">Artificial Neural Networks Applied to Survival Prediction in Breast Cancer</span>.”</span> <em>Oncology</em> 57 (4): 281–86. <a href="https://doi.org/10.1159/000012061">https://doi.org/10.1159/000012061</a>.
</div>
<div id="ref-Luxhoj1997" class="csl-entry" role="listitem">
Luxhoj, James T., and Huan Jyh Shyur. 1997. <span>“<span class="nocase">Comparison of proportional hazards models and neural networks for reliability estimation</span>.”</span> <em>Journal of Intelligent Manufacturing</em> 8 (3): 227–34. <a href="https://doi.org/10.1023/A:1018525308809">https://doi.org/10.1023/A:1018525308809</a>.
</div>
<div id="ref-Mani1999" class="csl-entry" role="listitem">
Mani, D R, James Drew, Andrew Betz, and Piew Datta. 1999. <span>“<span class="nocase">Statistics and data mining techniques for lifetime value modeling</span>.”</span> In <em>Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>, 94–103.
</div>
<div id="ref-Mariani1997" class="csl-entry" role="listitem">
Mariani, L, D Coradini, E Biganzoli, P Boracchi, E Marubini, S Pilotti, B Salvadori, et al. 1997. <span>“<span class="nocase">Prognostic factors for metachronous contralateral breast cancer: A comparison of the linear Cox regression model and its artificial neural network extension</span>.”</span> <em>Breast Cancer Research and Treatment</em> 44 (2): 167–78. <a href="https://doi.org/10.1023/A:1005765403093">https://doi.org/10.1023/A:1005765403093</a>.
</div>
<div id="ref-McKinney2020" class="csl-entry" role="listitem">
McKinney, Scott Mayer, Marcin Sieniek, Varun Godbole, Jonathan Godwin, Natasha Antropova, Hutan Ashrafian, Trevor Back, et al. 2020. <span>“<span class="nocase">International evaluation of an AI system for breast cancer screening</span>.”</span> <em>Nature</em> 577 (7788): 89–94. <a href="https://doi.org/10.1038/s41586-019-1799-6">https://doi.org/10.1038/s41586-019-1799-6</a>.
</div>
<div id="ref-pkgnnet" class="csl-entry" role="listitem">
N. Venables, W, and B D. Ripley. 2002. <em><span class="nocase">Modern Applied Statistics with S</span></em>. Springer. <a href="http://www.stats.ox.ac.uk/pub/MASS4">http://www.stats.ox.ac.uk/pub/MASS4</a>.
</div>
<div id="ref-Nair2010" class="csl-entry" role="listitem">
Nair, Vinod, and Geoffrey E Hinton. 2010. <span>“<span class="nocase">Rectified linear units improve restricted boltzmann machines</span>.”</span> In <em>Proceedings of the 27th International Conference on Machine Learning (ICML-10)</em>, 807–14.
</div>
<div id="ref-Oh2018" class="csl-entry" role="listitem">
Oh, Sung Eun, Sung Wook Seo, Min-Gew Choi, Tae Sung Sohn, Jae Moon Bae, and Sung Kim. 2018. <span>“<span class="nocase">Prediction of Overall Survival and Novel Classification of Patients with Gastric Cancer Using the Survival Recurrent Network</span>.”</span> <em>Annals of Surgical Oncology</em> 25 (5): 1153–59. <a href="https://doi.org/10.1245/s10434-018-6343-7">https://doi.org/10.1245/s10434-018-6343-7</a>.
</div>
<div id="ref-Ohno-Machado1996" class="csl-entry" role="listitem">
Ohno-Machado, Lucila. 1996. <span>“<span class="nocase">Medical applications of artificial neural networks: connectionist models of survival</span>.”</span> Stanford University Stanford, Calif.
</div>
<div id="ref-Ohno-Machado1997" class="csl-entry" role="listitem">
———. 1997. <span>“<span class="nocase">A COMPARISON OF COX PROPORTIONAL HAZARDS AND ARTIFICIAL NEURAL NETWORK MODELS FOR MEDICAL PROGNOSIS The theoretical advantages and disadvantages of using different methods for predicting survival have seldom been tested in real data sets [ 1 , 2 ]. Althou</span>.”</span> <em>Comput. Biol. Med</em> 27 (1): 55–65.
</div>
<div id="ref-Puddu2012" class="csl-entry" role="listitem">
Puddu, Paolo Emilio, and Alessandro Menotti. 2012. <span>“<span class="nocase">Artificial neural networks versus proportional hazards Cox models to predict 45-year all-cause mortality in the Italian Rural Areas of the Seven Countries Study</span>.”</span> <em>BMC Medical Research Methodology</em> 12 (1): 100. <a href="https://doi.org/10.1186/1471-2288-12-100">https://doi.org/10.1186/1471-2288-12-100</a>.
</div>
<div id="ref-Rietschel2018" class="csl-entry" role="listitem">
Rietschel, Carl, Jinsung Yoon, and Mihaela van der Schaar. 2018. <span>“<span class="nocase">Feature Selection for Survival Analysis with Competing Risks using Deep Learning</span>.”</span> <em>arXiv Preprint arXiv:1811.09317</em>.
</div>
<div id="ref-Ripley2001" class="csl-entry" role="listitem">
Ripley, Brian D, and Ruth M Ripley. 2001. <span>“<span class="nocase">Neural networks as statistical methods in survival analysis</span>.”</span> In <em>Clinical Applications of Artificial Neural Networks</em>, edited by Richard Dybowski and Vanya Gant, 237–55. Cambridge: Cambridge University Press. <a href="https://doi.org/DOI: 10.1017/CBO9780511543494.011">https://doi.org/DOI: 10.1017/CBO9780511543494.011</a>.
</div>
<div id="ref-Ripley1998" class="csl-entry" role="listitem">
Ripley, R M, A L Harris, and L Tarassenko. 1998. <span>“<span class="nocase">Neural network models for breast cancer prognosis</span>.”</span> <em>Neural Computing &amp; Applications</em> 7 (4): 367–75. <a href="https://doi.org/10.1007/BF01428127">https://doi.org/10.1007/BF01428127</a>.
</div>
<div id="ref-Schwarzer2000" class="csl-entry" role="listitem">
Schwarzer, Guido, Werner Vach, and Martin Schumacher. 2000. <span>“<span class="nocase">On the misuses of artificial neural networks for prognostic and diagnostic classification in oncology</span>.”</span> <em>Statistics in Medicine</em> 19 (4): 541–61. <a href="https://doi.org/10.1002/(SICI)1097-0258(20000229)19:4<541::AID-SIM355>3.0.CO;2-V">https://doi.org/10.1002/(SICI)1097-0258(20000229)19:4&lt;541::AID-SIM355&gt;3.0.CO;2-V</a>.
</div>
<div id="ref-Seker2002a" class="csl-entry" role="listitem">
Seker, H., M. O. Odetayo, D. Petrovic, R. N. G. Naguib, C. Bartoli, L. Alasio, M. S. Lakshmi, G. V. Sherbet, and O. R. Hinton. 2002. <span>“An Artificial Neural Network Based Feature Evaluation Index for the Assessment of Clinical Factors in Breast Cancer Survival Analysis.”</span> In <em>IEEE CCECE2002. Canadian Conference on Electrical and Computer Engineering. Conference Proceedings (Cat. No.02CH37373)</em>, 2:1211–1215 vol.2. <a href="https://doi.org/10.1109/CCECE.2002.1013121">https://doi.org/10.1109/CCECE.2002.1013121</a>.
</div>
<div id="ref-Seker2002" class="csl-entry" role="listitem">
Seker, Huseyin, Michael O Odetayo, Dobrila Petrovic, Raouf N G Naguib, C Bartoli, L Alasio, M S Lakshmi, and G V Sherbet. 2002. <span>“<span class="nocase">Assessment of nodal involvement and survival analysis in breast cancer patients using image cytometric data: statistical, neural network and fuzzy approaches</span>.”</span> <em>Anticancer Research</em> 22 (1A): 433–38. <a href="http://europepmc.org/abstract/MED/12017328">http://europepmc.org/abstract/MED/12017328</a>.
</div>
<div id="ref-pkgsurvivalmodels" class="csl-entry" role="listitem">
Sonabend, Raphael. 2020. <span>“<span class="nocase">survivalmodels: Models for Survival Analysis</span>.”</span> CRAN. <a href="https://raphaels1.r-universe.dev/ui#package:survivalmodels">https://raphaels1.r-universe.dev/ui#package:survivalmodels</a>.
</div>
<div id="ref-Sonabend2021b" class="csl-entry" role="listitem">
Sonabend, Raphael Edward Benjamin. 2021. <span>“<span class="nocase">A Theoretical and Methodological Framework for Machine Learning in Survival Analysis: Enabling Transparent and Accessible Predictive Modelling on Right-Censored Time-to-Event Data</span>.”</span> PhD, University College London (UCL). <a href="https://discovery.ucl.ac.uk/id/eprint/10129352/">https://discovery.ucl.ac.uk/id/eprint/10129352/</a>.
</div>
<div id="ref-Srivastava2014" class="csl-entry" role="listitem">
Srivastava, Nitish, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. <span>“<span class="nocase">Dropout: a simple way to prevent neural networks from overfitting</span>.”</span> <em>The Journal of Machine Learning Research</em> 15 (1): 1929–58.
</div>
<div id="ref-pkggamlssadd" class="csl-entry" role="listitem">
Stasinopoulos, Mikis, Bob Rigby, Vlasios Voudouris, and Daniil Kiose. 2020. <span>“<span class="nocase">gamlss.add: Extra Additive Terms for Generalized Additive Models for Location Scale and Shape</span>.”</span> CRAN. <a href="https://cran.r-project.org/package=gamlss.add">https://cran.r-project.org/package=gamlss.add</a>.
</div>
<div id="ref-Street1998" class="csl-entry" role="listitem">
Street, W Nick. 1998. <span>“<span class="nocase">A Neural Network Model for Prognostic Prediction.</span>”</span> In <em>Proceedings of the Fifteenth International Conference on Machine Learning</em>. San Francisco.
</div>
<div id="ref-Tutz2016" class="csl-entry" role="listitem">
Tutz, Gerhard, and Matthias Schmid. 2016. <em><span class="nocase">Modeling Discrete Time-to-Event Data</span></em>. Springer Series in Statistics. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-319-28158-2">https://doi.org/10.1007/978-3-319-28158-2</a>.
</div>
<div id="ref-pkgreticulate" class="csl-entry" role="listitem">
Ushey, Kevin, J J Allaire, and Yuan Tang. 2020. <span>“<span class="nocase">reticulate: Interface to ’Python’</span>.”</span> CRAN. <a href="https://cran.r-project.org/package=reticulate">https://cran.r-project.org/package=reticulate</a>.
</div>
<div id="ref-VanBelle2011b" class="csl-entry" role="listitem">
Van Belle, Vanya, Kristiaan Pelckmans, Sabine Van Huffel, and Johan A. K. Suykens. 2011. <span>“<span class="nocase">Support vector methods for survival analysis: A comparison between ranking and regression approaches</span>.”</span> <em>Artificial Intelligence in Medicine</em> 53 (2): 107–18. <a href="https://doi.org/10.1016/j.artmed.2011.06.006">https://doi.org/10.1016/j.artmed.2011.06.006</a>.
</div>
<div id="ref-Xiang2000" class="csl-entry" role="listitem">
Xiang, Anny, Pablo Lapuerta, Alex Ryutov, Jonathan Buckley, and Stanley Azen. 2000. <span>“<span class="nocase">Comparison of the performance of neural network methods and Cox regression for censored survival data</span>.”</span> <em>Computational Statistics &amp; Data Analysis</em> 34 (2): 243–57. https://doi.org/<a href="https://doi.org/10.1016/S0167-9473(99)00098-5">https://doi.org/10.1016/S0167-9473(99)00098-5</a>.
</div>
<div id="ref-Yang2010" class="csl-entry" role="listitem">
Yang, Yanying. 2010. <span>“<span>Neural Network Survival Analysis</span>.”</span> PhD thesis, Universiteit Gent.
</div>
<div id="ref-Yasodhara2018" class="csl-entry" role="listitem">
Yasodhara, Angeline, Mamatha Bhat, and Anna Goldenberg. 2018. <em><span class="nocase">Prediction of New Onset Diabetes after Liver Transplant</span></em>.
</div>
<div id="ref-Zhang2020" class="csl-entry" role="listitem">
Zhang, Yucheng, Edrise M Lobo-Mueller, Paul Karanicolas, Steven Gallinger, Masoom A Haider, and Farzad Khalvati. 2020. <span>“<span class="nocase">CNN-based survival model for pancreatic ductal adenocarcinoma in medical imaging</span>.”</span> <em>BMC Medical Imaging</em> 20 (1): 11. <a href="https://doi.org/10.1186/s12880-020-0418-1">https://doi.org/10.1186/s12880-020-0418-1</a>.
</div>
<div id="ref-Zhao2020" class="csl-entry" role="listitem">
Zhao, Lili, and Dai Feng. 2020. <span>“<span class="nocase">Deep Neural Networks for Survival Analysis Using Pseudo Values</span>.”</span> <em>IEEE Journal of Biomedical and Health Informatics</em> 24 (11): 3308–14. <a href="https://doi.org/10.1109/JBHI.2020.2980204">https://doi.org/10.1109/JBHI.2020.2980204</a>.
</div>
<div id="ref-Zhu2020" class="csl-entry" role="listitem">
Zhu, Wan, Longxiang Xie, Jianye Han, and Xiangqian Guo. 2020. <span>“<span class="nocase">The Application of Deep Learning in Cancer Prognosis Prediction</span>.”</span> <em>Cancers</em> 12 (3): 603. <a href="https://doi.org/10.3390/cancers12030603">https://doi.org/10.3390/cancers12030603</a>.
</div>
<div id="ref-Zhu2016" class="csl-entry" role="listitem">
Zhu, X, J Yao, and J Huang. 2016. <span>“<span class="nocase">Deep convolutional neural network for survival analysis with pathological images</span>.”</span> In <em>2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</em>, 544–47. <a href="https://doi.org/10.1109/BIBM.2016.7822579">https://doi.org/10.1109/BIBM.2016.7822579</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./P3C16_boosting.html" class="pagination-link" aria-label="Boosting Methods">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Boosting Methods</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./P4C19_reductions.html" class="pagination-link" aria-label="Reductions">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Reductions</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>All content licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> <br> © Raphael Sonabend, Andreas Bender.</p>
</div>   
    <div class="nav-footer-center">
<p><a href="https://www.mlsabook.com">Website</a> | <a href="https://github.com/mlsa-book/MLSA">GitHub</a></p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mlsa-book/MLSA/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/mlsa-book/MLSA/edit/main/book/P3C17_neural.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/mlsa-book/MLSA/blob/main/book/P3C17_neural.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>