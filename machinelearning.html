<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Raphael Sonabend and Andreas Bender">
<title>3&nbsp; Statistical Learning – Machine Learning in Survival Analysis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./survival.html" rel="next">
<link href="./preview.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="styles.css">
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./preview.html">Survival Analysis and Machine Learning</a></li><li class="breadcrumb-item"><a href="./machinelearning.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Statistical Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning in Survival Analysis</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/mlsa-book/MLSA/tree/main/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Machine-Learning-in-Survival-Analysis.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Symbols and Notation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Survival Analysis and Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">MLSA From Start to Finish</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./machinelearning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Statistical Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./survival.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Survival Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eha.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Event-history Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./survtsk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Survival Task</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Evaluation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_what.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">What are Survival Measures?</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_rank.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Discrimination Measures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_calib.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Calibration Measures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_rules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Evaluating Distributions by Scoring Rules</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_time.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Evaluating Survival Time</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_choosing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Choosing Measures</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Models</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Classical Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mlmodels.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Machine Learning Survival Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./forests.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Tree-Based Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./svm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Boosting Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./neuralnetworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./alternatives.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Alternative Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./models_choosing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Choosing Models</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Reduction Techniques</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reductions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Reductions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./competing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Competing Risks Pipelines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./discretetime.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Discrete Time Survival Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./poisson.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Connections to Poisson Regression and Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pseudo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Connections to Regression and Imputation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./advanced.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Advanced Methods</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Extensions and Outlook</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./common.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Common problems in survival analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./software.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Survival Software</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./next.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">What’s next for MLSA?</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#sec-surv-setml" id="toc-sec-surv-setml" class="nav-link active" data-scroll-target="#sec-surv-setml"><span class="header-section-number">3.1</span> Machine Learning</a>
  <ul class="collapse">
<li><a href="#sec-surv-setml-meth" id="toc-sec-surv-setml-meth" class="nav-link" data-scroll-target="#sec-surv-setml-meth"><span class="header-section-number">3.1.1</span> Terminology and Methods</a></li>
  <li><a href="#sec-surv-ml-car" id="toc-sec-surv-ml-car" class="nav-link" data-scroll-target="#sec-surv-ml-car"><span class="header-section-number">3.1.2</span> Machine Learning in Classification and Regression</a></li>
  </ul>
</li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/mlsa-book/MLSA/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/mlsa-book/MLSA/edit/main/book/machinelearning.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/mlsa-book/MLSA/blob/main/book/machinelearning.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./preview.html">Survival Analysis and Machine Learning</a></li><li class="breadcrumb-item"><a href="./machinelearning.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Statistical Learning</span></a></li></ol></nav><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-ml" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Statistical Learning</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    TODO (150-200 WORDS)
  </div>
</div>


</header><div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Major changes expected!
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>This page is a work in progress and major changes will be made over time.</strong></p>
</div>
</div>
<hr>
<p>TODO</p>
<ul>
<li>
<span class="math inline">\(\mathcal{D}\)</span> - labeled data set with <span class="math inline">\(n\)</span> observation</li>
<li>Observations: <span class="math inline">\((\mathbf{x}^{(i)}, y^{(i)})\)</span> where <span class="math inline">\(\mathbf{x}^{(i)} \in \mathcal{X}\)</span> is a <span class="math inline">\(p\)</span>-dimensional feature vector and <span class="math inline">\(y^{(i)} \in \mathcal{Y}\)</span> is a label</li>
<li>Data set: <span class="math inline">\(\mathcal{D}= ((\mathbf{x}^{(1)}, y^{(1)}) , . . . , (\mathbf{x}^{(n)}, y^{(n)}))\)</span>
</li>
<li>Assume <span class="math inline">\(\mathcal{D}\stackrel{i.i.d.}\sim(\mathbb{P}_{xy})^n\)</span> (unknown distribution)</li>
<li>ML model: <span class="math inline">\(f : \mathcal{X}\rightarrow \mathbb{R}^g\)</span> assigning a prediction in <span class="math inline">\(\mathbb{R}^g\)</span>
</li>
<li>ML learners, <span class="math inline">\(\mathcal{I}\)</span>, configured by hyperparameters <span class="math inline">\(\lambda \in \Lambda\)</span>, are <span class="math inline">\(\mathcal{I}: \mathbb{D} \times \Lambda \rightarrow \mathcal{H}, (\mathcal{D}, \lambda) \mapsto \hat{f}\)</span> where <span class="math inline">\(\mathcal{H}\)</span> is the function space to which a model belongs and <span class="math inline">\(\hat{f}\)</span> is a trained model with learned hyperparameters <span class="math inline">\(\hat{\theta} \in \mathcal{H}\)</span>.</li>
<li>Models are evaluated with loss functions which measure the discrepancy between predictions and true values <span class="math inline">\(L: \mathcal{Y}\times \mathbb{R}^g \rightarrow \mathbb{R}, (\hat{f}(\mathbf{x}), \mathbf{y}) \mapsto l(\hat{f}(\mathbf{x}), \mathbf{y})\)</span>
</li>
<li>To prevent overfitting, models are evaluated on unseen test data to ensure unbiased performance estimation and the generalization error <span class="math inline">\(GE(\mathcal{I}, \lambda, n_{train}, l) := \lim_{n_{test} \rightarrow \infty} \mathbb{E}_{\mathcal{D}_{train},\mathcal{D}_{test}\sim\mathbb{P}_{xy}}[l(\mathbf{y}_{test}, \mathbf{F}_{\mathcal{D}_{test},\mathcal{I}(\mathcal{D}_{train},\lambda)})]\)</span> where <span class="math inline">\(\mathbf{F}_{\mathcal{D}_{test},\mathcal{I}(\mathcal{D}_{train},\lambda)}\)</span> is the matrix of predictions from a model trained on <span class="math inline">\(\mathcal{D}_{train}\)</span> and making predictions on <span class="math inline">\(\mathcal{D}_{test}\)</span>.</li>
</ul>
<hr>
<section id="sec-surv-setml" class="level2" data-number="3.1"><h2 data-number="3.1" class="anchored" data-anchor-id="sec-surv-setml">
<span class="header-section-number">3.1</span> Machine Learning</h2>
<p>This section begins with a very brief introduction to machine learning and a focus on regression and classification; the survival machine learning task is then introduced (<a href="survtsk.html#sec-surv-setmltask" class="quarto-xref"><span>Section 6.2</span></a>). Of the many fields within machine learning (ML), the scope of this book is narrowed to supervised learning. Supervised learning is the sub-field of ML in which predictions are made for outcomes based on data with observed dependent and independent variables. For example predicting someone’s height is a supervised learning problem as data can be collected for features (independent variables) such as age and sex, and outcome (dependent variable), which is height. Predictive survival analysis problems fall naturally in the supervised learning framework as there are identifiable features and (multiple types of) outcomes.</p>
<section id="sec-surv-setml-meth" class="level3" data-number="3.1.1"><h3 data-number="3.1.1" class="anchored" data-anchor-id="sec-surv-setml-meth">
<span class="header-section-number">3.1.1</span> Terminology and Methods</h3>
<p>Common supervised learning methods are discussed in a simplified setting with features <span class="math inline">\(X \ t.v.i. \ \mathcal{X}\)</span> and outcomes <span class="math inline">\(Y \ t.v.i. \ \mathcal{Y}\)</span>; usually outcomes are referred to as ‘targets’ (a ‘target for prediction’). Let <span class="math inline">\(\mathcal{D}_0 = \{(X_1,Y_1),...,(X_n,Y_n)\}\)</span> be a (training) dataset where <span class="math inline">\((X_i, Y_i) \stackrel{i.i.d.}\sim(X, Y)\)</span>. The methods below extend naturally to the survival setting.</p>
<section id="strategies-and-models" class="level4 unnumbered unlisted"><h4 class="unnumbered unlisted anchored" data-anchor-id="strategies-and-models">Strategies and Models</h4>
<p>In order to clearly separate between similar objects, several terms for machine learning are now introduced and clearly distinguished.</p>
<p>Let <span class="math inline">\(g: \mathcal{X}\rightarrow \mathcal{Y}\)</span> be the true (but unknown) mapping from the features to outcomes, referred to as the <em>true prediction functional</em>. Let <span class="math inline">\(\mathcal{G}\)</span> be the set of <em>prediction functionals</em> such that <span class="math inline">\(\forall \Upsilon \in \mathcal{G}, \Upsilon: \mathcal{X}\rightarrow \mathcal{Y}\)</span>. A <em>learning</em> or <em>fitting algorithm</em> is defined to be any function of the form <span class="math inline">\(\mathcal{A}: \mathcal{X}^n \times \mathcal{Y}^n \rightarrow \mathcal{G}\)</span>. The goal of supervised learning is to <em>learn</em> <span class="math inline">\(g\)</span> with a learning algorithm <em>fit</em> on (i.e.&nbsp;the input to the algorithm is) training data, <span class="math inline">\(\hat{g}:= \mathcal{A}(\mathcal{D}_{train}) \in \mathcal{G}\)</span>. Note that <span class="math inline">\(\hat{g}\)</span> may take hyper-parameters that can be set or tuned (see below). The learning algorithm is ‘good’ if <span class="math inline">\(\hat{g}(X) \approx g(X)\)</span> (see ‘Evaluation’ below).</p>
<p>The learning algorithm is determined by the chosen <em>learning strategy</em> and <em>model</em>, where a model is a complete specification of a learning strategy including hyper-parameters. These terms are more clearly illustrated by example:</p>
<ol type="i">
<li>Learning strategy – simple linear regression</li>
<li>Model – <span class="math inline">\(y = \beta_0 + \beta_1 x\)</span> where <span class="math inline">\(x \in \mathbb{R}\)</span> is a single covariate, <span class="math inline">\(y \in \mathbb{R}\)</span> is the target, and <span class="math inline">\(\beta_0,\beta_1 \in \mathbb{R}\)</span> are model coefficients.</li>
<li>Learning algorithm (model fitting) – Minimise the residual sum of squares: <span class="math inline">\((\hat{\beta_0}, \hat{\beta_1}) := \operatorname{argmin}_{\beta_0,\beta_1} \{\sum^n_{i=1} (y_i - \beta_0 - \beta_1 x_i)^2\}\)</span> for <span class="math inline">\((x_i,y_i) \in \mathcal{D}_{train}, i = 1,...,n\)</span>.</li>
<li>Prediction functional – <span class="math inline">\(\hat{g}(x) = \hat{\beta_0} + \hat{\beta_1}x\)</span>
</li>
</ol>
<p>To further illustrate the difference between learning strategy and model, note that the same learning strategy ‘simple linear regression’ could either utilise the model above or instead a model without intercept, <span class="math inline">\(y = \beta x\)</span>, in which case the learning algorithm and prediction functional would also be modified.</p>
<p>The model in (ii) is called <em>unfitted</em> as the model coefficients are unknown and the model cannot be used for prediction. After step (iii) the model is said to be fit to the training data and therefore the model is <em>fitted</em>. It is common to refer to the learning algorithm (and associated hyper-parameters) as the unfitted model and to refer to the prediction functional (and associated hyper-parameters) as the fitted model.</p>
</section><section id="evaluation" class="level4 unnumbered unlisted"><h4 class="unnumbered unlisted anchored" data-anchor-id="evaluation">Evaluation</h4>
<p>Models are <em>evaluated</em> by evaluation measures called <em>losses</em> or <em>scores</em>, <span class="math inline">\(L: \mathcal{Y}\times \mathcal{Y}\rightarrow \bar{\mathbb{R}}\)</span>. Let <span class="math inline">\((X^*, Y^*) \sim (X,Y)\)</span> be test data (i.e.&nbsp;independent of <span class="math inline">\(\mathcal{D}_{train}\)</span>) and let <span class="math inline">\(\hat{g}: \mathcal{X}\rightarrow \mathcal{Y}\)</span> be a prediction functional fit on <span class="math inline">\(\mathcal{D}_{train}\)</span>, then these evaluation measures determine how closely predictions, <span class="math inline">\(\hat{g}(X^*)\)</span>, relate to the truth, <span class="math inline">\(Y^*\)</span>, thereby providing a method for determining if a model is ‘good’.</p>
</section><section id="task" class="level4 unnumbered unlisted"><h4 class="unnumbered unlisted anchored" data-anchor-id="task">Task</h4>
<p>A machine learning <em>task</em> is a simple mechanism to outline the problem of interest by providing: i) the data specification; ii) the definition of learning; iii) the definition of success (when is a prediction ‘good’?) <span class="citation" data-cites="Kiraly2021">(<a href="references.html#ref-Kiraly2021" role="doc-biblioref">Király et al. 2021</a>)</span>. All tasks in this paper have the same definitions of learning and success. For (ii), the aim is to learn the true prediction functional, <span class="math inline">\(g\)</span>, by fitting the learning algorithm on training data, <span class="math inline">\(\hat{g}:= \mathcal{A}(\mathcal{D}_0)\)</span>. For (iii), a predicted functional is considered ‘good’ if the <em>expected generalization error</em>, <span class="math inline">\(\mathbb{E}[L(Y^*, \hat{g}(X^*))]\)</span>, is low, where <span class="math inline">\((X^*, Y^*) \sim (X,Y)\)</span> is independent of the training data <span class="math inline">\(\mathcal{D}_0\)</span>, and <span class="math inline">\(L\)</span> is some loss that is chosen according to the domain of interest (regression, classification, survival).</p>
</section><section id="resampling" class="level4 unnumbered unlisted"><h4 class="unnumbered unlisted anchored" data-anchor-id="resampling">Resampling</h4>
<p>Models are <em>tested</em> on their ability to make predictions. In order to avoid ‘optimism of training error’ <span class="citation" data-cites="Hastie2013">(<a href="references.html#ref-Hastie2013" role="doc-biblioref">James et al. 2013</a>)</span> – overconfidence caused by testing the model on training data – models are tested on previously unseen or ‘held-out’ data. <em>Resampling</em> is the procedure of splitting one dataset into two or more for separated training and testing. In this paper only two resampling methods are utilised: <em>holdout</em> and <em>cross-validation</em>. Holdout is the process of splitting a primary dataset into training data for model fitting and testing data for model predicting. This is an efficient method but may not accurately estimate the expected generalisation error for future model performance, instead this is well-estimated by <span class="math inline">\(K\)</span>-fold cross-validation (KCV) <span class="citation" data-cites="Hastie2001">(<a href="references.html#ref-Hastie2001" role="doc-biblioref">Hastie, Tibshirani, and Friedman 2001</a>)</span>. In KCV, data is split into <span class="math inline">\(K \in \mathbb{N}_{&gt; 0}\)</span> ‘folds’ such that <span class="math inline">\(K-1\)</span> of the folds are used for model training and the final <span class="math inline">\(K\)</span>th fold for testing. The testing fold is iterated over all <span class="math inline">\(K\)</span> folds, so that each at some point is used for testing and then training (though never at the same time). In each iteration the model is fit on the training folds, and predictions are made and evaluated on the testing fold, giving a loss <span class="math inline">\(L_k := L(\hat{g}(X^k), Y^k)\)</span>, where <span class="math inline">\((X^k, Y^k)\)</span> are data from the <span class="math inline">\(k\)</span>th fold. A final loss is defined by, <span class="math inline">\(L^* := \frac{1}{K} \sum^K_{k = 1} L_k\)</span>. Commonly <span class="math inline">\(K = 5\)</span> or <span class="math inline">\(K = 10\)</span> <span class="citation" data-cites="Breiman1992 Kohavi1995">(<a href="references.html#ref-Breiman1992" role="doc-biblioref">Breiman and Spector 1992</a>; <a href="references.html#ref-Kohavi1995" role="doc-biblioref">Kohavi 1995</a>)</span>.</p>
</section><section id="model-performance-benchmarking" class="level4 unnumbered unlisted"><h4 class="unnumbered unlisted anchored" data-anchor-id="model-performance-benchmarking">Model Performance Benchmarking</h4>
<p>Whilst <em>benchmarking</em> often refers to speed tests, i.e.&nbsp;the time taken to complete an operation, it can also refer to any experiment in which objects (mathematical or computational) are compared. In this report, a benchmark experiment will either refer to the comparison of multiple models’ predictive abilities, or comparison of computational speeds and object sizes for model fitting; which of these will be clear from context.</p>
</section><section id="model-comparison" class="level4 unnumbered unlisted"><h4 class="unnumbered unlisted anchored" data-anchor-id="model-comparison">Model Comparison</h4>
<p>Models can be analytically compared on how well they make predictions for new data. Model comparison is a complex topic with many open questions <span class="citation" data-cites="Demsar2006 Dietterich1998 Nadeau2003">(<a href="references.html#ref-Demsar2006" role="doc-biblioref">Demšar 2006</a>; <a href="references.html#ref-Dietterich1998" role="doc-biblioref">Dietterich 1998</a>; <a href="references.html#ref-Nadeau2003" role="doc-biblioref">Nadeau and Bengio 2003</a>)</span> and as such discussion is limited here. When models are compared on multiple datasets, there is more of a consensus in how to evaluate models <span class="citation" data-cites="Demsar2006">(<a href="references.html#ref-Demsar2006" role="doc-biblioref">Demšar 2006</a>)</span> and this is expanded on further in <span class="citation" data-cites="Sonabend2021b">(<a href="references.html#ref-Sonabend2021b" role="doc-biblioref">Sonabend 2021</a>)</span>. Throughout this book there are small simulation experiments for model comparison on single datasets however as these are primarily intended to aid exposition and not to generalise results, it suffices to compare models with the conservative method of constructing confidence intervals around the sample mean and standard error of the loss when available <span class="citation" data-cites="Nadeau2003">(<a href="references.html#ref-Nadeau2003" role="doc-biblioref">Nadeau and Bengio 2003</a>)</span>.</p>
</section><section id="hyper-parameters-and-tuning" class="level4 unnumbered unlisted"><h4 class="unnumbered unlisted anchored" data-anchor-id="hyper-parameters-and-tuning">Hyper-Parameters and Tuning</h4>
<p>A <em>hyper-parameter</em> is a model parameter that can be set by the user, as opposed to coefficients that are estimated as part of model fitting. A hyper-parameter can be set before training, or it can be tuned. <em>Tuning</em> is the process of choosing the optimal hyper-parameter value via automation. In the simplest setting, tuning is performed by selecting a range of values for the hyper-parameter(s) and treating each choice (combination) as a different model. For example if tuning the number of trees in a random forest (<a href="forests.html#sec-surv-ml-models-ranfor" class="quarto-xref"><span>Section 15.1</span></a>), <span class="math inline">\(m_r\)</span>, then a range of values, say <span class="math inline">\(100, 200, 500\)</span> are chosen, and three models <span class="math inline">\(m_{r100}, m_{r200}, m_{r500}\)</span> are benchmarked. The optimal hyper-parameter is given by whichever model is the best performing. <em>Nested resampling</em> is a common method to prevent overfitting that could occur from using overlapping data for tuning, training, or testing. Nested resampling is the process of resampling the training set again for tuning.</p>
<div style="page-break-after: always;"></div>
</section></section><section id="sec-surv-ml-car" class="level3" data-number="3.1.2"><h3 data-number="3.1.2" class="anchored" data-anchor-id="sec-surv-ml-car">
<span class="header-section-number">3.1.2</span> Machine Learning in Classification and Regression</h3>
<p>Before introducing machine learning for survival analysis, which is considered ‘non-classical’, the more standard classification and regression set-ups are provided; these are referenced throughout this book.</p>
<section id="sec-surv-ml-car-class" class="level4" data-number="3.1.2.1"><h4 data-number="3.1.2.1" class="anchored" data-anchor-id="sec-surv-ml-car-class">
<span class="header-section-number">3.1.2.1</span> Classification</h4>
<p>Classification problems make predictions about categorical (or discrete) events, these may be <em>deterministic</em> or <em>probabilistic</em>. Deterministic classification predicts which category an observation falls into, whereas probabilistic classification predicts the probability of an observation falling into each category. In this brief introduction only binary single-label classification is discussed, though the multi-label case is considered in <span class="math inline">\(\ref{sec:car_reduxes_r7_mlc}\)</span>. In binary classification, there are two possible categories an observation can fall into, usually referred to as the ‘positive’ and ‘negative’ class. For example predicting the probability of death due to a virus is a probabilistic classification task where the ‘positive’ event is death.</p>
<p>A probabilistic prediction is more informative than a deterministic one as it encodes uncertainty about the prediction. For example it is clearly more informative to predict a <span class="math inline">\(70%\)</span> chance of rain tomorrow instead of simply ‘rain’. Moreover the latter prediction implicitly contains an erroneous assumption of certainty, e.g.&nbsp;‘it will rain tomorrow’.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Classification Task
</div>
</div>
<div class="callout-body-container callout-body">
<div id="cnj-task-classif" class="theorem conjecture">
<p><span class="theorem-title"><strong>Box 3.1</strong></span> Let <span class="math inline">\((X,Y)\)</span> be random variables t.v.i. <span class="math inline">\(\mathcal{X}\times \mathcal{Y}\)</span> where <span class="math inline">\(\mathcal{X}\subseteq \mathbb{R}^p\)</span> and <span class="math inline">\(\mathcal{Y}= \{0, 1\}\)</span>. Then,</p>
<ul>
<li>The <em>probabilistic classification task</em> is the problem of predicting the probability of a single event taking place and is specified by <span class="math inline">\(g: \mathcal{X}\rightarrow [0, 1]\)</span>.</li>
<li>The <em>deterministic classification task</em> is the problem of predicting if a single event takes place and is specified by <span class="math inline">\(g: \mathcal{X}\rightarrow \mathcal{Y}\)</span>.</li>
</ul>
<p>The estimated prediction functional <span class="math inline">\(\hat{g}\)</span> is fit on training data \<span class="math inline">\((X_1,Y_1),...,(X_n,Y_n) \stackrel{i.i.d.}\sim(X,Y)\)</span> and is considered ‘good’ if <span class="math inline">\(\mathbb{E}[L(Y^*, \hat{g}(X^*))]\)</span> is low, where <span class="math inline">\((X^*, Y^*) \sim (X, Y)\)</span> is independent of <span class="math inline">\((X_1,Y_1),...,(X_n,Y_n)\)</span> and <span class="math inline">\(\hat{g}\)</span>.</p>
<p>In the probabilistic case, the prediction <span class="math inline">\(\hat{g}\)</span> maps to the estimated probability mass function <span class="math inline">\(\hat{p}_Y\)</span> such that <span class="math inline">\(\hat{p}_Y(1) = 1 - \hat{p}_Y(0)\)</span>.</p>
</div>
</div>
</div>
</section><section id="sec-surv-ml-regr" class="level4" data-number="3.1.2.2"><h4 data-number="3.1.2.2" class="anchored" data-anchor-id="sec-surv-ml-regr">
<span class="header-section-number">3.1.2.2</span> Regression</h4>
<p>A regression prediction is one in which the goal is to predict a continuous outcome from a set of features. For example predicting the time until an event (without censoring) occurs, is a regression problem.</p>
<div class="callout callout-style-default callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Regression Task
</div>
</div>
<div class="callout-body-container callout-body">
<div id="cnj-task-regr" class="theorem conjecture">
<p><span class="theorem-title"><strong>Box 3.2</strong></span> Let <span class="math inline">\((X,Y)\)</span> be random variables t.v.i. <span class="math inline">\(\mathcal{X}\times \mathcal{Y}\)</span> where <span class="math inline">\(\mathcal{X}\subseteq \mathbb{R}^p\)</span> and <span class="math inline">\(\mathcal{Y}\subseteq \mathbb{R}\)</span>. Let <span class="math inline">\(\mathcal{S}\subset \operatorname{Distr}(\mathcal{Y})\)</span> be a convex set of distributions on <span class="math inline">\(\mathcal{Y}\)</span>. Then,</p>
<ul>
<li>The <em>probabilistic regression task</em> is the problem of predicting a conditional distribution over the Reals and is specified by <span class="math inline">\(g : \mathcal{X}\rightarrow \mathcal{S}\)</span>.</li>
<li>The <em>deterministic regression task</em> is the problem of predicting a single continuous value in the Reals and is specified by <span class="math inline">\(g: \mathcal{X}\rightarrow \mathcal{Y}\)</span>.</li>
</ul>
<p>The estimated prediction functional <span class="math inline">\(\hat{g}\)</span> is fit on training data \<span class="math inline">\((X_1,Y_1),...,(X_n,Y_n) \stackrel{i.i.d.}\sim(X,Y)\)</span> and is considered ‘good’ if <span class="math inline">\(\mathbb{E}[L(Y^*, \hat{g}(X^*))]\)</span> is low, where <span class="math inline">\((X^*, Y^*) \sim (X, Y)\)</span> is independent of <span class="math inline">\((X_1,Y_1),...,(X_n,Y_n)\)</span> and <span class="math inline">\(\hat{g}\)</span>.</p>
</div>
</div>
</div>
<p>Whilst regression can be either probabilistic or deterministic, the latter is much more common and therefore in this book ‘regression’ refers to the deterministic case unless otherwise stated.</p>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Breiman1992" class="csl-entry" role="listitem">
Breiman, Leo, and Philip Spector. 1992. <span>“<span class="nocase">Submodel Selection and Evaluation in Regression. The X-Random Case</span>.”</span> <em>International Statistical Review / Revue Internationale de Statistique</em> 60 (3): 291–319. <a href="https://doi.org/10.2307/1403680">https://doi.org/10.2307/1403680</a>.
</div>
<div id="ref-Demsar2006" class="csl-entry" role="listitem">
Demšar, Janez. 2006. <span>“<span class="nocase">Statistical comparisons of classifiers over multiple data sets</span>.”</span> <em>Journal of Machine Learning Research</em> 7 (Jan): 1–30.
</div>
<div id="ref-Dietterich1998" class="csl-entry" role="listitem">
Dietterich, Thomas G. 1998. <span>“<span class="nocase">Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms</span>.”</span> <em>Neural Computation</em> 10 (7): 1895–1923. <a href="https://doi.org/10.1162/089976698300017197">https://doi.org/10.1162/089976698300017197</a>.
</div>
<div id="ref-Hastie2001" class="csl-entry" role="listitem">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2001. <em><span class="nocase">The Elements of Statistical Learning</span></em>. Springer New York Inc.
</div>
<div id="ref-Hastie2013" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em><span class="nocase">An introduction to statistical learning</span></em>. Vol. 112. New York: Springer.
</div>
<div id="ref-Kiraly2021" class="csl-entry" role="listitem">
Király, Franz J., Markus Löning, Anthony Blaom, Ahmed Guecioueur, and Raphael Sonabend. 2021. <span>“<span class="nocase">Designing Machine Learning Toolboxes: Concepts, Principles and Patterns</span>.”</span> <em>arXiv</em>, January. <a href="http://arxiv.org/abs/2101.04938">http://arxiv.org/abs/2101.04938</a>.
</div>
<div id="ref-Kohavi1995" class="csl-entry" role="listitem">
Kohavi, Ron. 1995. <span>“<span class="nocase">A study of cross-validation and bootstrap for accuracy estimation and model selection</span>.”</span> <em>Ijcai</em> 14 (2): 1137–45.
</div>
<div id="ref-Nadeau2003" class="csl-entry" role="listitem">
Nadeau, Claude, and Yoshua Bengio. 2003. <span>“<span class="nocase">Inference for the Generalization Error</span>.”</span> <em>Machine Learning</em> 52 (3): 239–81. <a href="https://doi.org/10.1023/A:1024068626366">https://doi.org/10.1023/A:1024068626366</a>.
</div>
<div id="ref-Sonabend2021b" class="csl-entry" role="listitem">
Sonabend, Raphael Edward Benjamin. 2021. <span>“<span class="nocase">A Theoretical and Methodological Framework for Machine Learning in Survival Analysis: Enabling Transparent and Accessible Predictive Modelling on Right-Censored Time-to-Event Data</span>.”</span> PhD, University College London (UCL). <a href="https://discovery.ucl.ac.uk/id/eprint/10129352/">https://discovery.ucl.ac.uk/id/eprint/10129352/</a>.
</div>
</div>
</section></section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./preview.html" class="pagination-link" aria-label="MLSA From Start to Finish">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">MLSA From Start to Finish</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./survival.html" class="pagination-link" aria-label="Survival Analysis">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Survival Analysis</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">abstract:</span><span class="co"> TODO (150-200 WORDS)</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>{{&lt; include _setup.qmd &gt;}}</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu"># Statistical Learning {#sec-ml}</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>{{&lt; include _wip.qmd &gt;}}</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>___</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>TODO</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>$\calD$ - labeled data set with $n$ observation</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Observations: $(\xx^{(i)}, y^{(i)})$ where $\xx^{(i)} \in \calX$ is a $p$-dimensional feature vector and $y^{(i)} \in \calY$ is a label</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Data set: $\calD = ((\xx^{(1)}, y^{(1)}) , . . . , (\xx^{(n)}, y^{(n)}))$</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Assume $\calD \iid (\mathbb{P}_{xy})^n$ (unknown distribution)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>ML model: $f : \calX \rightarrow \mathbb{R}^g$ assigning a prediction in $\mathbb{R}^g$</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>ML learners, $\calI$, configured by hyperparameters $\lambda \in \Lambda$, are $\calI: \mathbb{D} \times \Lambda \rightarrow \calH, (\calD, \lambda) \mapsto \hatf$ where $\calH$ is the function space to which a model belongs and $\hatf$ is a trained model with learned hyperparameters $\hat{\theta} \in \calH$.</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Models are evaluated with loss functions which measure the discrepancy between predictions and true values $L: \calY \times \Reals^g \rightarrow \Reals, (\hatf(\xx), \yy) \mapsto l(\hatf(\xx), \yy)$</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>To prevent overfitting, models are evaluated on unseen test data to ensure unbiased performance estimation and the generalization error $GE(\calI, \lambda, n_{train}, l) := \lim_{n_{test} \rightarrow \infty} \EE_{\calD_{train},\calD_{test}\sim\PP_{xy}}[l(\yy_{test}, \mathbf{F}_{\calD_{test},\calI(\calD_{train},\lambda)})]$ where $\mathbf{F}_{\calD_{test},\calI(\calD_{train},\lambda)}$ is the matrix of predictions from a model trained on $\calD_{train}$ and making predictions on $\calD_{test}$.</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>___</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## Machine Learning {#sec-surv-setml}</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>This section begins with a very brief introduction to machine learning and a focus on regression and classification; the survival machine learning task is then introduced (@sec-surv-setmltask). Of the many fields within machine learning (ML), the scope of this book is narrowed to supervised learning. Supervised learning is the sub-field of ML in which predictions are made for outcomes based on data with observed dependent and independent variables. For example predicting someone's height is a supervised learning problem as data can be collected for features (independent variables) such as age and sex, and outcome (dependent variable), which is height. Predictive survival analysis problems fall naturally in the supervised learning framework as there are identifiable features and (multiple types of) outcomes.</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="fu">### Terminology and Methods {#sec-surv-setml-meth}</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>Common supervised learning methods are discussed in a simplified setting with features $X \ t.v.i. \ \calX$ and outcomes $Y \ t.v.i. \ \calY$; usually outcomes are referred to as 'targets' (a 'target for prediction'). Let $\calD_0 = <span class="sc">\{</span>(X_1,Y_1),...,(X_n,Y_n)<span class="sc">\}</span>$ be a (training) dataset where $(X_i, Y_i) \iid (X, Y)$. The methods below extend naturally to the survival setting.</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Strategies and Models {.unnumbered .unlisted}</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>In order to clearly separate between similar objects, several terms for machine learning are now introduced and clearly distinguished.</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>Let $g: \calX \rightarrow \calY$ be the true (but unknown) mapping from the features to outcomes, referred to as the *true prediction functional*. Let $\calG$ be the set of *prediction functionals* such that $\forall \Upsilon \in \calG, \Upsilon: \calX \rightarrow \calY$. A *learning* or *fitting algorithm* is defined to be any function of the form $\calA: \calX^n \times \calY^n \rightarrow \calG$. The goal of supervised learning is to *learn* $g$ with a learning algorithm *fit* on (i.e. the input to the algorithm is) training data, $\hatg := \calA(\dtrain) \in \calG$. Note that $\hatg$ may take hyper-parameters that can be set or tuned (see below). The learning algorithm is 'good' if $\hatg(X) \approx g(X)$ (see 'Evaluation' below).</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>The learning algorithm is determined by the chosen *learning strategy* and *model*, where a model is a complete specification of a learning strategy including hyper-parameters. These terms are more clearly illustrated by example:</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>i. Learning strategy -- simple linear regression</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>i. Model -- $y = \beta_0 + \beta_1 x$ where $x \in \Reals$ is a single covariate, $y \in \Reals$ is the target, and $\beta_0,\beta_1 \in \Reals$ are model coefficients.</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>i. Learning algorithm (model fitting) -- Minimise the residual sum of squares: $(\hat{\beta_0}, \hat{\beta_1}) := \argmin_{\beta_0,\beta_1} <span class="sc">\{</span>\sum^n_{i=1} (y_i - \beta_0 - \beta_1 x_i)^2<span class="sc">\}</span>$ for $(x_i,y_i) \in \dtrain, i = 1,...,n$.</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>i. Prediction functional -- $\hatg(x) = \hat{\beta_0} + \hat{\beta_1}x$</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>To further illustrate the difference between learning strategy and model, note that the same learning strategy 'simple linear regression' could either utilise the model above or instead a model without intercept, $y = \beta x$, in which case the learning algorithm and prediction functional would also be modified.</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>The model in (ii) is called *unfitted* as the model coefficients are unknown and the model cannot be used for prediction. After step (iii) the model is said to be fit to the training data and therefore the model is *fitted*.\footnote{The terms 'fitted' and 'unfitted' are used instead of 'fit' and 'unfit' to prevent confusion with words such as 'suitable' and 'unsuitable'.} It is common to refer to the learning algorithm (and associated hyper-parameters) as the unfitted model and to refer to the prediction functional (and associated hyper-parameters) as the fitted model.</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Evaluation {.unnumbered .unlisted}</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>Models are *evaluated* by evaluation measures called *losses* or *scores*,\footnote{The term 'loss' is usually utilised to refer to evaluation measures to be minimised, whereas 'scores' should be maximised, this is returned to in (@sec-eval).} $L: \calY \times \calY \rightarrow \ExtReals$. Let $(X^*, Y^*) \sim (X,Y)$ be test data (i.e. independent of $\dtrain$) and let $\hatg: \calX \rightarrow \calY$ be a prediction functional fit on $\dtrain$, then these evaluation measures determine how closely predictions, $\hatg(X^*)$, relate to the truth, $Y^*$, thereby providing a method for determining if a model is 'good'.\footnote{Here evaluation refers specifically to predictive ability; other forms of evaluation and further discussion of the area are provided in (@sec-eval).}</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Task {.unnumbered .unlisted}</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>A machine learning *task* is a simple mechanism to outline the problem of interest by providing: i) the data specification; ii) the definition of learning; iii) the definition of success (when is a prediction 'good'?)  [@Kiraly2021]. All tasks in this paper have the same definitions of learning and success. For (ii), the aim is to learn the true prediction functional, $g$, by fitting the learning algorithm on training data, $\hatg := \calA(\calD_0)$. For (iii), a predicted functional is considered 'good' if the *expected generalization error*, $\EE[L(Y^*, \hatg(X^*))]$, is low, where $(X^*, Y^*) \sim (X,Y)$ is independent of the training data $\calD_0$, and $L$ is some loss that is chosen according to the domain of interest (regression, classification, survival).</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Resampling {.unnumbered .unlisted}</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>Models are *tested* on their ability to make predictions. In order to avoid 'optimism of training error'  [@Hastie2013] -- overconfidence caused by testing the model on training data -- models are tested on previously unseen or 'held-out' data. *Resampling* is the procedure of splitting one dataset into two or more for separated training and testing. In this paper only two resampling methods are utilised: *holdout* and *cross-validation*. Holdout is the process of splitting a primary dataset into training data for model fitting and testing data for model predicting. This is an efficient method but may not accurately estimate the expected generalisation error for future model performance, instead this is well-estimated by $K$-fold cross-validation (KCV)  [@Hastie2001]. In KCV, data is split into $K \in \PNaturals$ 'folds' such that $K-1$ of the folds are used for model training and the final $K$th fold for testing. The testing fold is iterated over all $K$ folds, so that each at some point is used for testing and then training (though never at the same time). In each iteration the model is fit on the training folds, and predictions are made and evaluated on the testing fold, giving a loss $L_k := L(\hatg(X^k), Y^k)$, where $(X^k, Y^k)$ are data from the $k$th fold. A final loss is defined by, $L^* := \frac{1}{K} \sum^K_{k = 1} L_k$. Commonly $K = 5$ or $K = 10$  <span class="co">[</span><span class="ot">@Breiman1992; @Kohavi1995</span><span class="co">]</span>.</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Model Performance Benchmarking {.unnumbered .unlisted}</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>Whilst *benchmarking* often refers to speed tests, i.e. the time taken to complete an operation, it can also refer to any experiment in which objects (mathematical or computational) are compared. In this report, a benchmark experiment will either refer to the comparison of multiple models' predictive abilities, or comparison of computational speeds and object sizes for model fitting; which of these will be clear from context.</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Model Comparison {.unnumbered .unlisted}</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>Models can be analytically compared on how well they make predictions for new data. Model comparison is a complex topic with many open questions  <span class="co">[</span><span class="ot">@Demsar2006; @Dietterich1998; @Nadeau2003</span><span class="co">]</span> and as such discussion is limited here. When models are compared on multiple datasets, there is more of a consensus in how to evaluate models  <span class="co">[</span><span class="ot">@Demsar2006</span><span class="co">]</span> and this is expanded on further in <span class="co">[</span><span class="ot">@Sonabend2021b</span><span class="co">]</span>. Throughout this book there are small simulation experiments for model comparison on single datasets however as these are primarily intended to aid exposition and not to generalise results, it suffices to compare models with the conservative method of constructing confidence intervals around the sample mean and standard error of the loss when available  <span class="co">[</span><span class="ot">@Nadeau2003</span><span class="co">]</span>.</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Hyper-Parameters and Tuning {.unnumbered .unlisted}</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>A *hyper-parameter* is a model parameter that can be set by the user, as opposed to coefficients that are estimated as part of model fitting. A hyper-parameter can be set before training, or it can be tuned. *Tuning* is the process of choosing the optimal hyper-parameter value via automation. In the simplest setting, tuning is performed by selecting a range of values for the hyper-parameter(s) and treating each choice (combination) as a different model. For example if tuning the number of trees in a random forest (@sec-surv-ml-models-ranfor), $m_r$, then a range of values, say $100, 200, 500$ are chosen, and three models $m_{r100}, m_{r200}, m_{r500}$ are benchmarked. The optimal hyper-parameter is given by whichever model is the best performing. *Nested resampling* is a common method to prevent overfitting that could occur from using overlapping data for tuning, training, or testing. Nested resampling is the process of resampling the training set again for tuning.</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>\newpage</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="fu">### Machine Learning in Classification and Regression {#sec-surv-ml-car}</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>Before introducing machine learning for survival analysis, which is considered 'non-classical', the more standard classification and regression set-ups are provided; these are referenced throughout this book.</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Classification {#sec-surv-ml-car-class}</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>Classification problems make predictions about categorical (or discrete) events, these may be *deterministic* or *probabilistic*.  Deterministic classification predicts which category an observation falls into, whereas probabilistic classification predicts the probability of an observation falling into each category. In this brief introduction only binary single-label classification is discussed, though the multi-label case is considered in \ref{sec:car_reduxes_r7_mlc}. In binary classification, there are two possible categories an observation can fall into, usually referred to as the 'positive' and 'negative' class. For example predicting the probability of death due to a virus is a probabilistic classification task where the 'positive' event is death.</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>A probabilistic prediction is more informative than a deterministic one as it encodes uncertainty about the prediction. For example it is clearly more informative to predict a $70%$ chance of rain tomorrow instead of simply 'rain'. Moreover the latter prediction implicitly contains an erroneous assumption of certainty, e.g. 'it will rain tomorrow'.</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>:::: {.callout-note icon=false}</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a><span class="fu">## Classification Task</span></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>::: {#cnj-task-classif}</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>Let $(X,Y)$ be random variables t.v.i. $\calX \times \calY$ where $\calX \subseteq \Reals^p$ and $\calY = <span class="sc">\{</span>0, 1<span class="sc">\}</span>$. Then,</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The *probabilistic classification task* is the problem of predicting the probability of a single event taking place and is specified by $g: \calX \rightarrow <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>$.</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The *deterministic classification task* is the problem of predicting if a single event takes place and is specified by $g: \calX \rightarrow \calY$.</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>The estimated prediction functional $\hatg$ is fit on training data <span class="sc">\\</span>$(X_1,Y_1),...,(X_n,Y_n) \iid (X,Y)$ and is considered 'good' if $\EE<span class="co">[</span><span class="ot">L(Y^*, \hatg(X^*))</span><span class="co">]</span>$ is low, where $(X^*, Y^*) \sim (X, Y)$ is independent of $(X_1,Y_1),...,(X_n,Y_n)$ and $\hatg$.</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>In the probabilistic case, the prediction $\hat{g}$ maps to the estimated probability mass function $\hat{p}_Y$ such that $\hat{p}_Y(1) = 1 - \hat{p}_Y(0)$.</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>::::</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Regression {#sec-surv-ml-regr}</span></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>A regression prediction is one in which the goal is to predict a continuous outcome from a set of features. For example predicting the time until an event (without censoring) occurs, is a regression problem.</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>:::: {.callout-note icon=false}</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a><span class="fu">## Regression Task</span></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>::: {#cnj-task-regr}</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>Let $(X,Y)$ be random variables t.v.i. $\calX \times \calY$ where $\calX \subseteq \Reals^p$ and $\calY \subseteq \Reals$. Let $\calS \subset \Distr(\calY)$ be a convex set of distributions on $\calY$. Then,</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The *probabilistic regression task* is the problem of predicting a conditional distribution over the Reals and is specified by $g : \calX \rightarrow \calS$.</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>The *deterministic regression task* is the problem of predicting a single continuous value in the Reals and is specified by $g: \calX \rightarrow \calY$.</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>The estimated prediction functional $\hatg$ is fit on training data <span class="sc">\\</span>$(X_1,Y_1),...,(X_n,Y_n) \iid (X,Y)$ and is considered 'good' if $\EE<span class="co">[</span><span class="ot">L(Y^*, \hatg(X^*))</span><span class="co">]</span>$ is low, where $(X^*, Y^*) \sim (X, Y)$ is independent of $(X_1,Y_1),...,(X_n,Y_n)$ and $\hatg$.</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>::::</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>Whilst regression can be either probabilistic or deterministic, the latter is much more common and therefore in this book 'regression' refers to the deterministic case unless otherwise stated.</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>All content licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> <br> © Raphael Sonabend, Andreas Bender.</p>
</div>   
    <div class="nav-footer-center">
<p><a href="https://www.mlsabook.com">Website</a> | <a href="https://github.com/mlsa-book/MLSA">GitHub</a></p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mlsa-book/MLSA/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/mlsa-book/MLSA/edit/main/book/machinelearning.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/mlsa-book/MLSA/blob/main/book/machinelearning.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


</body></html>