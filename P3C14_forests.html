<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Raphael Sonabend and Andreas Bender">

<title>14&nbsp; Random Forests – Machine Learning in Survival Analysis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./P3C15_svm.html" rel="next">
<link href="./P3C13_classical.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./P3C13_classical.html">Models</a></li><li class="breadcrumb-item"><a href="./P3C14_forests.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Random Forests</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning in Survival Analysis</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/mlsa-book/MLSA/tree/main/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Machine-Learning-in-Survival-Analysis.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P0C0_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Symbols and Notation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P0C1_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Survival Analysis and Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P1C2_preview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">MLSA From Start to Finish</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P1C3_machinelearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Machine Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P1C4_survival.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Survival Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P1C5_eha.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Event-history Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P1C6_survtsk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Survival Task</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Evaluation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P2C7_what.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">What are Survival Measures?</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P2C8_rank.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Discrimination Measures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P2C9_calib.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Calibration Measures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P2C10_rules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Evaluating Distributions by Scoring Rules</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P2C11_time.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Evaluating Survival Time</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P2C12_choosing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Choosing Measures</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P3C13_classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Classical Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P3C14_forests.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Random Forests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P3C15_svm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P3C16_boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Boosting Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P3C17_neural.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P3C18_choosing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Choosing Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Reduction Techniques</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P4C19_reductions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Reductions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P4C20_competing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Competing Risks Pipelines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P4C21_discrete.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Discrete Time Survival Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P4C22_poisson.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Connections to Poisson Regression and Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P4C23_pseudo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Connections to Regression and Imputation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P5C24_conclusions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Conclusions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P5C25_exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P5C26_references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#random-forests-for-regression" id="toc-random-forests-for-regression" class="nav-link active" data-scroll-target="#random-forests-for-regression"><span class="header-section-number">14.1</span> Random Forests for Regression</a>
  <ul class="collapse">
  <li><a href="#decision-trees" id="toc-decision-trees" class="nav-link" data-scroll-target="#decision-trees"><span class="header-section-number">14.1.1</span> Decision Trees</a></li>
  <li><a href="#random-forests" id="toc-random-forests" class="nav-link" data-scroll-target="#random-forests"><span class="header-section-number">14.1.2</span> Random Forests</a></li>
  </ul></li>
  <li><a href="#random-survival-forests" id="toc-random-survival-forests" class="nav-link" data-scroll-target="#random-survival-forests"><span class="header-section-number">14.2</span> Random Survival Forests</a>
  <ul class="collapse">
  <li><a href="#splitting-rules" id="toc-splitting-rules" class="nav-link" data-scroll-target="#splitting-rules"><span class="header-section-number">14.2.1</span> Splitting Rules</a></li>
  <li><a href="#sec-surv-ml-models-ranfor-nodes" id="toc-sec-surv-ml-models-ranfor-nodes" class="nav-link" data-scroll-target="#sec-surv-ml-models-ranfor-nodes"><span class="header-section-number">14.2.2</span> Terminal Node Prediction</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">14.3</span> Conclusion</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/mlsa-book/MLSA/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/mlsa-book/MLSA/edit/main/book/P3C14_forests.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/mlsa-book/MLSA/blob/main/book/P3C14_forests.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./P3C13_classical.html">Models</a></li><li class="breadcrumb-item"><a href="./P3C14_forests.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Random Forests</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span id="sec-ranfor" class="quarto-section-identifier"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Random Forests</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    TODO (150-200 WORDS)
  </div>
</div>


</header>


<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Minor changes expected!
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>This page is a work in progress and minor changes will be made over time.</strong></p>
</div>
</div>
<p>Random forests are a composite (or ensemble) algorithm built by fitting many simpler component models, <em>decision trees</em>, and then averaging the results of predictions from these trees. Due to in-built variable importance properties, random forests are commonly used in high-dimensional settings when the number of variables in a dataset far exceeds the number of rows. High-dimensional datasets are very common in survival analysis, especially when considering omics, genetic and financial data. It is therefore no surprise that <em>random survival forests</em>, remain a popular and well-performing model in the survival setting.</p>
<section id="random-forests-for-regression" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="random-forests-for-regression"><span class="header-section-number">14.1</span> Random Forests for Regression</h2>
<p>Training of decision trees can include a large number of hyper-parameters and different training steps including ‘growing’ and subsequently ‘pruning’. However, when utilised in random forests, many of these parameters and steps can be safely ignored, hence this section only focuses on the components that primarily influence the resulting random forest. This section will start by discussing decision trees and will then introduce the <em>bagging</em> algorithm used to create random forests.</p>
<section id="decision-trees" class="level3" data-number="14.1.1">
<h3 data-number="14.1.1" class="anchored" data-anchor-id="decision-trees"><span class="header-section-number">14.1.1</span> Decision Trees</h3>
<p><em>Decision trees</em> are a (relatively) simple machine learning model that are comparatively easy to implement in software and are highly interpretable. The decision tree algorithm takes an input, a dataset, selects a variable that is used to partition the data according to some <em>splitting rule</em> into distinct non-overlapping datasets or <em>nodes</em> or <em>leaves</em>, and repeats this step for the resulting partitions, or <em>branches</em>, until some criterion has been reached. The final nodes are referred to as <em>terminal nodes</em>.</p>
<p>By example, (<a href="#fig-surv-ranfor" class="quarto-xref">Figure&nbsp;<span>14.1</span></a>) demonstrates a decision tree predicting the price that a car sells for in India (price in thousands of dollars). The dataset includes as variables the registration year, kilometers driven, fuel type (petrol or automatic), seller type (individual or dealer), transmission type (manual or automatic), and number of owners. The decision tree was trained with a maximum depth of 2 (the number of rows excluding the top), and it can be seen that with this restriction only the transmission type, registration year, and fuel type were selected variables. During training, the algorithm identified that the first optimal variable to split the data was transmission type, partitioning the data into manual and automatic cars. Manual cars are further subset by registration year whereas automatic cars are split by fuel type. It can also be seen how the average sale price (top value in each leaf) diverges between leaves as the tree splits – the average sale prices in the final leaves are the terminal node predictions.</p>
<p>The graphic highlights several core features of decision trees:</p>
<ol type="1">
<li>They can model non-linear and interaction effects: The hierarchical structure allows for complex interactions between variables with some variables being used to separate all observations (transmission) and others only applied to subsets (year and fuel);</li>
<li>They are highly interpretable: it is easy to visualise the tree and see how predictions are made;</li>
<li>They perform variable selection: not all variables were used to train the model.</li>
</ol>
<p>To understand how random forests work, it is worth looking a bit more into the most important components of decision trees: splitting rules, stopping rules, and terminal node predictions.</p>
<div id="fig-surv-ranfor" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Decision tree diagram with 7 green boxes in a hierarchical top-down structure. The top box splits the data by transmission type, manual or automatic. Manual cars are then split into registration before or after 2014. Manual cars registered before 2014 are predicted to sell for $3.4K, whereas manual cars registered on or after 2014 are predicted to sell for $7.6K. On the other side of the plot, automatic cars are split according to whether they use petrol or diesel. Automatic, petrol cars are predicted to sell for $10K whereas automatic, diesel cars are predicted to sell for $26K.">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-surv-ranfor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/forests/cars.png" class="img-fluid figure-img" alt="Decision tree diagram with 7 green boxes in a hierarchical top-down structure. The top box splits the data by transmission type, manual or automatic. Manual cars are then split into registration before or after 2014. Manual cars registered before 2014 are predicted to sell for $3.4K, whereas manual cars registered on or after 2014 are predicted to sell for $7.6K. On the other side of the plot, automatic cars are split according to whether they use petrol or diesel. Automatic, petrol cars are predicted to sell for $10K whereas automatic, diesel cars are predicted to sell for $26K.">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-surv-ranfor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.1: Predicting the price a vehicle is sold for in India using a regression tree, dataset from kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho. Rounded rectangles are leaves, which indicate the variable that is being split. Edges are branches, which indicate the cut-off at which the variable is split. Variables are car transmission type (manual or automatic), fuel type (petrol or diesel) and registration year. The number at the top of each leaf is the average selling price in thousands of dollars for all observations in that leaf. The numbers at the bottom of each leaf are the number of observations in the leaf, and the proportion of data contained in the leaf.
</figcaption>
</figure>
</div>
<section id="splitting-and-stopping-rules" class="level4 unnumbered unlisted">
<h4 class="unnumbered unlisted anchored" data-anchor-id="splitting-and-stopping-rules">Splitting and Stopping Rules</h4>
<p>Precisely how the data partitions/splits are derived and which variables are utilised is determined by the <em>splitting rule</em>. The goal in each partition is to find two resulting leaves/nodes that have the greatest difference between them and thus the maximal homogeneity within each leaf, hence with each split, the data in each node should become increasingly similar. The splitting rule provides a way to measure the homogeneity within the resulting nodes. In regression, the most common splitting rule is to select a variable and cut-off (a threshold on the variable at which to separate observations) that minimises the mean squared error in the two potential resulting leaves.</p>
<p>For all decision tree and random forest algorithms going forward, let <span class="math inline">\(L\)</span> denote some leaf, then let <span class="math inline">\(L_{xy}, L_x, L_y\)</span> respectively be the set of observations, features, and outcomes in leaf <span class="math inline">\(L\)</span>. Let <span class="math inline">\(L_{y;i}\)</span> be the <span class="math inline">\(i\)</span>th outcome in <span class="math inline">\(L_y\)</span> and finally let <span class="math inline">\(\bar{y}_L = \frac{1}{|L_y|} \sum^{|L_y|}_{i = 1} L_{y;i}\)</span> be the mean outcome in leaf <span class="math inline">\(L\)</span>.</p>
<p>Let <span class="math inline">\(j=1,\ldots,p\)</span> be the index of features and let <span class="math inline">\(c_j\)</span> be a possible cutoff value for feature <span class="math inline">\(\mathbf{x}_{;j}\)</span>. Define <span class="math display">\[
\begin{split}
L^a_{xy}(j,c_j) := \{(\mathbf{x}_i,y_i)|x_{i;j} \boldsymbol{&lt;} c_j, i = 1,...,n\} \\
L^b_{xy}(j,c_j) := \{(\mathbf{x}_i,y_i)|x_{i;j} \boldsymbol{\geq} c_j, i = 1,...,n\}
\end{split}
\]</span> as the two leaves containing the set of observations resulting from partitioning variable <span class="math inline">\(j\)</span> at cutoff <span class="math inline">\(c_j\)</span>. To simplify equations let <span class="math inline">\(L^a, L^b\)</span> be shorthands for <span class="math inline">\(L^a(j,c_j)\)</span> and <span class="math inline">\(L^b(j,c_j)\)</span>. Then a split is determined by finding the arguments, <span class="math inline">\((j^*,c_{j^*}^*)\)</span> that minimise the residual sum of squares across both leaves <span class="citation" data-cites="Hastie2013">(<a href="P5C26_references.html#ref-Hastie2013" role="doc-biblioref">James et al. 2013</a>)</span>, <span id="eq-dt-min"><span class="math display">\[
(j^*, c_{j^*}^*) = \mathop{\mathrm{arg\,min}}_{j, c_j} \sum_{y \in L^a_{y}} (y - \bar{y}_{L^a})^2 + \sum_{y \in L^b_{y}} (y - \bar{y}_{L^b})^2
\tag{14.1}\]</span></span></p>
<p>This method is repeated from the first leaf to the last such that observations are included in a given leaf <span class="math inline">\(L\)</span> if they satisfy all conditions from all previous branches (splits); features may be considered multiple times in the growing process allowing complex interaction effects to be captured.</p>
<p>Leaves are repeatedly split until a <em>stopping rule</em> has been triggered – a criterion that tells the algorithm to stop partitioning data. The stopping rule is usually a condition on the number of observations in each leaf such that leaves will continue to be split until some minimum number of observations has been reached in a leaf. Other conditions may be on the depth of the tree (as in <a href="#fig-surv-ranfor" class="quarto-xref">Figure&nbsp;<span>14.1</span></a> which is restricted to a maximum depth of 2), which corresponds to the number of levels of splitting. Stopping rules are often used together, for example by setting a maximum tree depth <em>and</em> determining a minimum number of observations per leaf. Deciding the number of minimum observations and/or the maximum depth can be performed with automated hyper-parameter optimisation.</p>
</section>
<section id="terminal-node-predictions" class="level4 unnumbered unlisted">
<h4 class="unnumbered unlisted anchored" data-anchor-id="terminal-node-predictions">Terminal Node Predictions</h4>
<p>The final major component of decision trees are <em>terminal node predictions</em>. As the name suggests, this is the part of the algorithm that determines how to actually make a prediction for a new observation. A prediction is made by ‘dropping’ the new data ‘down’ the tree according to the optimal splits that were found during training. The resulting prediction is then a simple baseline statistic computed from the training data that fell into the corresponding node. In regression, this is commonly the sample mean of the training outcome data.</p>
<p>Returning to <a href="#fig-surv-ranfor" class="quarto-xref">Figure&nbsp;<span>14.1</span></a>, say a new data point is {transmission = Manual, fuel = Diesel, year = 2015}, then in the first split the left branch is taken as ‘transmission = Manual’, in the second split the right branch is taken as ‘year’ <span class="math inline">\(= 2015 \geq 2014\)</span>, hence the new data point lands in the second terminal leaf and is predicted to sell for $7,600. The ‘fuel’ variable is ignored as it is only considered for automatic vehicles. As the final predictions are simple statistics based on training data, all potential predictions can be saved in the original trained model and no complex computations are required in the prediction algorithm.</p>
</section>
</section>
<section id="random-forests" class="level3" data-number="14.1.2">
<h3 data-number="14.1.2" class="anchored" data-anchor-id="random-forests"><span class="header-section-number">14.1.2</span> Random Forests</h3>
<p>Decision trees often overfit the training data, hence they have high variance, perform poorly on new data, and are not robust to even small changes in the original training data. Moreover, important variables can end up being ignored as only subsets of dominant variables are selected for splitting.</p>
<p>To counter these problems, <em>random forests</em> are designed to improve prediction accuracy and decrease variance. Random forests utilise bootstrap aggregation, or <em>bagging</em> <span class="citation" data-cites="Breiman1996a">(<a href="P5C26_references.html#ref-Breiman1996a" role="doc-biblioref">Leo Breiman 1996</a>)</span>, to aggregate many decision trees. Bagging is a relatively simple algorithm, as follows:</p>
<ol type="1">
<li><strong>For</strong> <span class="math inline">\(b = 1,...,B\)</span>:</li>
<li><span class="math inline">\(D_b \gets \text{ Randomly sample with replacement } \mathcal{D}_{train}\)</span></li>
<li><span class="math inline">\(\hat{g}_b \gets \text{ Train a decision tree on } D_b\)</span></li>
<li><strong>end For</strong></li>
<li><strong>return</strong> <span class="math inline">\(\{\hat{g}_b\}^B_{b=1}\)</span></li>
</ol>
<p>Step 2 is known as <em>bootstrapping</em>, which is the process of sampling a dataset <em>with</em> replacement – which is in contrast to more standard subsampling where data is sampled <em>without</em> replacement. Commonly, the bootstrapped sample size is the same as the original. However, as the same value may be sampled multiple times, on average the resulting data only contains around 63.2% unique observations <span class="citation" data-cites="Efron1997">(<a href="P5C26_references.html#ref-Efron1997" role="doc-biblioref">Efron and Tibshirani 1997</a>)</span>. Randomness is further injected to decorrelate the trees by randomly subsetting the candidates of features to consider at each split of a tree. Therefore, every split of every tree may consider a different subset of variables. This process is repeated for <span class="math inline">\(B\)</span> trees, with the final output being a collection of trained decision trees.</p>
<p>Prediction from a random forest for new data <span class="math inline">\(\mathbf{x}^*\)</span> follows by making predictions from the individual trees and aggregating the results by some function <span class="math inline">\(\sigma\)</span>, which is usually the sample mean for regression:</p>
<p><span class="math display">\[
\hat{g}(\mathbf{x}^*) = \sigma(\hat{g}_1(\mathbf{x}^*),...,\hat{g}_B(\mathbf{x}^*)) = \frac{1}{B} \sum^B_{b=1} \hat{g}_b(\mathbf{x}^*)
\]</span></p>
<p>where <span class="math inline">\(\hat{g}_b(\mathbf{x}^*)\)</span> is the prediction from the <span class="math inline">\(b\)</span>th tree for some new data <span class="math inline">\(\mathbf{x}^*\)</span> and <span class="math inline">\(B\)</span> are the total number of grown trees.</p>
<p>As discussed above, individual decision trees result in predictions with high variance that are not robust to small changes in the underlying data. Random forests decrease this variance by aggregating predictions over a large sample of decorrelated trees, where a high degree of difference between trees is promoted through the use of bootstrapped samples and random candidate feature selection at each split.</p>
<p>Usually many (hundreds or thousands) trees are grown, which makes random forests robust to changes in data and ‘confident’ about individual predictions. Other advantages include having tunable and meaningful hyper-parameters, including: the number of variables to consider for a single tree, the splitting rule, and the stopping rule. Random forests treat trees as <em>weak learners</em>, which are not intended to be individually optimized. Instead, each tree captures a small amount of information about the data, which are combined to form a powerful representation of the dataset.</p>
<p>Whilst random forests are considered a ‘black-box’, in that one cannot be reasonably expected to inspect thousands of individual trees, variable importance can still be aggregated across trees, for example by counting the frequency a variable was selected across trees, calculating the minimal depth at which a variable was used for splitting, or via permutation based feature importance. Hence the model remains more interpretable than many alternative methods. Finally, random forests are less prone to overfitting and this can be relatively easily controlled by using <em>early-stopping</em> methods, for example by continually growing trees until the performance of the model stops improving.</p>
</section>
</section>
<section id="random-survival-forests" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="random-survival-forests"><span class="header-section-number">14.2</span> Random Survival Forests</h2>
<p>Unlike other machine learning methods that may require complex changes to underlying algorithms, random forests can be relatively simply adapted to <em>random survival forests</em> by updating the splitting rules and terminal node predictions to those that can handle censoring and can make survival predictions. This chapter is therefore focused on outlining different choices of splitting rules and terminal node predictions, which can then be flexibly combined into different models.</p>
<section id="splitting-rules" class="level3" data-number="14.2.1">
<h3 data-number="14.2.1" class="anchored" data-anchor-id="splitting-rules"><span class="header-section-number">14.2.1</span> Splitting Rules</h3>
<p>Survival trees and RSFs have been studied for the past four decades and whilst there are many possible splitting rules <span class="citation" data-cites="Bou-Hamad2011">(<a href="P5C26_references.html#ref-Bou-Hamad2011" role="doc-biblioref">Bou-Hamad, Larocque, and Ben-Ameur 2011</a>)</span>, only two broad classes are commonly utilised (as judged by number of available implementations, e.g., <span class="citation" data-cites="pkgsksurvival">Pölsterl (<a href="P5C26_references.html#ref-pkgsksurvival" role="doc-biblioref">2020</a>)</span>; <span class="citation" data-cites="pkgranger">Wright and Ziegler (<a href="P5C26_references.html#ref-pkgranger" role="doc-biblioref">2017</a>)</span>; <span class="citation" data-cites="Ishwaran2011">H. Ishwaran et al. (<a href="P5C26_references.html#ref-Ishwaran2011" role="doc-biblioref">2011</a>)</span>). The first class rely on hypothesis tests, and primarily the log-rank test, to maximise dissimilarity between splits, the second class utilises likelihood-based measures. The first is discussed in more detail as this is common in practice and is relatively straightforward to implement and understand, moreover it has been demonstrated to outperform other splitting rules <span class="citation" data-cites="Bou-Hamad2011">(<a href="P5C26_references.html#ref-Bou-Hamad2011" role="doc-biblioref">Bou-Hamad, Larocque, and Ben-Ameur 2011</a>)</span>. Likelihood rules are more complex and require assumptions that may not be realistic, these are discussed briefly.</p>
<section id="hypothesis-tests" class="level4 unnumbered unlisted">
<h4 class="unnumbered unlisted anchored" data-anchor-id="hypothesis-tests">Hypothesis Tests</h4>
<p>The log-rank test statistic has been widely utilized as a splitting-rule for survival analysis <span class="citation" data-cites="Ciampi1986 Ishwaran2008 LeBlanc1993 Segal1988">(<a href="P5C26_references.html#ref-Ciampi1986" role="doc-biblioref">Ciampi et al. 1986</a>; <a href="P5C26_references.html#ref-Ishwaran2008" role="doc-biblioref">B. H. Ishwaran et al. 2008</a>; <a href="P5C26_references.html#ref-LeBlanc1993" role="doc-biblioref">LeBlanc and Crowley 1993</a>; <a href="P5C26_references.html#ref-Segal1988" role="doc-biblioref">Segal 1988</a>)</span>. The log-rank test compares the survival distributions of two groups under the null-hypothesis that both groups have the same underlying risk of (immediate) events, with the hazard function used to compare underlying risk.</p>
<p>Let <span class="math inline">\(L^a\)</span> and <span class="math inline">\(L^b\)</span> be two leaves and let <span class="math inline">\(h^a,h^b\)</span> be the (theoretical, true) hazard functions in the two leaves respectively and let <span class="math inline">\(i \in L\)</span> be a shorthand for the indices of the observations in leaf <span class="math inline">\(L\)</span> so that <span class="math inline">\(i = 1,\ldots,|L|\)</span>. Define:</p>
<ul>
<li><p><span class="math inline">\(\mathcal{U}_D\)</span>, the set of unique event times across the data (in both leaves)</p></li>
<li><p><span class="math inline">\(n_\tau^a\)</span>, the number of observations at risk at <span class="math inline">\(\tau\)</span> in leaf <span class="math inline">\(a\)</span></p></li>
</ul>
<p><span class="math display">\[
n_\tau^a = \sum_{i \in L^a} \mathbb{I}(t_i \geq \tau)
\]</span></p>
<ul>
<li><span class="math inline">\(o^a_{\tau}\)</span>, the observed number of events in leaf <span class="math inline">\(a\)</span> at <span class="math inline">\(\tau\)</span></li>
</ul>
<p><span class="math display">\[
o^a_{\tau} = \sum_{i \in L^a} \mathbb{I}(t_i = \tau, \delta_i = 1)
\]</span></p>
<ul>
<li><span class="math inline">\(n_\tau = n_\tau^a + n_\tau^b\)</span>, the number of observations at risk at <span class="math inline">\(\tau\)</span> in both leaves</li>
<li><span class="math inline">\(o_\tau = o^a_{\tau} + o^b_{\tau}\)</span>, the observed number of events at <span class="math inline">\(\tau\)</span> in both leaves</li>
</ul>
<p>Then, the log-rank hypothesis test is given by <span class="math inline">\(H_0: h^a = h^b\)</span> with test statistic <span class="citation" data-cites="Segal1988">(<a href="P5C26_references.html#ref-Segal1988" role="doc-biblioref">Segal 1988</a>)</span>, <span class="math display">\[
LR(L^a) = \frac{\sum_{\tau \in \mathcal{U}_D} (o^a_{\tau} - e^a_{\tau})}{\sqrt{\sum_{\tau \in \mathcal{U}_D} v_\tau^a}}
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(e^a_{\tau}\)</span> is the expected number of events in leaf <span class="math inline">\(a\)</span> at <span class="math inline">\(\tau\)</span></li>
</ul>
<p><span class="math display">\[
e^a_{\tau} := \frac{n_\tau^a o_\tau}{n_\tau}
\]</span></p>
<ul>
<li><span class="math inline">\(v^a_\tau\)</span> is the variance of the number of events in leaf <span class="math inline">\(a\)</span> at <span class="math inline">\(\tau\)</span></li>
</ul>
<p><span class="math display">\[
v^a_{\tau} := e^a_{\tau} \Big(\frac{n_\tau - o_\tau}{n_\tau}\Big)\Big(\frac{n_\tau - n^a_\tau}{n_\tau - 1}\Big)
\]</span></p>
<p>These results follow as under the assumption of equal hazards in both leafs, the number of events at each <span class="math inline">\(\tau \in \mathcal{U}_D\)</span> is distributed according to a Hypergeometric distribution. The same statistic results if <span class="math inline">\(L^b\)</span> is instead considered.</p>
<p>The higher the log-rank statistic, the greater the dissimilarity between the two groups (<a href="#fig-surv-ranfor-logrank" class="quarto-xref">Figure&nbsp;<span>14.2</span></a>), thereby making it a sensible splitting rule for survival, moreover it has been shown that it works well for splitting censored data <span class="citation" data-cites="LeBlanc1993">(<a href="P5C26_references.html#ref-LeBlanc1993" role="doc-biblioref">LeBlanc and Crowley 1993</a>)</span>. Additionally, the log-rank test requires no knowledge about the shape of the survival curves or distribution of the outcomes in either group <span class="citation" data-cites="Bland2004">(<a href="P5C26_references.html#ref-Bland2004" role="doc-biblioref">Bland and Altman 2004</a>)</span>, making it ideal for an automated process that requires no user intervention.</p>
<div id="fig-surv-ranfor-logrank" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Three Kaplan-Meier plots. First plot (a) shows a single curve descending from (0,1) to (1000,0). The next plot (b) shows two overlapping Kaplan-Meier curves and a label reads 'chi-squared = 1.17 (p=0.28)'. The final plot (c) shows two non-overlapping Kaplan-Meier curves with the label 'chi-squared=5.02 (p=0.03)'.">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-surv-ranfor-logrank-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/forests/logrank.png" class="img-fluid figure-img" alt="Three Kaplan-Meier plots. First plot (a) shows a single curve descending from (0,1) to (1000,0). The next plot (b) shows two overlapping Kaplan-Meier curves and a label reads 'chi-squared = 1.17 (p=0.28)'. The final plot (c) shows two non-overlapping Kaplan-Meier curves with the label 'chi-squared=5.02 (p=0.03)'.">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-surv-ranfor-logrank-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.2: Panel (a) is the Kaplan-Meier estimator fit on the complete <code>lung</code> dataset from the <span class="math inline">\(\textsf{R}\)</span> package <strong>survival</strong>. (b-c) is the same data stratified according to whether ‘age’ is greater or less than 50 (panel b) or 75 (panel c). The higher <span class="math inline">\(\chi^2\)</span> statistic (panel c) results in a lower <span class="math inline">\(p\)</span>-value and a greater difference between the stratified Kaplan-Meier curves. Hence splitting age at 75 results in a greater dissimilarity between the resulting branches and thus makes a better choice for splitting the variable.
</figcaption>
</figure>
</div>
<p>The log-rank <em>score</em> rule <span class="citation" data-cites="Hothorn2003">(<a href="P5C26_references.html#ref-Hothorn2003" role="doc-biblioref">Hothorn and Lausen 2003</a>)</span> is a standardized version of the log-rank rule that could be considered as a splitting rule, though simulation studies have demonstrated non-significant improvements in predictive performance when comparing the two <span class="citation" data-cites="Ishwaran2008">(<a href="P5C26_references.html#ref-Ishwaran2008" role="doc-biblioref">B. H. Ishwaran et al. 2008</a>)</span>. Alternative dissimiliarity measures and tests have also been suggested as splitting rules, including modified Kolmogorov-Smirnov test and Gehan-Wilcoxon tests <span class="citation" data-cites="Ciampi1988">(<a href="P5C26_references.html#ref-Ciampi1988" role="doc-biblioref">Ciampi et al. 1988</a>)</span>. Simulation studies have demonstrated that both of these may have higher power and produce ‘better’ results than the log-rank statistic <span class="citation" data-cites="Fleming1980">(<a href="P5C26_references.html#ref-Fleming1980" role="doc-biblioref">Fleming et al. 1980</a>)</span>, however neither appears to be commonly used.</p>
<p>In a competing risk setting, Gray’s test <span class="citation" data-cites="Gray1988">(<a href="P5C26_references.html#ref-Gray1988" role="doc-biblioref">Gray 1988</a>)</span> can be used instead of the log-rank test, as it compares cumulative incidence functions rather than all-cause hazards. Similarly to the log-rank test, Gray’s test also compares survival distributions using hypothesis tests to determine if there are significant differences between the groups, thus making it a suitable option to build competing risk RSFs.</p>
</section>
<section id="alternative-splitting-rules" class="level4 unnumbered unlisted">
<h4 class="unnumbered unlisted anchored" data-anchor-id="alternative-splitting-rules">Alternative Splitting Rules</h4>
<p>A common alternative to the log-rank test is to instead use <em>likelihood ratio</em>, or <em>deviance</em>, statistics. When building RSFs, the likelihood-ratio statistic can be used to test if the model fit is improved or worsened with each split, thus providing a way to partition data. However, as discussed in <a href="P1C4_survival.html#sec-surv-obj" class="quarto-xref"><span>Section 4.1.5</span></a>, there are many different likelihoods that can be assumed for survival data, and there is no obvious way to determine if one is more sensible than another. Furthermore the choice of likelihood must fit the underlying model assumptions. For example, one could assume the data fits the proportional hazards assumption and in each split one could calculate the likelihood-ratio using the Cox PH partial likelihood. Alternatively, a parametric form could be assumed and a likelihood proposed by <span class="citation" data-cites="LeBlanc1992">LeBlanc and Crowley (<a href="P5C26_references.html#ref-LeBlanc1992" role="doc-biblioref">1992</a>)</span> may be calculated to test model fit. While potentially useful, these methods are implemented in very few off-shelf software packages, thus empirical comparisons to other splitting rules are lacking.</p>
<p>Other rules have also been studied including comparison of residuals <span class="citation" data-cites="Therneau1990">(<a href="P5C26_references.html#ref-Therneau1990" role="doc-biblioref">Therneau, Grambsch, and Fleming 1990</a>)</span>, scoring rules <span class="citation" data-cites="pkgrfsrc">(<a href="P5C26_references.html#ref-pkgrfsrc" role="doc-biblioref">H. Ishwaran and Kogalur 2018</a>)</span>, distance metrics <span class="citation" data-cites="Gordon1985">(<a href="P5C26_references.html#ref-Gordon1985" role="doc-biblioref">Gordon and Olshen 1985</a>)</span>, and concordance metrics <span class="citation" data-cites="Schmid2016">(<a href="P5C26_references.html#ref-Schmid2016" role="doc-biblioref">Schmid, Wright, and Ziegler 2016</a>)</span>. Experiments have shown different splitting rules may perform better or worse depending on the underlying data <span class="citation" data-cites="Schmid2016">(<a href="P5C26_references.html#ref-Schmid2016" role="doc-biblioref">Schmid, Wright, and Ziegler 2016</a>)</span>, hence one could even consider treating the splitting rule as a hyper-parameter for tuning. However, if there is a clear goal in prediction, then the choice of splitting rule can be informed by the prediction type. For example, if the goal is to maximise separation, then a log-rank splitting rule to maximise homogeneity in terminal nodes is a natural starting point. Whereas if the goal is to accurately rank observations, then a concordance splitting rule may be optimal.</p>
</section>
</section>
<section id="sec-surv-ml-models-ranfor-nodes" class="level3" data-number="14.2.2">
<h3 data-number="14.2.2" class="anchored" data-anchor-id="sec-surv-ml-models-ranfor-nodes"><span class="header-section-number">14.2.2</span> Terminal Node Prediction</h3>
<p>As in the regression setting, the usual strategy for predictions is to create a simple estimate based on the training data that lands in the terminal nodes. However, as seen throughout this book, the choice of estimator in the survival setting depends on the prediction task of interest, which are now considered in turn. First, note that all terminal node predictions can only yield useful results if there are a sufficient number of uncensored observations in each terminal node. Hence, a common RSF stopping rule is the minimum number of <em>uncensored</em> observations per leaf, meaning a leaf is not split if that would result in too few uncensored observations in the resulting leaves.</p>
<section id="probabilistic-predictions" class="level4 unnumbered unlisted">
<h4 class="unnumbered unlisted anchored" data-anchor-id="probabilistic-predictions">Probabilistic Predictions</h4>
<p>Starting with the most common survival prediction type, the algorithm requires a simple estimate for the underlying survival distribution in each of the terminal nodes, which can be estimated using the Kaplan-Meier or Nelson-Aalen methods <span class="citation" data-cites="Hothorn2004 Ishwaran2008 LeBlanc1993 Segal1988">(<a href="P5C26_references.html#ref-Hothorn2004" role="doc-biblioref">Hothorn et al. 2004</a>; <a href="P5C26_references.html#ref-Ishwaran2008" role="doc-biblioref">B. H. Ishwaran et al. 2008</a>; <a href="P5C26_references.html#ref-LeBlanc1993" role="doc-biblioref">LeBlanc and Crowley 1993</a>; <a href="P5C26_references.html#ref-Segal1988" role="doc-biblioref">Segal 1988</a>)</span>.</p>
<!-- FIXME: REF KM DEFINITION ABOVE -->
<p>Denote <span class="math inline">\(b\)</span> as a decision tree and <span class="math inline">\(L^{b(h)}\)</span> as the terminal node <span class="math inline">\(h\)</span> in tree <span class="math inline">\(b\)</span>. Then the predicted survival function and cumulative hazard for a new observation <span class="math inline">\(\mathbf{x}^*\)</span> is,</p>
<p><span id="eq-surv-kaplan"><span class="math display">\[
\hat{S}_{b(h)}(\tau|\mathbf{x}^*) = \prod_{i:t_{(i)} \leq \tau} 1-\frac{d_{t_{(i)}}}{n_{t_{(i)}}}, \quad \{i \in L^{b(h)}: \mathbf{x}^* \in L^{b(h)}\}
\tag{14.2}\]</span></span></p>
<p><span id="eq-surv-nelson"><span class="math display">\[
\hat{H}_{b(h)}(\tau|\mathbf{x}^*) = \sum_{i:t_{(i)} \leq \tau} \frac{d_{t_{(i)}}}{n_{t_{(i)}}}, \quad \{i \in L^{b(h)}: \mathbf{x}^* \in L^{b(h)}\}
\tag{14.3}\]</span></span></p>
<p>where <span class="math inline">\(t_{(i)}\)</span> is the ordered event times and <span class="math inline">\(d_{t_{(i)}}\)</span> and <span class="math inline">\(n_{t_{(i)}}\)</span> are the observed number of events, and the number of observations at risk, respectively at <span class="math inline">\(t_{(i)}\)</span>. See <a href="#fig-surv-ranfor-lung" class="quarto-xref">Figure&nbsp;<span>14.3</span></a> for an example using the <code>lung</code> dataset <span class="citation" data-cites="pkgsurvival">(<a href="P5C26_references.html#ref-pkgsurvival" role="doc-biblioref">Therneau 2015</a>)</span>.</p>
<div id="fig-surv-ranfor-lung" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Decision tree with two splits, the first is 'ph.ecog' with a p-value of <0.001, when ph.ecog > 1 a terminal prediction is made, otherwise the data is split at 'sex' with - = 0.015. Terminal node predictions are plots of survival functions.">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-surv-ranfor-lung-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/forests/lung.png" class="img-fluid figure-img" alt="Decision tree with two splits, the first is 'ph.ecog' with a p-value of <0.001, when ph.ecog > 1 a terminal prediction is made, otherwise the data is split at 'sex' with - = 0.015. Terminal node predictions are plots of survival functions.">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-surv-ranfor-lung-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.3: Survival tree trained on the <code>lung</code> dataset from the <span class="math inline">\(\textsf{R}\)</span> package <strong>survival</strong>. The terminal node predictions are survival curves.
</figcaption>
</figure>
</div>
<p>The bootstrapped prediction is the cumulative hazard function or survival function averaged over individual trees. Note that understanding what these bootstrapped functions represents depends on how they are calculated. By definition, a mixture of <span class="math inline">\(n\)</span> distributions with cumulative distribution functions <span class="math inline">\(F_i, i = 1,...,n\)</span> is given by</p>
<p><span class="math display">\[
F(x) = \sum^n_{i=1} w_i F_i(x)
\]</span></p>
<p>Subsituting <span class="math inline">\(F = 1 - S\)</span> and noting <span class="math inline">\(\sum w_i = 1\)</span> gives the computation <span class="math inline">\(S(x) = \sum^n_{i=1} w_i S_i(x)\)</span>, allowing the bootstrapped survival function to exactly represent the mixture distribution averaged over all trees:</p>
<p><span id="eq-surv-kaplan-boot"><span class="math display">\[
\hat{S}_{Boot}(\tau|\mathbf{x}^*) = \frac{1}{B} \sum^B_{b=1} w_i\hat{S}_b(\tau|\mathbf{x}^*)
\tag{14.4}\]</span></span></p>
<p>usually with <span class="math inline">\(w_i = 1/B\)</span> where <span class="math inline">\(B\)</span> is the number of trees.</p>
<p>In contrast, if one were to instead substitute <span class="math inline">\(F = 1 - \exp(-H)\)</span>, then the mixture distribution depends on a logarithmic function that can only be approximately computed if predicted survival probabilities are close to one, which is an assumption that deteriorates over time. Therefore, to ensure the bootstrapped prediction accurately represents the underlying mixed probability distribution, the bootstrapped cumulative hazard function should be computed as:</p>
<p><span id="eq-surv-nelson-boot"><span class="math display">\[
\hat{H}_{Boot}(\tau|\mathbf{x}^*) = - \log (\hat{S}_{Boot}(\tau|\mathbf{x}^*))
\tag{14.5}\]</span></span></p>
<p>Another practical consideration to take into account is how to average the survival probabilities over the decision trees as each individual Kaplan-Meier estimate may have been trained on different time points. This is overcome by recognising that the Kaplan-Meier estimation results in a piece-wise function that can be linearly interpolated between training data. <a href="#fig-surv-ranfor-bootstrap" class="quarto-xref">Figure&nbsp;<span>14.4</span></a> demonstrates this process for three decision trees (panel a), where the survival probability is calculated at all possible time points (panels b-c), and the average is computed with linear interpolation added between time-points (panel d).</p>
<div id="fig-surv-ranfor-bootstrap" class="quarto-float quarto-figure quarto-figure-center anchored" alt="Four panels with 't' on x-axis and 'S(t)' on y-axis. Panel a) shows three survival functions as piece-wise linearly decreasing step functions. Panel (b) shows the same but with vertical dotted lines added at all time-points. Panel (c) shows dots at each of the intersections between the vertical lines and the plotted decision trees. Panel (d) shows a single line with dots which are the average of the points in panel (c).">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-surv-ranfor-bootstrap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/forests/bootstrap.png" class="img-fluid figure-img" alt="Four panels with 't' on x-axis and 'S(t)' on y-axis. Panel a) shows three survival functions as piece-wise linearly decreasing step functions. Panel (b) shows the same but with vertical dotted lines added at all time-points. Panel (c) shows dots at each of the intersections between the vertical lines and the plotted decision trees. Panel (d) shows a single line with dots which are the average of the points in panel (c).">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-surv-ranfor-bootstrap-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.4: Bootstrapping Kaplan-Meier estimators across three decision trees (red, blue, green). Panel a) shows the individual estimates, b) shows the time points to aggregate the trees over, c) is the predicted survival probability from each tree at the desired time points, and d) is the average survival probabilities connected by a step function.
</figcaption>
</figure>
</div>
<p>Extensions to competing risks follow naturally using bootstrapped cause-specific cumulative incidence functions.</p>
<!-- FIXME - ANDREAS TO ADD DETAILS ON COMPETING RISKS -->
</section>
<section id="deterministic-predictions" class="level4 unnumbered unlisted">
<h4 class="unnumbered unlisted anchored" data-anchor-id="deterministic-predictions">Deterministic Predictions</h4>
<p>As discussed in <a href="P2C11_time.html" class="quarto-xref"><span>Chapter 11</span></a>, predicting and evaluating survival times is a complex and fairly under-researched area. For RSFs, there is an inclination to estimate survival times based on the mean or median survival times of observations in terminal nodes, however this would lead to biased estimations. Therefore, research has tended to focus on relative risk predictions.</p>
<p>As discussed, relative risks are arbitrary values where only the resulting rank matters when comparing observations. In RSFs, each terminal node should be as homogeneous as possible, hence within a terminal node, the risk between observations should be the same. The most common method to estimate average risk appears to be a transformation from the Nelson-Aalen method <span class="citation" data-cites="Ishwaran2008">(<a href="P5C26_references.html#ref-Ishwaran2008" role="doc-biblioref">B. H. Ishwaran et al. 2008</a>)</span>, which exploits results from counting process to provide a measure of expected mortality <span class="citation" data-cites="dataapplied">(<a href="P5C26_references.html#ref-dataapplied" role="doc-biblioref">Hosmer Jr, Lemeshow, and May 2011</a>)</span> – the same result is used in the van Houwelingen calibration measure discussed in <a href="P2C9_calib.html#sec-alpha" class="quarto-xref"><span>Section 9.1.2</span></a>. Given new data, <span class="math inline">\(\mathbf{x}^*\)</span>, falling into terminal node <span class="math inline">\(b(h)\)</span>, the relative risk prediction is the sum of the predicted cumulative hazard, <span class="math inline">\(\hat{H}_{b(h)}\)</span>, computed at each observation’s observed outcome time:</p>
<p><span class="math display">\[
\phi_{b(h)}(\mathbf{x}^*) = \sum_{i \in \mathcal{U}_O} \hat{H}_{b(h)}(t_i|\mathbf{x}^*)
\]</span></p>
<p>where <span class="math inline">\(\hat{H}_{b(h)}\)</span> is the terminal node prediction as in <a href="#eq-surv-nelson" class="quarto-xref">Equation&nbsp;<span>14.3</span></a>. This is interpreted as the number of expected events in <span class="math inline">\(b(h)\)</span> and the assumption is that a terminal node with more expected events is a higher risk group than a node with less expected events. The bootstrapped risk prediction is the sample mean over all trees:</p>
<p><span class="math display">\[
\phi_{Boot}(\mathbf{x}^*) = \frac{1}{B} \sum^B_{b=1} \phi_{b(h)}(\mathbf{x}^*)
\]</span></p>
<p>More complex methods have also been proposed that are based on the likelihood-based splitting rule and assume a PH model form <span class="citation" data-cites="Ishwaran2004 LeBlanc1992">(<a href="P5C26_references.html#ref-Ishwaran2004" role="doc-biblioref">H. Ishwaran et al. 2004</a>; <a href="P5C26_references.html#ref-LeBlanc1992" role="doc-biblioref">LeBlanc and Crowley 1992</a>)</span>. However, these do not appear to be in wide-spread usage.</p>
</section>
</section>
</section>
<section id="conclusion" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">14.3</span> Conclusion</h2>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key takeaways
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Random forests are a highly flexible algorithm that allow the various components to be adapted and altered without major changes to the underlying algorithm. This allows random survival forests (RSFs) to be readily available ‘off-shelf’ in many open-source packages;</li>
<li>RSFs have in-built variable selection methods that mean they tend to perform well on high-dimensional data, routinely outperforming other models <span class="citation" data-cites="Herrmann2020">Burk et al. (<a href="P5C26_references.html#ref-Burk2024" role="doc-biblioref">2024</a>)</span>;</li>
<li>Despite having many potential hyper-parameters to tune, all are intuitive and many can even be ignored as sensible defaults exist in most off-shelf software implementations.</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Limitations
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Due to the number of trees and the constant bootstrapping procedures, RSFs can be more computationally intensive than other models, though still much less intensive than neural networks and other deep learning methods.</li>
<li>Despite having some in-built methods for model interpretation, RSFs are still black-boxes that can be difficult to fully interpret.</li>
<li>With too few trees random forests can have similar limitations to decision trees and with too many random forests can overfit the data. Though most software has sensible defaults to prevent either scenario.</li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Further reading
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>A comprehensive review of random survival forests (RSFs) is provided in Bou-Hamad (2011) <span class="citation" data-cites="Bou-Hamad2011">(<a href="P5C26_references.html#ref-Bou-Hamad2011" role="doc-biblioref">Bou-Hamad, Larocque, and Ben-Ameur 2011</a>)</span>, which includes extensions to time-varying covariates and different censoring types.</li>
<li>The discussion of decision trees omitted many methods for growing and pruning trees, if you are interest in those technical details see <span class="citation" data-cites="Breiman1984">L. Breiman et al. (<a href="P5C26_references.html#ref-Breiman1984" role="doc-biblioref">1984</a>)</span>.</li>
<li>RSFs have been shown to perform well in benchmark experiments on high-dimensional data, see <span class="citation" data-cites="Herrmann2020">Herrmann et al. (<a href="P5C26_references.html#ref-Herrmann2020" role="doc-biblioref">2021</a>)</span> and <span class="citation" data-cites="Spooner2020">Spooner et al. (<a href="P5C26_references.html#ref-Spooner2020" role="doc-biblioref">2020</a>)</span> for examples.</li>
<li>This chapter considered the most ‘traditional’ forms of RSFs. Conditional inference forests are popular in the regression setting and whilst they are under-researched in survival, see <span class="citation" data-cites="Hothorn2005">Hothorn et al. (<a href="P5C26_references.html#ref-Hothorn2005" role="doc-biblioref">2005</a>)</span> for literature on the topic. A more recent method that seems to perform well is the (accelerated) oblique random survival forest discussed in <span class="citation" data-cites="Jaeger2024">(<a href="P5C26_references.html#ref-Jaeger2024" role="doc-biblioref"><strong>Jaeger2024?</strong></a>)</span>.</li>
</ul>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Bland2004" class="csl-entry" role="listitem">
Bland, J Martin, and Douglas G. Altman. 2004. <span>“<span class="nocase">The logrank test</span>.”</span> <em>BMJ (Clinical Research Ed.)</em> 328 (7447): 1073. <a href="https://doi.org/10.1136/bmj.328.7447.1073">https://doi.org/10.1136/bmj.328.7447.1073</a>.
</div>
<div id="ref-Bou-Hamad2011" class="csl-entry" role="listitem">
Bou-Hamad, Imad, Denis Larocque, and Hatem Ben-Ameur. 2011. <span>“<span class="nocase">A review of survival trees</span>.”</span> <em>Statist. Surv.</em> 5: 44–71. <a href="https://doi.org/10.1214/09-SS047">https://doi.org/10.1214/09-SS047</a>.
</div>
<div id="ref-Breiman1996a" class="csl-entry" role="listitem">
Breiman, Leo. 1996. <span>“<span>Bagging Predictors</span>.”</span> <em>Machine Learning</em> 24 (2): 123–40. <a href="https://doi.org/10.1023/A:1018054314350">https://doi.org/10.1023/A:1018054314350</a>.
</div>
<div id="ref-Breiman1984" class="csl-entry" role="listitem">
Breiman, L, J Friedman, C J Stone, and R A Olshen. 1984. <em><span class="nocase">Classification and Regression Trees</span></em>. The Wadsworth and Brooks-Cole Statistics-Probability Series. Taylor &amp; Francis. <a href="https://books.google.co.uk/books?id=JwQx-WOmSyQC">https://books.google.co.uk/books?id=JwQx-WOmSyQC</a>.
</div>
<div id="ref-Burk2024" class="csl-entry" role="listitem">
Burk, Lukas, John Zobolas, Bernd Bischl, Andreas Bender, Marvin N. Wright, and Raphael Sonabend. 2024. <span>“<span class="nocase">A Large-Scale Neutral Comparison Study of Survival Models on Low-Dimensional Data</span>,”</span> June. <a href="http://arxiv.org/abs/2406.04098">http://arxiv.org/abs/2406.04098</a>.
</div>
<div id="ref-Ciampi1988" class="csl-entry" role="listitem">
Ciampi, Antonio, Sheilah A Hogg, Steve McKinney, and Johanne Thiffault. 1988. <span>“<span class="nocase">RECPAM: a computer program for recursive partition and amalgamation for censored survival data and other situations frequently occurring in biostatistics. I. Methods and program features</span>.”</span> <em>Computer Methods and Programs in Biomedicine</em> 26 (3): 239–56. https://doi.org/<a href="https://doi.org/10.1016/0169-2607(88)90004-1">https://doi.org/10.1016/0169-2607(88)90004-1</a>.
</div>
<div id="ref-Ciampi1986" class="csl-entry" role="listitem">
Ciampi, Antonio, Johanne Thiffault, Jean Pierre Nakache, and Bernard Asselain. 1986. <span>“<span class="nocase">Stratification by stepwise regression, correspondence analysis and recursive partition: a comparison of three methods of analysis for survival data with covariates</span>.”</span> <em>Computational Statistics and Data Analysis</em> 4 (3): 185–204. <a href="https://doi.org/10.1016/0167-9473(86)90033-2">https://doi.org/10.1016/0167-9473(86)90033-2</a>.
</div>
<div id="ref-Efron1997" class="csl-entry" role="listitem">
Efron, Bradley, and Robert Tibshirani. 1997. <span>“Improvements on Cross-Validation: The .632+ Bootstrap Method.”</span> <em>Journal of the American Statistical Association</em> 92 (438): 548–60. <a href="http://www.jstor.org/stable/2965703">http://www.jstor.org/stable/2965703</a>.
</div>
<div id="ref-Fleming1980" class="csl-entry" role="listitem">
Fleming, Thomas R, Judith R O’Fallon, Peter C O’Brien, and David P Harrington. 1980. <span>“<span class="nocase">Modified Kolmogorov-Smirnov Test Procedures with Application to Arbitrarily Right-Censored Data</span>.”</span> <em>Biometrics</em> 36 (4): 607–25. <a href="https://doi.org/10.2307/2556114">https://doi.org/10.2307/2556114</a>.
</div>
<div id="ref-Gordon1985" class="csl-entry" role="listitem">
Gordon, Louis, and Richard A Olshen. 1985. <span>“<span class="nocase">Tree-structured survival analysis.</span>”</span> <em>Cancer Treatment Reports</em> 69 (10): 1065–69.
</div>
<div id="ref-Gray1988" class="csl-entry" role="listitem">
Gray, Robert J. 1988. <span>“<span class="nocase">A Class of <span class="math inline">\(K\)</span>-Sample Tests for Comparing the Cumulative Incidence of a Competing Risk</span>.”</span> <em>The Annals of Statistics</em> 16 (3): 1141–54. <a href="https://doi.org/10.1214/aos/1176350951">https://doi.org/10.1214/aos/1176350951</a>.
</div>
<div id="ref-Herrmann2020" class="csl-entry" role="listitem">
Herrmann, Moritz, Philipp Probst, Roman Hornung, Vindi Jurinovic, and Anne-Laure Boulesteix. 2021. <span>“<span class="nocase">Large-scale benchmark study of survival prediction methods using multi-omics data</span>.”</span> <em>Briefings in Bioinformatics</em> 22 (3). <a href="https://doi.org/10.1093/bib/bbaa167">https://doi.org/10.1093/bib/bbaa167</a>.
</div>
<div id="ref-dataapplied" class="csl-entry" role="listitem">
Hosmer Jr, David W, Stanley Lemeshow, and Susanne May. 2011. <em><span class="nocase">Applied survival analysis: regression modeling of time-to-event data</span></em>. Vol. 618. John Wiley &amp; Sons.
</div>
<div id="ref-Hothorn2005" class="csl-entry" role="listitem">
Hothorn, Torsten, Peter Bühlmann, Sandrine Dudoit, Annette Molinaro, and Mark J Van Der Laan. 2005. <span>“<span class="nocase">Survival ensembles</span>.”</span> <em>Biostatistics</em> 7 (3): 355–73. <a href="https://doi.org/10.1093/biostatistics/kxj011">https://doi.org/10.1093/biostatistics/kxj011</a>.
</div>
<div id="ref-Hothorn2003" class="csl-entry" role="listitem">
Hothorn, Torsten, and Berthold Lausen. 2003. <span>“<span class="nocase">On the exact distribution of maximally selected rank statistics</span>.”</span> <em>Computational Statistics &amp; Data Analysis</em> 43 (2): 121–37. <a href="https://doi.org/10.1016/S0167-9473(02)00225-6">https://doi.org/10.1016/S0167-9473(02)00225-6</a>.
</div>
<div id="ref-Hothorn2004" class="csl-entry" role="listitem">
Hothorn, Torsten, Berthold Lausen, Axel Benner, and Martin Radespiel-Tröger. 2004. <span>“<span class="nocase">Bagging survival trees</span>.”</span> <em>Statistics in Medicine</em> 23 (1): 77–91. <a href="https://doi.org/10.1002/sim.1593">https://doi.org/10.1002/sim.1593</a>.
</div>
<div id="ref-Ishwaran2008" class="csl-entry" role="listitem">
Ishwaran, By Hemant, Udaya B Kogalur, Eugene H Blackstone, and Michael S Lauer. 2008. <span>“<span class="nocase">Random survival forests</span>.”</span> <em>The Annals of Statistics</em> 2 (3): 841–60. <a href="https://doi.org/10.1214/08-AOAS169">https://doi.org/10.1214/08-AOAS169</a>.
</div>
<div id="ref-Ishwaran2004" class="csl-entry" role="listitem">
Ishwaran, Hemant, Eugene H Blackstone, Claire E Pothier, and Michael S Lauer. 2004. <span>“<span class="nocase">Relative Risk Forests for Exercise Heart Rate Recovery as a Predictor of Mortality</span>.”</span> <em>Journal of the American Statistical Association</em> 99 (467): 591–600. <a href="https://doi.org/10.1198/016214504000000638">https://doi.org/10.1198/016214504000000638</a>.
</div>
<div id="ref-pkgrfsrc" class="csl-entry" role="listitem">
Ishwaran, Hemant, and Udaya B Kogalur. 2018. <span>“<span class="nocase">randomForestSRC</span>.”</span> <a href="https://cran.r-project.org/package=randomForestSRC">https://cran.r-project.org/package=randomForestSRC</a>.
</div>
<div id="ref-Ishwaran2011" class="csl-entry" role="listitem">
Ishwaran, Hemant, Udaya B Kogalur, Xi Chen, and Andy J Minn. 2011. <span>“<span class="nocase">Random Survival Forests for High-Dimensional Data</span>.”</span> <em>Statistical Analysis and Data Mining</em> 4 (1): 115–32. <a href="https://doi.org/10.1002/sam">https://doi.org/10.1002/sam</a>.
</div>
<div id="ref-Hastie2013" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em><span class="nocase">An introduction to statistical learning</span></em>. Vol. 112. New York: Springer.
</div>
<div id="ref-LeBlanc1992" class="csl-entry" role="listitem">
LeBlanc, Michael, and John Crowley. 1992. <span>“<span class="nocase">Relative Risk Trees for Censored Survival Data</span>.”</span> <em>Biometrics</em> 48 (2): 411–25. <a href="https://doi.org/10.2307/2532300">https://doi.org/10.2307/2532300</a>.
</div>
<div id="ref-LeBlanc1993" class="csl-entry" role="listitem">
———. 1993. <span>“<span class="nocase">Survival Trees by Goodness of Split</span>.”</span> <em>Journal of the American Statistical Association</em> 88 (422): 457–67. <a href="https://doi.org/10.2307/2290325">https://doi.org/10.2307/2290325</a>.
</div>
<div id="ref-pkgsksurvival" class="csl-entry" role="listitem">
Pölsterl, Sebastian. 2020. <span>“<span class="nocase">scikit-survival: A Library for Time-to-Event Analysis Built on Top of scikit-learn</span>.”</span> <em>Journal of Machine Learning Research</em> 21 (212): 1–6. <a href="http://jmlr.org/papers/v21/20-729.html">http://jmlr.org/papers/v21/20-729.html</a>.
</div>
<div id="ref-Schmid2016" class="csl-entry" role="listitem">
Schmid, Matthias, Marvin Wright, and Andreas Ziegler. 2016. <span>“On the Use of Harrell’s c for Clinical Risk Prediction via Random Survival Forests.”</span> <em>Expert Systems with Applications</em> 63 (July). <a href="https://doi.org/10.1016/j.eswa.2016.07.018">https://doi.org/10.1016/j.eswa.2016.07.018</a>.
</div>
<div id="ref-Segal1988" class="csl-entry" role="listitem">
Segal, Mark Robert. 1988. <span>“<span class="nocase">Regression Trees for Censored Data</span>.”</span> <em>Biometrics</em> 44 (1): 35–47.
</div>
<div id="ref-Spooner2020" class="csl-entry" role="listitem">
Spooner, Annette, Emily Chen, Arcot Sowmya, Perminder Sachdev, Nicole A Kochan, Julian Trollor, and Henry Brodaty. 2020. <span>“<span class="nocase">A comparison of machine learning methods for survival analysis of high-dimensional clinical data for dementia prediction</span>.”</span> <em>Scientific Reports</em> 10 (1): 20410. <a href="https://doi.org/10.1038/s41598-020-77220-w">https://doi.org/10.1038/s41598-020-77220-w</a>.
</div>
<div id="ref-pkgsurvival" class="csl-entry" role="listitem">
Therneau, Terry M. 2015. <span>“<span class="nocase">A Package for Survival Analysis in S</span>.”</span> <a href="https://cran.r-project.org/package=survival">https://cran.r-project.org/package=survival</a>.
</div>
<div id="ref-Therneau1990" class="csl-entry" role="listitem">
Therneau, Terry M., Patricia M. Grambsch, and Thomas R. Fleming. 1990. <span>“<span class="nocase">Martingale-based residuals for survival models</span>.”</span> <em>Biometrika</em> 77 (1): 147–60. <a href="https://doi.org/10.1093/biomet/77.1.147">https://doi.org/10.1093/biomet/77.1.147</a>.
</div>
<div id="ref-pkgranger" class="csl-entry" role="listitem">
Wright, Marvin N., and Andreas Ziegler. 2017. <span>“<span class="nocase">ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R</span>.”</span> <em>Journal of Statistical Software</em> 77 (1): 1–17.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./P3C13_classical.html" class="pagination-link" aria-label="Classical Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Classical Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./P3C15_svm.html" class="pagination-link" aria-label="Support Vector Machines">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>All content licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> <br> © Raphael Sonabend, Andreas Bender.</p>
</div>   
    <div class="nav-footer-center">
<p><a href="https://www.mlsabook.com">Website</a> | <a href="https://github.com/mlsa-book/MLSA">GitHub</a></p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mlsa-book/MLSA/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/mlsa-book/MLSA/edit/main/book/P3C14_forests.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/mlsa-book/MLSA/blob/main/book/P3C14_forests.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>