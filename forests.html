<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Raphael Sonabend and Andreas Bender">

<title>14&nbsp; Random Forests – Machine Learning in Survival Analysis</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./svm.html" rel="next">
<link href="./classical.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./classical.html">Models</a></li><li class="breadcrumb-item"><a href="./forests.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Random Forests</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning in Survival Analysis</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/mlsa-book/MLSA/tree/main/book/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Machine-Learning-in-Survival-Analysis.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Symbols and Notation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Survival Analysis and Machine Learning</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preview.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">MLSA From Start to Finish</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./machinelearning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Machine Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./survival.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Survival Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eha.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Event-history Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./survtsk.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Survival Task</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">Evaluation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_what.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">What are Survival Measures?</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_rank.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Discrimination Measures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_calib.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Calibration Measures</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_rules.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Evaluating Distributions by Scoring Rules</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_time.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Evaluating Survival Time</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./meas_choosing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Choosing Measures</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classical.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Classical Models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./forests.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Random Forests</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./svm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./boosting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Boosting Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./neuralnetworks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./models_choosing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Choosing Models</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">Reduction Techniques</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./reductions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Reductions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./competing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Competing Risks Pipelines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./discretetime.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Discrete Time Survival Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./poisson.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Connections to Poisson Regression and Processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pseudo.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Connections to Regression and Imputation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./conclusions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Conclusions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exercises.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exercises</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#random-forests-for-regression" id="toc-random-forests-for-regression" class="nav-link active" data-scroll-target="#random-forests-for-regression"><span class="header-section-number">14.1</span> Random Forests for Regression</a></li>
  <li><a href="#random-survival-forests" id="toc-random-survival-forests" class="nav-link" data-scroll-target="#random-survival-forests"><span class="header-section-number">14.2</span> Random Survival Forests</a>
  <ul class="collapse">
  <li><a href="#splitting-rules-1" id="toc-splitting-rules-1" class="nav-link" data-scroll-target="#splitting-rules-1"><span class="header-section-number">14.2.1</span> Splitting Rules</a></li>
  <li><a href="#sec-surv-ml-models-ranfor-nodes" id="toc-sec-surv-ml-models-ranfor-nodes" class="nav-link" data-scroll-target="#sec-surv-ml-models-ranfor-nodes"><span class="header-section-number">14.2.2</span> Terminal Node Prediction</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">14.3</span> Conclusion</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/mlsa-book/MLSA/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/mlsa-book/MLSA/edit/main/book/forests.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/mlsa-book/MLSA/blob/main/book/forests.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./classical.html">Models</a></li><li class="breadcrumb-item"><a href="./forests.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Random Forests</span></a></li></ol></nav>
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-surv-ml-models-ranfor" class="quarto-section-identifier"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Random Forests</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    TODO (150-200 WORDS)
  </div>
</div>


</header>


<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Major changes expected!
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>This page is a work in progress and major changes will be made over time.</strong></p>
</div>
</div>
<section id="random-forests-for-regression" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="random-forests-for-regression"><span class="header-section-number">14.1</span> Random Forests for Regression</h2>
<p>Random forests are a composite algorithm built by fitting many simpler component models, decision trees, and then averaging the results of predictions from these trees. Decision trees are first briefly introduced before the key ‘bagging’ algorithm, which creates the random forest. Woodland terminology is used throughout this chapter.</p>
<section id="decision-trees" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="decision-trees">Decision Trees</h3>
<p><em>Decision trees</em> are a common model class in machine learning and have the advantage of being (relatively) simple to implement and highly interpretable. A decision tree takes a set of inputs and a given <em>splitting rule</em> in order to create a series of splits, or branches, in the tree that culminates in a final <em>leaf</em>, or <em>terminal node</em>. Each terminal node has a corresponding prediction, which for regression is usually the sample mean of the training outcome data. This is made clearer by example, (<a href="#fig-surv-ranfor" class="quarto-xref">Figure&nbsp;<span>14.1</span></a>) demonstrates a decision tree predicting the miles per gallon (<code>mpg</code>) of a car from the <code>mtcars</code> <span class="citation" data-cites="datamtcars">(<a href="references.html#ref-datamtcars" role="doc-biblioref">Henderson and Velleman 1981</a>)</span> dataset. With this tree a new prediction is made by feeding the input variables from the top to the bottom, for example given new data, <span class="math inline">\(x = \{`wt` = 3, `disp` = 250\}\)</span>, then in the first split the right branch is taken as <code>wt</code> <span class="math inline">\(= 3 &gt; 2.32\)</span> and in the second split the left branch is taken as <code>disp</code> <span class="math inline">\(= 250 \leq 258\)</span>, therefore the new data point ‘lands’ in the final leaf and is predicted to have an <code>mpg</code> of <span class="math inline">\(20.8\)</span>. This value of <span class="math inline">\(20.8\)</span> arises as the sample mean of <code>mpg</code> for the <span class="math inline">\(11\)</span> (which can be seen in the box) observations in the training data who were sorted into this terminal node. Algorithmically, as splits are always binary, predictions are simply a series of conditional logical statements.</p>
<div id="fig-surv-ranfor" class="quarto-float quarto-figure quarto-figure-center anchored" alt="TODO">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-surv-ranfor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Figures/forests/iris_tree.png" class="img-fluid figure-img" alt="TODO">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-surv-ranfor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14.1: Demonstrating classification trees using the <code>mtcars</code> <span class="citation" data-cites="datamtcars">(<a href="references.html#ref-datamtcars" role="doc-biblioref">Henderson and Velleman 1981</a>)</span> dataset and the <span class="math inline">\(\textbf{party}\)</span> <span class="citation" data-cites="pkgparty">(<a href="references.html#ref-pkgparty" role="doc-biblioref">Hothorn, Hornik, and Zeileis 2006</a>)</span> package. Ovals are leaves, which indicate the variable that is being split. Edges are branches, which indicate the cut-off at which the variable is split. Rectangles are terminal nodes and include information about the number of training observations in the node and the terminal node prediction.
</figcaption>
</figure>
</div>
</section>
<section id="splitting-rules" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="splitting-rules">Splitting Rules</h3>
<p>Precisely how the splits are derived and which variables are utilised is determined by the splitting rule. In regression, the most common splitting rule is to select the cut-off for a given variable that minimises the mean squared error in each hypothetical resultant leaf. The goal is to find the variable and cutoff that leads to the greatest difference between the two resultant leaves and thus the maximal homogeneity within each leaf. For all decision tree and random forest algorithms going forward, let <span class="math inline">\(L\)</span> denote some leaf, then let <span class="math inline">\(L_{xy}, L_x, L_y\)</span> respectively be the set of observations, features, and outcomes in leaf <span class="math inline">\(L\)</span>. Let <span class="math inline">\(L_{y;i}\)</span> be the <span class="math inline">\(i\)</span>th outcome in <span class="math inline">\(L_y\)</span> and finally let <span class="math inline">\(L_{\bar{y}} = \frac{1}{n} \sum^{n}_{i = 1} L_{y;i}\)</span>. To simplify notation, <span class="math inline">\(i \in L\)</span> is taken to be equivalent to <span class="math inline">\(i \in \{i: X_i \in L_X\}\)</span>, i.e.&nbsp;the indices of the observations in leaf <span class="math inline">\(L\)</span>.</p>
<p>Let <span class="math inline">\(c \in \mathbb{R}\)</span> be some cutoff parameter and let <span class="math inline">\(L^a_{xy}(j,c) := \{(X_i,Y_i)|X_{ij} &lt; c, i = 1,...,n\}, L^b_{xy}(j,c) = \{(X_i,Y_i)|X_{ij} \geq c, i = 1,...,n\}\)</span> be the two leaves containing the set of observations resulting from partitioning variable <span class="math inline">\(j\)</span> at cutoff <span class="math inline">\(c\)</span>. Then a split is determined by finding the arguments, <span class="math inline">\((j^*,c^*)\)</span> that minimise the sum of the mean squared errors (MSE) in both leaves <span class="citation" data-cites="Hastie2013">(<a href="references.html#ref-Hastie2013" role="doc-biblioref">James et al. 2013</a>)</span>, <span id="eq-dt-min"><span class="math display">\[
(j^*, c^*) = \mathop{\mathrm{arg\,min}}_{j, c} \sum_{y \in L^a_{y}(j,c)} (y - L^a_{\bar{Y}}(j,c))^2 + \sum_{y \in L^b_{y}(j,c)} (y - L^b_{\bar{Y}}(j,c))^2
\tag{14.1}\]</span></span></p>
<p>This method is repeated from the first branch of the tree down to the very last such that observations are included in a given leaf <span class="math inline">\(L\)</span> if they satisfy all conditions from all previous branches; features may be considered multiple times in the growing process. This is an intuitive method as minimising the above sum results in the set of observations within each individual leaf being as similar as possible, thus as an observation is passed down the tree, it becomes more similar to the subsequent leaves, eventually landing in a leaf containing homogeneous observations. Controlling how many variables to consider at each split and how many splits to make are determined by hyper-parameter tuning.</p>
<p>Decision trees are a powerful method for high-dimensional data as only a small sample of variables will be used for growing a tree, and therefore they are also useful for variable importance by identifying which variables were utilised in growth (other importance methods are also available). Decision trees are also highly interpretable, as demonstrated by (<a href="#fig-surv-ranfor" class="quarto-xref">Figure&nbsp;<span>14.1</span></a>). The recursive pseudo-algorithm in (<span class="citation" data-cites="alg-dt-fit">(<a href="references.html#ref-alg-dt-fit" role="doc-biblioref"><strong>alg-dt-fit?</strong></a>)</span>) demonstrates the simplicity in growing a decision tree (again methods such as pruning are omitted).</p>
<!-- \begin{algorithm}
\caption{Fitting a decision tree. \\
**Input** Training data, $\dtrain$. Splitting rule, $SR$. \\
**Output** Fitted decision tree, $\hatg$.}
\begin{algorithmic}[1]
\State Compute $(j^*, c^*)$ as the optimisers of $SR$ (e.g. (@eq-dt-min)) to create the initial leaf and branches.
\State Repeat step 1 on all subsequent branches until a stopping rule is reached.
\State Return the fitted tree, $\hatg$, as the series of branches.
\end{algorithmic}
\end{algorithm} -->
<!-- {#alg-dt-fit} -->
</section>
<section id="stopping-rules" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="stopping-rules">Stopping Rules</h3>
<p>The ‘stopping rule’ in (<span class="citation" data-cites="alg-dt-fit">(<a href="references.html#ref-alg-dt-fit" role="doc-biblioref"><strong>alg-dt-fit?</strong></a>)</span>) is usually a condition on the number of observations in each leaf such that leaves will continue to be split until some minimum number of observations has been reached in a leaf. Other conditions may be on the ‘depth’ of the tree, which corresponds to the number of levels of splitting, for example the tree in (<a href="#fig-surv-ranfor" class="quarto-xref">Figure&nbsp;<span>14.1</span></a>) has a depth of <span class="math inline">\(2\)</span> (the first level is not counted).</p>
</section>
<section id="random-forests" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="random-forests">Random Forests</h3>
<p>Despite being more interpretable than other machine learning methods, decision trees usually have poor predictive performance, high variance and are not robust to changes in the data. As such, <em>random forests</em> are preferred to improve prediction accuracy and decrease variance. Random forests utilise bootstrap aggregation, or <em>bagging</em> <span class="citation" data-cites="Breiman1996a">(<a href="references.html#ref-Breiman1996a" role="doc-biblioref">Breiman 1996</a>)</span>, to aggregate many decision trees. A pseudo fitting algorithm is given in (<span class="citation" data-cites="alg-rsf-fit">(<a href="references.html#ref-alg-rsf-fit" role="doc-biblioref"><strong>alg-rsf-fit?</strong></a>)</span>).</p>
<!-- \begin{algorithm}
\caption{Fitting a random forest. \\
**Input** Training data, $\dtrain$. Total number of trees, $B \in \PNaturals$. \\
**Output** Fitted random forest, $\hatg$.}
\begin{algorithmic}[1]
\For{$b = 1,...,B$}
\State Create a bootstrapped sample of the data, $D_b$.
\State Grow a decision tree, $\hatg_b$, on $D_b$ with (@alg-dt-fit).
\EndFor
\State $\hatg \gets \{\hatg_b\}^B_{b=1}$
\Return $\hatg$
\end{algorithmic}
\end{algorithm} -->
<!-- {#alg-rsf-fit} -->
<p>Prediction from a random forest follows by making predictions from the individual trees and aggregating the results by some function <span class="math inline">\(\sigma\)</span> (<span class="citation" data-cites="alg-rsf-pred">(<a href="references.html#ref-alg-rsf-pred" role="doc-biblioref"><strong>alg-rsf-pred?</strong></a>)</span>); <span class="math inline">\(\sigma\)</span> is usually the sample mean for regression,</p>
<p><span class="math display">\[
\hat{g}(X^*) = \sigma(\hat{g}_1(X^*),...,\hat{g}_B(X^*)) = \frac{1}{B} \sum^B_{b=1} \hat{g}_b(X^*)
\]</span></p>
<p>where <span class="math inline">\(\hat{g}_b(X^*)\)</span> is the terminal node prediction from the <span class="math inline">\(b\)</span>th tree and <span class="math inline">\(B\)</span> are the total number of grown trees (<code>$B$' is commonly used instead of</code><span class="math inline">\(N\)</span>’ to note the relation to bootstrapped data).</p>
<!-- \begin{algorithm}
\caption{Predicting from a random forest. \\
**Input** Testing data $X^* \sim \calX$, fitted forest $\hatg$ with $B \in \PNaturals$ trees, aggregation method $\sigma$. \\
**Output** Prediction, $\hatY \sim \calY$.}
\begin{algorithmic}[1]
\For{$b = 1,...,B$}
\State 'Drop' $X^*$ down the tree $\hatg_b$ individually to return a prediction $\hatg_b(X^*)$.
\EndFor
\State $\hatY \gets \sigma(\hatg_1(X^*),...,\hatg_B(X^*))$
\Return $\hatY$
\end{algorithmic}
\end{algorithm} -->
<!-- {#alg-rsf-pred} -->
<p>Usually many (hundreds or thousands) trees are grown, which makes random forests robust to changes in data and ‘confident’ about individual predictions. Other advantages include having several tunable hyper-parameters, including: the number of trees to grow, the number of variables to include in a single tree, the splitting rule, and the minimum terminal node size. Machine learning models with many hyper-parameters, tend to perform better than other models as they can be fine-tuned to the data, which is why complex deep learning models are often the best performing. Although as a caveat: too many parameters can lead to over-fitting and tuning many parameters can take a long time and be highly intensive. Random forests lose the interpretability of decision trees and are considered ‘black-box’ models as individual predictions cannot be easily scrutinised.</p>
</section>
</section>
<section id="random-survival-forests" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="random-survival-forests"><span class="header-section-number">14.2</span> Random Survival Forests</h2>
<p>Unlike other machine learning methods that may require complex changes to underlying algorithms, random forests can be relatively simply adapted to <em>random survival forests</em> by updating the splitting rules and terminal node predictions to those that can handle censoring and can make survival predictions. This chapter is therefore focused on outlining different choices of splitting rules and terminal node predictions, which can then be flexibly combined into different models.</p>
<section id="splitting-rules-1" class="level3" data-number="14.2.1">
<h3 data-number="14.2.1" class="anchored" data-anchor-id="splitting-rules-1"><span class="header-section-number">14.2.1</span> Splitting Rules</h3>
<p>Survival trees and RSFs have been studied for the past four decades and whilst the amount of splitting rules to appear could be considered `‘numerous’’ <span class="citation" data-cites="Bou-Hamad2011">(<a href="references.html#ref-Bou-Hamad2011" role="doc-biblioref">Bou-Hamad, Larocque, and Ben-Ameur 2011</a>)</span>, only two broad classes are commonly utilised and implemented <span class="citation" data-cites="pkgrfsrc pkgsksurvival pkgrpart pkgranger">(<a href="references.html#ref-pkgrfsrc" role="doc-biblioref">H. Ishwaran and Kogalur 2018</a>; <a href="references.html#ref-pkgsksurvival" role="doc-biblioref">Pölsterl 2020</a>; <a href="references.html#ref-pkgrpart" role="doc-biblioref">Therneau and Atkinson 2019</a>; <a href="references.html#ref-pkgranger" role="doc-biblioref">Wright and Ziegler 2017</a>)</span>. The first class rely on hypothesis tests, and primarily the log-rank test, to maximise dissimilarity between splits, the second class utilises likelihood-based measures. The first is discussed in more detail as this is common in practice and is relatively straightforward to implement and understand, moreover it has been demonstrated to outperform other splitting rules <span class="citation" data-cites="Bou-Hamad2011">(<a href="references.html#ref-Bou-Hamad2011" role="doc-biblioref">Bou-Hamad, Larocque, and Ben-Ameur 2011</a>)</span>. Likelihood rules are more complex and require assumptions that may not be realistic, these are discussed briefly.</p>
</section>
<section id="hypothesis-tests" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="hypothesis-tests">Hypothesis Tests</h3>
<p>The log-rank test statistic has been widely utilised as the ‘natural’ splitting-rule for survival analysis <span class="citation" data-cites="Ciampi1986 Ishwaran2008 LeBlanc1993 Segal1988">(<a href="references.html#ref-Ciampi1986" role="doc-biblioref">Ciampi et al. 1986</a>; <a href="references.html#ref-Ishwaran2008" role="doc-biblioref">B. H. Ishwaran et al. 2008</a>; <a href="references.html#ref-LeBlanc1993" role="doc-biblioref">LeBlanc and Crowley 1993</a>; <a href="references.html#ref-Segal1988" role="doc-biblioref">Segal 1988</a>)</span>. The log-rank test compares the survival distributions of two groups and has the null-hypothesis that both groups have the same underlying risk of (immediate) death, i.e.&nbsp;identical hazard functions.</p>
<p>Let <span class="math inline">\(L^A\)</span> and <span class="math inline">\(L^B\)</span> be two leaves then using the notation above let <span class="math inline">\(h^A,h^B\)</span> be the (true) hazard functions derived from the observations in the two leaves respectively. The log-rank hypothesis test is given by <span class="math inline">\(H_0: h^A = h^B\)</span> with test statistic <span class="citation" data-cites="Segal1988">(<a href="references.html#ref-Segal1988" role="doc-biblioref">Segal 1988</a>)</span>, <span class="math display">\[
LR(L^A) = \frac{\sum_{\tau \in \mathcal{U}_D} (d^A_{\tau} - e^A_{\tau})}{\sqrt{\sum_{\tau \in \mathcal{U}_D} v_\tau^A}}
\]</span></p>
<p>where <span class="math inline">\(d^A_{\tau}\)</span> is the observed number of deaths in leaf <span class="math inline">\(A\)</span> at <span class="math inline">\(\tau\)</span>, <span class="math display">\[
d^A_{\tau} := \sum_{i \in L^A} \mathbb{I}(T_i = \tau, \Delta_i = 1)
\]</span></p>
<p><span class="math inline">\(e^A_{\tau}\)</span> is the expected number of deaths in leaf <span class="math inline">\(A\)</span> at <span class="math inline">\(\tau\)</span>, <span class="math display">\[
e^A_{\tau} := \frac{n_\tau^A d_\tau}{n_\tau}
\]</span></p>
<p>and <span class="math inline">\(v^A_\tau\)</span> is the variance of the number of deaths in leaf <span class="math inline">\(A\)</span> at <span class="math inline">\(\tau\)</span>, <span class="math display">\[
v^A_{\tau} := e^A_{\tau} \Big(\frac{n_\tau - d_\tau}{n_\tau}\Big)\Big(\frac{n_\tau - n^A_\tau}{n_\tau - 1}\Big)
\]</span></p>
<p>where <span class="math inline">\(\mathcal{U}_D\)</span> is the set of unique death times across the data (in both leaves), \ <span class="math inline">\(n_\tau = \sum_i \mathbb{I}(T_i \geq \tau)\)</span> is the number of observations at risk at <span class="math inline">\(\tau\)</span> in both leaves, \ <span class="math inline">\(n_\tau^A = \sum_{i \in L^A} \mathbb{I}(T_i \geq \tau)\)</span> is the number of observations at risk at <span class="math inline">\(\tau\)</span> in leaf A, and \ <span class="math inline">\(d_\tau = \sum_i \mathbb{I}(T_i = \tau, \Delta_i = 1)\)</span> is the number of deaths at <span class="math inline">\(\tau\)</span> in both leaves.</p>
<p>Intuitively these results follow as the number of deaths in a leaf is distributed according to <span class="math inline">\(\operatorname{Hyper}(n^A_\tau,n_\tau,d_\tau)\)</span>. The same statistic results if <span class="math inline">\(L^B\)</span> is instead considered. (<span class="citation" data-cites="alg-dt-fit">(<a href="references.html#ref-alg-dt-fit" role="doc-biblioref"><strong>alg-dt-fit?</strong></a>)</span>) follows for fitting decision trees with the log-rank splitting rule, <span class="math inline">\(SR\)</span>, to be maximised.</p>
<p>The higher the log-rank statistic, the greater the dissimilarity between the two groups, thereby making it a sensible splitting rule for survival, moreover it has been shown that it works well for splitting censored data <span class="citation" data-cites="LeBlanc1993">(<a href="references.html#ref-LeBlanc1993" role="doc-biblioref">LeBlanc and Crowley 1993</a>)</span>. When censoring is highly dependent on the outcome, the log-rank statistic does not perform well and is biased <span class="citation" data-cites="Bland2004">(<a href="references.html#ref-Bland2004" role="doc-biblioref">Bland and Altman 2004</a>)</span>, which tends to be true of the majority of survival models. Additionally, the log-rank test requires no knowledge about the shape of the survival curves or distribution of the outcomes in either group <span class="citation" data-cites="Bland2004">(<a href="references.html#ref-Bland2004" role="doc-biblioref">Bland and Altman 2004</a>)</span>, making it ideal for an automated process that requires no user intervention.</p>
<p>The log-rank <em>score</em> rule <span class="citation" data-cites="Hothorn2003">(<a href="references.html#ref-Hothorn2003" role="doc-biblioref">Hothorn and Lausen 2003</a>)</span> is a standardized version of the log-rank rule that could be considered as a splitting rule, though simulation studies have demonstrated non-significant predictive performance when comparing the two <span class="citation" data-cites="Ishwaran2008">(<a href="references.html#ref-Ishwaran2008" role="doc-biblioref">B. H. Ishwaran et al. 2008</a>)</span>.</p>
<p>Alternative dissimiliarity measures and tests have also been suggested as splitting rules, including modified Kolmogorov-Smirnov test and Gehan-Wilcoxon tests <span class="citation" data-cites="Ciampi1988">(<a href="references.html#ref-Ciampi1988" role="doc-biblioref">Ciampi et al. 1988</a>)</span>. Simulation studies have demonstrated that both of these may have higher power and produce ‘better’ results than the log-rank statistic <span class="citation" data-cites="Fleming1980">(<a href="references.html#ref-Fleming1980" role="doc-biblioref">Fleming et al. 1980</a>)</span>. Despite this, these do not appear to be in common usage and no implementation could be found that include these.</p>
<p>### Likelihood Based Rules {.unnumbered .unlisted} Likelihood ratio statistics, or deviance based splitting rules, assume a certain model form and thereby an assumption about the data. This may be viewed as an advantageous strategy, as it could arguably increase interpretability, or a disadvantage as it places restrictions on the data. For survival models, a full-likelihood can be estimated with a Cox form by estimating the cumulative hazard function <span class="citation" data-cites="LeBlanc1992">(<a href="references.html#ref-LeBlanc1992" role="doc-biblioref">LeBlanc and Crowley 1992</a>)</span>. LeBlanc and Crowley (1992) <span class="citation" data-cites="LeBlanc1992">(<a href="references.html#ref-LeBlanc1992" role="doc-biblioref">LeBlanc and Crowley 1992</a>)</span> advocate for selecting the optimal split by maximising the full PH likelihood, assuming the cumulative hazard function, <span class="math inline">\(H\)</span>, is known, <span class="math display">\[
\mathcal{L}:= \prod_{m = 1}^M \prod_{i \in L^m} h_m(T_i)^{\Delta_i} \exp(-H_m(T_i))
\]</span></p>
<p>where <span class="math inline">\(M\)</span> is the total number of terminal nodes, <span class="math inline">\(h_m\)</span> and <span class="math inline">\(H_m\)</span> are the (true) hazard and cumulative hazard functions in the <span class="math inline">\(m\)</span>th node, and again <span class="math inline">\(L^m\)</span> is the set of observations in terminal node <span class="math inline">\(m\)</span>. Estimation of <span class="math inline">\(h_m\)</span> and <span class="math inline">\(H_m\)</span> are described with the associated terminal node prediction below.</p>
<p>The primary advantage of this method is that any off-shelf regression software with a likelihood splitting rule can be utilised without any further adaptation to model fitting by supplying this likelihood with required estimates. However the additional costs of computing these estimates may outweigh the benefits once the likelihood has been calculated, and this could be why only one implementation of this method has been found <span class="citation" data-cites="Bou-Hamad2011 pkgrpart">(<a href="references.html#ref-Bou-Hamad2011" role="doc-biblioref">Bou-Hamad, Larocque, and Ben-Ameur 2011</a>; <a href="references.html#ref-pkgrpart" role="doc-biblioref">Therneau and Atkinson 2019</a>)</span>.</p>
</section>
<section id="other-splitting-rules" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="other-splitting-rules">Other Splitting Rules</h3>
<p>As well as likelihood and log-rank spitting rules, other papers have studied comparison of residuals <span class="citation" data-cites="Therneau1990">(<a href="references.html#ref-Therneau1990" role="doc-biblioref">Therneau, Grambsch, and Fleming 1990</a>)</span>, scoring rules <span class="citation" data-cites="pkgrfsrc">(<a href="references.html#ref-pkgrfsrc" role="doc-biblioref">H. Ishwaran and Kogalur 2018</a>)</span>, and distance metrics <span class="citation" data-cites="Gordon1985">(<a href="references.html#ref-Gordon1985" role="doc-biblioref">Gordon and Olshen 1985</a>)</span>. These splitting rules work similarly to the mean squared error in the regression setting, in which the score should be minimised across both leaves. The choice of splitting rule is usually data-dependent and can be treated as a hyper-parameter for tuning. However, if there is a clear goal in prediction, then the choice of splitting rule can be informed by the prediction type. For example, if the goal is to maximise separation, then a log-rank splitting rule to maximise homogeneity in terminal nodes is a natural starting point. Whereas if the goal is to estimate the linear predictor of a Cox PH model, then a likelihood splitting rule with a Cox form may be more sensible.</p>
</section>
<section id="sec-surv-ml-models-ranfor-nodes" class="level3" data-number="14.2.2">
<h3 data-number="14.2.2" class="anchored" data-anchor-id="sec-surv-ml-models-ranfor-nodes"><span class="header-section-number">14.2.2</span> Terminal Node Prediction</h3>
<p>Only two terminal node predictions appear in common usage.</p>
</section>
<section id="predict-ranking" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="predict-ranking">Predict: Ranking</h3>
<p>Terminal node ranking predictions for survival trees and forests have been limited to those that use a likelihood-based splitting rule and assume a PH model form <span class="citation" data-cites="Ishwaran2004 LeBlanc1992">(<a href="references.html#ref-Ishwaran2004" role="doc-biblioref">H. Ishwaran et al. 2004</a>; <a href="references.html#ref-LeBlanc1992" role="doc-biblioref">LeBlanc and Crowley 1992</a>)</span>. In model fitting the likelihood splitting rule model attempts to fit the (theoretical) PH model <span class="math inline">\(h_m(\tau) = h_0(\tau)\theta_m\)</span> for <span class="math inline">\(m \in 1,...,M\)</span> where <span class="math inline">\(M\)</span> is the total number of terminal nodes and <span class="math inline">\(\theta_m\)</span> is a parameter to estimate. The model returns predictions for <span class="math inline">\(\exp(\hat{\theta}_m)\)</span> where <span class="math inline">\(\hat{\theta}_m\)</span> is the estimate of <span class="math inline">\(\theta_m\)</span>. This is estimated via an iterative procedure in which in iteration <span class="math inline">\(j+1\)</span>, <span class="math inline">\(\hat{\theta}_m^{j+1}\)</span> is estimated by <span class="math display">\[
\hat{\theta}_m^{j+1} = \frac{\sum_{i \in L^m} \Delta_i}{\sum_{i \in L^m} \hat{H}^j_0(T_i)}
\]</span></p>
<p>where as before <span class="math inline">\(L^m\)</span> is the set of observations in leaf <span class="math inline">\(m\)</span> and <span class="math display">\[
\hat{H}^{j}_0(\tau) = \frac{\sum_{i:T_i \leq \tau} \Delta_i}{\sum_{m = 1}^M\sum_{\{i:i \in \mathcal{R}_\tau \cap L^a\}} \hat{\theta}^{j}_m}
\]</span></p>
<p>which is repeated until some stopping criterion is reached. The same cumulative hazard is estimated for all nodes however <span class="math inline">\(\hat{\theta}_m\)</span> varies across nodes. This method lends itself naturally to a composition to a full distribution (<a href="reductions.html" class="quarto-xref"><span>Chapter 19</span></a>) as it assumes a PH form and separately estimates the cumulative hazard and relative risk (<span class="quarto-unresolved-ref">?sec-surv-ml-models-ranfor-nov</span>), though no implementation of this composition could be found.</p>
</section>
<section id="predict-survival-distribution" class="level3 unnumbered unlisted">
<h3 class="unnumbered unlisted anchored" data-anchor-id="predict-survival-distribution">Predict: Survival Distribution</h3>
<p>The most common terminal node prediction appears to be predicting the survival distribution by estimating the survival function, using the Kaplan-Meier or Nelson-Aalen estimators, on the sample in the terminal node <span class="citation" data-cites="Hothorn2004 Ishwaran2008 LeBlanc1993 Segal1988">(<a href="references.html#ref-Hothorn2004" role="doc-biblioref">Hothorn et al. 2004</a>; <a href="references.html#ref-Ishwaran2008" role="doc-biblioref">B. H. Ishwaran et al. 2008</a>; <a href="references.html#ref-LeBlanc1993" role="doc-biblioref">LeBlanc and Crowley 1993</a>; <a href="references.html#ref-Segal1988" role="doc-biblioref">Segal 1988</a>)</span>. Estimating a survival function by a non-parametric estimator is a natural choice for terminal node prediction as these are natural ‘baselines’ in survival, similarly to taking the sample mean in regression. The prediction for SDTs is straightforward, the non-parametric estimator is fit on all observations in each of the terminal nodes. This is adapted to RSFs by bagging the estimator across all decision trees <span class="citation" data-cites="Hothorn2004">(<a href="references.html#ref-Hothorn2004" role="doc-biblioref">Hothorn et al. 2004</a>)</span>. Using the Nelson-Aalen estimator as an example, let <span class="math inline">\(m\)</span> be a terminal node in an SDT, then the terminal node prediction is given by,</p>
<p><span id="eq-surv-nelson"><span class="math display">\[
\hat{H}_m(\tau) = \sum_{\{i: i \in L^m \cap T_i \leq \tau\}} \frac{d_i}{n_i}
\tag{14.2}\]</span></span> where <span class="math inline">\(d_i\)</span> and <span class="math inline">\(n_i\)</span> are the number of events and observations at risk at time <span class="math inline">\(T_i\)</span> in terminal node <span class="math inline">\(m\)</span>. Ishwaran <span class="citation" data-cites="Ishwaran2008">(<a href="references.html#ref-Ishwaran2008" role="doc-biblioref">B. H. Ishwaran et al. 2008</a>)</span> defined the bootstrapped Nelson-Aalen estimator as</p>
<p><span id="eq-surv-nelson-boot"><span class="math display">\[
\hat{H}_{Boot}(\tau) = \frac{1}{B} \sum^B_{b=1} \hat{H}_{m, b}(\tau), \quad m \in 1,...,M
\tag{14.3}\]</span></span></p>
<p>where <span class="math inline">\(B\)</span> is the total number of bootstrapped estimators, <span class="math inline">\(M\)</span> is the number of terminal nodes, and <span class="math inline">\(\hat{H}_{m,b}\)</span> is the cumulative hazard for the <span class="math inline">\(m\)</span>th terminal node in the <span class="math inline">\(b\)</span>th tree. The bootstrapped Kaplan-Meier estimator is calculated analogously. More generally these can be considered as a uniform mixture of <span class="math inline">\(B\)</span> distributions (<a href="reductions.html" class="quarto-xref"><span>Chapter 19</span></a>).</p>
<p>All implemented RSFs can now be summarised into the following five algorithms:</p>
<p><strong>RRT</strong> {#mod-rrt}\ LeBlanc and Crowley’s (1992) <span class="citation" data-cites="LeBlanc1992">(<a href="references.html#ref-LeBlanc1992" role="doc-biblioref">LeBlanc and Crowley 1992</a>)</span> survival decision tree uses a deviance splitting rule with a terminal node ranking prediction, which assumes a PH model form. These ‘relative risk trees’ (RRTs) are implemented in the package <span class="math inline">\(\textbf{rpart}\)</span> <span class="citation" data-cites="pkgrpart">(<a href="references.html#ref-pkgrpart" role="doc-biblioref">Therneau and Atkinson 2019</a>)</span>. This model is considered the least accessible and transparent of all discussed in this section as: few implementations exist, it requires assumptions that may not be realistic, and predictions are harder to interpret than other models. Predictive performance of the model is expected to be worse than RSFs as this is a decision tree; this is confirmed in <span class="citation" data-cites="Sonabend2021b">(<a href="references.html#ref-Sonabend2021b" role="doc-biblioref">Sonabend 2021</a>)</span>.</p>
<p><strong>RRF</strong> {#mod-rrf}\ Ishwaran <span class="math inline">\(\textit{et al.}\)</span> (2004) <span class="citation" data-cites="Ishwaran2004">(<a href="references.html#ref-Ishwaran2004" role="doc-biblioref">H. Ishwaran et al. 2004</a>)</span> proposed a random forest framework for the relative risk trees, which makes a slight adaptation and applies the iteration of the terminal node prediction after the tree is grown as opposed to during the growing process. No implementation for these ‘relative risk forests’ (RRFs) could be found or any usage in the literature.</p>
<p><strong>RSDF-DEV</strong> {#mod-rsdfdev}\ Hothorn <span class="math inline">\(\textit{et al.}\)</span> (2004) <span class="citation" data-cites="Hothorn2004">(<a href="references.html#ref-Hothorn2004" role="doc-biblioref">Hothorn et al. 2004</a>)</span> expanded upon the RRT by introducing a bagging composition thus creating a random forest with a deviance splitting rule, again assuming a PH form. However the ranking prediction is altered to be a bootstrapped Kaplan-Meier prediction in the terminal node. This is implemented in <span class="math inline">\(\textbf{ipred}\)</span> <span class="citation" data-cites="pkgipred">(<a href="references.html#ref-pkgipred" role="doc-biblioref">Peters and Hothorn 2019</a>)</span>. This model improves upon the accessibility and transparency of the RRT by providing a more straightforward and interpretable terminal node prediction. However, as this is a decision tree, predictive performance is again expected to be worse than the RSFs.</p>
<p><strong>RSCIFF</strong> {#mod-rsciff}\ Hothorn <span class="math inline">\(\textit{et al.}\)</span> <span class="citation" data-cites="Hothorn2005">(<a href="references.html#ref-Hothorn2005" role="doc-biblioref">Hothorn et al. 2005</a>)</span> studied a conditional inference framework in order to predict log-survival time. In this case the splitting rule is based on an IPC weighted loss function, which allows implementation by off-shelf classical random forests. The terminal node predictions are a weighted average of the log-survival times in the node where weighting is determined by the Kaplan-Meier estimate of the censoring distribution. This ‘random survival conditional inference framework forest’ (RSCIFF) is implemented in <span class="math inline">\(\textbf{party}\)</span> <span class="citation" data-cites="pkgparty">(<a href="references.html#ref-pkgparty" role="doc-biblioref">Hothorn, Hornik, and Zeileis 2006</a>)</span> and <span class="math inline">\(\textbf{partykit}\)</span> <span class="citation" data-cites="pkgpartykit">(<a href="references.html#ref-pkgpartykit" role="doc-biblioref">Hothorn and Zeileis 2015</a>)</span>, which additionally includes a distribution terminal node prediction via the bootstrapped Kaplan-Meier estimator. The survival tree analogue (SDCIFT) is implemented in the same packages. Implementation of the RSCIFF is complex, which is likely why all implementations (in the above packages) are by the same authors. The complexity of conditional inference forests may also be the reason why several reviews, including this one, mention (or completely omit) RSCIFFs but do not include any comprehensive details that explain the fitting procedure <span class="citation" data-cites="Bou-Hamad2011 Wang2017a">(<a href="references.html#ref-Bou-Hamad2011" role="doc-biblioref">Bou-Hamad, Larocque, and Ben-Ameur 2011</a>; <a href="references.html#ref-Wang2017a" role="doc-biblioref">Wang and Li 2017</a>)</span>. In this regard, it is hard to claim that RSCIFFs are transparent or accessible. Moreover, the authors of the model state that random conditional inference forests are for `‘expert user[s] only and [their] current state is rather experimental’’ <span class="citation" data-cites="pkgpartykit">(<a href="references.html#ref-pkgpartykit" role="doc-biblioref">Hothorn and Zeileis 2015</a>)</span>. Finally, with respect to model performance, there is evidence that they can outperform RSDFs (below) dependent on the data type <span class="citation" data-cites="Nasejje2017">(<a href="references.html#ref-Nasejje2017" role="doc-biblioref">Nasejje et al. 2017</a>)</span> however no benchmark experiment could be found that compared them to other models.</p>
<p><strong>RSDF-STAT</strong> {#mod-rsdfstat}\ Finally Ishwaran <span class="math inline">\(\textit{et al.}\)</span> (2008) <span class="citation" data-cites="Ishwaran2008">(<a href="references.html#ref-Ishwaran2008" role="doc-biblioref">B. H. Ishwaran et al. 2008</a>)</span> proposed the most general form of RSFs with a choice of hypothesis tests (log-rank and log-rank score) and survival measure (Brier, concordance) splitting rules, and a bootstrapped Nelson-Aalen terminal node prediction. These are implemented in <span class="math inline">\(\textbf{randomForestSRC}\)</span> <span class="citation" data-cites="pkgrfsrc">(<a href="references.html#ref-pkgrfsrc" role="doc-biblioref">H. Ishwaran and Kogalur 2018</a>)</span> and <span class="math inline">\(\textbf{ranger}\)</span> <span class="citation" data-cites="pkgranger">(<a href="references.html#ref-pkgranger" role="doc-biblioref">Wright and Ziegler 2017</a>)</span>. There are several implementations of these models across programming languages, and extensive details for the fitting and predicting procedures, which makes them very accessible. The models utilise a standard random forest framework, which makes them transparent and familiar to those without expert Survival knowledge. Moreover they have been proven to perform well in benchmark experiments, especially on high-dimensional data <span class="citation" data-cites="Herrmann2020 Spooner2020">(<a href="references.html#ref-Herrmann2020" role="doc-biblioref">Herrmann et al. 2021</a>; <a href="references.html#ref-Spooner2020" role="doc-biblioref">Spooner et al. 2020</a>)</span>.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">14.3</span> Conclusion</h2>
<div class="callout callout-style-default callout-warning no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key takeaways
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Random forests are a highly flexible algorithm that allow the various components to be adapted and altered without major changes to the underlying algorithm;</li>
<li>This means that random survival forests (RSFs) are readily available ‘off-shelf’ in many open-source packages;</li>
<li>RSFs have been demonstrated to perform well on high-dimensional data, routinely outperforming other models. <!-- fixme: add citations above --></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-important no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Limitations
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li></li>
</ul>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Further reading
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>A comprehensive review of random survival forests (RSFs) is provided in Bou-Hamad (2011) <span class="citation" data-cites="Bou-Hamad2011">(<a href="references.html#ref-Bou-Hamad2011" role="doc-biblioref">Bou-Hamad, Larocque, and Ben-Ameur 2011</a>)</span>, which includes extensions to time-varying covariates and different censoring types.</li>
</ul>
</div>
</div>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Bland2004" class="csl-entry" role="listitem">
Bland, J Martin, and Douglas G. Altman. 2004. <span>“<span class="nocase">The logrank test</span>.”</span> <em>BMJ (Clinical Research Ed.)</em> 328 (7447): 1073. <a href="https://doi.org/10.1136/bmj.328.7447.1073">https://doi.org/10.1136/bmj.328.7447.1073</a>.
</div>
<div id="ref-Bou-Hamad2011" class="csl-entry" role="listitem">
Bou-Hamad, Imad, Denis Larocque, and Hatem Ben-Ameur. 2011. <span>“<span class="nocase">A review of survival trees</span>.”</span> <em>Statist. Surv.</em> 5: 44–71. <a href="https://doi.org/10.1214/09-SS047">https://doi.org/10.1214/09-SS047</a>.
</div>
<div id="ref-Breiman1996a" class="csl-entry" role="listitem">
Breiman, Leo. 1996. <span>“<span>Bagging Predictors</span>.”</span> <em>Machine Learning</em> 24 (2): 123–40. <a href="https://doi.org/10.1023/A:1018054314350">https://doi.org/10.1023/A:1018054314350</a>.
</div>
<div id="ref-Ciampi1988" class="csl-entry" role="listitem">
Ciampi, Antonio, Sheilah A Hogg, Steve McKinney, and Johanne Thiffault. 1988. <span>“<span class="nocase">RECPAM: a computer program for recursive partition and amalgamation for censored survival data and other situations frequently occurring in biostatistics. I. Methods and program features</span>.”</span> <em>Computer Methods and Programs in Biomedicine</em> 26 (3): 239–56. https://doi.org/<a href="https://doi.org/10.1016/0169-2607(88)90004-1">https://doi.org/10.1016/0169-2607(88)90004-1</a>.
</div>
<div id="ref-Ciampi1986" class="csl-entry" role="listitem">
Ciampi, Antonio, Johanne Thiffault, Jean Pierre Nakache, and Bernard Asselain. 1986. <span>“<span class="nocase">Stratification by stepwise regression, correspondence analysis and recursive partition: a comparison of three methods of analysis for survival data with covariates</span>.”</span> <em>Computational Statistics and Data Analysis</em> 4 (3): 185–204. <a href="https://doi.org/10.1016/0167-9473(86)90033-2">https://doi.org/10.1016/0167-9473(86)90033-2</a>.
</div>
<div id="ref-Fleming1980" class="csl-entry" role="listitem">
Fleming, Thomas R, Judith R O’Fallon, Peter C O’Brien, and David P Harrington. 1980. <span>“<span class="nocase">Modified Kolmogorov-Smirnov Test Procedures with Application to Arbitrarily Right-Censored Data</span>.”</span> <em>Biometrics</em> 36 (4): 607–25. <a href="https://doi.org/10.2307/2556114">https://doi.org/10.2307/2556114</a>.
</div>
<div id="ref-Gordon1985" class="csl-entry" role="listitem">
Gordon, Louis, and Richard A Olshen. 1985. <span>“<span class="nocase">Tree-structured survival analysis.</span>”</span> <em>Cancer Treatment Reports</em> 69 (10): 1065–69.
</div>
<div id="ref-datamtcars" class="csl-entry" role="listitem">
Henderson, and Velleman. 1981. <span>“<span class="nocase">Building multiple regression models interactively</span>.”</span> <em>Biometrics</em> 37: 391—–411.
</div>
<div id="ref-Herrmann2020" class="csl-entry" role="listitem">
Herrmann, Moritz, Philipp Probst, Roman Hornung, Vindi Jurinovic, and Anne-Laure Boulesteix. 2021. <span>“<span class="nocase">Large-scale benchmark study of survival prediction methods using multi-omics data</span>.”</span> <em>Briefings in Bioinformatics</em> 22 (3). <a href="https://doi.org/10.1093/bib/bbaa167">https://doi.org/10.1093/bib/bbaa167</a>.
</div>
<div id="ref-Hothorn2005" class="csl-entry" role="listitem">
Hothorn, Torsten, Peter Bühlmann, Sandrine Dudoit, Annette Molinaro, and Mark J Van Der Laan. 2005. <span>“<span class="nocase">Survival ensembles</span>.”</span> <em>Biostatistics</em> 7 (3): 355–73. <a href="https://doi.org/10.1093/biostatistics/kxj011">https://doi.org/10.1093/biostatistics/kxj011</a>.
</div>
<div id="ref-pkgparty" class="csl-entry" role="listitem">
Hothorn, Torsten, Kurt Hornik, and Achim Zeileis. 2006. <span>“<span>Unbiased Recursive Partitioning: A Conditional Inference Framework</span>.”</span> <em>Journal of Computational and Graphical Statistics</em> 15 (3): 651—–674.
</div>
<div id="ref-Hothorn2003" class="csl-entry" role="listitem">
Hothorn, Torsten, and Berthold Lausen. 2003. <span>“<span class="nocase">On the exact distribution of maximally selected rank statistics</span>.”</span> <em>Computational Statistics &amp; Data Analysis</em> 43 (2): 121–37. <a href="https://doi.org/10.1016/S0167-9473(02)00225-6">https://doi.org/10.1016/S0167-9473(02)00225-6</a>.
</div>
<div id="ref-Hothorn2004" class="csl-entry" role="listitem">
Hothorn, Torsten, Berthold Lausen, Axel Benner, and Martin Radespiel-Tröger. 2004. <span>“<span class="nocase">Bagging survival trees</span>.”</span> <em>Statistics in Medicine</em> 23 (1): 77–91. <a href="https://doi.org/10.1002/sim.1593">https://doi.org/10.1002/sim.1593</a>.
</div>
<div id="ref-pkgpartykit" class="csl-entry" role="listitem">
Hothorn, Torsten, and Achim Zeileis. 2015. <span>“<span class="nocase">partykit: A Modular Toolkit for Recursive Partytioning in R.</span>”</span> <em>Journal of Machine Learning Research</em> 16: 3905–9. <a href="http://jmlr.org/papers/v16/hothorn15a.html">http://jmlr.org/papers/v16/hothorn15a.html</a>.
</div>
<div id="ref-Ishwaran2008" class="csl-entry" role="listitem">
Ishwaran, By Hemant, Udaya B Kogalur, Eugene H Blackstone, and Michael S Lauer. 2008. <span>“<span class="nocase">Random survival forests</span>.”</span> <em>The Annals of Statistics</em> 2 (3): 841–60. <a href="https://doi.org/10.1214/08-AOAS169">https://doi.org/10.1214/08-AOAS169</a>.
</div>
<div id="ref-Ishwaran2004" class="csl-entry" role="listitem">
Ishwaran, Hemant, Eugene H Blackstone, Claire E Pothier, and Michael S Lauer. 2004. <span>“<span class="nocase">Relative Risk Forests for Exercise Heart Rate Recovery as a Predictor of Mortality</span>.”</span> <em>Journal of the American Statistical Association</em> 99 (467): 591–600. <a href="https://doi.org/10.1198/016214504000000638">https://doi.org/10.1198/016214504000000638</a>.
</div>
<div id="ref-pkgrfsrc" class="csl-entry" role="listitem">
Ishwaran, Hemant, and Udaya B Kogalur. 2018. <span>“<span class="nocase">randomForestSRC</span>.”</span> <a href="https://cran.r-project.org/package=randomForestSRC">https://cran.r-project.org/package=randomForestSRC</a>.
</div>
<div id="ref-Hastie2013" class="csl-entry" role="listitem">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <em><span class="nocase">An introduction to statistical learning</span></em>. Vol. 112. New York: Springer.
</div>
<div id="ref-LeBlanc1992" class="csl-entry" role="listitem">
LeBlanc, Michael, and John Crowley. 1992. <span>“<span class="nocase">Relative Risk Trees for Censored Survival Data</span>.”</span> <em>Biometrics</em> 48 (2): 411–25. <a href="https://doi.org/10.2307/2532300">https://doi.org/10.2307/2532300</a>.
</div>
<div id="ref-LeBlanc1993" class="csl-entry" role="listitem">
———. 1993. <span>“<span class="nocase">Survival Trees by Goodness of Split</span>.”</span> <em>Journal of the American Statistical Association</em> 88 (422): 457–67. <a href="https://doi.org/10.2307/2290325">https://doi.org/10.2307/2290325</a>.
</div>
<div id="ref-Nasejje2017" class="csl-entry" role="listitem">
Nasejje, Justine B, Henry Mwambi, Keertan Dheda, and Maia Lesosky. 2017. <span>“<span class="nocase">A comparison of the conditional inference survival forest model to random survival forests based on a simulation study as well as on two applications with time-to-event data</span>.”</span> <em>BMC Medical Research Methodology</em> 17 (1): 115. <a href="https://doi.org/10.1186/s12874-017-0383-8">https://doi.org/10.1186/s12874-017-0383-8</a>.
</div>
<div id="ref-pkgipred" class="csl-entry" role="listitem">
Peters, Andrea, and Torsten Hothorn. 2019. <span>“<span class="nocase">ipred: Improved Predictors</span>.”</span> CRAN. <a href="https://cran.r-project.org/package=ipred">https://cran.r-project.org/package=ipred</a>.
</div>
<div id="ref-pkgsksurvival" class="csl-entry" role="listitem">
Pölsterl, Sebastian. 2020. <span>“<span class="nocase">scikit-survival: A Library for Time-to-Event Analysis Built on Top of scikit-learn</span>.”</span> <em>Journal of Machine Learning Research</em> 21 (212): 1—–6. <a href="http://jmlr.org/papers/v21/20-729.html">http://jmlr.org/papers/v21/20-729.html</a>.
</div>
<div id="ref-Segal1988" class="csl-entry" role="listitem">
Segal, Mark Robert. 1988. <span>“<span class="nocase">Regression Trees for Censored Data</span>.”</span> <em>Biometrics</em> 44 (1): 35—–47.
</div>
<div id="ref-Sonabend2021b" class="csl-entry" role="listitem">
Sonabend, Raphael Edward Benjamin. 2021. <span>“<span class="nocase">A Theoretical and Methodological Framework for Machine Learning in Survival Analysis: Enabling Transparent and Accessible Predictive Modelling on Right-Censored Time-to-Event Data</span>.”</span> PhD, University College London (UCL). <a href="https://discovery.ucl.ac.uk/id/eprint/10129352/">https://discovery.ucl.ac.uk/id/eprint/10129352/</a>.
</div>
<div id="ref-Spooner2020" class="csl-entry" role="listitem">
Spooner, Annette, Emily Chen, Arcot Sowmya, Perminder Sachdev, Nicole A Kochan, Julian Trollor, and Henry Brodaty. 2020. <span>“<span class="nocase">A comparison of machine learning methods for survival analysis of high-dimensional clinical data for dementia prediction</span>.”</span> <em>Scientific Reports</em> 10 (1): 20410. <a href="https://doi.org/10.1038/s41598-020-77220-w">https://doi.org/10.1038/s41598-020-77220-w</a>.
</div>
<div id="ref-pkgrpart" class="csl-entry" role="listitem">
Therneau, Terry M., and Beth Atkinson. 2019. <span>“<span class="nocase">rpart: Recursive Partitioning and Regression Trees</span>.”</span> CRAN.
</div>
<div id="ref-Therneau1990" class="csl-entry" role="listitem">
Therneau, Terry M., Patricia M. Grambsch, and Thomas R. Fleming. 1990. <span>“<span class="nocase">Martingale-based residuals for survival models</span>.”</span> <em>Biometrika</em> 77 (1): 147–60. <a href="https://doi.org/10.1093/biomet/77.1.147">https://doi.org/10.1093/biomet/77.1.147</a>.
</div>
<div id="ref-Wang2017a" class="csl-entry" role="listitem">
Wang, Hong, and Gang Li. 2017. <span>“<span class="nocase">A Selective Review on Random Survival Forests for High Dimensional Data</span>.”</span> <em>Quantitative Bio-Science</em> 36 (2): 85–96. <a href="https://doi.org/10.22283/qbs.2017.36.2.85">https://doi.org/10.22283/qbs.2017.36.2.85</a>.
</div>
<div id="ref-pkgranger" class="csl-entry" role="listitem">
Wright, Marvin N., and Andreas Ziegler. 2017. <span>“<span class="nocase">ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R</span>.”</span> <em>Journal of Statistical Software</em> 77 (1): 1—–17.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./classical.html" class="pagination-link" aria-label="Classical Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Classical Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./svm.html" class="pagination-link" aria-label="Support Vector Machines">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Support Vector Machines</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">abstract:</span><span class="co"> TODO (150-200 WORDS)</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>{{&lt; include _setup.qmd &gt;}}</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu"># Random Forests {#sec-surv-ml-models-ranfor}</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>{{&lt; include _wip.qmd &gt;}}</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random Forests for Regression</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>Random forests are a composite algorithm built by fitting many simpler component models, decision trees, and then averaging the results of predictions from these trees.</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>Decision trees are first briefly introduced before the key 'bagging' algorithm, which creates the random forest.</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>Woodland terminology is used throughout this chapter.</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="fu">### Decision Trees {.unnumbered .unlisted}</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>*Decision trees* are a common model class in machine learning and have the advantage of being (relatively) simple to implement and highly interpretable.</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>A decision tree takes a set of inputs and a given *splitting rule* in order to create a series of splits, or branches, in the tree that culminates in a final *leaf*, or *terminal node*.</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>Each terminal node has a corresponding prediction, which for regression is usually the sample mean of the training outcome data.</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>This is made clearer by example, (@fig-surv-ranfor) demonstrates a decision tree predicting the miles per gallon (<span class="in">`mpg`</span>) of a car from the <span class="in">`mtcars`</span>  <span class="co">[</span><span class="ot">@datamtcars</span><span class="co">]</span> dataset.</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>With this tree a new prediction is made by feeding the input variables from the top to the bottom, for example given new data, $x = <span class="sc">\{</span><span class="in">`wt`</span> = 3, <span class="in">`disp`</span> = 250<span class="sc">\}</span>$, then in the first split the right branch is taken as <span class="in">`wt`</span> $= 3 &gt; 2.32$ and in the second split the left branch is taken as <span class="in">`disp`</span> $= 250 \leq 258$, therefore the new data point 'lands' in the final leaf and is predicted to have an <span class="in">`mpg`</span> of $20.8$.</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>This value of $20.8$ arises as the sample mean of <span class="in">`mpg`</span> for the $11$ (which can be seen in the box) observations in the training data who were sorted into this terminal node.</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>Algorithmically, as splits are always binary, predictions are simply a series of conditional logical statements.</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Demonstrating classification trees using the `mtcars`  [@datamtcars] dataset and the $\pkg{party}$  [@pkgparty] package. Ovals are leaves, which indicate the variable that is being split. Edges are branches, which indicate the cut-off at which the variable is split. Rectangles are terminal nodes and include information about the number of training observations in the node and the terminal node prediction.</span><span class="co">](Figures/forests/iris_tree.png)</span>{#fig-surv-ranfor fig-alt="TODO"}</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="fu">### Splitting Rules {.unnumbered .unlisted}</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>Precisely how the splits are derived and which variables are utilised is determined by the splitting rule.\footnote{Other methods for growing trees such as pruning are not discussed here as they are less relevant to random forests, which are primarily of interest. Instead see (e.g.) Breiman (1984)  <span class="co">[</span><span class="ot">@Breiman1984</span><span class="co">]</span>.}</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>In regression, the most common splitting rule is to select the cut-off for a given variable that minimises the mean squared error in each hypothetical resultant leaf.</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>The goal is to find the variable and cutoff that leads to the greatest difference between the two resultant leaves and thus the maximal homogeneity within each leaf.</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>For all decision tree and random forest algorithms going forward, let $L$ denote some leaf, then let $L_{xy}, L_x, L_y$ respectively be the set of observations, features, and outcomes in leaf $L$.</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>Let $L_{y;i}$ be the $i$th outcome in $L_y$ and finally let $L_{\bar{y}} = \mean{L_{y;i}}$.</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>To simplify notation, $i \in L$ is taken to be equivalent to $i \in <span class="sc">\{</span>i: X_i \in L_X<span class="sc">\}</span>$, i.e. the indices of the observations in leaf $L$.</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>Let $c \in \Reals$ be some cutoff parameter and let $L^a_{xy}(j,c) := <span class="sc">\{</span>(X_i,Y_i)|X_{ij} &lt; c, i = 1,...,n<span class="sc">\}</span>, L^b_{xy}(j,c) = <span class="sc">\{</span>(X_i,Y_i)|X_{ij} \geq c, i = 1,...,n<span class="sc">\}</span>$ be the two leaves containing the set of observations resulting from partitioning variable $j$ at cutoff $c$.</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>Then a split is determined by finding the arguments, $(j^*,c^*)$ that minimise the sum of the mean squared errors (MSE) in both leaves  <span class="co">[</span><span class="ot">@Hastie2013</span><span class="co">]</span>,</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>(j^*, c^*) = \argmin_{j, c} \sum_{y \in L^a_{y}(j,c)} (y - L^a_{\bar{Y}}(j,c))^2 + \sum_{y \in L^b_{y}(j,c)} (y - L^b_{\bar{Y}}(j,c))^2</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>$$ {#eq-dt-min}</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>This method is repeated from the first branch of the tree down to the very last such that observations are included in a given leaf $L$ if they satisfy all conditions from all previous branches; features may be considered multiple times in the growing process.</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>This is an intuitive method as minimising the above sum results in the set of observations within each individual leaf being as similar as possible, thus as an observation is passed down the tree, it becomes more similar to the subsequent leaves, eventually landing in a leaf containing homogeneous observations.</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>Controlling how many variables to consider at each split and how many splits to make are determined by hyper-parameter tuning.</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>Decision trees are a powerful method for high-dimensional data as only a small sample of variables will be used for growing a tree, and therefore they are also useful for variable importance by identifying which variables were utilised in growth (other importance methods are also available).</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>Decision trees are also highly interpretable, as demonstrated by (@fig-surv-ranfor).</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>The recursive pseudo-algorithm in (@alg-dt-fit) demonstrates the simplicity in growing a decision tree (again methods such as pruning are omitted).</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \begin{algorithm}</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="co">\caption{Fitting a decision tree. \\</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="co">**Input** Training data, $\dtrain$. Splitting rule, $SR$. \\</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="co">**Output** Fitted decision tree, $\hatg$.}</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="co">\begin{algorithmic}[1]</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a><span class="co">\State Compute $(j^*, c^*)$ as the optimisers of $SR$ (e.g. (@eq-dt-min)) to create the initial leaf and branches.</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="co">\State Repeat step 1 on all subsequent branches until a stopping rule is reached.</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="co">\State Return the fitted tree, $\hatg$, as the series of branches.</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a><span class="co">\end{algorithmic}</span></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="co">\end{algorithm} --&gt;</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- {#alg-dt-fit} --&gt;</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="fu">### Stopping Rules {.unnumbered .unlisted}</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>The 'stopping rule' in (@alg-dt-fit) is usually a condition on the number of observations in each leaf such that leaves will continue to be split until some minimum number of observations has been reached in a leaf.</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>Other conditions may be on the 'depth' of the tree, which corresponds to the number of levels of splitting, for example the tree in (@fig-surv-ranfor) has a depth of $2$ (the first level is not counted).</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random Forests {.unnumbered .unlisted}</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>Despite being more interpretable than other machine learning methods, decision trees usually have poor predictive performance, high variance and are not robust to changes in the data.</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>As such, *random forests* are preferred to improve prediction accuracy and decrease variance. Random forests utilise bootstrap aggregation, or *bagging*  <span class="co">[</span><span class="ot">@Breiman1996a</span><span class="co">]</span>, to aggregate many decision trees.</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>A pseudo fitting algorithm is given in  (@alg-rsf-fit).</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \begin{algorithm}</span></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="co">\caption{Fitting a random forest. \\</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="co">**Input** Training data, $\dtrain$. Total number of trees, $B \in \PNaturals$. \\</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a><span class="co">**Output** Fitted random forest, $\hatg$.}</span></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="co">\begin{algorithmic}[1]</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a><span class="co">\For{$b = 1,...,B$}</span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a><span class="co">\State Create a bootstrapped sample of the data, $D_b$.</span></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a><span class="co">\State Grow a decision tree, $\hatg_b$, on $D_b$ with (@alg-dt-fit).</span></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a><span class="co">\EndFor</span></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a><span class="co">\State $\hatg \gets \{\hatg_b\}^B_{b=1}$</span></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="co">\Return $\hatg$</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="co">\end{algorithmic}</span></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a><span class="co">\end{algorithm} --&gt;</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- {#alg-rsf-fit} --&gt;</span></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>Prediction from a random forest follows by making predictions from the individual trees and aggregating the results by some function $\sigma$ (@alg-rsf-pred); $\sigma$ is usually the sample mean for regression,</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>\hatg(X^*) = \sigma(\hatg_1(X^*),...,\hatg_B(X^*)) = \frac{1}{B} \sum^B_{b=1} \hatg_b(X^*)</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>where $\hatg_b(X^*)$ is the terminal node prediction from the $b$th tree and $B$ are the total number of grown trees (<span class="in">`$B$' is commonly used instead of `</span>$N$' to note the relation to bootstrapped data).</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- \begin{algorithm}</span></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a><span class="co">\caption{Predicting from a random forest. \\</span></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a><span class="co">**Input** Testing data $X^* \sim \calX$, fitted forest $\hatg$ with $B \in \PNaturals$ trees, aggregation method $\sigma$. \\</span></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a><span class="co">**Output** Prediction, $\hatY \sim \calY$.}</span></span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a><span class="co">\begin{algorithmic}[1]</span></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a><span class="co">\For{$b = 1,...,B$}</span></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a><span class="co">\State 'Drop' $X^*$ down the tree $\hatg_b$ individually to return a prediction $\hatg_b(X^*)$.</span></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a><span class="co">\EndFor</span></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a><span class="co">\State $\hatY \gets \sigma(\hatg_1(X^*),...,\hatg_B(X^*))$</span></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a><span class="co">\Return $\hatY$</span></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a><span class="co">\end{algorithmic}</span></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a><span class="co">\end{algorithm} --&gt;</span></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- {#alg-rsf-pred} --&gt;</span></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>Usually many (hundreds or thousands) trees are grown, which makes random forests robust to changes in data and 'confident' about individual predictions.</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>Other advantages include having several tunable hyper-parameters, including: the number of trees to grow, the number of variables to include in a single tree, the splitting rule, and the minimum terminal node size. Machine learning models with many hyper-parameters, tend to perform better than other models as they can be fine-tuned to the data, which is why complex deep learning models are often the best performing.</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>Although as a caveat: too many parameters can lead to over-fitting and tuning many parameters can take a long time and be highly intensive.</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>Random forests lose the interpretability of decision trees and are considered 'black-box' models as individual predictions cannot be easily scrutinised.</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random Survival Forests</span></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>Unlike other machine learning methods that may require complex changes to underlying algorithms, random forests can be relatively simply adapted to *random survival forests* by updating the splitting rules and terminal node predictions to those that can handle censoring and can make survival predictions.</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>This chapter is therefore focused on outlining different choices of splitting rules and terminal node predictions, which can then be flexibly combined into different models.</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a><span class="fu">### Splitting Rules</span></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>Survival trees and RSFs have been studied for the past four decades and whilst the amount of splitting rules to appear could be considered `'numerous'' <span class="co">[</span><span class="ot">@Bou-Hamad2011</span><span class="co">]</span>, only two broad classes are commonly utilised and implemented  <span class="co">[</span><span class="ot">@pkgrfsrc; @pkgsksurvival; @pkgrpart; @pkgranger</span><span class="co">]</span>.</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>The first class rely on hypothesis tests, and primarily the log-rank test, to maximise dissimilarity between splits, the second class utilises likelihood-based measures.</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>The first is discussed in more detail as this is common in practice and is relatively straightforward to implement and understand, moreover it has been demonstrated to outperform other splitting rules <span class="co">[</span><span class="ot">@Bou-Hamad2011</span><span class="co">]</span>.</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>Likelihood rules are more complex and require assumptions that may not be realistic, these are discussed briefly.</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hypothesis Tests {.unnumbered .unlisted}</span></span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>The log-rank test statistic has been widely utilised as the 'natural' splitting-rule for survival analysis  <span class="co">[</span><span class="ot">@Ciampi1986; @Ishwaran2008; @LeBlanc1993; @Segal1988</span><span class="co">]</span>.</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>The log-rank test compares the survival distributions of two groups and has the null-hypothesis that both groups have the same underlying risk of (immediate) death, i.e. identical hazard functions.</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>Let $L^A$ and $L^B$ be two leaves then using the notation above let $h^A,h^B$ be the (true) hazard functions derived from the observations in the two leaves respectively.</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>The log-rank hypothesis test is given by $H_0: h^A = h^B$ with test statistic  <span class="co">[</span><span class="ot">@Segal1988</span><span class="co">]</span>,</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>LR(L^A) = \frac{\sum_{\tau \in \calU_D} (d^A_{\tau} - e^A_{\tau})}{\sqrt{\sum_{\tau \in \calU_D} v_\tau^A}}</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>where $d^A_{\tau}$ is the observed number of deaths in leaf $A$ at $\tau$,</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>d^A_{\tau} := \sum_{i \in L^A} \II(T_i = \tau, \Delta_i = 1)</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>$e^A_{\tau}$ is the expected number of deaths in leaf $A$ at $\tau$,</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>e^A_{\tau} := \frac{n_\tau^A d_\tau}{n_\tau}</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>and $v^A_\tau$ is the variance of the number of deaths in leaf $A$ at $\tau$,</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>v^A_{\tau} := e^A_{\tau} \Big(\frac{n_\tau - d_\tau}{n_\tau}\Big)\Big(\frac{n_\tau - n^A_\tau}{n_\tau - 1}\Big)</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>where $\calU_D$ is the set of unique death times across the data (in both leaves), <span class="sc">\\</span> $n_\tau = \sum_i \II(T_i \geq \tau)$ is the number of observations at risk at $\tau$ in both leaves, <span class="sc">\\</span> $n_\tau^A = \sum_{i \in L^A} \II(T_i \geq \tau)$ is the number of observations at risk at $\tau$ in leaf A, and <span class="sc">\\</span> $d_\tau = \sum_i \II(T_i = \tau, \Delta_i = 1)$ is the number of deaths at $\tau$ in both leaves.</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>Intuitively these results follow as the number of deaths in a leaf is distributed according to $\Hyper(n^A_\tau,n_\tau,d_\tau)$.</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>The same statistic results if $L^B$ is instead considered. (@alg-dt-fit) follows for fitting decision trees with the log-rank splitting rule, $SR$, to be maximised.</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>The higher the log-rank statistic, the greater the dissimilarity between the two groups, thereby making it a sensible splitting rule for survival, moreover it has been shown that it works well for splitting censored data  <span class="co">[</span><span class="ot">@LeBlanc1993</span><span class="co">]</span>.\footnote{The results of this experiment are actually in LeBlanc's unpublished 1989 PhD thesis and therefore it has to be assumed that LeBlanc is accurately conveying its results in this 1993 paper.}</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>When censoring is highly dependent on the outcome, the log-rank statistic does not perform well and is biased  <span class="co">[</span><span class="ot">@Bland2004</span><span class="co">]</span>, which tends to be true of the majority of survival models.</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>Additionally, the log-rank test requires no knowledge about the shape of the survival curves or distribution of the outcomes in either group  <span class="co">[</span><span class="ot">@Bland2004</span><span class="co">]</span>, making it ideal for an automated process that requires no user intervention.</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>The log-rank *score* rule  <span class="co">[</span><span class="ot">@Hothorn2003</span><span class="co">]</span> is a standardized version of the log-rank rule that could be considered as a splitting rule, though simulation studies have demonstrated non-significant predictive performance when comparing the two  <span class="co">[</span><span class="ot">@Ishwaran2008</span><span class="co">]</span>.</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>Alternative dissimiliarity measures and tests have also been suggested as splitting rules, including modified Kolmogorov-Smirnov test and Gehan-Wilcoxon tests  <span class="co">[</span><span class="ot">@Ciampi1988</span><span class="co">]</span>.</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>Simulation studies have demonstrated that both of these may have higher power and produce 'better' results than the log-rank statistic  <span class="co">[</span><span class="ot">@Fleming1980</span><span class="co">]</span>.</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>Despite this, these do not appear to be in common usage and no implementation could be found that include these.</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>\noindent ### Likelihood Based Rules {.unnumbered .unlisted}</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>Likelihood ratio statistics, or deviance based splitting rules, assume a certain model form and thereby an assumption about the data.</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>This may be viewed as an advantageous strategy, as it could arguably increase interpretability, or a disadvantage as it places restrictions on the data. For survival models, a full-likelihood can be estimated with a Cox form by estimating the cumulative hazard function  <span class="co">[</span><span class="ot">@LeBlanc1992</span><span class="co">]</span>.</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>LeBlanc and Crowley (1992)  <span class="co">[</span><span class="ot">@LeBlanc1992</span><span class="co">]</span> advocate for selecting the optimal split by maximising the full PH likelihood, assuming the cumulative hazard function, $H$, is known,</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>\calL := \prod_{m = 1}^M \prod_{i \in L^m} h_m(T_i)^{\Delta_i} \exp(-H_m(T_i))</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>where $M$ is the total number of terminal nodes, $h_m$ and $H_m$ are the (true) hazard and cumulative hazard functions in the $m$th node, and again $L^m$ is the set of observations in terminal node $m$.</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>Estimation of $h_m$ and $H_m$ are described with the associated terminal node prediction below.</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>The primary advantage of this method is that any off-shelf regression software with a likelihood splitting rule can be utilised without any further adaptation to model fitting by supplying this likelihood with required estimates.</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>However the additional costs of computing these estimates may outweigh the benefits once the likelihood has been calculated, and this could be why only one implementation of this method has been found <span class="co">[</span><span class="ot">@Bou-Hamad2011; @pkgrpart</span><span class="co">]</span>.</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a><span class="fu">### Other Splitting Rules {.unnumbered .unlisted}</span></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>As well as likelihood and log-rank spitting rules, other papers have studied comparison of residuals  <span class="co">[</span><span class="ot">@Therneau1990</span><span class="co">]</span>, scoring rules  <span class="co">[</span><span class="ot">@pkgrfsrc</span><span class="co">]</span>, and distance metrics  <span class="co">[</span><span class="ot">@Gordon1985</span><span class="co">]</span>.</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>These splitting rules work similarly to the mean squared error in the regression setting, in which the score should be minimised across both leaves.</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>The choice of splitting rule is usually data-dependent and can be treated as a hyper-parameter for tuning.</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>However, if there is a clear goal in prediction, then the choice of splitting rule can be informed by the prediction type.</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>For example, if the goal is to maximise separation, then a log-rank splitting rule to maximise homogeneity in terminal nodes is a natural starting point.</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>Whereas if the goal is to estimate the linear predictor of a Cox PH model, then a likelihood splitting rule with a Cox form may be more sensible.</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a><span class="fu">### Terminal Node Prediction {#sec-surv-ml-models-ranfor-nodes}</span></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>Only two terminal node predictions appear in common usage.</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a><span class="fu">### Predict: Ranking {.unnumbered .unlisted}</span></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>Terminal node ranking predictions for survival trees and forests have been limited to those that use a likelihood-based splitting rule and assume a PH model form  <span class="co">[</span><span class="ot">@Ishwaran2004; @LeBlanc1992</span><span class="co">]</span>.</span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>In model fitting the likelihood splitting rule model attempts to fit the (theoretical) PH model $h_m(\tau) = h_0(\tau)\theta_m$ for $m \in 1,...,M$ where $M$ is the total number of terminal nodes and $\theta_m$ is a parameter to estimate.</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>The model returns predictions for $\exp(\hat{\theta}_m)$ where $\hat{\theta}_m$ is the estimate of $\theta_m$.</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>This is estimated via an iterative procedure in which in iteration $j+1$, $\hat{\theta}_m^{j+1}$ is estimated by</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>\hat{\theta}_m^{j+1} = \frac{\sum_{i \in L^m} \Delta_i}{\sum_{i \in L^m} \hat{H}^j_0(T_i)}</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>where as before $L^m$ is the set of observations in leaf $m$ and</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>\hat{H}^{j}_0(\tau) = \frac{\sum_{i:T_i \leq \tau} \Delta_i}{\sum_{m = 1}^M\sum_{\{i:i \in \calR_\tau \cap L^a<span class="sc">\}</span>} \hat{\theta}^{j}_m}</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>which is repeated until some stopping criterion is reached.</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>The same cumulative hazard is estimated for all nodes however $\hat{\theta}_m$ varies across nodes.</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>This method lends itself naturally to a composition to a full distribution (@sec-car) as it assumes a PH form and separately estimates the cumulative hazard and relative risk (@sec-surv-ml-models-ranfor-nov), though no implementation of this composition could be found.</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a><span class="fu">### Predict: Survival Distribution {.unnumbered .unlisted}</span></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>The most common terminal node prediction appears to be predicting the survival distribution by estimating the survival function, using the Kaplan-Meier or Nelson-Aalen estimators, on the sample in the terminal node  <span class="co">[</span><span class="ot">@Hothorn2004; @Ishwaran2008; @LeBlanc1993; @Segal1988</span><span class="co">]</span>.</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>Estimating a survival function by a non-parametric estimator is a natural choice for terminal node prediction as these are natural 'baselines' in survival, similarly to taking the sample mean in regression.</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a>The prediction for SDTs is straightforward, the non-parametric estimator is fit on all observations in each of the terminal nodes.</span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>This is adapted to RSFs by bagging the estimator across all decision trees  <span class="co">[</span><span class="ot">@Hothorn2004</span><span class="co">]</span>. Using the Nelson-Aalen estimator as an example, let $m$ be a terminal node in an SDT, then the terminal node prediction is given by,</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>\hat{H}_m(\tau) = \sum_{<span class="sc">\{</span>i: i \in L^m \cap T_i \leq \tau<span class="sc">\}</span>} \frac{d_i}{n_i}</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>$$ {#eq-surv-nelson}</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>where $d_i$ and $n_i$ are the number of events and observations at risk at time $T_i$ in terminal node $m$. Ishwaran  <span class="co">[</span><span class="ot">@Ishwaran2008</span><span class="co">]</span> defined the bootstrapped Nelson-Aalen estimator as</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>\hat{H}_{Boot}(\tau) = \frac{1}{B} \sum^B_{b=1} \hat{H}_{m, b}(\tau), \quad m \in 1,...,M</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>$$ {#eq-surv-nelson-boot}</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>where $B$ is the total number of bootstrapped estimators, $M$ is the number of terminal nodes, and $\hat{H}_{m,b}$ is the cumulative hazard for the $m$th terminal node in the $b$th tree.</span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>The bootstrapped Kaplan-Meier estimator is calculated analogously.</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>More generally these can be considered as a uniform mixture of $B$ distributions (@sec-car).</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>All implemented RSFs can now be summarised into the following five algorithms:</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>**RRT** {#mod-rrt}<span class="sc">\\</span></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>LeBlanc and Crowley's (1992)  <span class="co">[</span><span class="ot">@LeBlanc1992</span><span class="co">]</span> survival decision tree uses a deviance splitting rule with a terminal node ranking prediction, which assumes a PH model form.</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>These 'relative risk trees' (RRTs) are implemented in the package $\pkg{rpart}$  <span class="co">[</span><span class="ot">@pkgrpart</span><span class="co">]</span>.</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>This model is considered the least accessible and transparent of all discussed in this section as: few implementations exist, it requires assumptions that may not be realistic, and predictions are harder to interpret than other models.</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>Predictive performance of the model is expected to be worse than RSFs as this is a decision tree; this is confirmed in <span class="co">[</span><span class="ot">@Sonabend2021b</span><span class="co">]</span>.</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>**RRF** {#mod-rrf}<span class="sc">\\</span></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>Ishwaran $\etal$ (2004)  <span class="co">[</span><span class="ot">@Ishwaran2004</span><span class="co">]</span> proposed a random forest framework for the relative risk trees, which makes a slight adaptation and applies the iteration of the terminal node prediction after the tree is grown as opposed to during the growing process.</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>No implementation for these 'relative risk forests' (RRFs) could be found or any usage in the literature.</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>**RSDF-DEV** {#mod-rsdfdev}<span class="sc">\\</span></span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>Hothorn $\etal$ (2004)  <span class="co">[</span><span class="ot">@Hothorn2004</span><span class="co">]</span> expanded upon the RRT by introducing a bagging composition thus creating a random forest with a deviance splitting rule, again assuming a PH form.</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>However the ranking prediction is altered to be a bootstrapped Kaplan-Meier prediction in the terminal node.</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>This is implemented in $\pkg{ipred}$  <span class="co">[</span><span class="ot">@pkgipred</span><span class="co">]</span>.</span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>This model improves upon the accessibility and transparency of the RRT by providing a more straightforward and interpretable terminal node prediction.</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>However, as this is a decision tree, predictive performance is again expected to be worse than the RSFs.</span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>**RSCIFF** {#mod-rsciff}<span class="sc">\\</span></span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>Hothorn $\etal$  <span class="co">[</span><span class="ot">@Hothorn2005</span><span class="co">]</span> studied a conditional inference framework in order to predict log-survival time.</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>In this case the splitting rule is based on an IPC weighted loss function, which allows implementation by off-shelf classical random forests.</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>The terminal node predictions are a weighted average of the log-survival times in the node where weighting is determined by the Kaplan-Meier estimate of the censoring distribution.</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a>This 'random survival conditional inference framework forest' (RSCIFF) is implemented in $\pkg{party}$  <span class="co">[</span><span class="ot">@pkgparty</span><span class="co">]</span> and $\pkg{partykit}$  <span class="co">[</span><span class="ot">@pkgpartykit</span><span class="co">]</span>, which additionally includes a distribution terminal node prediction via the bootstrapped Kaplan-Meier estimator.</span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>The survival tree analogue (SDCIFT) is implemented in the same packages.</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>Implementation of the RSCIFF is complex, which is likely why all implementations (in the above packages) are by the same authors.</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a>The complexity of conditional inference forests may also be the reason why several reviews, including this one, mention (or completely omit) RSCIFFs but do not include any comprehensive details that explain the fitting procedure <span class="co">[</span><span class="ot">@Bou-Hamad2011; @Wang2017a</span><span class="co">]</span>.</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a>In this regard, it is hard to claim that RSCIFFs are transparent or accessible.</span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a>Moreover, the authors of the model state that random conditional inference forests are for `'expert user<span class="co">[</span><span class="ot">s</span><span class="co">]</span> only and <span class="co">[</span><span class="ot">their</span><span class="co">]</span> current state is rather experimental''  <span class="co">[</span><span class="ot">@pkgpartykit</span><span class="co">]</span>.</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a>Finally, with respect to model performance, there is evidence that they can outperform RSDFs (below) dependent on the data type  <span class="co">[</span><span class="ot">@Nasejje2017</span><span class="co">]</span> however no benchmark experiment could be found that compared them to other models.</span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>**RSDF-STAT** {#mod-rsdfstat}<span class="sc">\\</span></span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a>Finally Ishwaran $\etal$ (2008)  <span class="co">[</span><span class="ot">@Ishwaran2008</span><span class="co">]</span> proposed the most general form of RSFs with a choice of hypothesis tests (log-rank and log-rank score) and survival measure (Brier, concordance) splitting rules, and a bootstrapped Nelson-Aalen terminal node prediction.</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>These are implemented in $\pkg{randomForestSRC}$  <span class="co">[</span><span class="ot">@pkgrfsrc</span><span class="co">]</span> and $\pkg{ranger}$  <span class="co">[</span><span class="ot">@pkgranger</span><span class="co">]</span>.</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a>There are several implementations of these models across programming languages, and extensive details for the fitting and predicting procedures, which makes them very accessible.</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>The models utilise a standard random forest framework, which makes them transparent and familiar to those without expert Survival knowledge.</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a>Moreover they have been proven to perform well in benchmark experiments, especially on high-dimensional data  <span class="co">[</span><span class="ot">@Herrmann2020; @Spooner2020</span><span class="co">]</span>.</span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a>:::: {.callout-warning icon=false}</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a><span class="fu">## Key takeaways</span></span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>Random forests are a highly flexible algorithm that allow the various components to be adapted and altered without major changes to the underlying algorithm;</span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>This means that random survival forests (RSFs) are readily available 'off-shelf' in many open-source packages;</span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>RSFs have been demonstrated to perform well on high-dimensional data, routinely outperforming other models.</span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- fixme: add citations above --&gt;</span></span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>::::</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>:::: {.callout-important icon=false}</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a><span class="fu">## Limitations</span></span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a>*</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>::::</span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a>:::: {.callout-tip icon=false}</span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a><span class="fu">## Further reading</span></span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>A comprehensive review of random survival forests (RSFs) is provided in Bou-Hamad (2011) <span class="co">[</span><span class="ot">@Bou-Hamad2011</span><span class="co">]</span>, which includes extensions to time-varying covariates and different censoring types.</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a>::::</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>All content licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> <br> © Raphael Sonabend, Andreas Bender.</p>
</div>   
    <div class="nav-footer-center">
<p><a href="https://www.mlsabook.com">Website</a> | <a href="https://github.com/mlsa-book/MLSA">GitHub</a></p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/mlsa-book/MLSA/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/mlsa-book/MLSA/edit/main/book/forests.qmd" class="toc-action"><i class="bi empty"></i>Edit this page</a></li><li><a href="https://github.com/mlsa-book/MLSA/blob/main/book/forests.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>