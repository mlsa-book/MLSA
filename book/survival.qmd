---
abstract: TODO (150-200 WORDS)
---

{{< include _setup.qmd >}}

# Survival Analysis {#sec-surv}

{{< include _wip.qmd >}}

___

TODO

* Make sure intro is clear about censoring/truncation and that metrics can't highlight if this is setup wrong - analogously to hypothesis testing not testing the result but hypothesis, p-hacking, etc.
* If measures for right-censoring used in parts of pipelines hard to discern biases if wrong type of measure used
* Same as dependent/independent censoring and measures problem
* Estimation of transition hazards/transition probabilities
* Event-history analysis (Graph of different types of transitions and setups)

___

<!-- In their broadest and most basic definitions, survival analysis is the study of temporal data from a given origin until the occurrence of one or more events or 'end-points'  [@Collett2014], and machine learning is the study of models and algorithms that learn from data in order to make predictions or find patterns  [@Hastie2001]. Reducing either field to these definitions is ill-advised. -->
<!--
Survival analysis is concerned with time-to-event data, i.e. data where the outcome is a duration from some origin until the occurrence of one or multiple events of interest.
In many medical settings, this duration will often be literal survival time, i.e. time until death after some procedure or treatment.
When collecting such data, however, the outcome of interest can often be not observed (fully), e.g. because subjects drop out of the study, haven't experienced the event of interest until end of study period or due to the occurence of another, competing event.

Often analysis of time-to-event data targets the estimation of the (improper) distribution of the event times or, equivalently, modelling the transitions between different states (e.g. alive -> dead) while taking into account censoring and truncation as well as other peculiarities of time-to-event data. However, the target of estimation can also be a relative risk score or the expected time-to-event (cf. @sec-surv-set-types for details).
 -->


 As discussed in the introduction, *Survival Analysis* is concerned with data where the outcome is a time-to-event.
Because the collection of such data takes place in the temporal domain (it takes time to observe a duration), the event of interest is often unobservable.
For example, because the event did not occur by the end of the data collection period or because of the occurrence of another event that prevents the event of interest from being obseved.
 In survival analysis terminolgy these are refered to as *censoring* and *competing risks*.

This chapter defines these and related terms and introduces basic terminology and mathematical definitions.
@sec-surv-set-math starts with the common single-event, right-censored data setting with definitions then extended to further types of censoring as well as truncation.
@sec-eha introduces event-history analysis, which is a generalisation to settings with multiple, potentially competing or recurrent events.
@sec-surv-set-types defines common prediction types of survival models, which is particularly important for machine learning based survival analysis.
Finally, in order to cleanly discuss *machine learning survival analysis*, the *survival task* is introduced in @sec-surv-setmltask.

It is of utmost importance to identify and specify the survival task correctly, as misspecification cannot be detected by comparing the predictive performance of alternate models via evaluation measures.
Evaluation measure can only detect if one model is better suited to minimize the obejective function, but not whether or not the obejective function is correct.
The latter depends on the (assumptions about the) data generating process and has to be also reflected in the definition of the evaluation measure.


## Survival Data and Definitions {#sec-surv-set-math}

### Quantifying the Distribution of Event Times {#sec-distributions}

This section introduces functions that can be used to fully characteristise a probability distribution, termed here as \emph{distribution defining functions}.
Particular focus is given to distribution defining functions that are important in survival analysis.

For now, assume a continuous, positive, random variable $Y$ taking values in (t.v.i.) $\NNReals$.
A standard representation of the distribution of $Y$ is given by the probability density function (pdf), $f_Y: \NNReals \rightarrow \NNReals$, and cumulative distribution function (cdf), $F_Y: \NNReals \rightarrow [0,1]; (\tau) \mapsto P(Y \leq \tau)$.

As discussed in @sec-intro, it is more common to describe the distribution of event times $Y$ via the *survival function* and *hazard function* (often also refered to as *hazard rate*) than the pdf or cdf.

The survival function is defined as
$$
S_Y(\tau) = P(Y > \tau) = \int^\infty_\tau f_Y(u) \ du,
$$
is the probability of not observing an event until some point $\tau \geq 0$ and thus simply the compliment of the cdf: $S_Y(\tau) = 1-F_Y(\tau)$.

The hazard function is given by
$$
h_Y(\tau) = \frac{f_Y(\tau)}{S_Y(\tau)}.
$$

The hazard function is interpreted as the instantaneous risk of death given that the observation has survived up until that point.
This is not a probability and $h_Y$ can be greater than one.

The cumulative hazard function (chf) can be derived from the hazard function by
$$
H_Y(\tau) = \int^\tau_0 h_Y(u) \ du
$$

The cumulative hazard function relates to the survival function by
$$
H_Y(\tau) = \int^\tau_0 h_Y(u) \ du = \int^\tau_0 \frac{f_Y(u)}{S_Y(u)} \ du = \int^\tau_0 -\frac{S'_Y(u)}{S_Y(u)} \ du = -\log(S_Y(\tau))
$$

These last relationships are particularly important, as many methods estimate the hazard rate, which is then used to calculate the cumulative hazard and survival probability
$$S_Y(\tau) = \exp(-H_Y(\tau)) = \exp\left(-\int_0^\tau h_Y(u)\ du\right).$${#eq-surv-haz}


Unless necessary to avoid confusion, subscripts are dropped from $S_Y, h_Y$ etc. going forward and instead these functions are referred to as $S$ and $h$ (and so on).

Normally, these quantities could be estimated using standard techniques like regression modeling (estimation of parameters of an assumed distribution).
However, in contrast to standard settings, $Y$ is only observed partially, due to different types of censoring and truncation describeb below.


### Single-event, right-censored data {#sec-data-rc}

Survival analysis has a more complicated data setting than other fields as the 'true' data generating process is not directly observable but instead engineered variables are defined to capture observed information. Let,

* $X \ t.v.i. \ \calX \subseteq \Reals^p, p \in \PNaturals$ be the generative random variable  representing the data *features*/*covariates*/*independent variables*.
* $Y \ t.v.i. \ \calY \subseteq \NNReals$ be the (partially unobservable) *true survival time*.
* $C \ t.v.i. \ \calC \subseteq \NNReals$ be the (partially unobservable) *true censoring time*.


The object of interest in survival analysis is $Y$.
However, in the presence of censoring $C$, it is impossible to fully observe $Y$.
Instead, the observable variables are given by

* $T := \min\{Y,C\}$, the *outcome time* (realisations of this random variable will be refered to as *observed outcome time*).
* $\Delta := \II(Y = T) = \II(Y \leq C)$, the *event indicator* (also known as the *censoring* or *status* indicator).


Together $(T,\Delta)$ is referred to as the *survival outcome* or *survival tuple* and they form the dependent variables.
The survival outcome provides a concise mechanism for representing the outcome time and indicating which outcome (event or censoring) took place.

A *survival dataset* is a $n \times p$ Real-valued matrix defined by $\calD = ((\xx_1 \ t_1 \ \delta_1) \cdots (\xx_n,t_n,\delta_n))^\trans$, where $(t_i,\delta_i)$ are realisations of the respective random variables  $(T_i, \Delta_i)$ and $\xx_i$ is a $p$-dimensional row-vector, $\xx_i = (x_{i1} \ x_{i2} \cdots x_{ip})$.

Finally the following terms are used frequently throughout this book.
Let $(t_i, \delta_i) \iid (T,\Delta), i = 1,...,n$, be observed survival outcomes.
Then,

* The *set of unique* or *distinct time-points* refers to the set of time-points in which at least one observation dies or is censored, $\calU_O \subseteq \{t_i\}_{i \in \{1,...,n\}}\subseteq \calD$.
* The *set of unique observed event times* refers to the set of unique time-points in which an event (and not censoring) occurred, $\calU_D := \{t_{i} : \delta_i = 1\}_{i \in \{1,...,n\}}\subseteq \calD$. Sometimes the ordered, unique events times are also denoted by $t_{(i)},\ i=1,\ldots,m \leq n,\ t_{(1)} < t_{(2)} < \cdots < t_{(m)}$.
* The *risk set* at a given time-point, $\tau$, is the index-set of observation units at risk for the event just before $\tau$, $\calR_\tau := \{i: t_i \geq \tau\}$ where $i$ is a unique row/subject in the data. Consequently, for right-censored data, we have $\calR_0 = \{1,\ldots,n\}$ and $\calR_{\tau} \supseteq \calR_{\tau'}, \forall \tau < \tau'$. Note that in a continuous setting, 'just before' refers to an infinitesimally smaller time than $\tau$, in practice as this is unobservable the risk set is defined at $\tau$. 
* The *number of observations at risk* at $\tau$ is the cardinality of the risk set, $|\calR_\tau|$, and is denoted by $n_\tau := \sum_i \II(t_i \geq \tau)$.
* The *number of events* at $\tau$ is denoted by $d_\tau := \sum_i \II(t_i = \tau, \Delta_i = 1).$ Note: For truly continuous variables $T_i$ we would expect $d_{t_i} = 1,\forall i=,1\ldots,n$, however, in practice we often observe ties due to finite measurement precision, such that $d_{\tau} > 1$ occurs quite frequently in real-world datasets.

The quantities $\calR_\tau$, $n_\tau$, and $d_\tau$ underline many models and measures in survival analysis as non-parametric methods (@sec-models-classical) including the Kaplan-Meier estimator [@Kaplan1958] are based on the ratio $d_\tau \ n_\tau$.


@tbl-surv-data-rats exemplifies an observed survival dataset with a modified version of the `rats` data  [@pkgsurvival], which contains the time until occurence of a tumor ($\delta_i=1$ if a tumor occured at the outcome time $t_i$ and $\delta_i = 0$ otherwise).
In this example, the above quantities would be:

* $\calU_0 = \{49, 91, 101, 102, 104\}$: with $104$ included only once
* $\calU_D = \{49, 102, 104\}$: with the inclusion of $104$ due to the event at $t_5$, not censoring at $t_3$
* $\calR_{\tau = 102} = \{3, 5, 6\}$ (these rats are at risk for the event shortly before $\tau = 102$, rats $1$ and $4$ are censored beforehand and rat $2$ already developed a tumor).
* $n_{\tau = 102} = |\calR_{102}| = 3$
* $d_{\tau = 102} = 1$: As only $i = 5$ experienced the event (and not censoring) at this time.

| **ID** ($i$) | **litter** $(\xx_{;1})$ | **rx** $(\xx_{;2})$ | **sexF** $(\xx_{;3})$ | **time** ($\tt$) | **status** ($\dd$) |
| -- | -- | -- | --- | -- | --|
| 1 | 1 | 1 | 1 | 101 | 0 |
| 2 | 1 | 0 | 1 | 49 | 1 |
| 3 | 1 | 0 | 1 | 104 | 0 |
| 4 | 2 | 1 | 0 | 91 | 0 |
| 5 | 2 | 0 | 0 | 104 | 1 |
| 6 | 2 | 0 | 0 | 102 | 1 |

: Subset of the `rats`  [@pkgsurvival] time-to-event dataset.
Rows are individual observations ($ID$), $\xx;j$ columns are features, $t$ is observed time-to-event, $\delta$ is the event indicator. {#tbl-surv-data-rats}


* The Kaplan-Meier estimate of the average survival function of the training data *survival distribution* is the Kaplan-Meier estimator (@sec-surv-models-uncond) fit (@sec-surv-setml-meth) on training data $(T_i, \Delta_i)$ and is denoted by $\KMS$.
* The Kaplan-Meier estimate of the average survival function of the training data *censoring distribution* is the Kaplan-Meier estimator fit on training data $(T_i, 1 - \Delta_i)$ and is denoted by $\KMG$.



### Types of Censoring

In *Survival Analysis* three types of censoring are commonly defined

* right-censoring,
* left-censoring, and
* interval-censoring

The latter can be viewed as the most general case with the other types being special cases.
Multiple types of censoring and/or truncation (@sec-truncation) can occur in any given data set and it is vital to identify which types are present in order to correctly select models and measures for the data.
Incorrect specification of the data cannot be picked up by models and measures alone and may lead to misleading results.


#### Right-censoring Types {.unnumbered .unlisted}

Right-censoring is the most common form of censoring in survival data and it occurs either when an observation drops out of the study before the end and before experiencing the event, or when they experience the event after the observation period.
In either case, their true outcome time is unknown.
Formally let $[\tau_l, \tau_u]$ be the study period for some, $\tau_l,\tau_u \in \NNReals$.
Then right-censoring occurs when either $Y > \tau_u$, or when $Y \in [\tau_l,\tau_u]$ and $C \leq Y$.
In the first case $T = C = \tau_u$ and censoring is due to the true time-to-event being unknown as the observation period has finished.
In the latter case, a separate censoring event, such as drop-out or another competing risk, is observed.

![Dead and censored subjects (y-axis) over time (x-axis). Black diamonds indicate true death times and white circles indicate censoring times. Vertical line is the study end time. Subjects 1 and 2 die in the study time. Subject 3 is censored in the study and (unknown) dies within the study time. Subject 4 is censored in the study and (unknown) dies after the study. Subject 5 dies after the end of the study.](Figures/survival/censoring.png){#fig-survset-censor fig-alt="TODO"}
<-- FIXME: ADD REF TO THESIS IF WE KEEP THIS FIGURE -->
Sometimes, right-censoring is also subdivided into Type-I, Type-II and Type-III censoring.
In Type-I, or *administrative*, censoring occurs at the fixed, pre-defined end of the study $\tau_u$, in which case we get $(T_i = \min(Y_i, \tau_u), \II(Y_i \leq \tau_u))$.
Type-II censoring also occurs when the study ends, however, in this case the study ends when a pre-defined number of subjects experienced the event of interest and so $\tau_u$ is random.
Type-III censoring occurs when censoring times *randomly* follow an unknown distribution and we observe $(T_i=\min(Y_i,C_i), \II(Y_i\leq C_i))$.
Different types of right-censoring can, and often do, co-occur in any given data set.

In practice, these different types of right censoring are usually handled the same during modeling and evaluation and so this book refers to 'right censoring' generally, which could occur from any combination of the above types.


#### Left-censoring {.unnumbered .unlisted}
Left-censoring occurs when the event happens at some unknown time before the study start.
While quiet rare in medical settings, this type of data often occurs in sociology studies when retrospective interviews are conducted.

Consider a survey about phone use where smartphone users are all asked the question "How old were you when you used a smartphone for the first time?".
Let $A_i$ denote the age of the participants during the interview, let $T_i$ be the time at which they first used a smartphone, and let $\Delta_i$ be the usual censoring indicator with $1$ indicating the event of interest and $0$ otherwise.
If the participant remembers the age they first used a phone then $(T_i, \Delta_i) = (Y_i, 1)$ where $Y_i \leq A_i$.
However, if the individual does not remember when the event occured, then they are left-censored at $A_i$ and $(T_i, \Delta_i) = (A_i, 0)$, where $A_i < Y_i$.

In this example, right-censoring was ignored so $\Delta_i$ could be used as a simple binary ($0/1$) indicator.
In practice, when left- and right-censoring are both present, an additional variable is required to differentiate between left- and right-censoring as well as exact event times.
This is often solved by removing the censoring indicator from the data and adding a second time column, then left-censored data is given by $(-\infty, A_i)$, right-censored data is denoted by $(A_i, \infty)$, and actual event times are $(T_i, T_i)$.


#### Interval-censoring {.unnumbered .unlisted}

Interval-censoring occurs when the event takes place in some interval within the study period, but the exact time of event is unknown.
This often occurs in data resulting from regular or irregular check-ups, as often occurs in electronic health record data.
For example, say one patient is checked for skin cancer every year and another every two years.
This data would be collected on a yearly scale and over three years may look something like:

| Patient | Year | Status |
| --- | --- | --- |
| 1 | 1 | 0 |
| 1 | 2 | 1 |
| 1 | 3 | 1 |
| 2 | 1 | 0 |
| 2 | 2 | ? |
| 2 | 3 | 1 |

As data is collected annually for patient 1, it is *known* that they had skin cancer in year 2.
On the other hand, patient 2's data is only collected every other year, so whilst it is known they had cancer in year 3, it is unknown if they already had cancer the year before (assuming for know there's no method to test how 'old' the cancer is).
In this case, the second patient is interval censored between years one and three.

Formally, consider $L_i$ the subject-specific check-up time and $R_i$ the current check-up time.
If the event is observed at $R_i$ then it must have occurred sometime before then, hence $T_i = (L_i, R_i]$.
If no event occured then $Y_i > T_i = C_i = R_i$.



### Censoring 'Dependence' {.unnumbered .unlisted}

Censoring may be defined as *uninformative* if $Y \indep C$ and *informative* otherwise.
However, these definitions can be misleading as the term 'uninformative' could imply that $C$ is independent of both $X$ *and* $Y$, and not just $Y$.
To avoid misinterpretation, the following definitions are used in this book.

::: {#def-cens}

## Independent Censoring

Let $(X,T,\Delta,Y,C)$ be defined as above, then

* If $C \indep X$, censoring is *feature-independent*, otherwise censoring is *feature-dependent*.
* If $C \indep Y$, then censoring is *event-independent*, otherwise censoring is *event-dependent*.
* If $(C \indep Y) | X$, censoring is conditionally independent of the event given covariates, or *conditionally event-independent*.
* If $C \indep (X,Y)$ censoring is *uninformative*, otherwise censoring is *informative*.

:::

Non-informative censoring can generally be well-handled by models as true underlying patterns can still be detected and the reason for censoring does not affect model inference or predictions.
However, in the real-world, censoring is rarely non-informative as reasons for drop-out or missingness in outcomes tend to be related to the study of interest.
Event-dependent censoring is a tricky case that, if not handled appropriately (by a competing-risks framework), can easily lead to poor model development; the reason for this can be made clear by example.
Say a study is interested in predicting the time between relapses of stroke but a patient suffers a brain aneurysm due to some separate neurological condition, then there is a high possibility that a stroke may have occurred if the aneurysm had not.
A survival model is unlikely to distinguish the censoring event (aneurysm) from the event of interest (stroke) and will confuse predictions.
In practice, the majority of models and measures assume that censoring is conditionally event-independent and hence censoring can be predicted by the covariates whilst not directly depending on the event.
For example, if studying the survival time of ill pregnant patients in hospital, then dropping out of the study due to pregnancy is clearly dependent on how many weeks pregnant the patient is when the study starts (for the sake of argument assume no early/late pregnancy due to illness).


### Censoring vs. Truncation {#sec-truncation}
While sometimes confused or missnamed, it is very important to differentiate between censoring and truncation, as the methods for handling them differ substantially.
Truncation can occur in non time-to-event settings, however this usually refers to truncating (or removing) an entire subject from a dataset.
As discussed in @sec-intro, truncation in survival analysis refers to partially truncating a period of time.
Left truncation is especially common in survival analysis.

In general, while censored observations have incomplete information about the time-to-event, they are still part of the data set.
Truncation on the other hand often leads to observations not entering the data set (at least not at time 0).
This will usually introduce bias that needs to be accounted for.

#### Left-truncation {.unnumbered .unlisted}


Left-truncation often occurs when study participation is conditional on the occurence of another event.
This is best explained through examples:

In one famous study (reference), the interest was in infant survival during the first 365 days after birth based on whether the mother was alive.
Data was collected by adding an infant to the study when their mother died, then two other infants, whose mothers were alive, were matched into the study based on their age and features.
The age of the infant at entrance into the study therefore signifies a so-called *left-truncation event*, as infants who die before their mothers never enter the data and the *left-truncation time* is given by the infants' age at which the mother died.

More formally, let $T^l_i$ the subject-specific left truncation time.
Then we only observe subjects with $Y_i > T^l_i$ or $C_i > T^l_i$.
Subjects with $Y_i < T^l_i$ never enter the data.

Notably, left-truncation also plays an important role when modeling recurrent events or multi-state data, as the data generating process induces left-truncation (@sec-eha).


#### Right-truncation {.unnumbered .unlisted}

Right-truncation mostly occurs in analyses based on registry data.
When sampling the registry for events, some subjects might already have experienced the event, but the registry data at a given time-point does not reflect this yet.

Formally, let $T_i^r$ the right-truncation time, then subjects only enter the data set when $T_i < T_i^r$.


### Objective functions

This section describes how the likelihood is constructed for observations subject to different types of censoring and truncation.
For machine learning survival analysis this can be used to construct the objective (or loss) function for any survival task.

{{< include _wip.qmd >}}



## Event-history Analysis {#sec-eha}

In this section we will go beyond single-event data or at least view time-to-event data more generally as data in which one could observe multiple, potentially mutually exclusive events.
From this more general point of view such data is sometimes refered to *event-history data* and its analysis as *event-history analysis*.

It may be easiest to think of event history in terms of transitions between different states, as illustrated in @fig-eha.
Usually, a subject starts out in state 0 (for example, healthy) and transitions to different states from there.
States from which a further transitions are possible are called *transient*, otherwise a state is called *terminal*.
If no transition happens within the follow-up time, the subject remains in the initial state, in event history terms the observation is censored for this transition.

In the *single-event setting* (@fig-eha, left panel), a subject can only transition to one, terminal, state (for example, death caused by the event of interest).
In the *competing risks setting* (@fig-eha, middle panel, see @sec-competing-risks), a subject could transition to any of the $K$ mutually exclusive states, thus the subject is *at risk* for a transition to multiple states.
However, once one of them occurs, it is impossible to transition to another (for example, death from different causes).
In the most general case, the *multi-state setting* (@fig-eha right panel, see @sec-multi-state), there are multiple transient and terminal states with potential back transitions (for example, moving between different stages of an illness with the possibility of (partial) recovery and death as terminal event).
The *recurrent events setting* (@sec-recurrent-events) is as a special case of the multi-state setting but could also be treated as a more general case of the single event setting (with correlated observations) and is thus treated seperately.

![Illustration of different types of time-to-event outcomes. Left: Standard single-event setting with transition from initial state (0) to terminal state (1); Middle: Competing risks setting with $k$ competing events. Once one of the $\{1,\ldots, k\}$ events is observed, the others cannot be observed; Right: Multi-state setting where subjects can transition between multiple transient and terminal states with possible back-transitions.](Figures/survival/event-history-analysis.svg){#fig-eha fig-alt="Schematic illustration of event history analysis."}


### Competing Risks {#sec-competing-risks}

In constrast to single-event survival analysis, competing risks are concerned with the time to the first of many, mutually exclusive events.
Mutually exclusive here means that once one of the events occurs, we can not observe the others.
That doesn't mean, that the events have to be terminal (e.g. death), however, the observation of the process ends at the occurence of one of the events.

Formally, let $Y$ be the time-to-event as before.
In the competing risks setting, there is now an additional random variable $E_Y\in \{1,\ldots,k\}$, which denotes one of $k$ competing events that can occur at event time $Y$.
The survival outcome notation, $(T, \Delta)$ has to be extended to accomodate for the possible competing risks.
Previously, $\Delta = 1$ if right-censoring is observed, this can be modified by multiplying $\Delta$ by the index of the event that occurs.
Now, $\Delta \in \{0, 1, \ldots, k\}$ where $\Delta = 0$ still indicates censoring and $\Delta = j$ means the $j$th event occurred.

The predominate approach to competing-risks analysis is known as *cause-specific hazards*, where $h_{0e}$ is defined as the hazard for the transition from initial state $0$ to state $e =1,\ldots,k$:

$$
h_{0e}(\tau) = \lim_{\Delta \tau \to 0} \frac{P(\tau \leq Y \leq \tau + \Delta \tau, E = e\ |\ Y \geq \tau)}{\Delta \tau}, \; e = 1, \dots, k.
$$

As competing events are mutually exclusive at any time $\tau$, it is useful to further define the:

- *all-cause hazard*, $h(\tau) = \sum_{e = 1}^{k}h_{0e}(\tau)$: which is the...
- *all-cause cumulative hazard*: $H(\tau) = \int_{0}^\tau h(u)\ du = \sum_{e=1}^{k}\int_{0}^\tau h_{0e}(u)\ du = \sum_{e=1}^k H_{0e}(\tau)$, and
- *all-cause survival probability*: $S(\tau) = P(Y \geq \tau) = \exp(-H(\tau))$

Note that whilst the all-cause survival probability is defined in the same way as the standard survival probability, the interpretation is that *none* of the events occurred before $\tau$.
The all-cause survival probability is also not directly estimated but instead calculated from the cause specific hazards.

Another important distribution function in competing risks analysis is the *Cumulative Incidence Function* (CIF), which denotes the probability of experiencing an event $E$ before time $\tau$

$$
F_{e}(\tau) = P(Y \leq \tau, E = e) = \int_{0}^\tau h_{0e}(u)S(u)\ du.
$$
Intuitively the CIF is the probability of not experiencing any event, $S(u)$, multiplied with the hazard of experiencing an event of type $e$, $h_{oe}(u)$.

Calculating the CIF (in addition to the cause-specific hazards) is particularly important when making statements about the absolute risk (probability) for an event of type $e$, as it depends on all cause-specific hazards via $S(\tau)$.

This will be particularly relevant when assesing how features affect the probability for observing a specific event.
While some features might strongly affect some of the cause-specific hazards, if an event has overall low prevalence, then the absolute probability of observing the event will still be low.
Hence the CIF is useful for incorporating this information into an interpretable quantity.

To estimate the cause-specific hazards, an event-specific binary indicator is required, such that the indicator is $1$ if the event of interest is observed or $0$ for any other event, effectively treating them as censoring:
$$\delta_{ie} = \II(Y_i \leq C_i \wedge E_i = e) \in \{0, 1\},\ e=1,\ldots,k$$

With this definition, $k$ data sets $(t_i, \delta_{ie}, \bx_i)$, one for each of the $k$ events, can be fit to a separate model in order to obtain cause-specific hazards, as if in the single-event setting.
Once all hazards are estimated, the all-cause survival probability and CIFs can be calculated.

Following @beyersmann.competing.2012, this book assumes that the cause-specific hazards model fully describes the data generating processes and doesn't require an independence assumption for the competing events.



### Multi-state Models {#sec-multi-state}

{{< include _wip.qmd >}}


### Recurrent Events {#sec-recurrent-events}

{{< include _wip.qmd >}}


## Survival Prediction Problems {#sec-surv-set-types}

This section continues by defining the survival problem.
Defining a single 'survival prediction problem' (or 'task') is important mathematically as conflating survival problems could lead to confused interpretation and evaluation of models.
Let $(\XX,\tt,\dd)$ and $\calD$ be as defined in @sec-surv-set-math.
A general survival prediction problem is one in which:

* a survival dataset, $\calD$, is split (@sec-surv-setml-meth) for training, $\dtrain$, and testing, $\dtest$;
* a survival model is fit on $\dtrain$; and
* the model predicts some representation of the unknown true survival time, $Y$, given $\dtest$.


The process of fitting is model-dependent, and can range from simple maximum likelihood estimation of model coefficients, to complex algorithms.
The model fitting process is discussed in more abstract detail in (@sec-surv-setml) and then concrete algorithms are discussed in (@sec-review).
The different survival problems are separated by *prediction types* or *prediction problems*, which can also be thought of as predictions of different representations of $Y$.
Four prediction types are possible:

1. The *relative risk* of an individual for experiencing an event: A single continuous ranking.
2. The *time until an event* occurs: A single continuous value.
3. The *prognostic index* for a model: A single continuous value.
4. The *survival distribution*: A probability distribution.


The first three of these are referred to as *deterministic* problems as they predict a single value whereas the fourth is *probabilistic* and returns a full survival distribution.
Definitions of these are expanded on below but first note that survival predictions differ from other fields in two respects:

* The outcome of interest is $Y$, which is different to the outcome used for model training, $(T=\min(Y, C), \Delta=\II(Y\leq C))$.
This differs from, say, regression in which the same object (a single continuous variable) is used for fitting and predicting.
* With the exception of the time-to-event prediction, all other prediction types do not predict $\EE(Y)$ but some other related quantity.

Survival prediction problems must be clearly separated as they are inherently incompatible.
For example, it is not meaningful to compare a relative risk prediction from one observation to a survival distribution of another.
Whilst these prediction types are separated above, they can be viewed as special cases of each other. Both (1.) and (2.) may be viewed as variants of (3.); and (1.), (2.), and (3.) can all be derived from (4.); this is elaborated on below and discussed fully in @sec-car.

#### Relative Risk/Ranking {.unnumbered .unlisted}

This is a common survival problem and is defined as predicting a continuous rank for an individual's relative risk of experiencing the event.
For example, given three subjects, $\{i,j,k\}$, a relative risk prediction may predict the risk of event as $\{0.1, 0.5, 10\}$ respectively.
From these predictions, the following types of conclusions can be drawn:

* Conclusions comparing subjects. For example, $i$ is at the least risk; the risk of $j$ is only slightly higher than that of $i$ but the risk of $k$ is considerably higher than $j$; the corresponding ranks for $i,j,k,$ are $1,2,3$;
* Conclusions comparing risk groups. For example, thresholding the risks at $1.0$ means that $i$ and $j$ are in a low-risk group whilst $k$ is in a high-risk group.

So whilst many important conclusions can be drawn from these predictions, the values themselves have no meaning when not compared to other individuals.
Interpretation of these rankings has historically been conflicting, with some using the interpretation "higher ranking implies higher risk" whereas others may assume "higher ranking implies lower risk".
The difference is often due to model types (e.g., PH and AFT models have opposite interpretation, @sec-models-classical) but can also be due to software implementation differences.
In this book, a higher ranking will always imply a higher risk of event (as in the example above).

#### Time to Event {.unnumbered .unlisted}
Predicting a time to event is the problem of predicting the expectation $\hat{y}=\EE(y|\xx)$.
A time-to-event prediction is a special case of a ranking prediction as an individual with a longer survival time will have a lower overall risk: if $\hat{y}_i,\hat{y}_j$ and $\hat{r}_i,\hat{r}_j$ are survival time and ranking predictions for subjects $i$ and $j$ respectively, then $\hat{y}_i > \hat{y}_j \rightarrow \hat{r}_i < \hat{r}_j$.

For practical purposes, time-to-event would be the ideal prediction type as it is easy to interpret and communicate.
However, this type of prediction is rare because only a subset of the domain of $Y$ is observed due to censoring.
For non-parametric methods, the estimated cdf is improper and thus integration requires extrapolation of the cdf.
For parametric models, the distribution of event times is fully specified once the paramers of the assumed distribution have been estimated, however, if the parameters were estimated based on only a small subset of the possible domain of $Y$, this essentially still constitutes extraploation and will in most cases yield implausible predictions.
A popular alternative is therefore to estimate the *restricted mean survival time* (RMST) [@han.restricted.2022; @andersen.regression.2004].

<!-- FIXME add example -->

#### Prognostic Index {.unnumbered .unlisted}
Given covariates, $\XX \in \Reals^{n \times p}$, and a vector of model coefficients, $\bbeta \in \Reals^p$, the linear predictor is defined by $\boldsymbol{\eta}:= \XX^\trans\bbeta \in \Reals^n$.
The prognostic index is a term often used to refer to some transformation, $\phi$, on the linear predictor, $\phi(\eta)$, (which could simply be the identity function $(f(x) = x)$).
Assuming a predictive function (for survival time, risk, or distribution defining function) of the form $g(\varphi)\phi(\eta)$, for some function $g$ and variables $\varphi$ where $g(\varphi)$ is constant for all observations (e.g. Cox PH (@sec-surv-models-crank)), then predictions of $\eta$ are a special case of predicting a relative risk, as predictions of $\phi(\eta)$ if $\phi$ is rank preserving -- this assumptions is sometimes written as "if there is a one-to-one mapping between the prediction and the expected survival times".
A higher prognostic index may imply a higher or lower risk of event, dependent on the model structure.
As stated above, if the prognostic index is used for a rank prediction, then this book always assumes "higher value higher risk".

<!-- FIXME example needed -->

#### Survival Distribution {.unnumbered .unlisted}
Predicting a survival distribution refers specifically to predicting the distribution of an individual subject's survival time, i.e., modelling the distribution of the event occurring over $\NNReals$.
Therefore, this is seen as the probabilistic analogue to the deterministic time-to-event prediction, these definitions are motivated by similar terminology in machine learning regression problems (@sec-surv-setml).

Distributional prediction can in theory target any of the quantities introduced in @sec-distributions but predicting $S(t)$ and/or $h(t)$ is most common.
Hazard based approaches are particularly relevant for non- and semi-parametric estimation of the distribution, where no assumptions are made about the underlying distribution of event times.


## Survival Analysis Task {#sec-surv-setmltask}

The survival prediction problems identified in (@sec-surv-set-types) are now formalised as machine learning tasks.

:::: {.callout-note icon=false}

## Survival Task

::: {#cnj-task-surv}
Let $(X,T,\Delta)$ be random variables t.v.i. $\calX \times \calT \times \bset$ where $\calX \subseteq \Reals^p$ and $\calT \subseteq \NNReals$.
Let $\calS \subseteq \distrT$ be a convex set of distributions on $\calT$ and let $\calR \subseteq \Reals$.
Then,

* The *probabilistic survival task* is the problem of predicting a conditional distribution over the positive Reals and is specified by $g: \calX \rightarrow \calS$.
* The *deterministic survival task* is the problem of predicting a continuous value in the positive Reals and is specified by $g: \calX \rightarrow \calT$.
* The *survival ranking task* is specified by predicting a continuous ranking in the Reals and is specified by $g: \calX \rightarrow \calR$.

:::
::::

Any other survival prediction type falls within one of the tasks in @cnj-task-surv, for example predicting log-survival time is the deterministic task and predicting prognostic index or linear predictor is the ranking task.
Removing the separation between the prognostic index and ranking prediction types is due to them both making predictions over $\Reals$ and hence their difference lies in interpretation only.

In this book, unless otherwise specified, the term *survival task*, will be used to refer to the probabilistic survival task.
As a final note, these definitions are given in the most general case where the time variable is over $\NNReals$.
In practice, all models instead assume time is over $\PReals$ and so a subject $i$ that experiences an outcome at $0$ is either deleted or their outcome time is set to $T_i = \epsilon$ for some very small $\epsilon \in \PReals$. 

<!-- FIXME I got rid of these notation earlier, but don't see how the following is very usefull without them. On the other hand the connection to regression and classification could useful, but could also be introduced later when the actual reductions are introduced.
#### Survival Analysis and Regression {.unnumbered .unlisted}



Survival and regression tasks are closely related as can be observed from their respective definitions. Both are specified by $g : \calX \rightarrow \calS$ where for probabilistic regression $\calS \subseteq \Distr(\Reals)$ and for survival $\calS \subseteq \Distr(\NNReals)$.
Furthermore both settings can be viewed to use the same generative process.
In the survival setting in which there is no censoring then data is drawn from $(X,Y) \ t.v.i. \ \calX \times \calT, \calT \subseteq \NNReals$ and in regression from $(X,Y) \ t.v.i. \ \calX \times \calY, \calY \subseteq \Reals$, so that the only difference is whether the outcome data ranges over the Reals or positive Reals.

These closely related tasks are discussed in more detail in (@sec-car), with a particular focus on how the more popular regression setting can be used to solve survival tasks.
In (@sec-review) the models are first introduced in a regression setting and then the adaptations to survival are discussed, which is natural when considering that historically machine learning survival models have been developed by adapting regression models.
-->
