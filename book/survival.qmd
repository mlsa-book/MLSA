---
abstract: TODO (150-200 WORDS)
---

{{< include _setup.qmd >}}

# Survival Analysis {#sec-surv}

{{< include _wip.qmd >}}

___

TODO

* Make sure intro is clear about censoring/truncation and that metrics can't highlight if this is setup wrong - analogously to hypothesis testing not testing the result but hypothesis, p-hacking, etc.
* If measures for right-censoring used in parts of pipelines hard to discern biases if wrong type of measure used
* Same as dependent/independent censoring and measures problem
* Estimation of transition hazards/transition probabilities
* Event-history analysis (Graph of different types of transitions and setups)

___

<!-- In their broadest and most basic definitions, survival analysis is the study of temporal data from a given origin until the occurrence of one or more events or 'end-points'  [@Collett2014], and machine learning is the study of models and algorithms that learn from data in order to make predictions or find patterns  [@Hastie2001]. Reducing either field to these definitions is ill-advised. -->
<!--
Survival analysis is concerned with time-to-event data, i.e. data where the outcome is a duration from some origin until the occurrence of one or multiple events of interest.
In many medical settings, this duration will often be literal survival time, i.e. time until death after some procedure or treatment.
When collecting such data, however, the outcome of interest can often be not observed (fully), e.g. because subjects drop out of the study, haven't experienced the event of interest until end of study period or due to the occurence of another, competing event.

Often analysis of time-to-event data targets the estimation of the (improper) distribution of the event times or, equivalently, modelling the transitions between different states (e.g. alive -> dead) while taking into account censoring and truncation as well as other peculiarities of time-to-event data. However, the target of estimation can also be a relative risk score or the expected time-to-event (cf. @sec-surv-set-types for details).
 -->


 As discussed in the introduction (@sec-intro), *Survival Analysis* is concerned with data where the outcome is a time-to-event.
 Because the collection of such data takes place in the temporal domain (it takes time to observe a duration), the event of interest is often unobservable, e.g. because it didn't occur by the end of the data collection period or because of the occurrence of another, competing event.
 In survival analysis terminolgy these are refered to as *censoring* and *competing risks*.

In this chapter we define these and related terms, introduce some basic terminology and mathematical definitions in @sec-surv-set-math, starting of with the common single-event, right-censored data setting and then extend to further types of censoring as well as truncation.
In Section @sec-eha we will discuss settings with multiple, potentially competing or recurrent events.
Section @sec-surv-set-types defines common prediction types of survival models, which is particularly important for machine learning based survival analysis and a common source of confusion.
Finally, in order to cleanly discuss 'machine learning survival analysis', we will introduce the 'Survival Task' [@Kiraly2021] in Section @sec-surv-setmltask as an addition to the more common 'Regression' and 'Classification' tasks.
While the exposition in this book will mostly focus on the survival task with single-event, right-censored data, it is important to be aware of the other settings.

It is of utmost importance to identify and specify the survival task correctly, as misspecification cannot be detected by comparing the predictive performance of alternate models via evaluation measures.
Evaluation measure can only detect if one model is better suited to minimize the obejective function, but not whether or not the obejective function is correct.
The latter depends on the (assumptions about the) data generating process and has to be also reflected in the definition of the evaluation measure.


## Survival Data and Definitions {#sec-surv-set-math}




### Single-event, right-censored data {#sec-data-rc}

Survival analysis has a more complicated data setting than other fields as the 'true' data generating process is not directly observable but instead engineered variables are defined to capture observed information. Let,

* $X \ t.v.i. \ \calX \subseteq \Reals^p, p \in \PNaturals$ be the generative random variable  representing the data *features*/*covariates*/*independent variables*.
* $Y \ t.v.i. \ \calY \subseteq \NNReals$ be the (unobservable) *true survival time*.
* $C \ t.v.i. \ \calC \subseteq \NNReals$ be the (unobservable) *true censoring time*.


Note that we are interested in the properties of $Y$. However, in the presence of censoring $C$, it is impossible to fully observe both $Y$.
Instead, the observable variables are given by

* $T := \min\{Y,C\}$ be the *observed outcome time*.
* $\Delta := \II(Y = T) = \II(Y \leq C)$ be the *survival indicator* (also known as the *censoring* or *event* indicator).\footnote{Indicators are usually named to reflect a positive condition in the function (in this case the event when $Y = T$), but counter to this convention the 'censoring indicator' is possibly the more common term.}


Together $(T,\Delta)$ is referred to as the *survival outcome* or *survival tuple* and they form the dependent variables.
The survival outcome provides a concise mechanism for representing the time of the *observed* outcome and indicating which outcome (event or censoring) took place.

<!-- Now the full generative template for survival analysis is given by \\ $(X, C, Y) \ t.v.i. \ \calX \times \calC \times \calY$ and with $(X_i, C_i, Y_i)$ jointly i.i.d. -->
A *survival dataset* is defined by $\calD = \{(X_1,T_1,\Delta_1),...,(X_n,T_n,\Delta_n)\}$ where $(X_i,T_i,\Delta_i) \iid (X,T,\Delta)$ and $X_i$ is a $p$-vector, $X_i = (X_{i;1},...,X_{i;p})$. Though unobservable, the true outcome times are defined by $(Y_1,C_1),...,(Y_n,C_n)$ where $(Y_i,C_i) \iid (Y,C)$.

(@tab-surv-data-abs) exemplifies a random survival dataset with $n$ observations (rows) and $p$ features.

| X | X | X | T | $\Delta$ | Y | C |
| -- | -- | --- | -- | --| -- | -- |
| $X_{11}$ | $\cdots$ | $X_{1p}$ | $T_1$ | $\Delta_1$ | $Y_1$ | $C_1$ |
| $\vdots$ | $\ddots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ | $\vdots$ |
| $X_{n1}$ | $\cdots$ | $X_{np}$ | $T_n$ | $\Delta_n$ | $Y_n$ | $C_n$ |

: Theoretical time-to-event dataset. $(Y,C)$ are 'hypothetical' as they can never be directly observed. Rows are individual observations, $X$ columns are features, $T$ is observed time-to-event, $\Delta$ is the censoring indicator, and $(Y,C)$ are hypothetical true survival and censoring times. {#tbl-surv-data-abs}

(@tab-surv-data-rats) exemplifies an observed survival dataset with a modified version of the `rats` dataset  [@pkgsurvival].

| **litter** $(X_{.;1})$ | **rx** $(X_{.;2})$ | **sexF** $(X_{.;3})$ | **time** (T) | **status** ($\Delta$) | **survTime** (Y) | **censTime** (C) |
| -- | -- | --- | -- | --| -- | -- |
| 1 | 1 | 1 | 101 | 0 | 105 | 101  |
| 1 | 0 | 1 | 49 | 1 | 49 | 55|
| 1 | 0 | 1 | 104 | 0 | 200 | 104 |
| 2 | 1 | 0 | 91 | 0 | 92 | 91 |
| 2 | 0 | 0 | 104 | 1 | 104 | 104 |
| 2 | 0 | 0 | 102 | 1 | 102 | 120 |

: `rats`  [@pkgsurvival] time-to-event dataset with added hypothetical columns ($Y,C$). Rows are individual observations, $X$ columns are features, $T$ is observed time-to-event, $\Delta$ is the censoring indicator, and $(Y,C)$ are hypothetical (here arbitrary values dependent on $(T,\Delta)$) true survival and censoring times. {#tbl-surv-data-rats}

Both datasets includes two extra columns, on the right of the triple vertical line, which imagine hypothetical data for the unobserved true survival and censoring times.

Finally the following terms are used frequently throughout this report. Let $(T_i, \Delta_i) \iid (T,\Delta), i = 1,...,n$, be random survival outcomes. Then,

* The *set of unique* or *distinct time-points* refers to the set of time-points in which at least one observation dies or is censored, $\calU_O := \{T_i\}_{i \in \{1,...,n\}}$.
* The *set of unique death times* refers to the set of unique time-points in which death (and not censoring) occurred, $\calU_D := \{T_i : \Delta_i = 1\}_{i \in \{1,...,n\}}$.
* The *risk set* at a given time-point, $\tau$, is the set of subjects who are known to be alive (not dead or censored) just before that time, $\calR_\tau := \{i: T_i \geq \tau\}$ where $i$ is a unique row/subject in the data.
* The *number of observations alive* at $\tau$ is the cardinality of the risk set, $|\calR_\tau|$, and is denoted by $n_\tau := \sum_i \II(T_i \geq \tau)$.
* The *number of observations who die* at $\tau$ is denoted by $d_\tau := \sum_i \II(T_i = \tau, \Delta_i = 1)$.
* The Kaplan-Meier estimate of the average survival function of the training data *survival distribution* is the Kaplan-Meier estimator (@sec-surv-models-uncond) fit (@sec-surv-setml-meth) on training data $(T_i, \Delta_i)$ and is denoted by $\KMS$.
* The Kaplan-Meier estimate of the average survival function of the training data *censoring distribution* is the Kaplan-Meier estimator fit on training data $(T_i, 1 - \Delta_i)$ and is denoted by $\KMG$.


Notation and definitions will be recapped at the start of each chapter for convenience.

### Types of Censoring

In *Survival Analysis* three types of censoring are commonly defined

* right-censoring,
* left-censoring, and
* interval-censoring

The latter can be viewed as the most general case with the other types being special cases.
Notably, multiple types of censoring (or truncation, cf. (@sec-truncation)) can occur in any given data set.
It is therefore important to identify which types are present, in order to select a method that can deal with the respective data, as most methods (and particularly the available implementations) are limited with respect to which type of survival task they can handle.


#### Right-censoring Types {.unnumbered .unlisted}

Right-censoring is the most common form of censoring in survival models and it occurs either when a patient drops out (but doesn't experience the event) of the study before the end and thus their outcome is unknown, or if they experience the event at some unknown point after the study end. Formally let $[\tau_l, \tau_u]$ be the study period for some, $\tau_l,\tau_u \in \NNReals$. Then right-censoring occurs when either $Y > \tau_u$ or when $Y \in [\tau_l,\tau_u]$ and $C \leq Y$. In the first case $T = C = \tau_u$ and censoring is due to the true time-to-event being unknown as the observation period has finished. In the latter case, a separate censoring event, such as drop-out or another competing risk, is observed.

![Dead and censored subjects (y-axis) over time (x-axis). Black diamonds indicate true death times and white circles indicate censoring times. Vertical line is the study end time. Subjects 1 and 2 die in the study time. Subject 3 is censored in the study and (unknown) dies within the study time. Subject 4 is censored in the study and (unknown) dies after the study. Subject 5 dies after the end of the study.](Figures/survival/censoring.png){#fig-survset-censor fig-alt="TODO"}

Sometimes, right-censoring is also subdivided into Type-I, Type-II and Type-III censoring.
In Type-I, censoring occurs at the fixed, pre-difined end of the study $\tau_u$, in which case we observe $(T_i = \min(Y_i, \tau_u), \II(Y_i \leq \tau_u))$.
Sometimes this is also refered to as *administrative* censoring, .
Type-II censoring is similar, however, the study ends when a pre-defined number of subjects experienced the event of interest, in which case $\tau_u$ is random.
Type-III censoring refers to random censoring, i.e. we assume the censoring times follow some unknown distribution and we observe $(T_i=\min(Y_i,C_i), \II(Y_i\leq C_i))$.
Often, the different types of right-censorig can co-occur in any given data set.

For the purposes of this book, however, the difference will be largely irrelevant, as the different types are treated the same at the modeling/estimation stage.


#### Left-censoring {.unnumbered .unlisted}
Left-censoring is a rarer form of censoring and occurs when the event happens at some unknown time before the study start. While quiet rare in medical settings, this type of data often occurs in sociology studies when retrospective interviews are conducted.

For illustration, consider an interview whith questions alas "At what age did you ... for the first/last time?", where you can fill in the blank with "use a smartphone", "drink alcohol" or similar.
Let $A_i$ denote the age of the participants during the interview.
If they remember the age exactly\footnote{If one assumes that the subjects remember correctly, otherwise one would need to account for potential measuresment error}, then $T_i = Y_i \leq A_i$.
If the event didn't occur yet, then the subject is right-censored at $A_i$, i.e., $Y_i > T_i = C_i = A_i$.
However, if the event occured, but the subjects don't remember when, the subject is left-censored at $A_i$, i.e. we know that $T_i = Y_i < A_i$, but not the exact time.

<Note: maybe things like this should be put in a "practical tips" box>
Note that for this type of data you need an additional variable in order to differentiate between left- and right-censoring as well as exact event times. In practice this is often solved by including two columns that store time, such that left-censored data is given by $(-\infty, A_i)$, while right-censored data is denoted by $(A_i, \infty)$. In this convention, actual event times are denoted by $(T_i, T_i)$.


#### Interval-censoring {.unnumbered .unlisted}

Interval-censoring occurs when the event takes place in some interval within the study period, but the exact time of event is unknown. This often occurs in data resulting from regular or irregular check-ups, as it often occurs in electronic health record data. For example, patients might get periodic tests for skin cancer. Some patients might check once a year, for others the intervals between checks might be longer. If skin cancer is detected, one only knows that it occured between the last and current check up, but not the exact time.

Formally, consider $L_i$ the subject-specific check-up time and $R_i$ the current check-up time.
Then we know that if an event occured $T_i = Y_i \in (L_i, R_i]$.
If no event occured then $Y_i > T_i = C_i = R_i$.



### Censoring 'Dependence' {.unnumbered .unlisted}

Censoring is often defined as *uninformative* if $Y \indep C$ and *informative* otherwise however these definitions can be misleading as the term 'uninformative' appears to imply that censoring is independent of both $X$ and $Y$, and not just $Y$. Instead the following more precise definitions are used in this book.

::: {#def-cens}

## Independent Censoring

Let $(X,T,\Delta,Y,C)$ be defined as above, then

* If $C \indep X$, censoring is *feature-independent*, otherwise censoring is *feature-dependent*.
* If $C \indep Y$, then censoring is *event-independent*, otherwise censoring is *event-dependent*.
* If $(C \indep Y) | X$, censoring is conditionally independent of the event given covariates, or *conditionally event-independent*.
* If $C \indep (X,Y)$ censoring is *uninformative*, otherwise censoring is *informative*.

:::

Non-informative censoring can generally be well-handled by models as true underlying patterns can still be detected and the reason for censoring does not affect model inference or predictions. However in the real-world, censoring is rarely non-informative as reasons for drop-out or missingness in outcomes tend to be related to the study of interest. Event-dependent censoring is a tricky case that, if not handled appropriately (by a competing-risks framework), can easily lead to poor model development; the reason for this can be made clear by example: Say a study is interested in predicting the time between relapses of stroke but a patient suffers a brain aneurysm due to some separate neurological condition, then there is a high possibility that a stroke may have occurred if the aneurysm had not. Therefore a survival model is unlikely to distinguish the censoring event (aneurysm) from the event of interest (stroke) and will confuse predictions. In practice, the majority of models and measures assume that censoring is conditionally event-independent and hence censoring can be predicted by the covariates whilst not directly depending on the event. For example if studying the survival time of ill pregnant patients in hospital, then dropping out of the study due to pregnancy is clearly dependent on how many weeks pregnant the patient is when the study starts (for the sake of argument assume no early/late pregnancy due to illness).


### Censoring vs. Truncation {#sec-truncation}
While sometimes confused or missnamed, it is very important to differentiate
between censoring and truncation, as the way the are handled and the methods suitable to handle respective survival tasks differs substantially.
While truncation also occurs in non time-to-event settings, it is quit common in survival analysis, especially left-truncation.

In general, while censored observations have incomplete information about the time-to-event, they are still part of the data set.
Truncation on the other hand often leads to observations not entering the data set (at least not at time 0).
This will usually introduce bias that needs to be accounted for.

#### Left-truncation {.unnumbered .unlisted}


Left-truncation often occurs when study participation is conditional of the occurence of another event. This is easiest explained through some examples:

In one famous study (reference), the interest was in infant survival during the first 365 days after birth based on whether the mother was alive.
The study is from the 18the century, when childhood and maternal mortality were compartively high.
Infants entered the study when the mother died, and two other infants with same age and other features were matched into the study, except that their mothers were still alive.
The age of the infant at entrance into the study therefore signifies a so-called *left-truncation event*, as infants who die before their mothers never enter the data and the *left-truncation time* is given by the infants' age at which the mother died.

More formally, let $T^l_i$ the subject-specific left truncation time.
Then we only observe subjects with $Y_i > T^l_i$ or $C_i > T^l_i$. Subjects with $Y_i < T^l_i$ never enter the data.

Notably, left-truncation also plays an important role when modeling recurrent events or multi-state data, as the data generating process induces left-truncation (more on this in Section @sec-eha).


#### Right-truncation {.unnumbered .unlisted}




### Setting up the objective function

Here we summarise how the likelihood is constructed for observations subject to different types of censoring and truncation. For machine learning survival analysis this can be used to construct the objective (or loss) function for any survival task.


## Event-history Analysis {#sec-eha}

- Here standard illustration of single-event, competing risks, multi-state models.
- Also talk about recurrent events setting, that can be viewd as multi-state, but also single-event with further dependencies.
- Make sure to refer back to left-truncation


### Competing Risks {#sec-competing-risks}


### Multi-state Models {#sec-multi-state}


### Recurrent Events {#sec-recurrent-events}



## Survival Prediction Problems {#sec-surv-set-types}

This section continues by defining the survival problem narrowed to the scope described in the previous section. Defining a single 'survival prediction problem' (or 'task') is important mathematically as conflating survival problems could lead to confused interpretation and evaluation of models. Let $(X,T,\Delta)$ and $\calD$ be as defined above. A general survival prediction problem is one in which:

* a survival dataset, $\calD$, is split (@sec-surv-setml-meth) for training, $\dtrain$, and testing, $\dtest$;
* a survival model is fit on $\dtrain$; and
* the model predicts some representation of the unknown true survival time, $Y$, given $\dtest$.


The process of 'fitting' is model-dependent, and can range from simple maximum likelihood estimation of model coefficients, to complex algorithms.  The model fitting process is discussed in more abstract detail in (@sec-surv-setml) and then concrete algorithms are discussed in (@sec-review). The different survival problems are separated by 'prediction types' or 'prediction problems', these can also be thought of as predictions of different 'representations' of $Y$. Four prediction types are discussed in this paper, these may be the only possible survival prediction types and are certainly the most common as identified in chapters (@sec-review) and (@sec-eval).  They are predicting:


* The *relative risk* of an individual experiencing an event -- A single continuous ranking.
* The *time until an event* occurs -- A single continuous value.
* The *prognostic index* for a model -- A single continuous value.
* The *survival distribution* (conditional on features) -- A probability distribution.


The first three of these are referred to as *deterministic* problems as they predict a single value whereas the fourth is *probabilistic* and returns a full survival distribution. Definitions of these are expanded on below.

Survival predictions differ from other fields in two respects. Firstly, the predicted outcome, $Y$, is a different object than the outcome used for model training, $(T, \Delta)$. This differs from, say, regression in which the same object (a single continuous variable) is used for fitting and predicting. Secondly, with the exception of the time-to-event prediction, all other prediction types do not predict $Y$ but some other related quantity.

Survival prediction problems must be clearly separated as they are inherently incompatible. For example it is not meaningful to compare a relative risk prediction from one observation to a survival distribution of another. Whilst these prediction types are separated above, they can be viewed as special cases of each other. Both (1) and (2) may be viewed as variants of (3); and (1), (2), and (3) can all be derived from (4); this is elaborated on below.

#### Relative Risk/Ranking {.unnumbered .unlisted}

This is perhaps the most common survival problem and is defined as predicting a continuous rank for an individual's 'relative risk of experiencing the event'. For example, given three patients, $\{i,j,k\}$, a relative risk prediction may predict the 'risk of event' as $\{0.1, 0.5, 10\}$ respectively. From these predictions, the following types of conclusions can be drawn:


* Conclusions comparing patients. e.g. $i$ is at the least risk; the risk of $j$ is only slightly higher than that of $i$ but the risk of $k$ is considerably higher than $j$; the corresponding ranks for $i,j,k,$ are $1,2,3$.
* Conclusions comparing risk groups. e.g. thresholding risks at $1.0$ places $i$ and $j$ in a 'low-risk' group and $k$ in a 'high-risk' group

So whilst many important conclusions can be drawn from these predictions, the values themselves have no meaning when not compared to other individuals. Interpretation of these rankings has historically been conflicting in implementation, with some software having the interpretation 'higher ranking implies higher risk' whereas others may indicate 'higher ranking implies lower risk' \ref{sec:tools_mlr3proba_api_learn}. In this book, a higher ranking will always imply a higher risk of event (as in the example above).

#### Time to Event {.unnumbered .unlisted}
Predicting a time to event is the problem of predicting the deterministic survival time of a patient, i.e. the amount of time for which they are predicted to be alive after some given start time. Part of the reason this problem is less common in survival analysis is because it borders regression -- a single continuous value is predicted -- and survival -- the handling of censoring is required -- but neither is designed to solve this problem directly. Time-to-event predictions can be seen as a special-case of the ranking problem as an individual with a predicted longer survival time will have a lower overall risk, i.e. if $t_i,t_j$ and $r_i,r_j$ are survival time and ranking predictions for patients $i$ and $j$ respectively, then $t_i > t_j \rightarrow r_i < r_j$.

#### Prognostic Index {.unnumbered .unlisted}
Given covariates, $x \in \Reals^{n \times p}$, and a vector of model coefficients, $\beta \in \Reals^p$, the linear predictor is defined by $\eta := x\beta \in \Reals^n$. The 'prognostic index' is a term that is often used in survival analysis papers that usually refers to some transformation (possibly identity), $\phi$, on the linear predictor, $\phi(\eta)$. Assuming a predictive function (for survival time, risk, or distribution defining function (see below)) of the form $g(\varphi)\phi(\eta)$, for some function $g$ and variables $\varphi$ where $g(\varphi)$ is constant for all observations (e.g. Cox PH (@sec-surv-models-crank)), then predictions of $\eta$ are a special case of predicting a relative risk, as are predictions of $\phi(\eta)$ if $\phi$ is rank preserving. A higher prognostic index may imply a higher or lower risk of event, dependent on the model structure.

#### Survival Distribution {.unnumbered .unlisted}
Predicting a survival distribution refers specifically to predicting the distribution of an individual patient's survival time, i.e. modelling the distribution of the event occurring over $\NNReals$. Therefore this is seen as the probabilistic analogue to the deterministic time-to-event prediction, these definitions are motivated by similar terminology in machine learning regression problems (@sec-surv-setml). The above three prediction types can all be derived from a probabilistic survival distribution prediction (@sec-car).

A survival distribution is a mathematical object that is estimated by predicting a *representation* of the distribution. Let $W$ be a continuous random variable t.v.i. $\NNReals$ with probability density function (pdf), $f_W: \NNReals \rightarrow \NNReals$, and cumulative distribution function (cdf), $F_W: \NNReals \rightarrow [0,1]; (\tau) \mapsto P(W \leq \tau)$. The pdf, $f_W(\tau)$, is the likelihood of an observation dying in a small interval around time $\tau$, and $F_W(\tau) = \int^\tau_0 f_W(\tau)$ is the probability of an observation being dead at time $\tau$ (i.e. dying at or before $\tau$). In survival analysis, it is generally more interesting to model the risk of the event taking place or the probability of the patient being alive, leading to other distribution representations of interest.

The survival function is defined as
$$
S_W: \NNReals \rightarrow [0,1]; \quad
(\tau) \mapsto P(W \geq \tau) = \int^\infty_\tau f_W(u) \ du
$$
and so $S_W(\tau) = 1-F_W(\tau)$. This function is known as the survival function as it can be interpreted as the probability that a given individual survives until some point $\tau \geq 0$.

Another common representation is the hazard function,
$$
h_W: \NNReals \rightarrow \NNReals; \quad
(\tau) \mapsto  \frac{f_W(\tau)}{S_W(\tau)}
$$
The hazard function is interpreted as the instantaneous risk of death given that the observation has survived up until that point; note this is not a probability as $h_W$ can be greater than one.

The cumulative hazard function (chf) can be derived from the hazard function by
$$
H_W: \NNReals \rightarrow \NNReals; \quad
(\tau) \mapsto \int^\tau_0 h_W(u) \ du
$$

The cumulative hazard function relates simply to the survival function by
$$
H_W(\tau) = \int^\tau_0 h_W(u) \ du = \int^\tau_0 \frac{f_W(u)}{S_W(u)} \ du = \int^\tau_0 -\frac{S'_W(u)}{S_W(u)} \ du = -\log(S_W(\tau))
$$

Any of these representations may be predicted conditionally on covariates for an individual by a probabilistic survival distribution prediction. Once a function has been estimated, predictions can be made conditional on the given data. For example if $n$ survival functions are predicted, $\hat{S}_1,...,\hat{S}_n$, then $\hat{S}_i$ is interpreted as the predicted survival function given covariates of observation $i$, and analogously for the other representation functions.

## Survival Analysis Task {#sec-surv-setmltask}

The survival prediction problems identified in (@sec-surv-set-types) are now formalised as machine learning tasks.

:::: {.callout-note icon=false}

## Survival Task

::: {#cnj-task-surv}
Let $(X,T,\Delta)$ be random variables t.v.i. $\calX \times \calT \times \bset$ where $\calX \subseteq \Reals^p$ and $\calT \subseteq \NNReals$. Let $\calS \subseteq \distrT$ be a convex set of distributions on $\calT$ and let $\calR \subseteq \Reals$. Then,


* The *probabilistic survival task* is the problem of predicting a conditional distribution over the positive Reals and is specified by $g: \calX \rightarrow \calS$.
* The *deterministic survival task* is the problem of predicting a continuous value in the positive Reals and is specified by $g: \calX \rightarrow \calT$.
* The *survival ranking task* is specified by predicting a continuous ranking in the Reals and is specified by $g: \calX \rightarrow \calR$.


The estimated prediction functional $\hatg$ is fit on training data \\$(X_1,T_1,\Delta_1),...,(X_n,T_n,\Delta_n) \iid (X,T,\Delta)$ and is considered 'good' if \\$\EE[L(T^*, \Delta^*, \hatg(X^*))]$ is low, where $(X^*, T^*, \Delta^*) \sim (X, T, \Delta)$ is independent of $(X_1,T_1,\Delta_1),...,(X_n,T_n,\Delta_n)$ and $\hatg$.
:::
::::

Any other survival prediction type falls within one of these tasks above, for example predicting log-survival time is the deterministic task and predicting prognostic index or linear predictor is the ranking task. Removing the separation between the prognostic index and ranking prediction types is due to them both making predictions over the Reals; their mathematical difference lies in interpretation only. In general, the survival task will assume that $\calT \subseteq \NNReals$, and the terms 'discrete' or 'reduced survival task' will refer to the case when $\calT \subseteq \PNaturals$. Unless otherwise specified, the term 'survival task', will be used to refer to the probabilistic survival task.\footnote{These definitions are given in the most general case where the time variable is over $\NNReals$. In practice, all models instead assume time is over $\PReals$ and any death at $T_i = 0$ is set to $T_i = \epsilon$ for some very small $\epsilon \in \PReals$. Analogously for the discrete survival task. This assumption may not reflect reality as a patient could die at the study start however models cannot typically include this information in training.}

#### Survival Analysis and Regression {.unnumbered .unlisted}

Survival and regression tasks are closely related as can be observed from their respective definitions. Both are specified by $g : \calX \rightarrow \calS$ where for probabilistic regression $\calS \subseteq \Distr(\Reals)$ and for survival $\calS \subseteq \Distr(\NNReals)$. Furthermore both settings can be viewed to use the same generative process. In the survival setting in which there is no censoring then data is drawn from $(X,Y) \ t.v.i. \ \calX \times \calT, \calT \subseteq \NNReals$ and in regression from $(X,Y) \ t.v.i. \ \calX \times \calY, \calY \subseteq \Reals$, so that the only difference is whether the outcome data ranges over the Reals or positive Reals.

These closely related tasks are discussed in more detail in (@sec-car), with a particular focus on how the more popular regression setting can be used to solve survival tasks. In (@sec-review) the models are first introduced in a regression setting and then the adaptations to survival are discussed, which is natural when considering that historically machine learning survival models have been developed by adapting regression models.
