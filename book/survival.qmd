---
abstract: TODO (150-200 WORDS)
---

{{< include _setup.qmd >}}

# Survival Analysis {#sec-surv}

{{< include _wip.qmd >}}

___

TODO

* Make sure intro is clear about censoring/truncation and that metrics can't highlight if this is setup wrong - analogously to hypothesis testing not testing the result but hypothesis, p-hacking, etc.
* If measures for right-censoring used in parts of pipelines hard to discern biases if wrong type of measure used
* Same as dependent/independent censoring and measures problem
* Estimation of transition hazards/transition probabilities
* Event-history analysis (Graph of different types of transitions and setups)

___

<!-- In their broadest and most basic definitions, survival analysis is the study of temporal data from a given origin until the occurrence of one or more events or 'end-points'  [@Collett2014], and machine learning is the study of models and algorithms that learn from data in order to make predictions or find patterns  [@Hastie2001]. Reducing either field to these definitions is ill-advised. -->
<!--
Survival analysis is concerned with time-to-event data, i.e. data where the outcome is a duration from some origin until the occurrence of one or multiple events of interest.
In many medical settings, this duration will often be literal survival time, i.e. time until death after some procedure or treatment.
When collecting such data, however, the outcome of interest can often be not observed (fully), e.g. because subjects drop out of the study, haven't experienced the event of interest until end of study period or due to the occurence of another, competing event.

Often analysis of time-to-event data targets the estimation of the (improper) distribution of the event times or, equivalently, modelling the transitions between different states (e.g. alive -> dead) while taking into account censoring and truncation as well as other peculiarities of time-to-event data. However, the target of estimation can also be a relative risk score or the expected time-to-event (cf. @sec-surv-set-types for details).
 -->


 As discussed in the introduction, *Survival Analysis* is concerned with data where the outcome is a time-to-event.
Because the collection of such data takes place in the temporal domain (it takes time to observe a duration), the event of interest is often unobservable.
For example, because the event did not occur by the end of the data collection period or because of the occurrence of another event that prevents the event of interest from being obseved.
 In survival analysis terminolgy these are refered to as *censoring* and *competing risks*.

This chapter defines these and related terms and introduces basic terminology and mathematical definitions.
@sec-surv-set-math starts with the common single-event, right-censored data setting with definitions then extended to further types of censoring as well as truncation.
@sec-eha introduces event-history analysis, which is a generalisation to settings with multiple, potentially competing or recurrent events.
@sec-surv-set-types defines common prediction types of survival models, which is particularly important for machine learning based survival analysis.
Finally, in order to cleanly discuss *machine learning survival analysis*, the *survival task* is introduced in @sec-surv-setmltask.

It is of utmost importance to identify and specify the survival task correctly, as misspecification cannot be detected by comparing the predictive performance of alternate models via evaluation measures.
Evaluation measure can only detect if one model is better suited to minimize the obejective function, but not whether or not the obejective function is correct.
The latter depends on the (assumptions about the) data generating process and has to be also reflected in the definition of the evaluation measure.


## Survival Data and Definitions {#sec-surv-set-math}

### Quantifying the Distribution of Event Times {#sec-distributions}

This section introduces functions that can be used to fully characteristise a probability distribution, termed here as \emph{distribution defining functions}.
Particular focus is given to distribution defining functions that are important in survival analysis.

For now, assume a continuous, positive, random variable $Y$ taking values in (t.v.i.) $\NNReals$.
A standard representation of the distribution of $Y$ is given by the probability density function (pdf), $f_Y: \NNReals \rightarrow \NNReals$, and cumulative distribution function (cdf), $F_Y: \NNReals \rightarrow [0,1]; (\tau) \mapsto P(Y \leq \tau)$.

As discussed in @sec-intro, it is more common to describe the distribution of event times $Y$ via the *survival function* and *hazard function* (often also refered to as *hazard rate*) than the pdf or cdf.

The survival function is defined as
$$
S_Y(\tau) = P(Y > \tau) = \int^\infty_\tau f_Y(u) \ du,
$$
is the probability of not observing an event until some point $\tau \geq 0$ and thus simply the compliment of the cdf: $S_Y(\tau) = 1-F_Y(\tau)$.

The hazard function is given by
$$
h_Y: \NNReals \rightarrow \NNReals; \quad
(\tau) \mapsto  \frac{f_Y(\tau)}{S_Y(\tau)}.
$$

The hazard function is interpreted as the instantaneous risk of death given that the observation has survived up until that point.
Note this is not a probability as $h_Y$ can be greater than one.

The cumulative hazard function (chf) can be derived from the hazard function by
$$
H_Y: \NNReals \rightarrow \NNReals; \quad
(\tau) \mapsto \int^\tau_0 h_Y(u) \ du
$$

The cumulative hazard function relates simply to the survival function by
$$
H_Y(\tau) = \int^\tau_0 h_Y(u) \ du = \int^\tau_0 \frac{f_Y(u)}{S_Y(u)} \ du = \int^\tau_0 -\frac{S'_Y(u)}{S_Y(u)} \ du = -\log(S_Y(\tau))
$$

These last relationships are particularly important, as many methods estimate the hazard rate, which is then used to calculate the cumulative hazard and survival probability
$$S_Y(\tau) = \exp(-H_Y(\tau)) = \exp\left(-\int_0^\tau h_Y(u)\ du\right).$${#eq-surv-haz}


From now on we will often refer to these quantities as $S$, $h$, etc. rather than $S_Y$, $h_Y$ for convenience, unless it's necessary to use the subscript to avoid confusion.

Normally, these quantities could be estimated using standard techniques like regression modeling (estimation of parameters of an assumed distribution).
However, in contrast to standard settings, $Y$ is only observed partially, due to different types of censoring and truncation describeb below.


### Single-event, right-censored data {#sec-data-rc}

Survival analysis has a more complicated data setting than other fields as the 'true' data generating process is not directly observable but instead engineered variables are defined to capture observed information. Let,

* $X \ t.v.i. \ \calX \subseteq \Reals^p, p \in \PNaturals$ be the generative random variable  representing the data *features*/*covariates*/*independent variables*.
* $Y \ t.v.i. \ \calY \subseteq \NNReals$ be the (patially unobservable) *true survival time*.
* $C \ t.v.i. \ \calC \subseteq \NNReals$ be the (partially unobservable) *true censoring time*.


Note that we are interested in the properties of $Y$.
However, in the presence of censoring $C$, it is impossible to fully observe $Y$.
Instead, the observable variables are given by

* $T := \min\{Y,C\}$, the *outcome time* (realisations of this random variable will be refered to as *observed outcome time*).
* $\Delta := \II(Y = T) = \II(Y \leq C)$ be the *event indicator* (also known as the *censoring* or *status* indicator).\footnote{Indicators are usually named to reflect a positive condition in the function (in this case the event when $Y = T$), but counter to this convention the censoring indicator is possibly the more common term.}


Together $(T,\Delta)$ is referred to as the *survival outcome* or *survival tuple* and they form the dependent variables.
The survival outcome provides a concise mechanism for representing the outcome time and indicating which outcome (event or censoring) took place.

A *survival dataset* is defined by $\calD = \{(\xx_1,t_1,\delta_1),...,(\xx_n,t_n,\delta_n)\}$, where $(t_i,\delta_i)$ are realisations of the respective random variables  $(T_i, \Delta_i)$ and $\xx_i$ is a $p$-dimensional row-vector, $\xx_i = (x_{i1},...,x_{ip})$.

Finally the following terms are used frequently throughout this book.
Let $(t_i, \delta_i) \iid (T,\Delta), i = 1,...,n$, be observed survival outcomes.
Then,

* The *set of unique* or *distinct time-points* refers to the set of time-points in which at least one observation dies or is censored, $\calU_O \subseteq \{t_i\}_{i \in \{1,...,n\}}\subseteq \calD$.
* The *set of unique observed event times* refers to the set of unique time-points in which an event (and not censoring) occurred, $\calU_D := \{t_{i} : \delta_i = 1\}_{i \in \{1,...,n\}}\subseteq \calD$. Sometimes the ordered, unique events times are also denoted by $t_{(i)},\ i=1,\ldots,m \leq n,\ t_{(1)} < t_{(2)} < \cdots < t_{(m)}$.
* The *risk set* at a given time-point, $\tau$, is the index-set of observation units at risk for the event just before $\tau$, $\calR_\tau := \{i: t_i \geq \tau\}$ where $i$ is a unique row/subject in the data. Consequently, for right-censored data, we have $\calR_0 = \{1,\ldots,n\}$ and $\calR_{\tau} \supseteq \calR_{\tau'}, \forall \tau < \tau'$.
* The *number of observations at risk* at $\tau$ is the cardinality of the risk set, $|\calR_\tau|$, and is denoted by $n_\tau := \sum_i \II(t_i \geq \tau)$.
* The *number of events* at $\tau$ is denoted by $d_\tau := \sum_i \II(t_i = \tau, \Delta_i = 1).$ Note: For truly continuous variables $T_i$ we would expect $d_{t_i} = 1,\forall i=,1\ldots,n$, however, in practice we often observe ties due to finite measurement precision, such that $d_{\tau} > 1$ occurs quite frequently in real-world datasets.

The notion of the risk-set $\calR_\tau$ as well as $n_\tau$ and $d_\tau$ will be very important later in @sec-models-classical, as many non-parametric methods (that is methods that don't make an assumption about the underlying distribution of event times) like the famous Kaplan-Meier estimator [@Kaplan1958] are based on the ratio $\frac{d_\tau}{n_\tau}$.


@tbl-surv-data-rats exemplifies an observed survival dataset with a modified version of the `rats` data  [@pkgsurvival], which contains the time until occurence of a tumor ($\delta_i=1$ if a tumor occured during the outcome time $t_i$ and $\delta_i = 0$ otherwise).
In this example, the above terms are be given as

* $\calU_0 = \{49, 91, 101, 102, 104\}$ (note that time $104$ occurs only once)
* $\calU_D = \{49, 102, 104\}$ (here $104$ refers to $t_5$, not $t_3$)
* $\calR_{\tau = 102} = \{3, 5, 6\}$ (these rats are at risk for the event shortly before $\tau = 102$, rats $1$ and $4$ are censored beforehand and rat $2$ already developed a tumor).
* $n_{\tau = 102} = |\calR_{102}| = 3$
* $d_{\tau = 102} = 1$ (note that while the observed outcome time is $104$ for rats 3 and 5, only the latter developed a tumor ($\delta_3=0$, but $\delta_5=1$), thus $d_{\tau = 104}=1$)

| **ID** ($i$) | **litter** $(\xx_{;1})$ | **rx** $(\xx_{;2})$ | **sexF** $(\xx_{;3})$ | **time** ($\tt$) | **status** ($\dd$) |
| -- | -- | -- | --- | -- | --|
| 1 | 1 | 1 | 1 | 101 | 0 |
| 2 | 1 | 0 | 1 | 49 | 1 |
| 3 | 1 | 0 | 1 | 104 | 0 |
| 4 | 2 | 1 | 0 | 91 | 0 |
| 5 | 2 | 0 | 0 | 104 | 1 |
| 6 | 2 | 0 | 0 | 102 | 1 |

: Subset of the `rats`  [@pkgsurvival] time-to-event dataset.
Rows are individual observations ($ID$), $\xx;j$ columns are features, $t$ is observed time-to-event, $\delta$ is the event indicator. {#tbl-surv-data-rats}


* The Kaplan-Meier estimate of the average survival function of the training data *survival distribution* is the Kaplan-Meier estimator (@sec-surv-models-uncond) fit (@sec-surv-setml-meth) on training data $(T_i, \Delta_i)$ and is denoted by $\KMS$.
* The Kaplan-Meier estimate of the average survival function of the training data *censoring distribution* is the Kaplan-Meier estimator fit on training data $(T_i, 1 - \Delta_i)$ and is denoted by $\KMG$.


Notation and definitions will be recapped at the start of each chapter for convenience.

### Types of Censoring

In *Survival Analysis* three types of censoring are commonly defined

* right-censoring,
* left-censoring, and
* interval-censoring

The latter can be viewed as the most general case with the other types being special cases.
Notably, multiple types of censoring and/or truncation, cf. (@sec-truncation) can occur in any given data set.
It is therefore important to identify which types are present, in order to select a method that can deal with the respective data, as most methods (and particularly the available implementations) are limited with respect to which type of survival task they can handle.


#### Right-censoring Types {.unnumbered .unlisted}

Right-censoring is the most common form of censoring in survival models and it occurs either when a patient drops out (but doesn't experience the event) of the study before the end and thus their outcome is unknown, or if they experience the event at some unknown point after the study end.
Formally let $[\tau_l, \tau_u]$ be the study period for some, $\tau_l,\tau_u \in \NNReals$.
Then right-censoring occurs when either $Y > \tau_u$ or when $Y \in [\tau_l,\tau_u]$ and $C \leq Y$.
In the first case $T = C = \tau_u$ and censoring is due to the true time-to-event being unknown as the observation period has finished.
In the latter case, a separate censoring event, such as drop-out or another competing risk, is observed.

![Dead and censored subjects (y-axis) over time (x-axis). Black diamonds indicate true death times and white circles indicate censoring times. Vertical line is the study end time. Subjects 1 and 2 die in the study time. Subject 3 is censored in the study and (unknown) dies within the study time. Subject 4 is censored in the study and (unknown) dies after the study. Subject 5 dies after the end of the study.](Figures/survival/censoring.png){#fig-survset-censor fig-alt="TODO"}

Sometimes, right-censoring is also subdivided into Type-I, Type-II and Type-III censoring.
In Type-I, censoring occurs at the fixed, pre-difined end of the study $\tau_u$, in which case we get $(T_i = \min(Y_i, \tau_u), \II(Y_i \leq \tau_u))$.
Sometimes this is also refered to as *administrative* censoring, .
Type-II censoring is similar, however, the study ends when a pre-defined number of subjects experienced the event of interest, in which case $\tau_u$ is random.
Type-III censoring refers to random censoring, i.e. we assume the censoring times follow some unknown distribution and we observe $(T_i=\min(Y_i,C_i), \II(Y_i\leq C_i))$.
Often, the different types of right-censorig can co-occur in any given data set.

For the purposes of this book, however, the difference will be largely irrelevant, as the different types are treated similarly at the modeling/estimation stage.


#### Left-censoring {.unnumbered .unlisted}
Left-censoring is a rarer form of censoring and occurs when the event happens at some unknown time before the study start.
While quiet rare in medical settings, this type of data often occurs in sociology studies when retrospective interviews are conducted.

For illustration, consider an interview whith questions alas "At what age did you ... for the first/last time?", where you can fill in the blank with "use a smartphone", "drink alcohol" or similar.
Let $A_i$ denote the age of the participants during the interview.
If they remember the age exactly\footnote{If one assumes that the subjects remember correctly, otherwise one would need to account for potential measuresment error}, then $T_i = Y_i \leq A_i$.
If the event didn't occur yet, then the subject is right-censored at $A_i$, i.e., $Y_i > T_i = C_i = A_i$.
However, if the event occured, but the subjects don't remember when, the subject is left-censored at $A_i$, i.e. we know that $T_i = Y_i < A_i$, but not the exact time.

Note that for this type of data you need an additional variable in order to differentiate between left- and right-censoring as well as exact event times.
In practice this is often solved by including two columns that store time, such that left-censored data is given by $(-\infty, A_i)$, while right-censored data is denoted by $(A_i, \infty)$.
In this convention, actual event times are denoted by $(T_i, T_i)$.


#### Interval-censoring {.unnumbered .unlisted}

Interval-censoring occurs when the event takes place in some interval within the study period, but the exact time of event is unknown.
This often occurs in data resulting from regular or irregular check-ups, as it often occurs in electronic health record data.
For example, patients might get periodic tests for skin cancer.
Some patients might check once a year, for others the intervals between checks might be longer.
If skin cancer is detected, one only knows that it occured between the last and current check up, but not the exact time.

Formally, consider $L_i$ the subject-specific check-up time and $R_i$ the current check-up time.
Then we know that if an event occured $T_i = Y_i \in (L_i, R_i]$.
If no event occured then $Y_i > T_i = C_i = R_i$.



### Censoring 'Dependence' {.unnumbered .unlisted}

Censoring is often defined as *uninformative* if $Y \indep C$ and *informative* otherwise however these definitions can be misleading as the term 'uninformative' appears to imply that censoring is independent of both $X$ and $Y$, and not just $Y$.
Instead the following more precise definitions are used in this book.

::: {#def-cens}

## Independent Censoring

Let $(X,T,\Delta,Y,C)$ be defined as above, then

* If $C \indep X$, censoring is *feature-independent*, otherwise censoring is *feature-dependent*.
* If $C \indep Y$, then censoring is *event-independent*, otherwise censoring is *event-dependent*.
* If $(C \indep Y) | X$, censoring is conditionally independent of the event given covariates, or *conditionally event-independent*.
* If $C \indep (X,Y)$ censoring is *uninformative*, otherwise censoring is *informative*.

:::

Non-informative censoring can generally be well-handled by models as true underlying patterns can still be detected and the reason for censoring does not affect model inference or predictions.
However in the real-world, censoring is rarely non-informative as reasons for drop-out or missingness in outcomes tend to be related to the study of interest.
Event-dependent censoring is a tricky case that, if not handled appropriately (by a competing-risks framework), can easily lead to poor model development; the reason for this can be made clear by example: Say a study is interested in predicting the time between relapses of stroke but a patient suffers a brain aneurysm due to some separate neurological condition, then there is a high possibility that a stroke may have occurred if the aneurysm had not.
Therefore a survival model is unlikely to distinguish the censoring event (aneurysm) from the event of interest (stroke) and will confuse predictions.
In practice, the majority of models and measures assume that censoring is conditionally event-independent and hence censoring can be predicted by the covariates whilst not directly depending on the event.
For example if studying the survival time of ill pregnant patients in hospital, then dropping out of the study due to pregnancy is clearly dependent on how many weeks pregnant the patient is when the study starts (for the sake of argument assume no early/late pregnancy due to illness).


### Censoring vs. Truncation {#sec-truncation}
While sometimes confused or missnamed, it is very important to differentiate
between censoring and truncation, as the way the are handled and the methods suitable to handle respective survival tasks differs substantially.
While truncation also occurs in non time-to-event settings, it is quit common in survival analysis, especially left-truncation.

In general, while censored observations have incomplete information about the time-to-event, they are still part of the data set.
Truncation on the other hand often leads to observations not entering the data set (at least not at time 0).
This will usually introduce bias that needs to be accounted for.

#### Left-truncation {.unnumbered .unlisted}


Left-truncation often occurs when study participation is conditional of the occurence of another event. This is easiest explained through some examples:

In one famous study (reference), the interest was in infant survival during the first 365 days after birth based on whether the mother was alive.
The study is from the 18the century, when childhood and maternal mortality were compartively high.
Infants entered the study when the mother died, and two other infants with same age and other features were matched into the study, except that their mothers were still alive.
The age of the infant at entrance into the study therefore signifies a so-called *left-truncation event*, as infants who die before their mothers never enter the data and the *left-truncation time* is given by the infants' age at which the mother died.

More formally, let $T^l_i$ the subject-specific left truncation time.
Then we only observe subjects with $Y_i > T^l_i$ or $C_i > T^l_i$. Subjects with $Y_i < T^l_i$ never enter the data.

Notably, left-truncation also plays an important role when modeling recurrent events or multi-state data, as the data generating process induces left-truncation (@sec-eha).


#### Right-truncation {.unnumbered .unlisted}

Right-truncation mostly occurs in analyses based on registry data.
When sampling the registry for events, some subjects might already have experienced the event, but the registry data at a given time-point does not reflect this yet.

Formally, let $T_i^r$ the right-truncation time, then subjects only enter the data set when $T_i < T_i^r$.


### Setting up the objective function

Here we summarise how the likelihood is constructed for observations subject to different types of censoring and truncation. For machine learning survival analysis this can be used to construct the objective (or loss) function for any survival task.


## Event-history Analysis {#sec-eha}

In this section we will go beyond single-event data or at least view time-to-event data more generally as data in which we could observe multiple, potentially mutually exclusive events.
From this more general point of view such data is sometimes refered to *event-history data* and its analysis as *event-history analysis*.

It is easiest to think of event history in terms of transition between different states, as illustrated in @fig-eha.
Usually, a subject starts out in state 0 (e.g. healthy) and transitions to different states from there.
States from which a further transition to another state is possible are called *transient*, while states from which no transitions are possible are called *terminal*.
If no transition happens within the follow-up time, the subject remains in the initial state, i.e. its censored for this transition.

In the *single-event setting* (left panel), we can only transition to one state (denoted by 1, for example death).

In the *competing risks setting* (middle panel, @sec-competing-risks), a subject could transition to any of the $K$ mutually exclusive states, thus the subject is *at risk* for a transition to multiple states. However, once one of them occurs observation of the other one is impossible (think of time until death from different causes).

In the most general, the *multi-state setting* (right panel, @sec-multi-state), we can have multiple transient and terminal states with potential back transitions (think moving between different stages of an illnes with potential (partial) recovery and death as terminal event).

The *recurrent events setting* (not pictured, @sec-recurrent-events) can be viewed as a special case of the multi-state setting but can also be treated as a more general case of the single event setting (with correlated observations) and is thus treated seperately.

![Illustration of different types of time-to-event outcomes. Left: Standard single-event setting with transition from initial state (0) to terminal state (1); Middle: Competing risks setting with $k$ competing events. Once one of the $\{1,\ldots, k\}$ events is observed, the others cannot be observed any more; Right: More general multi-state setting where subjects can transition between multiple transient and terminal states with possible back-transitions to previous states.](Figures/survival/event-history-analysis.svg){#fig-eha fig-alt="Schematic illustration of event history analysis."}


### Competing Risks {#sec-competing-risks}

In constrast to single-event survival analysis, competing risks are concerned with time-to first of many, mutually exclusive events $E \in {1,\ldots, k}$.
Mutually exclusive means that once one of the events occurs, we can not observe the other events any more.
That doesn't mean, that the events have to be terminal (e.g. death), however, the observation of the process ends at the occurence of one of the events.

Formally, let $Y$ the time-to-event as before.
In CR we now have an additional random variable $E_Y\in \{1,\ldots,k\}$ which denotes one of $k$ competing events that can occur at event time $Y$.
Absent of censoring, the data would be given as$(Y, E)$.
However, in the presence of right-censoring we have $T = \min(Y, C), \Delta = I(Y\leq C) \cdot E$.
Note that the status indicator can now take values $\Delta \in \{0, 1,\ldots, k\}$, compared to \Delta\in\{0,1\} in the single-event setting.

The predominate approach to competing-risks analysis is the so called *cause-specific hazards* approach, where we define $h_{0e}$ the hazard for the transition from initial state 0 to state $e =1,\ldots,k$:

$$
h_{0e}(\tau) = \lim_{\Delta \tau \to 0} \frac{P(\tau \leq Y \leq \tau + \Delta \tau, E = e\ |\ Y \geq \tau)}{\Delta \tau}, \; e = 1, \dots, k.
$$

As at any time $\tau$ the competing events are mutually exclusive, we can also define the

- *all-cause hazard*: $h(\tau) = \sum_{e = 1}^{k}h_{0e}(\tau)$,
- *all-cause cumulative hazard*: $H(\tau) = \int_{0}^\tau h(u)\ du = \sum_{e=1}^{k}\int_{0}^\tau h_{0e}(u)\ du = \sum_{e=1}^k H_{0e}(\tau)$, and
- *all-cause survival probability*: $S(\tau) = P(Y \geq \tau) = \exp(-H(\tau))$

Note that now the survival probability has the interpretation of non of the events occuring until time $\tau$ and can be calculated from the cause specific hazards.

Another important quantity often of interest in competing risks analysis is the *Cumulative Incidence Function* (CIF) which denote the probability of experiencing an event $E$ before time $\tau$

$$
F_{e}(\tau) = P(Y \leq \tau, E = e) = \int_{0}^\tau h_{0e}(u)S(u)\ du.
$$
Intuitively the integrants are given as the probability of not experiencing any event,S(u), multiplied with the hazard of experiencing an event of type $e$, $h_{oe}(u)$.

Calculating the CIF (in addition to the cause-specific hazards) is particularly important when making statements about the absolute risk (probability) for an event of type $e$, as it depends on all cause-specific hazards via $S(\tau)$.

This will be particularly relevant when assesing how features affect the probability for observing a specific event.
While some feautures might strongly affect some of the cause-specific hazards, events of type $e$ might have overall low prevalence and occur rarely (because competing events occur earlier on average), thus the absolut probability of observing an event of type $e$ might still be low.

In order to estimate the cause-specific hazards, one creates a new event indicator for each event type, such that all events of a different type are considered censoring:
$$\delta_{ie} = I(Y_i \leq C_i \wedge E_i = e) \in \{0, 1\},\ e=1,\ldots,k$$

Using this definition, we can construct a data set $(t_i, \delta_{ie}, \bx_i)$ and fit a separate model to each of the $k$ data sets in order to obtain cause-specific hazards using standard methods for the single-event case.
Once we have the hazards, we can calculate the all-cause survival probability and the CIFs.

Note that in this book we follow the exposition of competing risks offered in @beyersmann.competing.2012, particularly w.r.t. to the view that the cause-specific hazards model fully describes the data generating processes and doesn't require an independence assumption for the competing events.



### Multi-state Models {#sec-multi-state}

{{< include _wip.qmd >}}


### Recurrent Events {#sec-recurrent-events}

{{< include _wip.qmd >}}


## Survival Prediction Problems {#sec-surv-set-types}

This section continues by defining the survival problem.
Defining a single 'survival prediction problem' (or 'task') is important mathematically as conflating survival problems could lead to confused interpretation and evaluation of models.
Let $(\XX,\tt,\dd)$ and $\calD$ be as defined in @sec-surv-set-math.
A general survival prediction problem is one in which:

* a survival dataset, $\calD$, is split (@sec-surv-setml-meth) for training, $\dtrain$, and testing, $\dtest$;
* a survival model is fit on $\dtrain$; and
* the model predicts some representation of the unknown true survival time, $Y$, given $\dtest$.


The process of fitting is model-dependent, and can range from simple maximum likelihood estimation of model coefficients, to complex algorithms.
The model fitting process is discussed in more abstract detail in (@sec-surv-setml) and then concrete algorithms are discussed in (@sec-review).
The different survival problems are separated by *prediction types* or *prediction problems*, which can also be thought of as predictions of different representations of $Y$.
Four prediction types are discussed in this paper, these may not be the only possible survival prediction types but are certainly the most common as identified in chapters (@sec-review) and (@sec-eval).
They are predicting (conditional on features):

1. The *relative risk* of an individual for experiencing an event -- A single continuous ranking.
2. The *time until an event* occurs -- A single continuous value.
3. The *prognostic index* for a model -- A single continuous value.
4. The *survival distribution* -- A probability distribution.


The first three of these are referred to as *deterministic* problems as they predict a single value whereas the fourth is *probabilistic* and returns a full survival distribution.
Definitions of these are expanded on below.

Survival predictions differ from other fields in two respects:

* The outcome of interest is $Y$, which is different to the outcome used for model training, $(T=\min(Y, C), \Delta=\II(Y\leq C))$.
This differs from, say, regression in which the same object (a single continuous variable) is used for fitting and predicting.
* With the exception of the time-to-event prediction, all other prediction types do not predict $\EE(Y)$ but some other related quantity.

Survival prediction problems must be clearly separated as they are inherently incompatible.
For example it is not meaningful to compare a relative risk prediction from one observation to a survival distribution of another.
Whilst these prediction types are separated above, they can be viewed as special cases of each other. Both (1.) and (2.) may be viewed as variants of (3.); and (1.), (2.), and (3.) can all be derived from (4.); this is elaborated on below.

#### Relative Risk/Ranking {.unnumbered .unlisted}

This is a common survival problem and is defined as predicting a continuous rank for an individual's relative risk of experiencing the event.
For example, given three patients, $\{i,j,k\}$, a relative risk prediction may predict the risk of event as $\{0.1, 0.5, 10\}$ respectively.
From these predictions, the following types of conclusions can be drawn:

* Conclusions comparing patients. For example: $i$ is at the least risk; the risk of $j$ is only slightly higher than that of $i$ but the risk of $k$ is considerably higher than $j$; the corresponding ranks for $i,j,k,$ are $1,2,3$;
* Conclusions comparing risk groups. For example thresholding risks at $1.0$ places $i$ and $j$ in a low-risk group and $k$ in a high-risk group.

So whilst many important conclusions can be drawn from these predictions, the values themselves have no meaning when not compared to other individuals.
Interpretation of these rankings has historically been conflicting in implementation, with some software having the interpretation "higher ranking implies higher risk" whereas others may indicate "higher ranking implies lower risk".
In this book, a higher ranking will always imply a higher risk of event (as in the example above).

#### Time to Event {.unnumbered .unlisted}
Predicting a time to event is the problem of predicting the expectation $\hat{y}=\EE(y|\xx)$.
The ranking problem can be seen as a special case of time-to-event prediction as an individual with a predicted longer survival time will have a lower overall risk, i.e. if $\hat{y}_i,\hat{y}_j$ and $\hat{r}_i,\hat{r}_j$ are survival time and ranking predictions for patients $i$ and $j$ respectively, then $\hat{y}_i > \hat{y}_j \rightarrow \hat{r}_i < \hat{r}_j$.

For practical purposes time-to-event would be the ideal prediction type, as it is easy to interpret and communicate.
However, due to the nature of survival data this type of prediction is rare in survival analysis because we only observe a subset of the domain of $Y$, for example due to random censoring or limited follow-up.
For non-parametric methods, the estimated cdf is improper and thus integration requires extrapolation of the cdf.
For parametric models, the distribution of event times is fully specified once the paramers of the assumed distribution have been estimated, however, if the parameters were estimated based on only a small subset of the possible domain of $Y$, this essentially still constitutes extraploation and will in most cases yield implausible predictions.
A popular alternative is therefore to estimate the *restricted mean survival time* (RMST) [@han.restricted.2022; @andersen.regression.2004].

<!-- FIXME add example -->

#### Prognostic Index {.unnumbered .unlisted}
Given covariates, $\XX \in \Reals^{n \times p}$, and a vector of model coefficients, $\bbeta \in \Reals^p$, the linear predictor is defined by $\boldsymbol{\eta}:= \xx^\top\bbeta \in \Reals^n$.
The prognostic index is a term that is often used in survival analysis papers that usually refers to some transformation (possibly identity), $\phi$, on the linear predictor, $\phi(\eta)$.
Assuming a predictive function (for survival time, risk, or distribution defining function) of the form $g(\varphi)\phi(\eta)$, for some function $g$ and variables $\varphi$ where $g(\varphi)$ is constant for all observations (e.g. Cox PH (@sec-surv-models-crank)), then predictions of $\eta$ are a special case of predicting a relative risk, as predictions of $\phi(\eta)$ if $\phi$ is rank preserving.
A higher prognostic index may imply a higher or lower risk of event, dependent on the model structure.

<!-- FIXME example needed -->

#### Survival Distribution {.unnumbered .unlisted}
Predicting a survival distribution refers specifically to predicting the distribution of an individual patient's survival time, i.e. modelling the distribution of the event occurring over $\NNReals$.
Therefore this is seen as the probabilistic analogue to the deterministic time-to-event prediction, these definitions are motivated by similar terminology in machine learning regression problems (@sec-surv-setml).

Distributional prediction can in theory target any of the quantities introduced in @sec-distributions but predicting $S(t)$ and/or $h(t)$ is most common.
Hazard based approaches are particularly relevant for non- and semi-parametric estimation of the distribution, where no assumptions are made about the underlying distribution of event times.


## Survival Analysis Task {#sec-surv-setmltask}

The survival prediction problems identified in (@sec-surv-set-types) are now formalised as machine learning tasks.

:::: {.callout-note icon=false}

## Survival Task

::: {#cnj-task-surv}
Let $(X,T,\Delta)$ be random variables t.v.i. $\calX \times \calT \times \bset$ where $\calX \subseteq \Reals^p$ and $\calT \subseteq \NNReals$.
Let $\calS \subseteq \distrT$ be a convex set of distributions on $\calT$ and let $\calR \subseteq \Reals$.
Then,

* The *probabilistic survival task* is the problem of predicting a conditional distribution over the positive Reals and is specified by $g: \calX \rightarrow \calS$.
* The *deterministic survival task* is the problem of predicting a continuous value in the positive Reals and is specified by $g: \calX \rightarrow \calT$.
* The *survival ranking task* is specified by predicting a continuous ranking in the Reals and is specified by $g: \calX \rightarrow \calR$.

:::
::::

Any other survival prediction type falls within one of the tasks in @cnj-task-surv, for example predicting log-survival time is the deterministic task and predicting prognostic index or linear predictor is the ranking task.
Removing the separation between the prognostic index and ranking prediction types is due to them both making predictions over $\Reals$ and hence their difference lies in interpretation only.

In this book, unless otherwise specified, the term *survival task*, will be used to refer to the probabilistic survival task.
As a final note, these definitions are given in the most general case where the time variable is over $\NNReals$.
In practice, all models instead assume time is over $\PReals$ and so a subject $i$ that experiences an outcome at $0$ is either deleted or their outcome time is set to $T_i = \epsilon$ for some very small $\epsilon \in \PReals$. 

<!-- FIXME I got rid of these notation earlier, but don't see how the following is very usefull without them. On the other hand the connection to regression and classification could useful, but could also be introduced later when the actual reductions are introduced.
#### Survival Analysis and Regression {.unnumbered .unlisted}



Survival and regression tasks are closely related as can be observed from their respective definitions. Both are specified by $g : \calX \rightarrow \calS$ where for probabilistic regression $\calS \subseteq \Distr(\Reals)$ and for survival $\calS \subseteq \Distr(\NNReals)$.
Furthermore both settings can be viewed to use the same generative process.
In the survival setting in which there is no censoring then data is drawn from $(X,Y) \ t.v.i. \ \calX \times \calT, \calT \subseteq \NNReals$ and in regression from $(X,Y) \ t.v.i. \ \calX \times \calY, \calY \subseteq \Reals$, so that the only difference is whether the outcome data ranges over the Reals or positive Reals.

These closely related tasks are discussed in more detail in (@sec-car), with a particular focus on how the more popular regression setting can be used to solve survival tasks.
In (@sec-review) the models are first introduced in a regression setting and then the adaptations to survival are discussed, which is natural when considering that historically machine learning survival models have been developed by adapting regression models.
-->
