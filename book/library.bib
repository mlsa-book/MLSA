@article{vakulenko-lagun.inverse.2020,
  author       = {{Vakulenko-Lagun}, Bella and Mandel, Micha and Betensky, Rebecca A.},
  date         = {2020-06},
  doi          = {10.1111/biom.13162},
  issn         = {0006-341X},
  journaltitle = {Biometrics},
  note         = {https://doi.org/10.1111/biom.13162},
  number       = {2},
  pages        = {484--495},
  title        = {Inverse {{Probability Weighting Methods}} for {{Cox Regression}} with {{Right-Truncated Data}}},
  urldate      = {2024-04-11},
  volume       = {76},
}

@article{han.restricted.2022,
  author       = {Han, Kyunghwa and Jung, Inkyung},
  date         = {2022-05},
  doi          = {10.3348/kjr.2022.0061},
  issn         = {1229-6929},
  journaltitle = {Korean Journal of Radiology},
  note         = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9081686/},
  number       = {5},
  pages        = {495--499},
  shorttitle   = {Restricted {{Mean Survival Time}} for {{Survival Analysis}}},
  title        = {Restricted {{Mean Survival Time}} for {{Survival Analysis}}: {{A Quick Guide}} for {{Clinical Researchers}}},
  urldate      = {2024-01-28},
  volume       = {23},
}

@article{karrison.restricted.1987a,
  author       = {Karrison, Theodore},
  publisher    = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  date         = {1987},
  doi          = {10.2307/2289396},
  eprint       = {2289396},
  eprinttype   = {jstor},
  issn         = {0162-1459},
  journaltitle = {Journal of the American Statistical Association},
  note         = {https://www.jstor.org/stable/2289396},
  number       = {400},
  pages        = {1169--1176},
  title        = {Restricted {{Mean Life With Adjustment}} for {{Covariates}}},
  urldate      = {2023-12-07},
  volume       = {82},
}

@article{andersen.regression.2004,
  author       = {Andersen, Per Kragh and Hansen, Mette Gerster and Klein, John P.},
  date         = {2004-12},
  doi          = {10.1007/s10985-004-4771-0},
  issn         = {1380-7870, 1572-9249},
  journaltitle = {Lifetime Data Analysis},
  langid       = {english},
  note         = {http://link.springer.com/10.1007/s10985-004-4771-0},
  number       = {4},
  pages        = {335--350},
  title        = {Regression {{Analysis}} of {{Restricted Mean Survival Time Based}} on {{Pseudo-Observations}}},
  urldate      = {2023-11-10},
  volume       = {10},
}

@book{beyersmann.competing.2012,
  author     = {Beyersmann, Jan and Allignol, Arthur and Schumacher, Martin},
  location   = {New York},
  publisher  = {Springer},
  annotation = {OCLC: 796201252},
  date       = {2012},
  isbn       = {978-1-4614-2034-7 978-1-4614-2035-4},
  langid     = {english},
  series     = {Use {{R}}!},
  title      = {Competing Risks and Multistate Models with {{R}}},
}

@misc{pakbin.boxhed2.2023,
  author       = {Pakbin, Arash and Wang, Xiaochen and Mortazavi, Bobak J. and Lee, Donald K. K.},
  publisher    = {arXiv},
  date         = {2023-09},
  doi          = {10.48550/arXiv.2103.12591},
  eprint       = {2103.12591},
  eprintclass  = {cs, stat},
  eprinttype   = {arxiv},
  howpublished = {http://arxiv.org/abs/2103.12591},
  number       = {arXiv:2103.12591},
  shorttitle   = {{{BoXHED2}}.0},
  title        = {{{BoXHED2}}.0: {{Scalable}} Boosting of Dynamic Survival Analysis},
  urldate      = {2023-09-08},
}

@article{Mack2014,
  abstract     = {Clinicians are sometimes reluctant to discuss prognosis with parents of children with life-threatening illness, usually because they worry about the emotional impact of this information. However, parents often want this prognostic information because it underpins informed decision-making, especially near the end of life. In addition, despite understandable clinician concerns about its emotional impact, prognostic disclosure can actually support hope and peace of mind among parents struggling to live with a child's illness. Children, too, may need to understand what is ahead to manage uncertainty and make plans for the ways their remaining life will be lived. In this article, we describe the ethical issues involved in disclosure of prognostic information to parents and children with life-threatening illness and offer practical guidance for these conversations.},
  author       = {Mack, Jennifer W and Joffe, Steven},
  url          = {http://pediatrics.aappublications.org/content/133/Supplement_1/S24.abstract},
  date         = {2014-02},
  doi          = {10.1542/peds.2013-3608E},
  journaltitle = {Pediatrics},
  number       = {Supplement 1},
  pages        = {S24 LP -- S30},
  title        = {{Communicating About Prognosis: Ethical Responsibilities of Pediatricians and Parents}},
  volume       = {133},
}

@article{Jones2021,
  abstract     = {Prognostic uncertainty is frequently cited as a barrier to communication between physicians and patients and is particularly burdensome for surrogate decision-makers, who must make choices on behalf of their incapacitated family members. The Conceptual Taxonomy of Uncertainty is one model through which physician and surrogate communication can be analyzed to identify strategies for reducing uncertainty in surrogate decision-making. Our objective was to examine themes of uncertainty in physician communication of prognosis and surrogate goals-of-care decision-making for critically ill patients with traumatic brain injury (TBI).},
  author       = {Jones, Kelsey and Quinn, Thomas and Mazor, Kathleen M and Muehlschlegel, Susanne},
  url          = {https://doi.org/10.1007/s12028-021-01230-3},
  annotation   = {Uses 2011 taxonomy without adaptation},
  date         = {2021},
  doi          = {10.1007/s12028-021-01230-3},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Jones et al. - 2021 - Prognostic Uncertainty in Critically Ill Patients with Traumatic Brain Injury A Multicenter Qualitative Study.pdf:pdf},
  issn         = {1556-0961},
  journaltitle = {Neurocritical Care},
  number       = {2},
  pages        = {311--321},
  title        = {{Prognostic Uncertainty in Critically Ill Patients with Traumatic Brain Injury: A Multicenter Qualitative Study}},
  volume       = {35},
}

@article{Ripley1998,
  abstract     = {Estimating the risk of relapse for breast cancer patients is necessary, since it affects the choice of treatment. This problem involves analysing data of times to relapse of patients and relating them to prognostic variables. Some of the times to relapse will usually be censored.We investigate various ways of using neural network models to extend traditional statistical models in this situation. Such models are better able to model both non-linear effects of prognostic factors and interactions between them, than linear logistic or Cox regression models. With the dataset used in our study, however, the prediction of the risk of relapse is not significantly improved when using a neural network model. Predicting the risk that a patient will relapse within three years, say, is possible from this data, but not when any relapse will happen.},
  author       = {Ripley, R M and Harris, A L and Tarassenko, L},
  url          = {https://doi.org/10.1007/BF01428127},
  annotation   = {Classification},
  date         = {1998},
  doi          = {10.1007/BF01428127},
  issn         = {1433-3058},
  journaltitle = {Neural Computing & Applications},
  number       = {4},
  pages        = {367--375},
  title        = {{Neural network models for breast cancer prognosis}},
  volume       = {7},
}

@article{Murphy1973,
  author       = {Murphy, Allan H},
  language     = {English},
  location     = {Boston MA, USA},
  publisher    = {American Meteorological Society},
  url          = {https://journals.ametsoc.org/view/journals/apme/12/4/1520-0450_1973_012_0595_anvpot_2_0_co_2.xml},
  date         = {1973},
  doi          = {10.1175/1520-0450(1973)012<0595:ANVPOT>2.0.CO;2},
  journaltitle = {Journal of Applied Meteorology and Climatology},
  number       = {4},
  pages        = {595--600},
  title        = {{A New Vector Partition of the Probability Score}},
  volume       = {12},
}

@article{Schendel2018,
  author       = {Schendel, Thomas and Jung, Christian and Lindtner, Oliver and Greiner, Matthias},
  url          = {http://doi.wiley.com/10.2903/sp.efsa.2018.EN-1472},
  date         = {2018-07},
  doi          = {10.2903/sp.efsa.2018.EN-1472},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Schendel et al. - 2018 - Guidelines for Uncertainty Analysis Application of the respective Documents of EFSA and BfR for Exposure Assess.pdf:pdf},
  issn         = {23978325},
  journaltitle = {EFSA Supporting Publications},
  number       = {7},
  title        = {{Guidelines for Uncertainty Analysis: Application of the respective Documents of EFSA and BfR for Exposure Assessments}},
  volume       = {15},
}

@article{Breiman1996b,
  abstract     = {In bagging, predictors are constructed using bootstrap samples from the training set and then aggregated to form a bagged predictor. Each bootstrap sample leaves out about 37% of the examples. These left-out examples can be used to form accurate estimates of important quantities. For instance, they can be used to give much improved estimates of node probabilities and node error rates in decision trees. Using estimated outputs instead of the observed outputs improves accuracy in regression trees. They can also be used to give nearly optimal estimates of generalization errors for bagged predictors. * Partially supported by NSF Grant 1-444063-21445 Introduction: We assume that there is a training set T= {(y n ,x n), n=1, ... ,N} and a method for constructing a predictor Q(x,T) using the given training set. The output variable y can either be a class label (classification) or numerical (regression). In bagging (Breiman[1996a]) a sequence of training sets T B,1 , ... , T B,K are generated of the same size as T by bootstrap selection from T. Then K predictors are constructed such that the kth predictor Q(x,T k,B) is based on the kth bootstrap training set. It was shown that if these predictors are aggregated--averaging in regression or voting in classification, then the resultant predictor can be considerably more accurate than the original predictor. Accuracy is increased if the prediction method is unstable, i.e. if small changes in the training set or in the parameters used in construction can result in large changes in the resulting predictor. The examples generated in Breiman[1996a] were based on trees and subset selection in regression, but it is known that neural nets are also unstable, as are other well-known prediction methods. Other methods such as nearest neighbors, are stable.},
  author       = {Breiman, Leo},
  url          = {https://www.stat.berkeley.edu/$\sim$breiman/OOBestimation.pdf},
  date         = {1996},
  doi          = {10.1016/j.patcog.2009.05.010},
  eprint       = {arXiv:1011.1669v3},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Breiman - 1996 - Out-of-Bag Estimation.pdf:pdf},
  isbn         = {1612-9210 (Electronic)1̊612-9202 (Linking)},
  issn         = {00313203},
  journaltitle = {Technical Report},
  keywords     = {bagging,machine learning,oob,out-of-bag,random forests},
  pages        = {1--13},
  title        = {{Out-of-Bag Estimation}},
}

@article{Chi2007,
  abstract     = {This paper applies artificial neural networks (ANNs) to the survival analysis problem. Because ANNs can easily consider variable interactions and create a non-linear prediction model, they offer more flexible prediction of survival time than traditional methods. This study compares ANN results on two different breast cancer datasets, both of which use nuclear morphometric features. The results show that ANNs can successfully predict recurrence probability and separate patients with good (more than five years) and bad (less than five years) prognoses. Results are not as clear when the separation is done within subgroups such as lymph node positive or negative.},
  author       = {Chi, Chih-Lin and Street, W Nick and Wolberg, William H},
  language     = {eng},
  publisher    = {American Medical Informatics Association},
  url          = {https://pubmed.ncbi.nlm.nih.gov/18693812 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2813661/},
  date         = {2007-10},
  issn         = {1942-597X},
  journaltitle = {AMIA ... Annual Symposium proceedings. AMIA Symposium},
  keywords     = {*Neoplasm Recurrence,Local,*Neural Networks,Computer,*Survival Analysis,Breast Neoplasms/*mortality/surgery,Databases as Topic,Decision Support Techniques,Disease-Free Survival,Humans,Kaplan-Meier Estimate,Models,Biological,Prognosis,Statistics,Nonparametric},
  pages        = {130--134},
  title        = {{Application of artificial neural network-based survival analysis on two breast cancer datasets}},
  volume       = {2007},
}

@misc{pkgrpart,
  author    = {Therneau, Terry M. and Atkinson, Beth},
  publisher = {CRAN},
  date      = {2019},
  title     = {{rpart: Recursive Partitioning and Regression Trees}},
}

@article{Segal1988,
  author       = {Segal, Mark Robert},
  date         = {1988},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Segal - 1988 - Regression Trees for Censored Data.pdf:pdf},
  journaltitle = {Biometrics},
  keywords     = {Censoring,Pruning,Regression tree,Splitting rule,Tarone-Ware class,logrank,random forests,splitting rules,survival forests,survival trees},
  number       = {1},
  pages        = {35--47},
  title        = {{Regression Trees for Censored Data}},
  volume       = {44},
}

@misc{pkgtidymodels,
  author     = {Kuhn, Max and Wickham, Hadley},
  publisher  = {CRAN},
  url        = {https://cran.r-project.org/package=tidymodels},
  annotation = {R package version 0.1.0},
  date       = {2020},
  title      = {{tidymodels: Easily Install and Load the 'Tidymodels' Packages}},
}

@article{Goli2016b,
  abstract     = {The Support Vector Regression (SVR) model has been broadly used for response prediction. However, few researchers have used SVR for survival analysis. In this study, a new SVR model is proposed and SVR with different kernels and the traditional Cox model are trained. The models are compared based on different performance measures. We also select the best subset of features using three feature selection methods: combination of SVR and statistical tests, univariate feature selection based on concordance index, and recursive feature elimination. The evaluations are performed using available medical datasets and also a Breast Cancer (BC) dataset consisting of 573 patients who visited the Oncology Clinic of Hamadan province in Iran. Results show that, for the BC dataset, survival time can be predicted more accurately by linear SVR than nonlinear SVR. Based on the three feature selection methods, metastasis status, progesterone receptor status, and human epidermal growth factor receptor 2 status are the best features associated to survival. Also, according to the obtained results, performance of linear and nonlinear kernels is comparable. The proposed SVR model performs similar to or slightly better than other models. Also, SVR performs similar to or better than Cox when all features are included in model.},
  author       = {Goli, Shahrbanoo and Mahjub, Hossein and Faradmal, Javad and Mashayekhi, Hoda and Soltanian, Ali-Reza},
  editor       = {Pappalardo, Francesco},
  publisher    = {Hindawi Publishing Corporation},
  url          = {https://doi.org/10.1155/2016/2157984},
  date         = {2016},
  doi          = {10.1155/2016/2157984},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Goli et al. - 2016 - Survival Prediction and Feature Selection in Patients with Breast Cancer Using Support Vector Regression.pdf:pdf},
  issn         = {1748-670X},
  journaltitle = {Computational and Mathematical Methods in Medicine},
  keywords     = {SVCR,SVR,SVR-MRL,SVRc,benchmark experiment,censoring,comparison,machine learning,survey,survival,svm},
  pages        = {2157984},
  title        = {{Survival Prediction and Feature Selection in Patients with Breast Cancer Using Support Vector Regression}},
  volume       = {2016},
}

@misc{pkgcoxnnet,
  author = {Ching, Travers},
  url    = {https://github.com/lanagarmire/cox-nnet},
  date   = {2015},
  title  = {cox-nnet},
}

@article{Li2018,
  abstract     = {The time-dependent receiver operating characteristic curve is often used to study the diagnostic accuracy of a single continuous biomarker, measured at baseline, on the onset of a disease condition when the disease onset may occur at different times during the follow-up and hence may be right censored. Due to right censoring, the true disease onset status prior to the pre-specified time horizon may be unknown for some patients, which causes difficulty in calculating the time-dependent sensitivity and specificity. We propose to estimate the time-dependent sensitivity and specificity by weighting the censored data by the conditional probability of disease onset prior to the time horizon given the biomarker, the observed time to event, and the censoring indicator, with the weights calculated nonparametrically through a kernel regression on time to event. With this nonparametric weighting adjustment, we derive a novel, closed-form formula to calculate the area under the time-dependent receiver operating characteristic curve. We demonstrate through numerical study and theoretical arguments that the proposed method is insensitive to misspecification of the kernel bandwidth, produces unbiased and efficient estimators of time-dependent sensitivity and specificity, the area under the curve, and other estimands from the receiver operating characteristic curve, and outperforms several other published methods currently implemented in R packages.},
  author       = {Li, Liang and Greene, Tom and Hu, Bo},
  publisher    = {SAGE Publications Ltd STM},
  url          = {https://doi.org/10.1177/0962280216680239},
  annotation   = {doi: 10.1177/0962280216680239},
  date         = {2018-11},
  doi          = {10.1177/0962280216680239},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Li, Greene, Hu - 2018 - A simple method to estimate the time-dependent receiver operating characteristic curve and the area under the cu.pdf:pdf},
  issn         = {0962-2802},
  journaltitle = {Statistical Methods in Medical Research},
  number       = {8},
  pages        = {2264--2278},
  title        = {{A simple method to estimate the time-dependent receiver operating characteristic curve and the area under the curve with right censored data}},
  volume       = {27},
}

@article{Peters2007a,
  author       = {Peters, Ellen and Dieckmann, Nathan and Dixon, Anna and Hibbard, Judith H and Mertz, C K},
  publisher    = {Sage Publications},
  date         = {2007},
  issn         = {1077-5587},
  journaltitle = {Medical Care Research and Review},
  number       = {2},
  pages        = {169--190},
  title        = {{Less is more in presenting quality information to consumers}},
  volume       = {64},
}

@article{Cui2020,
  abstract     = {Lung cancer is the leading cause of cancer-related deaths in both men and women in the United States, and it has a much lower five-year survival rate than many other cancers. Accurate survival analysis is urgently needed for better disease diagnosis and treatment management.},
  author       = {Cui, Lei and Li, Hansheng and Hui, Wenli and Chen, Sitong and Yang, Lin and Kang, Yuxin and Bo, Qirong and Feng, Jun},
  url          = {https://doi.org/10.1186/s12859-020-3431-z},
  annotation   = {imaging},
  date         = {2020},
  doi          = {10.1186/s12859-020-3431-z},
  issn         = {1471-2105},
  journaltitle = {BMC Bioinformatics},
  number       = {1},
  pages        = {112},
  title        = {{A deep learning-based framework for lung cancer survival analysis with biomarker interpretation}},
  volume       = {21},
}

@book{Beauchamp2013,
  author    = {Beauchamp, Tom L and Childress, James F and Press., Oxford University},
  language  = {English},
  location  = {New York; Oxford},
  publisher = {Oxford University Press},
  date      = {2013},
  isbn      = {9780199924585 0199924589},
  title     = {{Principles of biomedical ethics}},
}

@article{Liu1985,
  author       = {Liu, Regina Y. C. and Ryzin, John Van},
  date         = {1985},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Ryzin - 1985 - A Histogram Estimator of the Hazard Rate with Censored Data.pdf:pdf},
  journaltitle = {The Annals of Statistics},
  number       = {2},
  pages        = {592--605},
  title        = {{A Histogram Estimator of the Hazard Rate with Censored Data}},
  volume       = {13},
}

@article{pkgmistr,
  author       = {Sablica, Lukas and Hornik, Kurt},
  publisher    = {CRAN},
  url          = {https://journal.r-project.org/archive/2020/RJ-2020-003/index.html},
  date         = {2020},
  doi          = {10.32614/RJ-2020-003},
  issn         = {2073-4859},
  journaltitle = {The R Journal},
  number       = {1},
  pages        = {283},
  title        = {{mistr: A Computational Framework for Mixture and Composite Distributions}},
  volume       = {12},
}

@article{Han2017,
  abstract     = {PURPOSE: Clinical next-generation sequencing (CNGS) is introducing new opportunities and challenges into the practice of medicine. Simultaneously, these technologies are generating uncertainties of an unprecedented scale that laboratories, clinicians, and patients are required to address and manage. We describe in this report the conceptual design of a new taxonomy of uncertainties around the use of CNGS in health care. METHODS: Interviews to delineate the dimensions of uncertainty in CNGS were conducted with genomics experts and themes were extracted in order to expand on a previously published three-dimensional taxonomy of medical uncertainty. In parallel, we developed an interactive website to disseminate the CNGS taxonomy to researchers and engage them in its continued refinement. RESULTS: The proposed taxonomy divides uncertainty along three axes-source, issue, and locus-and further discriminates the uncertainties into five layers with multiple domains. Using a hypothetical clinical example, we illustrate how the taxonomy can be applied to findings from CNGS and used to guide stakeholders through interpretation and implementation of variant results. CONCLUSION: The utility of the proposed taxonomy lies in promoting consistency in describing dimensions of uncertainty in publications and presentations, to facilitate research design and management of the uncertainties inherent in the implementation of CNGS.Genet Med advance online publication 19 January 2017.},
  author       = {Han, Paul K J and Umstead, Kendall L and Bernhardt, Barbara A and Green, Robert C and Joffe, Steven and Koenig, Barbara and Krantz, Ian and Waterston, Leo B and Biesecker, Leslie G and Biesecker, Barbara B},
  language     = {eng},
  url          = {https://pubmed.ncbi.nlm.nih.gov/28102863 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5517355/},
  date         = {2017-08},
  doi          = {10.1038/gim.2016.212},
  edition      = {2017/01/19},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Han et al. - 2017 - A taxonomy of medical uncertainties in clinical genome sequencing.pdf:pdf},
  issn         = {1530-0366},
  journaltitle = {Genetics in medicine : official journal of the American College of Medical Genetics},
  keywords     = {*Uncertainty,*Whole Genome Sequencing,Adult,Genetic Predisposition to Disease,Humans,Hydroxymethylglutaryl-CoA Reductase Inhibitors/adv,Male,Molecular Diagnostic Techniques,Muscular Diseases/chemically induced/genetics,Predictive Value of Tests},
  number       = {8},
  pages        = {918--925},
  title        = {{A taxonomy of medical uncertainties in clinical genome sequencing}},
  volume       = {19},
}

@article{Ching2018,
  abstract     = {Artificial neural networks (ANN) are computing architectures with many interconnections of simple neural-inspired computing elements, and have been applied to biomedical fields such as imaging analysis and diagnosis. We have developed a new ANN framework called Cox-nnet to predict patient prognosis from high throughput transcriptomics data. In 10 TCGA RNA-Seq data sets, Cox-nnet achieves the same or better predictive accuracy compared to other methods, including Cox-proportional hazards regression (with LASSO, ridge, and mimimax concave penalty), Random Forests Survival and CoxBoost. Cox-nnet also reveals richer biological information, at both the pathway and gene levels. The outputs from the hidden layer node provide an alternative approach for survival-sensitive dimension reduction. In summary, we have developed a new method for accurate and efficient prognosis prediction on high throughput data, with functional biological insights. The source code is freely available at https://github.com/lanagarmire/cox-nnet.},
  author       = {Ching, Travers and Zhu, Xun and Garmire, Lana X.},
  editor       = {Markowetz, Florian},
  url          = {https://dx.plos.org/10.1371/journal.pcbi.1006076},
  date         = {2018-04},
  doi          = {10.1371/journal.pcbi.1006076},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Ching, Zhu, Garmire - 2018 - Cox-nnet An artificial neural network method for prognosis prediction of high-throughput omics data.pdf:pdf},
  isbn         = {15537358 (Electronic)},
  issn         = {1553-7358},
  journaltitle = {PLOS Computational Biology},
  number       = {4},
  pages        = {e1006076},
  title        = {{Cox-nnet: An artificial neural network method for prognosis prediction of high-throughput omics data}},
  volume       = {14},
}

@article{Uno2011,
  abstract     = {For modern evidence-based medicine, a well thought-out risk scoring system for predicting the occurrence of a clinical event plays an important role in selecting prevention and treatment strategies. Such an index system is often established based on the subject's 'baseline' genetic or clinical markers via a working parametric or semi-parametric model. To evaluate the adequacy of such a system, C-statistics are routinely used in the medical literature to quantify the capacity of the estimated risk score in discriminating among subjects with different event times. The C-statistic provides a global assessment of a fitted survival model for the continuous event time rather than focussing on the prediction of bit-year survival for a fixed time. When the event time is possibly censored, however, the population parameters corresponding to the commonly used C-statistics may depend on the study-specific censoring distribution. In this article, we present a simple C-statistic without this shortcoming. The new procedure consistently estimates a conventional concordance measure which is free of censoring. We provide a large sample approximation to the distribution of this estimator for making inferences about the concordance measure. Results from numerical studies suggest that the new procedure performs well in finite sample.},
  author       = {Uno, Hajime and Cai, Tianxi and Pencina, Michael J. and D'Agostino, Ralph B. and Wei, L J},
  annotation   = {Uno's c-statistic used to remove the bias in Harrel's which occurs in the precense of censoring. It's also non-parametric in contract to semi-parametric alternatives that require certain assumptions like normally distributed prognostic indexes.},
  date         = {2011},
  doi          = {10.1002/sim.4154},
  eprint       = {NIHMS150003},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Uno et al. - 2011 - On the C-statistics for Evaluating Overall Adequacy of Risk Prediction Procedures with Censored Survival Data.pdf:pdf},
  isbn         = {1097-0258},
  issn         = {02776715},
  journaltitle = {Statistics in Medicine},
  keywords     = {auc,c-index,concordance,cox,discrimination,evaluation,framingham risk score,measure,roc,s proportional hazards model,survival},
  number       = {10},
  pages        = {1105--1117},
  title        = {{On the C-statistics for Evaluating Overall Adequacy of Risk Prediction Procedures with Censored Survival Data}},
  volume       = {30},
}

@book{dataaidsid,
  author  = {Carlin, Bradley P and Louis, Thomas A},
  date    = {2018},
  edition = {3},
  title   = {{Supplemental Materials to Bayesian Methods for Data Analysis, 3rd Edition}},
}

@book{Laan2011,
  abstract  = {The statistics profession is at a unique point in history. The need for valid statistical tools is greater than ever; data sets are massive, often measuring hundreds of thousands of measurements for a single subject. The field is ready to move towards clear objective benchmarks under which tools can be evaluated. Targeted learning allows (1) the full generalization and utilization of cross-validation as an estimator selection tool so that the subjective choices made by humans are now made by the machine, and (2) targeting the fitting of the probability distribution of the data toward the target parameter representing the scientific question of interest. This book is aimed at both statisticians and applied researchers interested in causal inference and general effect estimation for observational and experimental data. Part I is an accessible introduction to super learning and the targeted maximum likelihood estimator, including related concepts necessary to understand and apply these methods. Parts II-IX handle complex data structures and topics applied researchers will immediately recognize from their own research, including time-to-event outcomes, direct and indirect effects, positivity violations, case-control studies, censored data, longitudinal data, and genomic studies."Targeted Learning, by Mark J. van der Laan and Sherri Rose, fills a much needed gap in statistical and causal inference. It protects us from wasting computational, analytical, and data resources on irrelevant aspects of a problem and teaches us how to focus on what is relevant answering questions that researchers truly care about."-Judea Pearl, Computer Science Department, University of California, Los Angeles"In summary, this book should be on the shelf of every investigator who conducts observational research and randomized controlled trials. The concepts and methodology are foundational for causal inference and at the same time stay true to what the data at hand can say about the questions that motivate their collection."-Ira B. Tager, Division of Epidemiology, University of California, Berkeley},
  author    = {van der Laan, Mark J. and Rose, Sherri},
  booktitle = {Springer Series in Statistics},
  date      = {2011},
  doi       = {10.1007/978-1-4419-9782-1},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Laan, Rose - 2011 - Targeted Learning - preface.pdf:pdf},
  isbn      = {1441997814},
  number    = {2},
  pages     = {626},
  title     = {{Targeted Learning - preface}},
  volume    = {27},
}

@article{Wang2019,
  abstract     = {Some interesting recent studies have shown that neural network models are useful alternatives in modeling survival data when the assumptions of a classical parametric or semiparametric survival model such as the Cox (1972) model are seriously violated. However, to the best of our knowledge, the plausibility of adapting the emerging extreme learning machine (ELM) algorithm for single-hidden-layer feedforward neural networks to survival analysis has not been explored. In this paper, we present a kernel ELM Cox model regularized by an L0-based broken adaptive ridge (BAR) penalization method. Then, we demonstrate that the resulting method, referred to as ELMCoxBAR, can outperform some other state-of-art survival prediction methods such as L1- or L2-regularized Cox regression, random survival forest with various splitting rules, and boosted Cox model, in terms of its predictive performance using both simulated and real world datasets. In addition to its good predictive performance, we illustrate that the proposed method has a key computational advantage over the above competing methods in terms of computation time efficiency using an a real-world ultra?high-dimensional survival data.},
  author       = {Wang, Hong and Li, Gang},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/sim.8090},
  annotation   = {doi: 10.1002/sim.8090},
  date         = {2019-05},
  doi          = {10.1002/sim.8090},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Li - 2019 - Extreme learning machine Cox model for high-dimensional survival analysis.pdf:pdf},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  keywords     = {censored data,extreme learning machine,machine learning,regularized Cox model,survival analysis},
  number       = {12},
  pages        = {2139--2156},
  title        = {{Extreme learning machine Cox model for high-dimensional survival analysis}},
  volume       = {38},
}

@inproceedings{Goldstein2020,
  author    = {Goldstein, Mark and Han, Xintian and Puli, Aahlad M. and Perotte, Adler J. and Ranganath, Rajesh},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/d4a93297083a23cc099f7bd6a8621131-Abstract.html},
  booktitle = {Advances in Neural Information Processing Systems},
  date      = {2020},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Goldstein et al. - 2020 - X-CAL Explicit Calibration for Survival Analysis.pdf:pdf},
  title     = {{X-CAL: Explicit Calibration for Survival Analysis}},
}

@article{Land2011,
  abstract     = {While the role of survival analysis in medicine has continued to be increasingly essential in making treatment and other health care decisions, the common clinical methods used for performing these analyses, such as Cox Proportional Hazard models and Kaplan-Meier curves, have become antiquated. We have developed a new survival analysis technique of the Evolutionary Programming / Evolutionary Strategies Support Vector Regression Hybrid for censored and non-censored event data. This method provides the benefits of optimized statistical learning theory to be used as a replacement for or in addition to existing survival analysis protocols. The technique was tested on an artificially censored data from a well-known benchmark dataset as well as actual clinical data with encouraging results.},
  author       = {Land, Walker H and Qiao, Xingye and Margolis, Dan and Gottlieb, Ron},
  url          = {http://www.sciencedirect.com/science/article/pii/S1877050911005151},
  date         = {2011},
  doi          = {https://doi.org/10.1016/j.procs.2011.08.050},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Land et al. - 2011 - A new tool for survival analysis evolutionary programmingevolutionary strategies (EPES) support vector regression h.pdf:pdf},
  issn         = {1877-0509},
  journaltitle = {Procedia Computer Science},
  keywords     = {Evolutionary Programming,SVRc,Statistical Learning Theory,Survival Analysis,machine learning,model,support vector machine,survival,svrc},
  pages        = {267--272},
  title        = {{A new tool for survival analysis: evolutionary programming/evolutionary strategies (EP/ES) support vector regression hybrid using both censored / non-censored (event) data}},
  volume       = {6},
}

@inproceedings{Zhu2016,
  abstract   = {Traditional Cox proportional hazard model for survival analysis are based on structured features like patients' sex, smoke years, BMI, etc. With the development of medical imaging technology, more and more unstructured medical images are available for diagnosis, treatment and survival analysis. Traditional survival models utilize these unstructured images by extracting human-designed features from them. However, we argue that those hand-crafted features have limited abilities in representing highly abstract information. In this paper, we for the first time develop a deep convolutional neural network for survival analysis (DeepConvSurv) with pathological images. The deep layers in our model could represent more abstract information compared with hand-crafted features from the images. Hence, it will improve the survival prediction performance. From our extensive experiments on the National Lung Screening Trial (NLST) lung cancer data, we show that the proposed DeepConvSurv model improves significantly compared with four state-of-the-art methods.},
  author     = {Zhu, X and Yao, J and Huang, J},
  annotation = {imaging},
  booktitle  = {2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
  date       = {2016},
  doi        = {10.1109/BIBM.2016.7822579},
  isbn       = {VO -},
  keywords   = {Analytical models,Data models,Deep learning,DeepConvSurv,Feature extraction,Hazards,Lung cancer,Lungs,NLST lung cancer data,Pathological images,Pathology,Predictive models,Survival analysis,cancer,cox proportional hazard model,deep convolutional neural network,feature extraction,hand-crafted feature,lung,medical image processing,medical imaging technology,national lung screening trial,neural nets,pathological image,survival analysis,survival prediction,unstructured medical image},
  pages      = {544--547},
  title      = {{Deep convolutional neural network for survival analysis with pathological images}},
}

@article{Kuhn2014,
  author   = {Kuhn, Max},
  date     = {2014-05},
  keywords = {futility,machine learning,optimisation,toolboxes,tuning},
  title    = {{Futility Analysis in the Cross-Validation of Machine Learning Models}},
}

@article{Schratz2021,
  abstract   = {Spatial and spatiotemporal machine-learning models require a suitable framework for their model assessment, model selection, and hyperparameter tuning, in order to avoid error estimation bias and over-fitting. This contribution reviews the state-of-the-art in spatial and spatiotemporal cross-validation, and introduces the {R} package {mlr3spatiotempcv} as an extension package of the machine-learning framework {mlr3}. Currently various {R} packages implementing different spatiotemporal partitioning strategies exist: {blockCV}, {CAST}, {skmeans} and {sperrorest}. The goal of {mlr3spatiotempcv} is to gather the available spatiotemporal resampling methods in {R} and make them available to users through a simple and common interface. This is made possible by integrating the package directly into the {mlr3} machine-learning framework, which already has support for generic non-spatiotemporal resampling methods such as random partitioning. One advantage is the use of a consistent nomenclature in an overarching machine-learning toolkit instead of a varying package-specific syntax, making it easier for users to choose from a variety of spatiotemporal resampling methods. This package avoids giving recommendations which method to use in practice as this decision depends on the predictive task at hand, the autocorrelation within the data, and the spatial structure of the sampling design or geographic objects being studied.},
  author     = {Schratz, Patrick and Becker, Marc and Lang, Michel and Brenning, Alexander},
  url        = {http://arxiv.org/abs/2110.12674},
  date       = {2021-10},
  eprint     = {2110.12674},
  eprinttype = {arXiv},
  title      = {{mlr3spatiotempcv: Spatiotemporal resampling methods for machine learning in R}},
}

@article{Bergstra2012,
  author       = {Bergstra, James and Bengio, Yoshua},
  publisher    = {JMLR. org},
  date         = {2012},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Bergstra, Bengio - 2012 - Random search for hyper-parameter optimization.pdf:pdf},
  issn         = {1532-4435},
  journaltitle = {The Journal of Machine Learning Research},
  number       = {1},
  pages        = {281--305},
  title        = {{Random search for hyper-parameter optimization}},
  volume       = {13},
}

@article{Lazer2014,
  author       = {Lazer, David and Kennedy, Ryan and King, Gary and Vespignani, Alessandro},
  url          = {http://science.sciencemag.org/content/343/6176/1203.abstract},
  date         = {2014-03},
  doi          = {10.1126/science.1248506},
  journaltitle = {Science},
  number       = {6176},
  pages        = {1203 LP -- 1205},
  title        = {{The Parable of Google Flu: Traps in Big Data Analysis}},
  volume       = {343},
}

@misc{DataStudyGroupteam2019,
  abstract  = {This report presents the output of a week-long collaboration between the Alan Turing Institute, NHS Scotland, and the National Services Scotland's Information Services Division (ISD) to investigate and update the current decision support tool for identifying patients at risk of admission - the SPARRA (Scottish Patients at Risk of Readmission and Admission) model.},
  author    = {{Data Study Group team}},
  publisher = {Zenodo},
  url       = {https://zenodo.org/record/2539563#.XftPipP7TOQ},
  date      = {2019},
  doi       = {10.5281/zenodo.2539563},
  title     = {{Data Study Group Final Report: NHS Scotland.}},
}

@article{Lipkus2009,
  author       = {Lipkus, Isaac M and Peters, Ellen},
  publisher    = {Sage Publications Sage CA: Los Angeles, CA},
  date         = {2009},
  issn         = {1090-1981},
  journaltitle = {Health Education & Behavior},
  number       = {6},
  pages        = {1065--1081},
  title        = {{Understanding the role of numeracy in health: proposed theoretical framework and practical insights}},
  volume       = {36},
}

@misc{pkgmlr3verse,
  author    = {Schratz, Patrick and Lang, Michel},
  publisher = {CRAN},
  url       = {https://cran.r-project.org/package=mlr3verse},
  date      = {2019},
  title     = {{mlr3verse: Easily Install and Load the 'mlr3' package family}},
}

@article{Gonen2005,
  author       = {Gönen, Mithat and Heller, Glenn},
  date         = {2005},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/G{\"{o}}nen, Heller - 2005 - Concordance Probability and Discriminatory Power in Proportional Hazards Regression.pdf:pdf},
  journaltitle = {Biometrika},
  keywords     = {c-index,concordance,discrimination,measure,survival},
  number       = {4},
  pages        = {965--970},
  title        = {{Concordance Probability and Discriminatory Power in Proportional Hazards Regression}},
  volume       = {92},
}

@article{Schemper2009,
  abstract     = {Abstract Often the effect of at least one of the prognostic factors in a Cox regression model changes over time, which violates the proportional hazards assumption of this model. As a consequence, the average hazard ratio for such a prognostic factor is under- or overestimated. While there are several methods to appropriately cope with non-proportional hazards, in particular by including parameters for time-dependent effects, weighted estimation in Cox regression is a parsimonious alternative without additional parameters. The methodology, which extends the weighted k-sample logrank tests of the Tarone-Ware scheme to models with multiple, binary and continuous covariates, has been introduced in the nineties of the last century and is further developed and re-evaluated in this contribution. The notion of an average hazard ratio is defined and its connection to the effect size measure P(X<Y) is emphasized. The suggested approach accomplishes estimation of intuitively interpretable average hazard ratios and provides tools for inference. A Monte Carlo study confirms the satisfactory performance. Advantages of the approach are exemplified by comparing standard and weighted analyses of an international lung cancer study. SAS and R programs facilitate application. Copyright ? 2009 John Wiley & Sons, Ltd.},
  author       = {Schemper, Michael and Wakounig, Samo and Heinze, Georg},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/sim.3623},
  annotation   = {doi: 10.1002/sim.3623},
  date         = {2009-08},
  doi          = {10.1002/sim.3623},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Schemper, Wakounig, Heinze - 2009 - The estimation of average hazard ratios by weighted Cox regression.pdf:pdf},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  keywords     = {Prentice test,converging hazards,effect size,proportional hazards model,survival analysis,weighted estimation},
  number       = {19},
  pages        = {2473--2489},
  title        = {{The estimation of average hazard ratios by weighted Cox regression}},
  volume       = {28},
}

@misc{HosmerJr2008,
  abstract   = {Summary This chapter contains sections titled: Introduction Residuals Assessing the Proportional Hazards Assumption Identification of Influential and Poorly Fit Subjects Assessing Overall Goodness-of-Fit Interpreting and Presenting Results From the Final Model},
  author     = {Hosmer, David W and Lemeshow, Stanley},
  url        = {https://doi.org/10.1002/9780470258019.ch6},
  annotation = {https://doi.org/10.1002/9780470258019.ch6},
  booktitle  = {Applied Survival Analysis},
  date       = {2008-02},
  doi        = {https://doi.org/10.1002/9780470258019.ch6},
  isbn       = {9780470258019},
  keywords   = {logistic regression,model adequacy,proportional hazards assumption,regression surface,residuals},
  pages      = {169--206},
  series     = {Wiley Series in Probability and Statistics},
  title      = {{Assessment of Model Adequacy}},
}

@article{Patel2006,
  abstract     = {The proportional hazards (PH) model is routinely employed for the analysis of time-to-event data in medical research when it is required to assess the effect of an intervention in the presence of covariates. The assumption of PH required for the PH approach may not hold, especially in circumstances where the effect of the intervention is to delay or accelerate the onset of an event rather than to reduce or increase the overall proportion of subjects who observe the event through time. If the assumption of PH is violated, the results from a PH model will be difficult to generalize to situations where the length of follow-up is different to that used in the analysis. It is also difficult to translate the results into the effect upon the expected median duration of illness for a patient in a clinical setting. The accelerated failure time (AFT) approach is an alternative strategy for the analysis of time-to-event data and can be suitable even when hazards are not proportional and this family of models contains a certain form of PH as a special case. The framework can allow for different forms of the hazard function and may provide a closer description of the data in certain circumstances. In addition, the results of the AFT model may be easier to interpret and more relevant to clinicians, as they can be directly translated into expected reduction or prolongation of the median time to event, unlike the hazard ratio. We recommend that consideration is given to an AFT modelling approach in the analysis of time-to-event data in medical research.},
  author       = {Patel, Katie and Kay, Richard and Rowell, Lucy},
  date         = {2006},
  doi          = {10.1002/pst.213},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Patel, Kay, Rowell - 2006 - Comparing proportional hazards and accelerated failure time models An application in influenza.pdf:pdf},
  isbn         = {1539-1604 (Print)1̊539-1604 (Linking)},
  issn         = {15391604},
  journaltitle = {Pharmaceutical Statistics},
  keywords     = {Accelerated failure time model,Influenza,Proportional hazards model,Time-to-event data},
  number       = {3},
  pages        = {213--224},
  title        = {{Comparing proportional hazards and accelerated failure time models: An application in influenza}},
  volume       = {5},
}

@article{Han2011,
  abstract     = {Uncertainty is a pervasive and important problem that has attracted increasing attention in health care, given the growing emphasis on evidence-based medicine, shared decision making, and patient-centered care. However, our understanding of this problem is limited, in part because of the absence of a unified, coherent concept of uncertainty. There are multiple meanings and varieties of uncertainty in health care that are not often distinguished or acknowledged although each may have unique effects or warrant different courses of action. The literature on uncertainty in health care is thus fragmented, and existing insights have been incompletely translated to clinical practice. This article addresses this problem by synthesizing diverse theoretical and empirical literature from the fields of communication, decision science, engineering, health services research, and psychology and developing a new integrative conceptual taxonomy of uncertainty. A 3-dimensional taxonomy is proposed that characterizes uncertainty in health care according to its fundamental sources, issues, and locus. It is shown how this new taxonomy facilitates an organized approach to the problem of uncertainty in health care by clarifying its nature and prognosis and suggesting appropriate strategies for its analysis and management.},
  author       = {Han, Paul K J and Klein, William M P and Arora, Neeraj K},
  language     = {eng},
  url          = {https://pubmed.ncbi.nlm.nih.gov/22067431 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3146626/},
  date         = {2011},
  doi          = {10.1177/0272989x11393976},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Han, Klein, Arora - 2011 - Varieties of uncertainty in health care a conceptual taxonomy.pdf:pdf},
  issn         = {1552-681X},
  journaltitle = {Medical decision making : an international journal of the Society for Medical Decision Making},
  keywords     = {*Delivery of Health Care,*Uncertainty,Decision Making,Evidence-Based Medicine},
  number       = {6},
  pages        = {828--838},
  title        = {{Varieties of uncertainty in health care: a conceptual taxonomy}},
  volume       = {31},
}

@misc{pkgxts,
  author = {Ryan, Jeffrey A. and Ulrich, Joshua M. and Bennett, Ross and Joy, Corwin},
  url    = {https://cran.r-project.org/package=xts},
  date   = {2018},
  title  = {{xts: eXtensible Time Series}},
}

@article{Brizzi2022,
  abstract     = {<p>The severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) Gamma variant of concern has spread rapidly across Brazil since late 2020, causing substantial infection and death waves. Here we used individual-level patient records after hospitalization with suspected or confirmed coronavirus disease 2019 (COVID-19) between 20 January 2020 and 26 July 2021 to document temporary, sweeping shocks in hospital fatality rates that followed the spread of Gamma across 14 state capitals, during which typically more than half of hospitalized patients aged 70 years and older died. We show that such extensive shocks in COVID-19 in-hospital fatality rates also existed before the detection of Gamma. Using a Bayesian fatality rate model, we found that the geographic and temporal fluctuations in Brazil's COVID-19 in-hospital fatality rates were primarily associated with geographic inequities and shortages in healthcare capacity. We estimate that approximately half of the COVID-19 deaths in hospitals in the 14 cities could have been avoided without pre-pandemic geographic inequities and without pandemic healthcare pressure. Our results suggest that investments in healthcare resources, healthcare optimization and pandemic preparedness are critical to minimize population-wide mortality and morbidity caused by highly transmissible and deadly pathogens such as SARS-CoV-2, especially in low- and middle-income countries.</p>},
  author       = {Brizzi, Andrea and Whittaker, Charles and Servo, Luciana M. S. and Hawryluk, Iwona and Prete, Carlos A. and de Souza, William M. and Aguiar, Renato S. and Araujo, Leonardo J. T. and Bastos, Leonardo S. and Blenkinsop, Alexandra and Buss, Lewis F. and Candido, Darlan and Castro, Marcia C. and Costa, Silvia F. and Croda, Julio and {de Souza Santos}, Andreza Aruska and Dye, Christopher and Flaxman, Seth and Fonseca, Paula L. C. and Geddes, Victor E. V. and Gutierrez, Bernardo and Lemey, Philippe and Levin, Anna S. and Mellan, Thomas and Bonfim, Diego M. and Miscouridou, Xenia and Mishra, Swapnil and Monod, Mélodie and Moreira, Filipe R. R. and Nelson, Bruce and Pereira, Rafael H. M. and Ranzani, Otavio and Schnekenberg, Ricardo P. and Semenova, Elizaveta and Sonabend, Raphael and Souza, Renan P. and Xi, Xiaoyue and Sabino, Ester C. and Faria, Nuno R. and Bhatt, Samir and Ratmann, Oliver},
  date         = {2022-05},
  doi          = {10.1038/s41591-022-01807-1},
  issn         = {1078-8956},
  journaltitle = {Nature Medicine},
  title        = {{Spatial and temporal fluctuations in COVID-19 fatality rates in Brazilian hospitals}},
}

@misc{Ohno-Machado1996,
  author    = {Ohno-Machado, Lucila},
  publisher = {Stanford University Stanford, Calif},
  date      = {1996},
  title     = {{Medical applications of artificial neural networks: connectionist models of survival}},
}

@report{Sonabend2021c,
  abstract    = {In this report, we summarise the findings of our evaluation of the easing of non-pharmaceutical interventions (NPIs) as set out in the UK Government's Roadmap out of Lockdown focusing on the potential impact of B.1.617.2. Full methods, data used, and parameter values assumed for forward projections are given in the technical appendix. Key parameters relevant to interpretation of findings are provided in the text. Results and assumptions refer to England unless otherwise specified. 1. We estimate the current level of transmission, R eff , is approximately 0.8 for B.1.1.7 and 1.5 for B.1.617.2 in England, with an overall R eff of $\sim$1.4 across both variants. This may not capture all changes in transmissibility since the 17 May reopening as R is a lagging indicator by 3 weeks. 2. Based on Public Health England (PHE) data available to 1 June 2021, 74% of the adult population in England have received at least one vaccine dose and 50% have received two doses. 3. Across all transmissibility and immune escape scenarios explored, we estimate that B.1.617.2 could lead to a significant third wave of hospitalisations and deaths similar to or larger than the winter wave. 4. Cases, hospitalisations, and deaths in the next month could grow rapidly. Large uncertainty remains regarding the scale of the future epidemic and resulting additional burden. 5. Delaying step 4 releases beyond 21 June should delay the projected third wave and reduce the estimated number of hospitalisation and deaths. This will also allow more time for alternative control strategies such as boosters doses and vaccination of <18 years to be considered and implemented. 6. In the range of parameters we examined, immune escape properties of B.1.617.2 affected the magnitude of the third wave more than assumptions about transmissibility. However, there is considerable uncertainty regarding the levels of transmissibility and immune escape of B.1.617.2 which translate into large uncertainty on the possible future epidemic trajectory. 7. Global collaborative efforts to control transmission abroad will be vital in preventing further emergence and importation of new VOCs which may trigger another wave and necessitate further reconsideration or reversal of the current roadmap. VOC importations over time should also be monitored carefully. Careful testing and quarantine measures will be critical as international travel restrictions are lifted. 8. Given the many uncertainties involved in making these projections and in light of the increasing B.1.617.2 cases in England, more time may be needed to fully assess the impact of Step 3 and better characterise the transmissibility, severity, and immune escape properties of B.1.617.2 before committing to Step 4 which will pose the greatest risk to increased transmission.},
  author      = {Sonabend, Raphael and Whittles, Lilith K and Imai, Natsuko and Knock, Edward S and Perez-Guzman, Pablo N and Rawson, Thomas and Mangal, Tara and Volz, Erik M and Ferguson, Neil M and Baguelin, Marc and Cori, Anne},
  institution = {Imperial College COVID-19 Response Team},
  url         = {https://www.gov.uk/government/publications/imperial-college-london-evaluating-the-roadmap-out-of-lockdown-modelling-step-4-of-the-roadmap-in-the-context-of-b16172-delta-9-june-2021},
  date        = {2021},
  title       = {{Evaluating the Roadmap out of Lockdown: modelling step 4 of the roadmap in the context of B.1.617.2}},
  type        = {techreport},
}

@article{Cox1972,
  author       = {Cox, D. R.},
  annotation   = {The seminal paper that introduced the world to the Cox Proportional Hazards Regression Model. Nuissance parameter: Any parameter not of direct interest but must be accounted for. e.g. when parameter of interest in a Normal distribution is its mean then the variance is a nuissance parameter The paper covers the introduction to the standard PH model as well as an alternative (and now less used I think) parameterisation of it. Brief discussions of time-covariate models are discussed as well as computations for when time is measured as a discrete variable (this will be important for our reduction). Key-points: Cox PH Model Time-varying covariates Conditional likelihood Discrete time},
  date         = {1972},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Cox - 1972 - Regression Models and Life-Tables.pdf:pdf},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords     = {accelerated life tests,age-specific failure rate,asymptotic theory,censored data,classical,conditional inference,cox,hazard function,life table,limit estimate,medical applications,ph,product,regression,reliability,survival,theory,two-sample rank tests},
  number       = {2},
  pages        = {187--220},
  title        = {{Regression Models and Life-Tables}},
  volume       = {34},
}

@article{Aalen1978,
  author       = {Aalen, Odd},
  annotation   = {Another half of the Nelson-Aalen estimator},
  date         = {1978},
  journaltitle = {The Annals of Statistics},
  number       = {4},
  pages        = {701--726},
  title        = {{Nonparametric Inference for a Family of Counting Processes}},
  volume       = {6},
}

@article{datawbc,
  author       = {{The Benelux C M L Study Group}},
  url          = {https://doi.org/10.1182/blood.V91.8.2713.2713_2713_2721 https://ashpublications.org/blood/article/91/8/2713/107615/Randomized-Study-on-Hydroxyurea-Alone-Versus},
  date         = {1998-04},
  doi          = {10.1182/blood.V91.8.2713.2713_2713_2721},
  issn         = {1528-0020},
  journaltitle = {Blood},
  number       = {8},
  pages        = {2713--2721},
  title        = {{Randomized Study on Hydroxyurea Alone Versus Hydroxyurea Combined With Low-Dose Interferon-$\alpha$2b for Chronic Myeloid Leukemia}},
  volume       = {91},
}

@article{Zare2015,
  abstract     = {Background: Gastric cancer is the one of the most prevalent reason of\ncancer-related death in the world. Survival of patients after surgery\ninvolves identifying risk factors. There are various models to detect\nthe effect of risk factors on patients' survival. The present study aims\nat evaluating these models.\nMethods: Data from 330 gastric cancer patients diagnosed at the Iran\ncancer institute during 1995-99 and followed up the end of 2011 were\nanalyzed. The survival status of these patients in 2011 was determined\nby reopening the files as well as phone calls and the effect of various\nfactors such as demographic, clinical, treatment, and post-surgical on\npatients' survival was studied. To compare various models of survival,\nAkaike Information Criterion and Cox-Snell Residuals were used. STATA 11\nwas used for data analyses.\nResults: Based on Cox-Snell Residuals and Akaike Information Criterion,\nthe exponential (AIC=969.14) and Gompertz (AIC=970.70) models were more\nefficient than other accelerated failure-time models. Results of Cox\nproportional hazard model as well as the analysis of accelerated\nfailure-time models showed that variables such as age (at diagnosis),\nmarital status, relapse, number of supplementary treatments, disease\nstage, and type of surgery were among factors affecting survival\n(P<0.05).\nConclusion: Although most cancer researchers tend to use proportional\nhazard model, accelerated failure-time models in analogous conditions -\nas they do not require proportional hazards assumption and consider a\nparametric statistical distribution for survival time-will be credible\nalternatives to proportional hazard model.},
  author       = {Zare, Ali and Hosseini, Mostafa and Mahmoodi, Mahmood and Mohammad, Kazem and Zeraati, Hojjat and {Holakouie Naieni}, Kourosh},
  url          = {http://www.ncbi.nlm.nih.gov/pubmed/26587473%0Ahttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4645729},
  date         = {2015},
  doi          = {10.1007/s00606-006-0435-8},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Zare et al. - 2015 - A Comparison between Accelerated Failure-time and Cox Proportional Hazard Models in Analyzing the Survival of Gastr.pdf:pdf},
  issn         = {03044556},
  journaltitle = {Iranian journal of public health},
  keywords     = {Accelerated failure-time models,Akaike information criterion,Cox proportional hazard model,Cox-Snell residuals,Gastric cancer},
  number       = {8},
  pages        = {1095--102},
  title        = {{A Comparison between Accelerated Failure-time and Cox Proportional Hazard Models in Analyzing the Survival of Gastric Cancer Patients.}},
  volume       = {44},
}

@article{Seker2002,
  author       = {Seker, Huseyin and Odetayo, Michael O and Petrovic, Dobrila and Naguib, Raouf N G and Bartoli, C and Alasio, L and Lakshmi, M S and Sherbet, G V},
  language     = {eng},
  location     = {BIOCORE, School of Mathematical and Information Sciences, Coventry University, UK.},
  url          = {http://europepmc.org/abstract/MED/12017328},
  annotation   = {Classification},
  date         = {2002},
  issn         = {0250-7005},
  journaltitle = {Anticancer research},
  keywords     = {Fuzzy Logic,Neural Networks (Computer),Survival Analysis},
  number       = {1A},
  pages        = {433--438},
  title        = {{Assessment of nodal involvement and survival analysis in breast cancer patients using image cytometric data: statistical, neural network and fuzzy approaches}},
  volume       = {22},
}

@article{Gandon2021,
  author       = {Gandon, Enora and Nonaka, Tetsushi and Coyle, Thelma and Coyle, Erin and Sonabend, Raphael and Ogbonnaya, Chibueze and Endler, John and Roux, Valentine},
  url          = {https://www.sciencedirect.com/science/article/pii/S0278416521000672},
  date         = {2021},
  doi          = {10.1016/j.jaa.2021.101334},
  journaltitle = {Journal of Anthropological Archaeology},
  keywords     = {Artifact variation,Craft skill,Cultural transmission,Shape analysis,Shape perception,Wheel-throwing pottery},
  title        = {{Cultural transmission and perception of vessel shapes among Hebron potters}},
  volume       = {63},
}

@misc{pkgbujar,
  author     = {Wang, Zhu},
  publisher  = {CRAN},
  url        = {https://cran.r-project.org/package=bujar},
  annotation = {R package version 0.2-7},
  date       = {2019},
  title      = {{bujar: Buckley-James Regression for Survival Data with High-Dimensional Covariates}},
}

@article{Bellazzi2008,
  abstract     = {Background: The widespread availability of new computational methods and tools for data analysis and predictive modeling requires medical informatics researchers and practitioners to systematically select the most appropriate strategy to cope with clinical prediction problems. In particular, the collection of methods known as 'data mining' offers methodological and technical solutions to deal with the analysis of medical data and construction of prediction models. A large variety of these methods requires general and simple guidelines that may help practitioners in the appropriate selection of data mining tools, construction and validation of predictive models, along with the dissemination of predictive models within clinical environments. Purpose: The goal of this review is to discuss the extent and role of the research area of predictive data mining and to propose a framework to cope with the problems of constructing, assessing and exploiting data mining models in clinical medicine. Methods: We review the recent relevant work published in the area of predictive data mining in clinical medicine, highlighting critical issues and summarizing the approaches in a set of learned lessons. Results: The paper provides a comprehensive review of the state of the art of predictive data mining in clinical medicine and gives guidelines to carry out data mining studies in this field. Conclusions: Predictive data mining is becoming an essential instrument for researchers and clinical practitioners in medicine. Understanding the main issues underlying these methods and the application of agreed and standardized procedures is mandatory for their deployment and the dissemination of results. Thanks to the integration of molecular and clinical data taking place within genomic medicine, the area has recently not only gained a fresh impulse but also a new set of complex problems it needs to address. {©} 2006 Elsevier Ireland Ltd. All rights reserved.},
  author       = {Bellazzi, Riccardo and Zupan, Blaz},
  annotation   = {Naive bayes for survival},
  date         = {2008},
  doi          = {10.1016/j.ijmedinf.2006.11.006},
  eprint       = {arXiv:1011.1669v3},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Bellazzi, Zupan - 2008 - Predictive data mining in clinical medicine Current issues and guidelines.pdf:pdf},
  isbn         = {1386-5056},
  issn         = {13865056},
  journaltitle = {International Journal of Medical Informatics},
  keywords     = {Clinical medicine,Data analysis,Data mining,Data mining process,Predictive models},
  number       = {2},
  pages        = {81--97},
  title        = {{Predictive data mining in clinical medicine: Current issues and guidelines}},
  volume       = {77},
}

@book{datacgd,
  author    = {Fleming, Thomas R and Harrington, David P},
  publisher = {John Wiley & Sons},
  date      = {2011},
  isbn      = {111815066X},
  title     = {{Counting processes and survival analysis}},
  volume    = {169},
}

@misc{pkgobliquersf,
  author     = {Jaeger, Byron},
  publisher  = {CRAN},
  url        = {https://cran.r-project.org/package=obliqueRSF},
  annotation = {R package version 0.1.1},
  date       = {2019},
  title      = {{obliqueRSF: Oblique Random Forests for Right-Censored Time-to-Event Data}},
}

@article{Witteman2011,
  author       = {Witteman, Holly O and Zikmund-Fisher, Brian J and Waters, Erika A and Gavaruzzi, Teresa and Fagerlin, Angela},
  publisher    = {JMIR Publications Inc., Toronto, Canada},
  date         = {2011},
  journaltitle = {Journal of medical Internet research},
  number       = {3},
  pages        = {e54},
  title        = {{Risk estimates from an online risk calculator are more believable and recalled better when expressed as integers}},
  volume       = {13},
}

@incollection{pkgmlr3booksurv,
  author    = {Becker, Marc and Binder, Martin and Bischl, Bernd and Lang, Michel and Pfisterer, Florian and Reich, Nicholas G. and Richter, Jakob and Schratz, Patrick and Sonabend, Raphael},
  url       = {https://mlr3book.mlr-org.com/survival.html},
  booktitle = {mlr3book},
  chapter   = {7.1},
  date      = {2021},
  title     = {{Survival Analysis}},
}

@article{pkgjmbayes,
  author       = {Rizopoulos, Dimitris},
  date         = {2016},
  doi          = {10.18637/jss.v072.i07},
  journaltitle = {Journal of Statistical Software},
  number       = {7},
  pages        = {1--45},
  title        = {{The R Package JMbayes for Fitting Joint Models for Longitudinal and Time-to-Event Data Using MCMC}},
  volume       = {72},
}

@misc{pkgR,
  author    = {{R Core Team}},
  location  = {Vienna},
  publisher = {R Foundation for Statistical Computing},
  date      = {2017},
  title     = {{R: A Language and Environment for Statistical Computing}},
}

@report{Whittles2021b,
  author      = {Whittles, Lilith K and Imai, Natsuko and Sonabend, Raphael and Knock, Edward S and Perez-Guzman, Pablo N and Mangal, Tara and Hogan, Alexandra B and Ghani, Azra and Ferguson, Neil M and Baguelin, Marc and Cori, Anne},
  institution = {Imperial College COVID-19 Response Team},
  url         = {https://www.gov.uk/government/publications/imperial-college-london-evaluating-englands-roadmap-out-of-lockdown-30-march-2021},
  date        = {2021},
  title       = {{Evaluating England's Roadmap out of Lockdown}},
  type        = {techreport},
}

@article{Nelson1972,
  author       = {Nelson, Wayne},
  annotation   = {One half of the Nelson-Aalen Estimator},
  date         = {1972},
  journaltitle = {Technometrics},
  number       = {4},
  pages        = {945--966},
  title        = {{Theory and Applications of Hazard Plotting for Censored Failure Data}},
  volume       = {14},
}

@misc{pkgipred,
  author    = {Peters, Andrea and Hothorn, Torsten},
  publisher = {CRAN},
  url       = {https://cran.r-project.org/package=ipred},
  date      = {2019},
  title     = {{ipred: Improved Predictors}},
}

@article{Katzman2016,
  author = {Katzman, Jared and Shaham, Uri and Cloninger, Alexander and Bates, Jonathan and Jiang, Tingting and Kluger, Yuval},
  date   = {2016-06},
  title  = {{Deep Survival: A Deep Cox Proportional Hazards Network}},
}

@article{Hong2020,
  author       = {Hong, Soo Jung},
  publisher    = {Taylor & Francis},
  url          = {https://doi.org/10.1080/10810730.2020.1745963},
  annotation   = {doi: 10.1080/10810730.2020.1745963},
  date         = {2020-03},
  doi          = {10.1080/10810730.2020.1745963},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Hong - 2020 - Uncertainty in the Process of Communicating Cancer-related Genetic Risk Information with Patients A Scoping Review.pdf:pdf},
  issn         = {1081-0730},
  journaltitle = {Journal of Health Communication},
  number       = {3},
  pages        = {251--270},
  title        = {{Uncertainty in the Process of Communicating Cancer-related Genetic Risk Information with Patients: A Scoping Review}},
  volume       = {25},
}

@article{Lin2007,
  abstract     = {In his discussion of Cox's (1972) paper on proportional hazards regression, Breslow (1972) provided the maximum likelihood estimator for the cumulative baseline hazard function. This estimator is commonly used in practice. The estimator has also been highly valuable in the further development of Cox regression and semiparametric inference with censored data. The present paper describes the Breslow estimator and its tremendous impact on the theory and practice of survival analysis.},
  author       = {Lin, D. Y.},
  date         = {2007},
  doi          = {10.1007/s10985-007-9048-y},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Lin - 2007 - On the Breslow estimator.pdf:pdf},
  issn         = {13807870},
  journaltitle = {Lifetime Data Analysis},
  keywords     = {Cox model,Maximum likelihood,Partial likelihood,Proportional hazards,Semiparametric inference,Survival data},
  number       = {4},
  pages        = {471--480},
  title        = {{On the Breslow estimator}},
  volume       = {13},
}

@inproceedings{Street1998,
  author    = {Street, W Nick},
  location  = {San Francisco},
  booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning},
  date      = {1998},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Street - 1998 - A Neural Network Model for Prognostic Prediction.pdf:pdf},
  title     = {{A Neural Network Model for Prognostic Prediction.}},
}

@article{Stone1974,
  abstract     = {The method of cross-validatory choice and assessment is applied to prediction of a multinomial indicator. The resulting predictor is compared with analogous expressions due to Good and Fienberg & Holland. Some numerical comparisons are made.},
  author       = {Stone, M.},
  date         = {1974},
  doi          = {10.1093/biomet/61.3.509},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Stone - 1974 - Cross-validation and multinomial prediction.pdf:pdf},
  isbn         = {0006-3444},
  issn         = {00063444},
  journaltitle = {Biometrika},
  keywords     = {Cross-validation,Estimation,Flattening,Multinomial,Prediction,Smoothing,Squashing},
  number       = {3},
  pages        = {509--515},
  title        = {{Cross-validation and multinomial prediction}},
  volume       = {61},
}

@article{Akaike1974,
  author       = {Akaike, Hirotugu},
  annotation   = {AIC},
  date         = {1974},
  doi          = {10.1093/ietfec/e90-a.12.2762},
  eprint       = {arXiv:1011.1669v3},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Akaike - 1974 - A New Look at the Statistical Model Identification.pdf:pdf},
  isbn         = {1-59593-058-2},
  issn         = {17451337},
  journaltitle = {IEEE Transactions on Automatic Control},
  keywords     = {Binary decision diagram,Characteristic function,Functional decomposition,Incompletely specified function,LUT cascade},
  number       = {6},
  pages        = {716--723},
  title        = {{A New Look at the Statistical Model Identification}},
  volume       = {19},
}

@article{Reid2011a,
  abstract     = {We unify f-divergences, Bregman divergences, surrogate loss bounds (regret bounds), proper scoring rules, matching losses, cost curves, ROC-curves and information. We do this by systematically studying integral and variational representations of these objects and in so doing identify their primitives which all are related to cost-sensitive binary classification. As well as clarifying relationships between generative and discriminative views of learning, the new machinery leads to tight and more general surrogate loss bounds and generalised Pinsker inequalities relating f-divergences to variational divergence. The new viewpoint illuminates existing algorithms: it provides a new derivation of Support Vector Machines in terms of divergences and relates Maximum Mean Discrepancy to Fisher Linear Discriminants. It also suggests new techniques for estimating f-divergences.},
  author       = {Reid, Mark D. and Williamson, Robert C.},
  url          = {http://arxiv.org/abs/0901.0356},
  annotation   = {Chapter 6 in particular. Fig.1 p.734},
  date         = {2009},
  eprint       = {0901.0356},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Reid, Williamson - 2009 - Information, Divergence and Risk for Binary Experiments.pdf:pdf},
  isbn         = {1532-4435},
  issn         = {1532-4435},
  journaltitle = {Journal of Machine Learning Research},
  keywords     = {classification,divergence,loss functions,regret bounds,statistical information},
  pages        = {731--817},
  title        = {{Information, Divergence and Risk for Binary Experiments}},
  volume       = {12},
}

@article{Habibi2018,
  abstract     = {Objective: There are a number of models for determining risk factors for survival of patients with gastric cancer. This study was conducted to select the model showing the best fit with available data. Methods: Cox regression and parametric models (Exponential, Weibull, Gompertz, Log normal, Log logistic and Generalized Gamma) were utilized in unadjusted and adjusted forms to detect factors influencing mortality of patients. Comparisons were made with Akaike Information Criterion (AIC) by using STATA 13 and R 3.1.3 softwares. Results: The results of this study indicated that all parametric models outperform the Cox regression model. The Log normal, Log logistic and Generalized Gamma provided the best performance in terms of AIC values (179.2, 179.4 and 181.1, respectively). On unadjusted analysis, the results of the Cox regression and parametric models indicated stage, grade, largest diameter of metastatic nest, largest diameter of LM, number of involved lymph nodes and the largest ratio of metastatic nests to lymph nodes, to be variables influencing the survival of patients with gastric cancer. On adjusted analysis, according to the best model (log normal), grade was found as the significant variable. Conclusion: The results suggested that all parametric models outperform the Cox model. The log normal model provides the best fit and is a good substitute for Cox regression.},
  author       = {Habibi, Danial and Rafiei, Mohammad and Chehrei, Ali and Shayan, Zahra and Tafaqodi, Soheil},
  url          = {http://www.ncbi.nlm.nih.gov/pubmed/29582630%0Ahttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5980851},
  date         = {2018},
  doi          = {10.22034/APJCP.2018.19.3.749},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Habibi et al. - 2018 - Comparison of Survival Models for Analyzing Prognostic Factors in Gastric Cancer Patients.pdf:pdf},
  issn         = {2476-762X},
  journaltitle = {Asian Pacific journal of cancer prevention : APJCP},
  keywords     = {AIC,Cox regression,Gastric cancer,Parametric models},
  number       = {3},
  pages        = {749--753},
  title        = {{Comparison of Survival Models for Analyzing Prognostic Factors in Gastric Cancer Patients}},
  volume       = {19},
}

@article{Demler2015,
  abstract     = {To access the calibration of a predictive model in a survival analysis setting, several authors have extended the Hosmer-Lemeshow goodness-of-fit test to survival data. Gr{ø}nnesby and Borgan developed a test under the proportional hazards assumption, and Nam and D'Agostino developed a nonparametric test that is applicable in a more general survival setting for data with limited censoring. We analyze the performance of the two tests and show that the Gr{ø}nnesby-Borgan test attains appropriate size in a variety of settings, whereas the Nam-D'Agostino method has a higher than nominal Type 1 error when there is more than trivial censoring. Both tests are sensitive to small cell sizes. We develop a modification of the Nam-D'Agostino test to allow for higher censoring rates. We show that this modified Nam-D'Agostino test has appropriate control of Type 1 error and comparable power to the Gr{ø}nnesby-Borgan test and is applicable to settings other than proportional hazards. We also discuss the application to small cell sizes.},
  author       = {Demler, Olga V and Paynter, Nina P and Cook, Nancy R},
  language     = {eng},
  url          = {https://pubmed.ncbi.nlm.nih.gov/25684707 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4555993/},
  date         = {2015-05},
  doi          = {10.1002/sim.6428},
  edition      = {2015/02/11},
  issn         = {1097-0258},
  journaltitle = {Statistics in medicine},
  keywords     = {*Models,Theoretical,*Survival Analysis,Adult,Aged,Bias,Calibration,Computer Simulation,Coronary Disease/epidemiology/etiology,Female,Humans,Longitudinal Studies,Middle Aged,Proportional Hazards Models,Risk Assessment/methods,Sample Size,Statistics,Nonparametric,calibration,goodness-of-fit,survival analysis},
  number       = {10},
  pages        = {1659--1680},
  title        = {{Tests of calibration and goodness-of-fit in the survival setting}},
  volume       = {34},
}

@article{Hill1965,
  author       = {{Bradford Hill}, Austin},
  language     = {eng},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/14283879 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1898525/},
  date         = {1965-05},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Bradford Hill - 1965 - THE ENVIRONMENT AND DISEASE ASSOCIATION OR CAUSATION.pdf:pdf},
  issn         = {0035-9157},
  journaltitle = {Proceedings of the Royal Society of Medicine},
  keywords     = {*ENVIRONMENTAL HEALTH,*Environment,*Environmental Health,*INDUSTRIAL MEDICINE,*Occupational Medicine,Humans},
  number       = {5},
  pages        = {295--300},
  title        = {{THE ENVIRONMENT AND DISEASE: ASSOCIATION OR CAUSATION?}},
  volume       = {58},
}

@article{pkgpenalized,
  author       = {Goeman, J. J.},
  date         = {2010},
  journaltitle = {Biometrical Journal},
  number       = {1},
  pages        = {---14},
  title        = {{L1 penalized estimation in the Cox proportional hazards model}},
  volume       = {52},
}

@misc{pkgportion,
  author    = {Decan, Alexandre},
  publisher = {PyPi},
  url       = {https://pypi.org/project/portion/},
  date      = {2020},
  title     = {portion - data structure and operations for intervals},
}

@article{pkgsasflex,
  author = {Dewar, Ron and Khan, Iftekhar},
  title  = {{A new SAS macro for flexible parametric survival modeling: applications to clinical trials and surveillance data}},
}

@article{Dawid1984,
  abstract     = {[The prequential approach is founded on the premises that the purpose of statistical inference is to make sequential probability forecasts for future observations, rather than to express information about parameters. Many traditional parametric concepts, such as consistency and efficiency, prove to have natural counterparts in this formulation, which sheds new light on these and suggests fruitful extensions.]},
  author       = {Dawid, A P},
  publisher    = {[Royal Statistical Society, Wiley]},
  url          = {http://www.jstor.org/stable/2981683},
  date         = {1984-06},
  doi          = {10.2307/2981683},
  issn         = {00359238},
  journaltitle = {Journal of the Royal Statistical Society. Series A (General)},
  number       = {2},
  pages        = {278--292},
  title        = {{Present Position and Potential Developments: Some Personal Views: Statistical Theory: The Prequential Approach}},
  volume       = {147},
}

@book{Han2021,
  author    = {Han, Paul K J},
  publisher = {Oxford University Press},
  date      = {2021},
  isbn      = {0190270586},
  title     = {{Uncertainty in medicine: a framework for tolerance}},
}

@article{Gardner2011,
  author       = {Gardner, Peter H and McMillan, Brian and Raynor, David K and Woolf, Elizabeth and Knapp, Peter},
  publisher    = {Elsevier},
  date         = {2011},
  issn         = {0738-3991},
  journaltitle = {Patient education and counseling},
  number       = {3},
  pages        = {398--403},
  title        = {{The effect of numeracy on the comprehension of information about medicines in users of a patient information website}},
  volume       = {83},
}

@misc{pkgmicrobm,
  author    = {Mersmann, Olaf},
  publisher = {CRAN},
  url       = {https://cran.r-project.org/package=microbenchmark},
  date      = {2019},
  title     = {{microbenchmark: Accurate Timing Functions}},
}

@article{Bennett1983,
  abstract     = {Abstract A model is presented for the analysis of lifetime data in which the rates of mortality for separate groups of patients converge with time. A non-parametric estimate is given for the survivor function. The theoretical basis for the model assumes that prognostic factors have a multiplicative effect on the odds against survival beyond any given time. The model is fitted to data using maximum likelihood estimation, and an example of its use in the analysis of a lung cancer trial is given.},
  author       = {Bennett, Steve},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/sim.4780020223},
  annotation   = {https://doi.org/10.1002/sim.4780020223},
  date         = {1983-04},
  doi          = {https://doi.org/10.1002/sim.4780020223},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Bennett - 1983 - Analysis of survival data by the proportional odds model.pdf:pdf},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  keywords     = {Censored observations,Convergent hazards,Lung cancer,Odds ratio,Proportional odds,Survival data},
  number       = {2},
  pages        = {273--277},
  title        = {{Analysis of survival data by the proportional odds model}},
  volume       = {2},
}

@article{Spiegelhalter2017,
  abstract     = {This review briefly examines the vast range of techniques used to communicate risk assessments arising from statistical analysis. After discussing essential psychological and sociological issues, I focus on individual health risks and relevant research on communicating numbers, verbal expressions, graphics, and conveying deeper uncertainty. I then consider practice in a selection of diverse case studies, including gambling, the benefits and risks of pharmaceuticals, weather forecasting, natural hazards, climate change, environmental exposures, security and intelligence, industrial reliability, and catastrophic national and global risks. There are some tentative final conclusions, but the primary message is to acknowledge expert guidance, be clear about objectives, and work closely with intended audiences.},
  author       = {Spiegelhalter, David},
  date         = {2017},
  doi          = {10.1146/annurev-statistics-010814-020148},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Spiegelhalter - 2017 - Risk and Uncertainty Communication.pdf:pdf},
  isbn         = {978-0-8243-3604-2},
  issn         = {2326-8298},
  journaltitle = {Ssrn},
  title        = {{Risk and Uncertainty Communication}},
}

@article{Srivastava2014,
  author       = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  publisher    = {JMLR. org},
  date         = {2014},
  issn         = {1532-4435},
  journaltitle = {The journal of machine learning research},
  number       = {1},
  pages        = {1929--1958},
  title        = {{Dropout: a simple way to prevent neural networks from overfitting}},
  volume       = {15},
}

@article{Therneau1990,
  abstract     = {Graphical methods based on the analysis of residuals are considered for the setting of the highly-used Cox (1972) regression model and for the Andersen-Gill (1982) generalization of that model. We start with a class of martingale-based residuals as proposed by Barlow & Prentice (1988). These residuals and/or their transforms are useful for investigating the functional form of a covariate, the proportional hazards assumption, the leverage of each subject upon the estimates of beta, and the lack of model fit to a given subject.},
  author       = {Therneau, Terry M. and Grambsch, Patricia M. and Fleming, Thomas R.},
  date         = {1990},
  doi          = {10.1093/biomet/77.1.147},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Therneau, Grambsch, Fleming - 1990 - Martingale-based residuals for survival models.pdf:pdf},
  isbn         = {DR000881 00063444 DI992407 99P07212},
  issn         = {00063444},
  journaltitle = {Biometrika},
  keywords     = {Cox regression,Deviance,Influence function,Martingale,Outlier detection,Proportional hazards,Regression diagnostic,Residual},
  number       = {1},
  pages        = {147--160},
  title        = {{Martingale-based residuals for survival models}},
  volume       = {77},
}

@article{Choodari2012a,
  abstract     = {Measures of predictive ability play an important role in quantifying the clinical significance of prognostic factors. Several measures have been proposed to evaluate the predictive ability of survival models in the last two decades, but no single measure is consistently used. The proposed measures can be classified into the following categories: explained variation, explained randomness, and predictive accuracy. The three categories are conceptually different and are based on different principles. Several new measures have been proposed since Schemper and Stare's study in 1996 on some of the existing measures. This paper is the first of two papers that study the proposed measures systematically by applying a set of criteria that a measure of predictive ability should possess in the context of survival analysis. The present paper focuses on the explained variation category, and part II studies the proposed measures in the other categories. Simulation studies are used to examine the performance of five explained variation measures with respect to these criteria, discussing their strengths and shortcomings. Our simulation studies show that the measures proposed by Kent and O'Quigley, R, and Royston and Sauerbrei, R, appear to be the best overall at quantifying predictive ability. However, it should be noted that neither measure is perfect; R is sensitive to outliers and R to (marked) non-normality of the distribution of the prognostic index. The results show that the other measures perform poorly, primarily because they are adversely affected by censoring. Copyright {©} 2011 John Wiley & Sons, Ltd.},
  author       = {Choodari-Oskooei, Babak and Royston, Patrick and Parmar, Mahesh K.B.},
  annotation   = {A useful paper for guidelines on how to simulate survival data with multiple types of censoring. Practical paper using simulations to test explained variation measures. The key result is that no measure is perfect but the best so far are Kent and O'Quigley's, R_PM, and Royston and Sauerbrei's, R_D.},
  date         = {2012},
  doi          = {10.1002/sim.4242},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Choodari-Oskooei, Royston, Parmar - 2012 - A simulation study of predictive ability measures in a survival model I Explained variation m.pdf:pdf},
  isbn         = {1097-0258 (Electronic)0̊277-6715 (Linking)},
  issn         = {02776715},
  journaltitle = {Statistics in Medicine},
  keywords     = {Breast cancer,Cox proportional hazards model,Explained variation,Predictive ability,Survival analysis},
  number       = {23},
  pages        = {2627--2643},
  title        = {{A simulation study of predictive ability measures in a survival model I: Explained variation measures}},
  volume       = {31},
}

@article{Faraggi1995,
  abstract     = {Neural networks have received considerable attention recently, mostly by non-statisticians. They are considered by many to be very promising tools for classification and prediction. In this paper we present an approach to modelling censored survival data using the input-output relationship associated with a simple feed-forward neural network as the basis for a non-linear proportional hazards model. This approach can be extended to other models used with censored survival data. The proportional hazards neural network parameters are estimated using the method of maximum likelihood. These maximum likelihood based models can be compared, using readily available techniques such as the likelihood ratio test and the Akaike criterion. The neural network models are illustrated using data on the survival of men with prostatic carcinoma. A method of interpreting the neural network predictions based on the factorial contrasts is presented.},
  author       = {Faraggi, David and Simon, Richard},
  annotation   = {Extended Cox PH to nnet},
  date         = {1995},
  doi          = {10.1002/sim.4780140108},
  eprint       = {arXiv:1011.1669v3},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Faraggi, Simon - 1995 - A neural network model for survival data.pdf:pdf},
  isbn         = {9788578110796},
  issn         = {10970258},
  journaltitle = {Statistics in Medicine},
  number       = {1},
  pages        = {73--82},
  title        = {{A neural network model for survival data}},
  volume       = {14},
}

@article{Harrell1984,
  abstract     = {Regression models such as the Cox proportional hazards model have had increasing use in modelling and estimating the prognosis of patients with a variety of diseases. Many applications involve a large number of variables to be modelled using a relatively small patient sample. Problems of overfitting and of identifying important covariates are exacerbated in analysing prognosis because the accuracy of a model is more a function of the number of events than of the sample size. We used a general index of predictive discrimination to measure the ability of a model developed on training samples of varying sizes to predict survival in an independent test sample of patients suspected of having coronary artery disease. We compared three methods of model fitting: (1) standard 'step-up' variable selection, (2) incomplete principal components regression, and (3) Cox model regression after developing clinical indices from variable clusters. We found regression using principal components to offer superior predictions in the test sample, whereas regression using indices offers easily interpretable models nearly as good as the principal components models. Standard variable selection has a number of deficiencies.},
  author       = {Harrell, F E Jr and Lee, K L and Califf, R M and Pryor, D B and Rosati, R A},
  language     = {eng},
  date         = {1984},
  doi          = {10.1002/sim.4780030207},
  issn         = {0277-6715 (Print)},
  journaltitle = {Statistics in medicine},
  keywords     = {Coronary Disease,Female,Humans,Male,Models,Cardiovascular,Probability,Prognosis,Regression Analysis,Risk,mortality},
  number       = {2},
  pages        = {143--152},
  title        = {{Regression modelling strategies for improved prognostic prediction.}},
  volume       = {3},
}

@article{Breslow1972,
  author       = {Breslow, N.},
  date         = {1972},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  number       = {2},
  pages        = {187--220},
  title        = {{Discussion following ``Regression models and life tables'' by D. R. Cox}},
  volume       = {34},
}

@article{Dirick2017,
  abstract     = {We investigate the performance of various survival analysis techniques applied to ten actual credit data sets from Belgian and UK financial institutions. In the comparison we consider classical survival analysis techniques, namely the accelerated failure time models and Cox proportional hazards regression models, as well as Cox proportional hazard regression models with splines in the hazard function. Mixture cure models for single and multiple events were more recently introduced in the credit risk context. The performance of these models is evaluated using both a statistical evaluation and an economic approach through the use of annuity theory. It is found that spline-based methods and the single event mixture cure model perform well in the credit risk context.},
  author       = {Dirick, Lore and Claeskens, Gerda and Baesens, Bart},
  publisher    = {Palgrave Macmillan UK},
  date         = {2017},
  doi          = {10.1057/s41274-016-0128-9},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Dirick, Claeskens, Baesens - 2017 - Time to default in credit scoring using survival analysis A benchmark study.pdf:pdf},
  isbn         = {0471358339},
  issn         = {14769360},
  journaltitle = {Journal of the Operational Research Society},
  keywords     = {Benchmarking,competing risks,credit risk modeling,mixture cure model,survival analysis},
  number       = {6},
  pages        = {652--665},
  title        = {{Time to default in credit scoring using survival analysis: A benchmark study}},
  volume       = {68},
}

@article{dataall,
  abstract     = {This paper considers the problem of obtaining a dynamic prediction for 5-year failure free survival after bone marrow transplantation in ALL patients using data from the EBMT, the European Group for Blood and Marrow Transplantation. The paper compares the new landmark methodology as developed by the first author and the established multi-state modeling as described in a recent Tutorial in Biostatistics in Statistics in Medicine by the second author and colleagues. As expected the two approaches give similar results. The landmark methodology does not need complex modeling and leads to easy prediction rules. On the other hand, it does not give the insight in the biological processes as obtained for the multi-state model.},
  author       = {van Houwelingen, Hans C and Putter, Hein},
  language     = {eng},
  publisher    = {Springer US},
  url          = {https://pubmed.ncbi.nlm.nih.gov/18836831 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2798037/},
  date         = {2008-12},
  doi          = {10.1007/s10985-008-9099-8},
  edition      = {2008/10/03},
  issn         = {1380-7870},
  journaltitle = {Lifetime data analysis},
  keywords     = {Data Interpretation,Disease-Free Survival,Europe,Graft vs Host Disease/complications/*mortality,Humans,Leukemia,Lymphoid/*mortality,Markov Chains,Proportional Hazards Models,Risk Factors,Statistical,Time Factors},
  number       = {4},
  pages        = {447--463},
  title        = {{Dynamic predicting by landmarking as an alternative for multi-state modeling: an application to acute lymphoid leukemia data}},
  volume       = {14},
}

@misc{IPCS2008,
  author    = {{International Programme on Chemical Safety}},
  language  = {en},
  location  = {Geneva PP - Geneva},
  publisher = {World Health Organization},
  url       = {https://apps.who.int/iris/handle/10665/44017},
  date      = {2008},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/International Programme on Chemical Safety - 2008 - Uncertainty and data quality in exposure assessment.pdf:pdf},
  isbn      = {9789241563765},
  keywords  = {Data Collection,Environmental Exposure,Risk Assessment,Uncertainty,standards},
  series    = {IPCS harmonization project document ; no. 6},
  title     = {{Uncertainty and data quality in exposure assessment}},
}

@article{Hald1949,
  author       = {Hald, A},
  publisher    = {Taylor & Francis},
  url          = {https://doi.org/10.1080/03461238.1949.10419767},
  annotation   = {doi: 10.1080/03461238.1949.10419767},
  date         = {1949-01},
  doi          = {10.1080/03461238.1949.10419767},
  issn         = {0346-1238},
  journaltitle = {Scandinavian Actuarial Journal},
  number       = {1},
  pages        = {119--134},
  title        = {{Maximum Likelihood Estimation of the Parameters of a Normal Distribution which is Truncated at a Known Point}},
  volume       = {1949},
}

@article{datapharmaco,
  author       = {Steinberg, Michael B and Greenhaus, Shelley and Schmelzer, Amy C and Bover, Michelle T and Foulds, Jonathan and Hoover, Donald R and Carson, Jeffrey L},
  publisher    = {American College of Physicians},
  date         = {2009},
  issn         = {0003-4819},
  journaltitle = {Annals of internal medicine},
  number       = {7},
  pages        = {447--454},
  title        = {{Triple-combination pharmacotherapy for medically ill smokers: a randomized trial}},
  volume       = {150},
}

@article{Sonabend2021,
  abstract     = {Summary Background England's COVID-19 roadmap out of lockdown policy set out the timeline and conditions for the stepwise lifting of non-pharmaceutical interventions (NPIs) as vaccination roll-out continued, with step one starting on March 8, 2021. In this study, we assess the roadmap, the impact of the delta (B.1.617.2) variant of SARS-CoV-2, and potential future epidemic trajectories. Methods This mathematical modelling study was done to assess the UK Government's four-step process to easing lockdown restrictions in England, UK. We extended a previously described model of SARS-CoV-2 transmission to incorporate vaccination and multi-strain dynamics to explicitly capture the emergence of the delta variant. We calibrated the model to English surveillance data, including hospital admissions, hospital occupancy, seroprevalence data, and population-level PCR testing data using a Bayesian evidence synthesis framework, then modelled the potential trajectory of the epidemic for a range of different schedules for relaxing NPIs. We estimated the resulting number of daily infections and hospital admissions, and daily and cumulative deaths. Three scenarios spanning a range of optimistic to pessimistic vaccine effectiveness, waning natural immunity, and cross-protection from previous infections were investigated. We also considered three levels of mixing after the lifting of restrictions. Findings The roadmap policy was successful in offsetting the increased transmission resulting from lifting NPIs starting on March 8, 2021, with increasing population immunity through vaccination. However, because of the emergence of the delta variant, with an estimated transmission advantage of 76% (95% credible interval [95% CrI] 69–83) over alpha, fully lifting NPIs on June 21, 2021, as originally planned might have led to 3900 (95% CrI 1500–5700) peak daily hospital admissions under our central parameter scenario. Delaying until July 19, 2021, reduced peak hospital admissions by three fold to 1400 (95% CrI 700–1700) per day. There was substantial uncertainty in the epidemic trajectory, with particular sensitivity to the transmissibility of delta, level of mixing, and estimates of vaccine effectiveness. Interpretation Our findings show that the risk of a large wave of COVID-19 hospital admissions resulting from lifting NPIs can be substantially mitigated if the timing of NPI relaxation is carefully balanced against vaccination coverage. However, with the delta variant, it might not be possible to fully lift NPIs without a third wave of hospital admissions and deaths, even if vaccination coverage is high. Variants of concern, their transmissibility, vaccine uptake, and vaccine effectiveness must be carefully monitored as countries relax pandemic control measures. Funding National Institute for Health Research, UK Medical Research Council, Wellcome Trust, and UK Foreign, Commonwealth and Development Office.},
  author       = {Sonabend, Raphael and Whittles, Lilith K and Imai, Natsuko and Perez-Guzman, Pablo N and Knock, Edward S and Rawson, Thomas and Gaythorpe, Katy A M and Djaafara, Bimandra A and Hinsley, Wes and FitzJohn, Richard G and Lees, John A and Kanapram, Divya Thekke and Volz, Erik M and Ghani, Azra C and Ferguson, Neil M and Baguelin, Marc and Cori, Anne},
  url          = {https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(21)02276-5/fulltext},
  date         = {2021},
  doi          = {https://doi.org/10.1016/S0140-6736(21)02276-5},
  issn         = {0140-6736},
  journaltitle = {The Lancet},
  title        = {{Non-pharmaceutical interventions, vaccination, and the SARS-CoV-2 delta variant in England: a mathematical modelling study}},
}

@misc{pkgparadox,
  author    = {Lang, Michel and Bischl, Bernd and Richter, Jakob and Sun, Xudong and Binder, Martin},
  publisher = {CRAN},
  url       = {https://cran.r-project.org/package=paradox},
  date      = {2019},
  title     = {{paradox: Define and Work with Parameter Spaces for Complex Algorithms}},
}

@article{Goli2016a,
  author       = {Goli, Shahrbanoo and Mahjub, Hossein and Faradmal, Javad and Soltanian, Ali-Reza},
  date         = {2016-06},
  doi          = {10.14569/IJACSA.2016.070650},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Goli et al. - 2016 - Performance Evaluation of Support Vector Regression Models for Survival Analysis A Simulation Study.pdf:pdf},
  journaltitle = {International Journal of Advanced Computer Science and Applications},
  keywords     = {SVCR,SVR,SVR-MRL,SVRc,benchmark experiment,censoring,comparison,machine learning,survey,survival,svm},
  title        = {{Performance Evaluation of Support Vector Regression Models for Survival Analysis: A Simulation Study}},
  volume       = {7},
}

@inproceedings{pkgpandas,
  author    = {McKinney, Wes},
  editor    = {van der Walt, Stéfan and Millman, Jarrod},
  booktitle = {Proceedings of the 9th Python in Science Conference},
  date      = {2010},
  pages     = {51--56},
  title     = {{Data Structures for Statistical Computing in Python}},
}

@article{Burke1994,
  abstract     = {Abstract The use of artificial neural networks in biological and medical research has increased tremendously in the last few years. Artificial neural networks are being used in cancer research for image processing, the analysis of laboratory data for breast cancer diagnosis, the discovery of chemotherapeutic agents, and for cancer outcome prediction. A neural network generalizes from the input data to patterns inherent in the data, and it uses these patterns to make predictions or to classify. This paper explains how neural networks work, and it shows that a neural network is more accurate at predicting breast cancer patient outcome than the current staging system. ? 1994 Wiley-Liss, Inc.},
  author       = {Burke, Harry B},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/ssu.2980100111},
  annotation   = {doi: 10.1002/ssu.2980100111},
  date         = {1994-01},
  doi          = {10.1002/ssu.2980100111},
  issn         = {8756-0437},
  journaltitle = {Seminars in Surgical Oncology},
  keywords     = {ROC,SEER,TNM,breast cancer,c index,outcome,prediction,staging},
  number       = {1},
  pages        = {73--79},
  title        = {{Artificial neural networks for cancer research: Outcome prediction}},
  volume       = {10},
}

@misc{pkgsurvivalanalysis.jl,
  author = {Sonabend, Raphael},
  url    = {https://github.com/RaphaelS1/SurvivalAnalysis.jl},
  date   = {2022},
  title  = {{SurvivalAnalysis.jl}},
}

@article{Saverino2021,
  abstract     = {<p> <bold>Background:</bold> Health-related quality of life (HRQL) is important for evaluating the impact of a disease in the longer term across the physical and psychological domains of human functioning. The aim of this study is to evaluate HRQL in COVID-19 survivors in Italy using the short form 36-items questionnaire (SF-36). </p>},
  author       = {Saverino, Alessia and Zsirai, Eva and Sonabend, Raphael and Gaggero, Lorenza and Cevasco, Isabella and Pistarini, Caterina and Cremonesi, Paolo},
  url          = {https://f1000research.com/articles/10-282/v1},
  date         = {2021-04},
  doi          = {10.12688/f1000research.50781.1},
  issn         = {2046-1402},
  journaltitle = {F1000Research},
  number       = {282},
  pages        = {282},
  title        = {{Health related quality of life in COVID-19 survivors discharged from acute hospitals: results of a short-form 36-item survey}},
  volume       = {10},
}

@article{datapatient,
  abstract     = {We propose a novel approach for the flexible modeling of complex exposure-lag-response associations in time-to-event data, where multiple past exposures within a defined time window are cumulatively associated with the hazard. Our method allows for the estimation of a wide variety of effects, including potentially smooth and smoothly time-varying effects as well as cumulative effects with leads and lags, taking advantage of the inference methods that have recently been developed for generalized additive mixed models. We apply our method to data from a large observational study of intensive care patients in order to analyze the association of both the timing and the amount of artificial nutrition with the short term survival of critically ill patients. We evaluate the properties of the proposed method by performing extensive simulation studies and provide a systematic comparison with related approaches.},
  author       = {Bender, Andreas and Scheipl, Fabian and Hartl, Wolfgang and Day, Andrew G and Küchenhoff, Helmut},
  url          = {https://doi.org/10.1093/biostatistics/kxy003},
  date         = {2018-02},
  doi          = {10.1093/biostatistics/kxy003},
  issn         = {1465-4644},
  journaltitle = {Biostatistics},
  number       = {2},
  pages        = {315--331},
  title        = {{Penalized estimation of complex, non-linear exposure-lag-response associations}},
  volume       = {20},
}

@article{Box1976,
  author       = {Box, George E. P},
  url          = {http://links.jstor.org/sici?sici=0162-1459%28197612%2971%3A356%3C791%3ASAS%3E2.0.CO%3B2-W},
  date         = {1976},
  isbn         = {0471810339},
  journaltitle = {Journal of the American Statistical Association},
  number       = {356},
  pages        = {791--799},
  title        = {{Science and Statistics}},
  volume       = {71},
}

@inproceedings{VanBelle2009,
  author    = {{Van Belle}, Vanya and Pelckmans, Kristiaan and Suykens, Johan A K and {Van Huffel}, Sabine},
  publisher = {Springer},
  booktitle = {In Proceedings ofthe International Conference on Artificial Neural Net- works (ICANN2009)},
  date      = {2009},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Van Belle et al. - 2009 - MINLIP Efficient learning of transformation models.pdf:pdf},
  keywords  = {concordance,machine learning,ranking,support vector machine,survival,svm},
  pages     = {60--69},
  title     = {{MINLIP: Efficient learning of transformation models}},
}

@article{Wynants2020,
  abstract     = {Objective To review and appraise the validity and usefulness of published and preprint reports of prediction models for diagnosing coronavirus disease 2019 (covid-19) in patients with suspected infection, for prognosis of patients with covid-19, and for detecting people in the general population at increased risk of becoming infected with covid-19 or being admitted to hospital with the disease.Design Living systematic review and critical appraisal by the COVID-PRECISE (Precise Risk Estimation to optimise covid-19 Care for Infected or Suspected patients in diverse sEttings) group.Data sources PubMed and Embase through Ovid, arXiv, medRxiv, and bioRxiv up to 5 May 2020.Study selection Studies that developed or validated a multivariable covid-19 related prediction model.Data extraction At least two authors independently extracted data using the CHARMS (critical appraisal and data extraction for systematic reviews of prediction modelling studies) checklist; risk of bias was assessed using PROBAST (prediction model risk of bias assessment tool).Results 14 217 titles were screened, and 107 studies describing 145 prediction models were included. The review identified four models for identifying people at risk in the general population; 91 diagnostic models for detecting covid-19 (60 were based on medical imaging, nine to diagnose disease severity); and 50 prognostic models for predicting mortality risk, progression to severe disease, intensive care unit admission, ventilation, intubation, or length of hospital stay. The most frequently reported predictors of diagnosis and prognosis of covid-19 are age, body temperature, lymphocyte count, and lung imaging features. Flu-like symptoms and neutrophil count are frequently predictive in diagnostic models, while comorbidities, sex, C reactive protein, and creatinine are frequent prognostic factors. C index estimates ranged from 0.73 to 0.81 in prediction models for the general population, from 0.65 to more than 0.99 in diagnostic models, and from 0.68 to 0.99 in prognostic models. All models were rated at high risk of bias, mostly because of non-representative selection of control patients, exclusion of patients who had not experienced the event of interest by the end of the study, high risk of model overfitting, and vague reporting. Most reports did not include any description of the study population or intended use of the models, and calibration of the model predictions was rarely assessed.Conclusion Prediction models for covid-19 are quickly entering the academic literature to support medical decision making at a time when they are urgently needed. This review indicates that proposed models are poorly reported, at high risk of bias, and their reported performance is probably optimistic. Hence, we do not recommend any of these reported prediction models for use in current practice. Immediate sharing of well documented individual participant data from covid-19 studies and collaboration are urgently needed to develop more rigorous prediction models, and validate promising ones. The predictors identified in included models should be considered as candidate predictors for new models. Methodological guidance should be followed because unreliable predictions could cause more harm than benefit in guiding clinical decisions. Finally, studies should adhere to the TRIPOD (transparent reporting of a multivariable prediction model for individual prognosis or diagnosis) reporting guideline.Systematic review registration Protocol https://osf.io/ehc47/, registration https://osf.io/wy245.Readers' note This article is a living systematic review that will be updated to reflect emerging evidence. Updates may occur for up to two years from the date of original publication. This version is update 2 of the original article published on 7 April 2020 (BMJ 2020;369:m1328), and previous updates can be found as data supplements (https://www.bmj.com/content/369/bmj.m1328/related#datasupp).},
  author       = {Wynants, Laure and {Van Calster}, Ben and Collins, Gary S and Riley, Richard D and Heinze, Georg and Schuit, Ewoud and Bonten, Marc M J and Dahly, Darren L and Damen, Johanna A A and Debray, Thomas P A and de Jong, Valentijn M T and {De Vos}, Maarten and Dhiman, Paula and Haller, Maria C and Harhay, Michael O and Henckaerts, Liesbet and Heus, Pauline and Kreuzberger, Nina and Lohmann, Anna and Luijken, Kim and Ma, Jie and Martin, Glen P and {Andaur Navarro}, Constanza L and Reitsma, Johannes B and Sergeant, Jamie C and Shi, Chunhu and Skoetz, Nicole and Smits, Luc J M and Snell, Kym I E and Sperrin, Matthew and Spijker, René and Steyerberg, Ewout W and Takada, Toshihiko and Tzoulaki, Ioanna and van Kuijk, Sander M J and van Royen, Florien S and Verbakel, Jan Y and Wallisch, Christine and Wilkinson, Jack and Wolff, Robert and Hooft, Lotty and Moons, Karel G M and van Smeden, Maarten},
  url          = {http://www.bmj.com/content/369/bmj.m1328.abstract},
  date         = {2020-04},
  doi          = {10.1136/bmj.m1328},
  journaltitle = {BMJ},
  pages        = {m1328},
  title        = {{Prediction models for diagnosis and prognosis of covid-19: systematic review and critical appraisal}},
  volume       = {369},
}

@article{Mateen2021,
  abstract     = {Background Executive dysregulation and impulsivity can both predispose individuals to risk-prone actions. Although the risk of falls is well established in people with poor executive function, its association to impulsivity is less clear. Purpose To describe and assess the prognostic capabilities of the relationship between impulsivity, executive function, functional capability, and falls in the in-patient neurorehabilitation population. Materials and Methods A prospective cohort study in a 26-bed neurorehabilitation unit in London, recruiting 121 patients, of whom 94 were deemed eligible for inclusion. Cognitive-behavioural assessment was undertaken using the short (16-item) version of the Urgency-Premeditation-Perseverance-Sensation Seeking-Positive Urgency (UPPS) impulsive behaviour scale, and the Trail Making Test (TMT). Patients also underwent a functional assessment at admission and discharge using the UK Functional Independence and Assessment Measure tool (FIM + FAM). The main outcome of interest was falling during an in-patient episode, which are routinely recorded in a computerized registry of adverse incidents. Results Measurements of impulsivity (based on the UPPS-Short form) and executive function (based on the Trail Making Test) were not found to be significantly associated with functional improvement, or risk of falling. Predictive modelling experiments demonstrated that neither of the aforementioned results were capable of identifying individuals at risk of falling more accurately than an informed guess. Conclusion Where impulsivity is present, measurement using structured tools such as the UPPS may be informative to guide individualized rehabilitation programmes; however, its usefulness as the basis of risk prediction models for falls is less likely given the results of this study.},
  author       = {Mateen, Bilal A and Boakye, Ndidi and Sonabend, Raphael and Russell, Noreen and Saverino, Alessia},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1111/jnp.12239},
  annotation   = {https://doi.org/10.1111/jnp.12239},
  date         = {2021-09},
  doi          = {https://doi.org/10.1111/jnp.12239},
  issn         = {1748-6645},
  journaltitle = {Journal of Neuropsychology},
  keywords     = {accidental falls,cognition,executive function,impulsivity,neurorehabilitation,rehabilitation},
  number       = {3},
  pages        = {379--395},
  title        = {{The role of impulsivity in neurorehabilitation: A prospective cohort study of a potential cognitive biomarker for fall risk?}},
  volume       = {15},
}

@article{Kappen1993,
  abstract     = {Background Quantitative methods for the analysis of prognostic information are important in order to use this knowledge optimally. The neural network is a new quantitative method where the fundamental building blocks are units which can be likened to neurons, and weighted connections which can be likened to synapses. The more the hidden units, the more complex the patterns that can be learnt. Materials and methods Data from two Dutch studies in ovarian cancer were used to compare the previously reported survival rates predicted by the Cox's prognostic index with the prediction obtained by a neural network Results Both the Cox's analysis and the neural network agreed on residual tumour size, stage, and performance status as being important for survival. The neural network identified additional predictive factors such as place of diagnosis and age. As the Cox's prognostic index has not been tested to predict survival on an independent data set a comparison with the results obtained in the neural network test set could not be performed. Conclusions Neural networks perform at least as well as Cox's method for the prediction of survival, and prognostic factors can easily be identified. The analysis not only revealed the predictive power of some characteristics, but also the non-predictive power of the others.},
  author       = {Kappen, H J and Neijt, J P},
  url          = {https://doi.org/10.1093/annonc/4.suppl_4.S31},
  date         = {1993-12},
  doi          = {10.1093/annonc/4.suppl_4.S31},
  issn         = {0923-7534},
  journaltitle = {Annals of Oncology},
  number       = {suppl_4},
  pages        = {S31--S34},
  title        = {{Neural network analysis to predict treatment outcome}},
  volume       = {4},
}

@book{BfR2015,
  author    = {{Bundesinstitut für Risikobewertung}},
  location  = {Berlin},
  publisher = {Federal Institute for Risk Assessment (BfR)},
  date      = {2015},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Bundesinstitut f{\"{u}}r Risikobewertung - 2015 - Guidelines on Uncertainty Analysis in Exposure Assessments.pdf:pdf},
  isbn      = {978-3-943963-33-5},
  title     = {{Guidelines on Uncertainty Analysis in Exposure Assessments}},
}

@article{Lundin1999,
  abstract     = {In this study, we evaluated the accuracy of a neural network in predicting 5-, 10- and 15-year breast-cancer-specific survival. A series of 951 breast cancer patients was divided into a training set of 651 and a validation set of 300 patients. Eight variables were entered as input to the network: tumor size, axillary nodal status, histological type, mitotic count, nuclear pleomorphism, tubule formation, tumor necrosis and age. The area under the ROC curve (AUC) was used as a measure of accuracy of the prediction models in generating survival estimates for the patients in the independent validation set. The AUC values of the neural network models for 5-, 10- and 15-year breast-cancer-specific survival were 0.909, 0.886 and 0.883, respectively.The corresponding AUC values for logistic regression were 0.897, 0.862 and 0.858. Axillary lymph node status (N0 vs. N+) predicted 5-year survival with a specificity of 71% and a sensitivity of 77%. The sensitivity of the neural network model was 91% at this specificity level. The rate of false predictions at 5 years was 82/300 for nodal status and 40/300 for the neural network. When nodal status was excluded from the neural network model, the rate of false predictions increased only to 49/300 (AUC 0.877). An artificial neural network is very accurate in the 5-, 10- and 15-year breast-cancer-specific survival prediction. The consistently high accuracy over time and the good predictive performance of a network trained without information on nodal status demonstrate that neural networks can be important tools for cancer survival prediction.},
  author       = {Lundin, M and Lundin, J and Burke, H B and Toikkanen, S and Pylkkänen, L and Joensuu, H},
  url          = {https://www.karger.com/DOI/10.1159/000012061},
  annotation   = {classification},
  date         = {1999},
  doi          = {10.1159/000012061},
  issn         = {0030-2414},
  journaltitle = {Oncology},
  number       = {4},
  pages        = {281--286},
  title        = {{Artificial Neural Networks Applied to Survival Prediction in Breast Cancer}},
  volume       = {57},
}

@article{Sonabend2022,
  abstract     = {In this paper we consider how to evaluate survival distribution predictions with measures of discrimination. This is a non-trivial problem as discrimination measures are the most commonly used in survival analysis and yet there is no clear method to derive a risk prediction from a distribution prediction. We survey methods proposed in literature and software and consider their respective advantages and disadvantages. Whilst distributions are frequently evaluated by discrimination measures, we find that the method for doing so is rarely described in the literature and often leads to unfair comparisons. We find that the most robust method of reducing a distribution to a risk is to sum over the predicted cumulative hazard. We recommend that machine learning survival analysis software implements clear transformations between distribution and risk predictions in order to allow more transparent and accessible model evaluation. The code used in the final experiment is available at https://github.com/RaphaelS1/distribution_discrimination.},
  author       = {Sonabend, Raphael and Bender, Andreas and Vollmer, Sebastian},
  editor       = {Lu, Zhiyong},
  url          = {https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btac451/6640155 https://academic.oup.com/bioinformatics/article/38/17/4178/6640155},
  date         = {2022-09},
  doi          = {10.1093/bioinformatics/btac451},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Sonabend, Bender, Vollmer - 2022 - Avoiding C-hacking when evaluating survival distribution predictions with discrimination measures.pdf:pdf},
  issn         = {1367-4803},
  journaltitle = {Bioinformatics},
  number       = {17},
  pages        = {4178--4184},
  title        = {{Avoiding C-hacking when evaluating survival distribution predictions with discrimination measures}},
  volume       = {38},
}

@unpublished{Guecioueur2018,
  author = {Guecioueur, Ahmed},
  date   = {2018},
  file   = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Guecioueur - 2018 - Supervised Learning.pdf:pdf},
  title  = {{Supervised Learning}},
}

@article{vanderLaan2007,
  author       = {van der Laan, Mark K and Polley, Eric C and Hubbard, Alan E},
  url          = {https://www.degruyter.com/view/j/sagmb.2007.6.issue-1/sagmb.2007.6.1.1309/sagmb.2007.6.1.1309.xml},
  date         = {2007},
  doi          = {10.2202/1544-6115.1309},
  isbn         = {15446115},
  journaltitle = {Statistical Applications in Genetics and Molecular Biology},
  number       = {1},
  title        = {{Super Learner}},
  volume       = {6},
}

@misc{pkgdevtools,
  author    = {Wickham, Hadley and Hester, Jim and Chang, Winston},
  publisher = {CRAN},
  date      = {2019},
  title     = {{devtools: Tools to Make Developing R Packages Easier}},
}

@article{Bøvelstad2011,
  abstract     = {Abstract Survival prediction from high-dimensional genomic data is dependent on a proper regularization method. With an increasing number of such methods proposed in the literature, comparative studies are called for and some have been performed. However, there is currently no consensus on which prediction assessment criterion should be used for time-to-event data. Without a firm knowledge about whether the choice of evaluation criterion may affect the conclusions made as to which regularization method performs best, these comparative studies may be of limited value. In this paper, four evaluation criteria are investigated: the log-rank test for two groups, the area under the time-dependent ROC curve (AUC), an R2-measure based on the Cox partial likelihood, and an R2-measure based on the Brier score. The criteria are compared according to how they rank six widely used regularization methods that are based on the Cox regression model, namely univariate selection, principal components regression (PCR), supervised PCR, partial least squares regression, ridge regression, and the lasso. Based on our application to three microarray gene expression data sets, we find that the results obtained from the widely used log-rank test deviate from the other three criteria studied. For future studies, where one also might want to include non-likelihood or non-model-based regularization methods, we argue in favor of AUC and the R2-measure based on the Brier score, as these do not suffer from the arbitrary splitting into two groups nor depend on the Cox partial likelihood.},
  author       = {B{ø}velstad, Hege M and Borgan, {Ø}rnulf},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/bimj.201000048},
  annotation   = {doi: 10.1002/bimj.201000048},
  date         = {2011-03},
  doi          = {10.1002/bimj.201000048},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/B{\o}velstad, Borgan - 2011 - Assessment of evaluation criteria for survival prediction from genomic data.pdf:pdf},
  issn         = {0323-3847},
  journaltitle = {Biometrical Journal},
  keywords     = {AUC,Brier Score,Cox regression,Explained variation,Microarray gene expression data},
  number       = {2},
  pages        = {202--216},
  title        = {{Assessment of evaluation criteria for survival prediction from genomic data}},
  volume       = {53},
}

@article{Breiman1996a,
  abstract     = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
  author       = {Breiman, Leo},
  url          = {http://statistics.berkeley.edu/sites/default/files/tech-reports/421.pdf},
  date         = {1996},
  doi          = {10.1023/A:1018054314350},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Breiman - 1996 - Bagging Predictors.pdf:pdf},
  isbn         = {0885-6125},
  issn         = {1573-0565},
  journaltitle = {Machine Learning},
  keywords     = {aggregation,averaging,bagging,bootstrap,cart,combining,machine learning,models,random forests},
  number       = {2},
  pages        = {123--140},
  title        = {{Bagging Predictors}},
  volume       = {24},
}

@article{Ohno-Machado2001,
  abstract     = {Medical prognosis has played an increasing role in health care. Reliable prognostic models that are based on survival analysis techniques have been recently applied to a variety of domains, with varying degrees of success. In this article, we review some methods commonly used to model time-oriented data, such as Kaplan-Meier curves, Cox proportional hazards, and logistic regression, and discuss their applications in medical prognosis. Nonlinear, nonparametric models such as neural networks have increasingly been used for building prognostic models. We review their use in several medical domains and discuss different implementation strategies. Advantages and disadvantages of these methods are outlined, as well as pointers to pertinent literature.},
  author       = {Ohno-Machado, L},
  language     = {eng},
  date         = {2001-12},
  doi          = {10.1006/jbin.2002.1038},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Ohno-Machado - 2001 - Modeling medical prognosis survival analysis techniques.pdf:pdf},
  issn         = {1532-0464 (Print)},
  journaltitle = {Journal of biomedical informatics},
  keywords     = {Computer,Humans,Life Tables,Logistic Models,Medical Informatics,Models,Neural Networks,Prognosis,Proportional Hazards Models,Statistical,Survival Analysis},
  number       = {6},
  pages        = {428--439},
  title        = {{Modeling medical prognosis: survival analysis techniques.}},
  volume       = {34},
}

@article{datagbsg,
  abstract     = {PURPOSE: In 1984, the German Breast Cancer Study Group started a multicenter randomized trial to compare six versus three cycles of cyclophosphamide, methotrexate, and fluorouracil (CMF) starting perioperatively and to investigate the additional effect of tamoxifen as adjuvant treatment in node-positive breast cancer patients treated with mastectomy. PATIENTS AND METHODS: From 1984 to 1989, 473 patients were randomized from 41 institutions. After a median follow-up of approximately 10 years for overall survival (OS) and 9 years for event-free survival (EFS), the treatment groups were compared with respect to OS and EFS. Results based on a median follow-up of 56 months have been published earlier. RESULTS: Estimated cumulative locoregional incidence rate after 10 years was 19.9%; the corresponding rate of distant recurrences was 41.3%. Concerning duration of chemotherapy, we did not find any difference between six and three cycles of CMF (EFS: relative risk [RR] in multivariate analysis = 0.95; 95% confidence interval [CI], 0.74 to 1.21 OS: RR = 0.90; 95% CI, = 0.69 to 1.18). Treatment with tamoxifen resulted in an improvement in outcome (EFS: RR = 0.81; 95% CI, 0.61 to 1.07, OS: RR = 0.74; 95% CI, 0.55 to 1.0) although it proved not significant. Number of positive lymph nodes and progesterone receptor were the dominant prognostic factors. CONCLUSION: In this study, we observed some tendency in favor of hormonal treatment, which is in agreement with the literature. Concerning duration of chemotherapy, the results of this study provide further evidence that a reduction to three cycles of CMF is possible without increasing the risk of recurrence or death. For a definitive conclusion, however, further investigations are required.},
  author       = {Sauerbrei, W and Bastert, G and Bojar, H and Beyerle, C and Neumann, R L A and Schmoor, C and Schumacher, M},
  publisher    = {American Society of Clinical Oncology},
  url          = {https://doi.org/10.1200/JCO.2000.18.1.94},
  annotation   = {doi: 10.1200/JCO.2000.18.1.94},
  date         = {2000-01},
  doi          = {10.1200/JCO.2000.18.1.94},
  issn         = {0732-183X},
  journaltitle = {Journal of Clinical Oncology},
  number       = {1},
  pages        = {94},
  title        = {{Randomized 2 × 2 Trial Evaluating Hormonal Treatment and the Duration of Chemotherapy in Node-Positive Breast Cancer Patients: An Update Based on 10 Years' Follow-Up}},
  volume       = {18},
}

@article{Spruance2004,
  author       = {Spruance, Spotswood L and Reid, Julia E and Grace, Michael and Samore, Matthew},
  language     = {eng},
  publisher    = {American Society for Microbiology},
  url          = {https://pubmed.ncbi.nlm.nih.gov/15273082 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC478551/},
  date         = {2004-08},
  doi          = {10.1128/AAC.48.8.2787-2792.2004},
  issn         = {0066-4804},
  journaltitle = {Antimicrobial agents and chemotherapy},
  keywords     = {*Proportional Hazards Models,Antiviral Agents/therapeutic use,Clinical Trials as Topic/*statistics & numerical d,Herpes Genitalis/drug therapy,Humans,Odds Ratio,Research Design},
  number       = {8},
  pages        = {2787--2792},
  title        = {{Hazard ratio in clinical trials}},
  volume       = {48},
}

@misc{pgmyriad,
  url     = {http://wiki.rc.ucl.ac.uk/index.php/RC_Systems#Myriad_technical_specs},
  title   = {{Myriad technical specs}},
  urldate = {2021-03-07},
}

@article{Thoma2019,
  author       = {Thoma, Brian C. and Salk, Rachel H. and Choukas-Bradley, Sophia and Goldstein, Tina R. and Levine, Michele D. and Marshal, Michael P.},
  date         = {2019-11},
  doi          = {10.1542/peds.2019-1183},
  issn         = {0031-4005},
  journaltitle = {Pediatrics},
  number       = {5},
  title        = {{Suicidality Disparities Between Transgender and Cisgender Adolescents}},
  volume       = {144},
}

@article{Kamarudin2017,
  abstract     = {BACKGROUND ROC (receiver operating characteristic) curve analysis is well established for assessing how well a marker is capable of discriminating between individuals who experience disease onset and individuals who do not. The classical (standard) approach of ROC curve analysis considers event (disease) status and marker value for an individual as fixed over time, however in practice, both the disease status and marker value change over time. Individuals who are disease-free earlier may develop the disease later due to longer study follow-up, and also their marker value may change from baseline during follow-up. Thus, an ROC curve as a function of time is more appropriate. However, many researchers still use the standard ROC curve approach to determine the marker capability ignoring the time dependency of the disease status or the marker. METHODS We comprehensively review currently proposed methodologies of time-dependent ROC curves which use single or longitudinal marker measurements, aiming to provide clarity in each methodology, identify software tools to carry out such analysis in practice and illustrate several applications of the methodology. We have also extended some methods to incorporate a longitudinal marker and illustrated the methodologies using a sequential dataset from the Mayo Clinic trial in primary biliary cirrhosis (PBC) of the liver. RESULTS From our methodological review, we have identified 18 estimation methods of time-dependent ROC curve analyses for censored event times and three other methods can only deal with non-censored event times. Despite the considerable numbers of estimation methods, applications of the methodology in clinical studies are still lacking. CONCLUSIONS The value of time-dependent ROC curve methods has been re-established. We have illustrated the methods in practice using currently available software and made some recommendations for future research.},
  author       = {Kamarudin, Adina Najwa and Cox, Trevor and Kolamunnage-Dona, Ruwanthi},
  publisher    = {BMC Medical Research Methodology},
  date         = {2017},
  doi          = {10.1186/s12874-017-0332-6},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Kamarudin, Cox, Kolamunnage-Dona - 2017 - Time-dependent ROC curve analysis in medical research Current methods and applications.pdf:pdf},
  isbn         = {1471-2288 (Electronic) 1471-2288 (Linking)},
  issn         = {14712288},
  journaltitle = {BMC Medical Research Methodology},
  keywords     = {Biomarker evaluation,Event-time,Longitudinal data,ROC curve,Software,Time-dependent AUC},
  number       = {1},
  pages        = {1--19},
  title        = {{Time-dependent ROC curve analysis in medical research: Current methods and applications}},
  volume       = {17},
}

@article{Bower2019,
  author       = {Bower, Hannah and Crowther, Michael J and Rutherford, Mark J and Andersson, Therese M.-L. and Clements, Mark and Liu, Xing-Rong and Dickman, Paul W and Lambert, Paul C},
  publisher    = {Taylor & Francis},
  url          = {https://doi.org/10.1080/03610918.2019.1634201},
  annotation   = {doi: 10.1080/03610918.2019.1634201},
  date         = {2019-07},
  doi          = {10.1080/03610918.2019.1634201},
  issn         = {0361-0918},
  journaltitle = {Communications in Statistics - Simulation and Computation},
  keywords     = {bias,flexible,flexible parametric,proportional hazards,proportional odds,review,royston-parmar,simulation,splines,survival},
  pages        = {1--17},
  title        = {{Capturing simple and complex time-dependent effects using flexible parametric survival models: A simulation study}},
}

@book{Hume1779,
  author = {Hume, David},
  date   = {1779},
  pages  = {3--212},
  title  = {{An enquiry concerning human understanding.}},
}

@article{Borra2010,
  author       = {Borra, Simone and {Di Ciaccio}, Agostino},
  publisher    = {Elsevier B.V.},
  url          = {http://dx.doi.org/10.1016/j.csda.2010.03.004},
  date         = {2010},
  doi          = {10.1016/j.csda.2010.03.004},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Borra, Di Ciaccio - 2010 - Measuring the prediction error . A comparison of cross-validation , bootstrap and covariance penalty methods.pdf:pdf},
  issn         = {0167-9473},
  journaltitle = {Computational Statistics and Data Analysis},
  number       = {12},
  pages        = {2976--2989},
  title        = {{Measuring the prediction error . A comparison of cross-validation , bootstrap and covariance penalty methods}},
  volume       = {54},
}

@article{Wang2017a,
  abstract     = {Over the past decades, there has been considerable interest in applying statistical machine learning methods in survival analysis. Ensemble based approaches, especially random survival forests, have been developed in a variety of contexts due to their high precision and non-parametric nature. This article aims to provide a timely review on recent developments and applications of random survival forests for time-to-event data with high dimensional covariates. This selective review begins with an introduction to the random survival forest framework, followed by a survey of recent developments on splitting criteria, variable selection, and other advanced topics of random survival forests for time-to-event data in high dimensional settings. We also discuss potential research directions for future research.},
  author       = {Wang, Hong and Li, Gang},
  language     = {eng},
  url          = {https://pubmed.ncbi.nlm.nih.gov/30740388 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6364686/},
  date         = {2017},
  doi          = {10.22283/qbs.2017.36.2.85},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Li - 2017 - A Selective Review on Random Survival Forests for High Dimensional Data.pdf:pdf},
  issn         = {2508-7185},
  journaltitle = {Quantitative bio-science},
  keywords     = {Censoring,Random survival forest,Survival ensemble,Survival tree,Time-to-event data},
  number       = {2},
  pages        = {85--96},
  title        = {{A Selective Review on Random Survival Forests for High Dimensional Data}},
  volume       = {36},
}

@article{Schemper2003,
  abstract     = {Abstract Measures of the predictive accuracy of regression models quantify the extent to which covariates determine an individual outcome. Explained variation measures the relative gains in predictive accuracy when prediction based on covariates replaces unconditional prediction. A unified concept of predictive accuracy and explained variation based on the absolute prediction error is presented for models with continuous, binary, polytomous and survival outcomes. The measures are given both in a model-based formulation and in a formulation directly contrasting observed and expected outcomes. Various aspects of application are demonstrated by examples from three forms of regression models. It is emphasized that the likely degree of absolute or relative predictive accuracy often is low even if there are highly significant and relatively strong covariates. Copyright ? 2003 John Wiley & Sons, Ltd.},
  author       = {Schemper, Michael},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/sim.1486},
  annotation   = {doi: 10.1002/sim.1486},
  date         = {2003-07},
  doi          = {10.1002/sim.1486},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Schemper - 2003 - Predictive accuracy and explained variation.pdf:pdf},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  keywords     = {Cox regression,Poisson regression,general linear model,logistic regression,prediction error},
  number       = {14},
  pages        = {2299--2308},
  title        = {{Predictive accuracy and explained variation}},
  volume       = {22},
}

@article{Hullermeier2021,
  abstract     = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
  author       = {Hüllermeier, Eyke and Waegeman, Willem},
  url          = {https://doi.org/10.1007/s10994-021-05946-3},
  date         = {2021},
  doi          = {10.1007/s10994-021-05946-3},
  issn         = {1573-0565},
  journaltitle = {Machine Learning},
  number       = {3},
  pages        = {457--506},
  title        = {{Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods}},
  volume       = {110},
}

@article{Fard2016,
  abstract     = {Predicting event occurrence at the early stage of a longitudinal study\nis an important and challenging problem which has high practical value\nin many real-world applications. As opposed to the standard\nclassification and regression problems where a domain expert can provide\nlabels for the data in a reasonably short period of time, training data\nin such longitudinal studies must be obtained only by waiting for the\noccurrence of a sufficient number of events. Survival analysis aims at\ndirectly predicting the time to an event of interest using the data\ncollected in the past for a certain duration. However, it cannot give an\nanswer to the open question of ``how to forecast whether a subject will\nexperience an event by end of a longitudinal study using event\noccurrence information of other subjects at the early stage of the\nstudy?{''}. The goal of this work is to predict the event occurrence at\na future time point using only the information about a limited number of\nevents that occurred at the initial stages of a longitudinal study. This\nproblem exhibits two major challenges: (1) absence of complete\ninformation about event occurrence (censoring) and (2) availability of\nonly a partial set of events that occurred during the initial phase of\nthe study. We propose a novel Early Stage Prediction (ESP) framework for\nbuilding event prediction models which are trained at the early stages\nof longitudinal studies. First, we develop a novel approach to address\nthe first challenge by introducing a new method for handling censored\ndata using Kaplan-Meier estimator. We then extend the Naive Bayes,\nTree-Augmented Naive Bayes (TAN), and Bayesian Network methods based on\nthe proposed framework, and develop three algorithms, namely, ESP-NB,\nESP-TAN, and ESPBN, to effectively predict event occurrence using\ntraining data obtained at an early stage of the study. More\nspecifically, our approach effectively integrates Bayesian methods with\nan Accelerated Failure Time (AFT) model by adapting the prior\nprobability of the event occurrence for future time points. The proposed\nframework is evaluated using a wide range of synthetic and real-world\nbenchmark datasets. Our extensive set of experiments show that the\nproposed ESP framework is, on an average, 20 percent more accurate\ncompared to existing schemes when using only limited event information\nin the training data.},
  author       = {Fard, Mahtab Jahanbani and Wang, Ping and Chawla, Sanjay and Reddy, Chandan K.},
  annotation   = {Bayesian network for AFT Early stage prediction},
  date         = {2016},
  doi          = {10.1109/TKDE.2016.2608347},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Fard et al. - 2016 - A Bayesian Perspective on Early Stage Event Prediction in Longitudinal Data.pdf:pdf},
  issn         = {10414347},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  keywords     = {Bayesian network,Naive Bayes,early stage prediction,event data,longitudinal data,regression,survival analysis},
  number       = {12},
  pages        = {3126--3139},
  title        = {{A Bayesian Perspective on Early Stage Event Prediction in Longitudinal Data}},
  volume       = {28},
}

@article{datalung,
  abstract     = {PURPOSE: This study was developed to determine whether descriptive information from a patient-completed questionnaire could provide prognostic information that was independent from that already obtained by the patient's physician. PATIENTS AND METHODS: An initial detailed questionnaire was administered to approximately 150 patients with advanced cancer. This questionnaire was subsequently revised and given to a total of 1,115 patients with advanced colorectal or lung cancer. Univariate and multivariate analyses were performed to evaluate the data from these questionnaires. RESULTS: A total of 36 variables showed statistically significant prognostic information for survival in univariate analyses, even though many of these variables were associated with only a minimal increase in risk. A multivariate analysis demonstrated that there was a high correlation between many variables. Three major groups of variables became apparent as providing strong prognostic information. These included the following: (1) a physician's assessment of performance status (PS); (2) a patient's assessment of their own PS; and (3) a nutritional factor such as appetite, caloric intake, or overall food intake. CONCLUSION: Data generated by a patient-completed questionnaire can provide important prognostic information independent from that obtained by other physician-determined prognostic factors.},
  author       = {Loprinzi, C L and Laurie, J A and Wieand, H S and Krook, J E and Novotny, P J and Kugler, J W and Bartel, J and Law, M and Bateman, M and Klatt, N E},
  language     = {eng},
  date         = {1994-03},
  doi          = {10.1200/JCO.1994.12.3.601},
  issn         = {0732-183X (Print)},
  journaltitle = {Journal of clinical oncology : official journal of the American Society of Clinical Oncology},
  keywords     = {Analysis of Variance,Colorectal Neoplasms,Humans,Karnofsky Performance Status,Lung Neoplasms,Neoplasms,Prognosis,Proportional Hazards Models,Prospective Studies,Severity of Illness Index,Surveys and Questionnaires,physiopathology},
  number       = {3},
  pages        = {601--607},
  title        = {{Prospective evaluation of prognostic variables from patient-completed questionnaires. North Central Cancer Treatment Group.}},
  volume       = {12},
}

@misc{pkgneuralnet,
  author     = {Fritsch, Stefan and Guenther, Frauke and {N. Wright}, Marvin},
  publisher  = {CRAN},
  url        = {https://cran.r-project.org/package=neuralnet},
  annotation = {R package version 1.44.2},
  date       = {2019},
  title      = {{neuralnet: Training of Neural Networks}},
}

@article{pkgzoo,
  author       = {Zeileis, Achiem and Grothendieck, Gabor},
  date         = {2005},
  doi          = {10.18637/jss.v014.i06},
  journaltitle = {Journal of Statistical Software},
  number       = {6},
  pages        = {1--27},
  title        = {{zoo: S3 Infrastructure for Regular and Irregular Time Series}},
  volume       = {14},
}

@article{Hung2010,
  abstract     = {[The performance of clinical tests for disease screening is often evaluated using the area under the receiver-operating characteristic (ROC) curve (AUC). Recent developments have extended the traditional setting to the AUC with binary time-varying failure status. Without considering covariates, our first theme is to propose a simple and easily computed nonparametric estimator for the time-dependent AUC. Moreover, we use generalized linear models with time-varying coefficients to characterize the time-dependent AUC as a function of covariate values. The corresponding estimation procedures are proposed to estimate the parameter functions of interest. The derived limiting Gaussian processes and the estimated asymptotic variances enable us to construct the approximated confidence regions for the AUCs. The finite sample properties of our proposed estimators and inference procedures are examined through extensive simulations. An analysis of the AIDS Clinical Trials Group (ACTG) 175 data is further presented to show the applicability of the proposed methods. La performance des tests cliniques pour le dépistage de maladie est souvent évaluée en utilisant l'aire sous la courbe caractéristique de fonctionnements du récepteur (« ROC »), notée « AUC ». Des développements récents ont généralisé le cadre traditionnel à l'AUC avec un statut de panne binaire variant dans le temps. Sans considérer les covariables, nous commençons par proposer un estimateur non paramétrique pour l'AUC simple et facile à calculer. De plus, nous utilisons des modèles linéaires généralisés avec des coefficients dépendant du temps pour caractériser les AUC, dépendant du temps, comme fonction des covariables. Les procédures d'estimation asociées correspondantes sont proposées afin d'estimer les fonctions paramètres d'intérêt. Les processus gaussiens limites sont obtenus ainsi que les variances asymptotiques estimées afin de construire des régions de confiance approximatives pour les AUC. À l'aide de nombreuses simulations, les propriétés pour de petits échantillons des estimateurs proposés et des procédures d'inférence sont étudiées. Une analyse du groupe d'essais cliniques sur le sida 175 (ACTG 175) est aussi présentée afin de montrer l'applicabilité des méthodes proposées.]},
  author       = {Hung, Hung and Chiang, Chin-Tsang},
  publisher    = {[Statistical Society of Canada, Wiley]},
  url          = {http://www.jstor.org/stable/27805213},
  date         = {2010},
  issn         = {03195724},
  journaltitle = {The Canadian Journal of Statistics / La Revue Canadienne de Statistique},
  number       = {1},
  pages        = {8--26},
  title        = {{Estimation methods for time-dependent AUC models with survival data}},
  volume       = {38},
}

@article{Johri2021,
  abstract     = {The aim of this study was to compare machine learning (ML) methods with conventional statistical methods to investigate the predictive ability of carotid plaque characteristics for assessing the risk of coronary artery disease (CAD) and cardiovascular (CV) events. Focused carotid B-mode ultrasound, contrast-enhanced ultrasound, and coronary angiography were performed on 459 participants. These participants were followed for 30 days. Plaque characteristics such as carotid intima-media thickness (cIMT), maximum plaque height (MPH), total plaque area (TPA), and intraplaque neovascularization (IPN) were measured at baseline. Two ML-based algorithms—random forest (RF) and random survival forest (RSF) were used for CAD and CV event prediction. The performance of these algorithms was compared against (i) univariate and multivariate analysis for CAD prediction using the area-under-the-curve (AUC) and (ii) Cox proportional hazard model for CV event prediction using the concordance index (c-index). There was a significant association between CAD and carotid plaque characteristics [cIMT (odds ratio (OR) = 1.49, p = 0.03), MPH (OR = 2.44, p < 0.0001), TPA (OR = 1.61, p < 0.0001), and IPN (OR = 2.78, p < 0.0001)]. IPN alone reported significant CV event prediction (hazard ratio = 1.24, p < 0.0001). CAD prediction using the RF algorithm reported an improvement in AUC by $\sim$ 3% over the univariate analysis with IPN alone (0.97 vs. 0.94, p < 0.0001). Cardiovascular event prediction using RSF demonstrated an improvement in the c-index by $\sim$ 17.8% over the Cox-based model (0.86 vs. 0.73). Carotid imaging phenotypes and IPN were associated with CAD and CV events. The ML-based system is superior to the conventional statistically-derived approaches for CAD prediction and survival analysis.},
  author       = {Johri, Amer M and Mantella, Laura E and Jamthikar, Ankush D and Saba, Luca and Laird, John R and Suri, Jasjit S},
  url          = {https://doi.org/10.1007/s10554-021-02294-0},
  date         = {2021},
  doi          = {10.1007/s10554-021-02294-0},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Johri et al. - 2021 - Role of artificial intelligence in cardiovascular risk prediction and outcomes comparison of machine-learning and.pdf:pdf},
  issn         = {1573-0743},
  journaltitle = {The International Journal of Cardiovascular Imaging},
  number       = {11},
  pages        = {3145--3156},
  title        = {{Role of artificial intelligence in cardiovascular risk prediction and outcomes: comparison of machine-learning and conventional statistical approaches for the analysis of carotid ultrasound features and intra-plaque neovascularization}},
  volume       = {37},
}

@article{pkgactuar,
  author       = {Dutang, Christophe and Goulet, Vincent and Pigeon, Mathieu},
  date         = {2008},
  journaltitle = {Journal of Statistical Software},
  number       = {7},
  pages        = {38},
  title        = {{actuar: An R Package for Actuarial Science}},
  volume       = {25},
}

@article{Chen2013,
  abstract     = {Survival analysis focuses on modeling and predicting the time to an event of interest. Many statistical models have been proposed for survival analysis. They often impose strong assumptions on hazard functions, which describe how the risk of an event changes over time depending on covariates associated with each individual. In particular, the prevalent proportional hazards model assumes that covariates are multiplicatively related to the hazard. Here we propose a nonparametric model for survival analysis that does not explicitly assume particular forms of hazard functions. Our nonparametric model utilizes an ensemble of regression trees to determine how the hazard function varies according to the associated covariates. The ensemble model is trained using a gradient boosting method to optimize a smoothed approximation of the concordance index, which is one of the most widely used metrics in survival model performance evaluation. We implemented our model in a software package called GBMCI (gradient boosting machine for concordance index) and benchmarked the performance of our model against other popular survival models with a large-scale breast cancer prognosis dataset. Our experiment shows that GBMCI consistently outperforms other methods based on a number of covariate settings. GBMCI is implemented in R and is freely available online.},
  author       = {Chen, Yifei and Jia, Zhenyu and Mercola, Dan and Xie, Xiaohui},
  editor       = {Klebanov, Lev},
  publisher    = {Hindawi Publishing Corporation},
  url          = {https://doi.org/10.1155/2013/873595},
  annotation   = {Strangely these authors submitted their paper just after Mayr and Schmid.This also optimises c-index using sigmoid approximation but here they optimise Harrell's and not Uno. Perhaps this is why it didn't take off (also they're less well known) Have no idea what's going on with this 'package' https://github.com/uci-cbcl/GBMCI},
  date         = {2013},
  doi          = {10.1155/2013/873595},
  issn         = {1748-670X},
  journaltitle = {Computational and Mathematical Methods in Medicine},
  keywords     = {boosting,concordance,ensemble,ml,survival},
  pages        = {873595},
  title        = {{A Gradient Boosting Algorithm for Survival Analysis via Direct Optimization of Concordance Index}},
  volume       = {2013},
}

@article{Chambers2014,
  abstract     = {This paper reviews some programming techniques in R that have proved useful, particularly for substantial projects. These include several versions of object-oriented programming, used in a large number of R packages. The review tries to clarify the origins and ideas behind the various versions, each of which is valuable in the appropriate context. R has also been strongly influenced by the ideas of functional programming and, in particular, by the desire to combine functional with object oriented programming. To clarify how this particular mix of ideas has turned out in the current R language and supporting software, the paper will first review the basic ideas behind object-oriented and functional programming, and then examine the evolution of R with these ideas providing context. Functional programming supports well-defined, defensible software giving reproducible results. Object-oriented programming is the mechanism par excellence for managing complexity while keeping things simple for the user. The two paradigms have been valuable in supporting major software for fitting models to data and numerous other statistical applications. The paradigms have been adopted, and adapted, distinctively in R. Functional programming motivates much of R but R does not enforce the paradigm. Object-oriented programming from a functional perspective differs from that used in non-functional languages, a distinction that needs to be emphasized to avoid confusion. R initially replicated the S language from Bell Labs, which in turn was strongly influenced by earlier program libraries. At each stage, new ideas have been added, but the previous software continues to show its influence in the design as well. Outlining the evolution will further clarify why we currently have this somewhat unusual combination of ideas.},
  author       = {Chambers, John M},
  language     = {en},
  publisher    = {The Institute of Mathematical Statistics},
  url          = {https://projecteuclid.org:443/euclid.ss/1408368569},
  date         = {2014},
  doi          = {10.1214/13-STS452},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Chambers - 2014 - Object-Oriented Programming, Functional Programming and R.pdf:pdf},
  issn         = {0883-4237},
  journaltitle = {Statist. Sci.},
  keywords     = {COOP,FOOP,Programming languages,R,functional programming,object-oriented programming,programming},
  number       = {2},
  pages        = {167--180},
  title        = {{Object-Oriented Programming, Functional Programming and R}},
  volume       = {29},
}

@article{Kim2018,
  author       = {Kim, Minyoung and Pavlovic, Vladimir},
  date         = {2018},
  journaltitle = {UAI},
  keywords     = {bayesian,gaussian process,gp,mcmc,ml,poisson process,simulation,survival},
  pages        = {435--445},
  title        = {{Variational Inference for Gaussian Process Models for Survival Analysis.}},
}

@article{Georgousopoulou2015,
  author       = {Georgousopoulou, Ekavi N and Pitsavos, Christos and Yannakoulia, Christos Mary and Panagiotakos, Demosthenes B},
  date         = {2015},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Georgousopoulou et al. - 2015 - Comparisons between Survival Models in Predicting Cardiovascular Disease Events Application in the ATTI.pdf:pdf},
  journaltitle = {Journal of Statistics Applications & Probability},
  keywords     = {cardiovascular risk,model performance,parametric models,semi-parametric models,survival analysis},
  number       = {2},
  pages        = {203--210},
  title        = {{Comparisons between Survival Models in Predicting Cardiovascular Disease Events : Application in the ATTICA Study ( 2002-2012 ).}},
  volume       = {4},
}

@article{Tibshirani1997,
  author       = {Tibshirani, Robert},
  date         = {1997},
  doi          = {10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Tibshirani - 1997 - The Lasso Method for Variable Selection in the Cox Model.pdf:pdf},
  isbn         = {0277-6715 (Print)},
  issn         = {02776715},
  journaltitle = {Statistics in Medicine},
  pages        = {385--395},
  title        = {{The Lasso Method for Variable Selection in the Cox Model}},
  volume       = {16},
}

@article{Oh2018,
  abstract     = {Artificial neural networks (ANNs) have been applied to many prediction and classification problems, and could also be used to develop a prediction model of survival outcomes for cancer patients.},
  author       = {Oh, Sung Eun and Seo, Sung Wook and Choi, Min-Gew and Sohn, Tae Sung and Bae, Jae Moon and Kim, Sung},
  url          = {https://doi.org/10.1245/s10434-018-6343-7},
  date         = {2018},
  doi          = {10.1245/s10434-018-6343-7},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Oh et al. - 2018 - Prediction of Overall Survival and Novel Classification of Patients with Gastric Cancer Using the Survival Recurrent.pdf:pdf},
  issn         = {1534-4681},
  journaltitle = {Annals of Surgical Oncology},
  number       = {5},
  pages        = {1153--1159},
  title        = {{Prediction of Overall Survival and Novel Classification of Patients with Gastric Cancer Using the Survival Recurrent Network}},
  volume       = {25},
}

@article{Gordon1985,
  author       = {Gordon, Louis and Olshen, Richard A},
  date         = {1985},
  issn         = {0361-5960},
  journaltitle = {Cancer treatment reports},
  number       = {10},
  pages        = {1065--1069},
  title        = {{Tree-structured survival analysis.}},
  volume       = {69},
}

@misc{pkgdeephit,
  author = {{Lee, Changhee} and {Zame, William R} and {Yoon, Jinsung} and van der Schaar, Mihaela},
  url    = {https://github.com/chl8856/DeepHit},
  date   = {2019},
  title  = {{DeepHit}},
}

@misc{pkgkeras,
  author     = {Allaire, J J and Chollet, François},
  publisher  = {CRAN},
  url        = {https://cran.r-project.org/package=keras},
  annotation = {R package version 2.3.0.0},
  date       = {2020},
  title      = {{keras: R Interface to 'Keras'}},
}

@article{pkgparty,
  author       = {Hothorn, Torsten and Hornik, Kurt and Zeileis, Achim},
  date         = {2006},
  journaltitle = {Journal of Computational and Graphical Statistics},
  number       = {3},
  pages        = {651--674},
  title        = {{Unbiased Recursive Partitioning: A Conditional Inference Framework}},
  volume       = {15},
}

@article{Rahman2017,
  abstract     = {When developing a prediction model for survival data it is essential to validate its performance in external validation settings using appropriate performance measures. Although a number of such measures have been proposed, there is only limited guidance regarding their use in the context of model validation. This paper reviewed and evaluated a wide range of performance measures to provide some guidelines for their use in practice. An extensive simulation study based on two clinical datasets was conducted to investigate the performance of the measures in external validation settings. Measures were selected from categories that assess the overall performance, discrimination and calibration of a survival prediction model. Some of these have been modified to allow their use with validation data, and a case study is provided to describe how these measures can be estimated in practice. The measures were evaluated with respect to their robustness to censoring and ease of interpretation. All measures are implemented, or are straightforward to implement, in statistical software. Most of the performance measures were reasonably robust to moderate levels of censoring. One exception was Harrell's concordance measure which tended to increase as censoring increased. We recommend that Uno's concordance measure is used to quantify concordance when there are moderate levels of censoring. Alternatively, Gönen and Heller's measure could be considered, especially if censoring is very high, but we suggest that the prediction model is re-calibrated first. We also recommend that Royston's D is routinely reported to assess discrimination since it has an appealing interpretation. The calibration slope is useful for both internal and external validation settings and recommended to report routinely. Our recommendation would be to use any of the predictive accuracy measures and provide the corresponding predictive accuracy curves. In addition, we recommend to investigate the characteristics of the validation data such as the level of censoring and the distribution of the prognostic index derived in the validation setting before choosing the performance measures.},
  author       = {Rahman, M. Shafiqur and Ambler, Gareth and Choodari-Oskooei, Babak and Omar, Rumana Z.},
  publisher    = {BMC Medical Research Methodology},
  annotation   = {Out of all the measures assessed, are there any that can/should be used to compare multiple Survival models (not necessarily Cox). e.g. is it meaningful to compare Uno's C or Gonen and Heller's between models to find the 'best' model. In this case does it still make sense to use the prognostic index in the function if the index is coming from a Weibull model vs Cox model? Is the interpretation the same... Are there equivalents for AFTs that can be compared to PHs? Or is concordance index always the best and therefore the one measure that should be used? Do we care at all about distance measures such as brier or logloss that instead compare the true time to event to the predicted time to event? As opposed to comparing the concordance of the linear predictor to the even time},
  date         = {2017},
  doi          = {10.1186/s12874-017-0336-2},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Rahman et al. - 2017 - Review and evaluation of performance measures for survival prediction models in external validation settings.pdf:pdf},
  issn         = {14712288},
  journaltitle = {BMC Medical Research Methodology},
  keywords     = {Prognostic model,Survival analysis,Validation,calibration,discrimination},
  number       = {1},
  pages        = {1--15},
  title        = {{Review and evaluation of performance measures for survival prediction models in external validation settings}},
  volume       = {17},
}

@misc{Sedgwick2003,
  author    = {Sedgwick, Philip and Hall, Angela},
  publisher = {British Medical Journal Publishing Group},
  date      = {2003},
  isbn      = {0959-8138},
  title     = {{Teaching medical students and doctors how to communicate risk}},
}

@article{Aalen2009,
  author       = {{O Aalen}, Odd and Andersen, Per and Borgan, Ornulf and Gill, Richard and Keiding, Niels},
  date         = {2009-02},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/O Aalen et al. - 2009 - History of applications of martingales in survival analysis.pdf:pdf},
  journaltitle = {Electronic Journal for History of Probability and Statistics},
  pages        = {1--28},
  title        = {{History of applications of martingales in survival analysis}},
  volume       = {51},
}

@article{Fournier2018,
  abstract     = {In the context of chronic diseases, patient's health evolution is often evaluated through the study of longitudinal markers and major clinical events such as relapses or death. Dynamic predictions of such types of events may be useful to improve patients management all along their follow-up. Dynamic predictions consist of predictions that are based on information repeatedly collected over time, such as measurements of a biomarker, and that can be updated as soon as new information becomes available. Several techniques to derive dynamic predictions have already been suggested, and computation of dynamic predictions is becoming increasingly popular. In this work, we focus on assessing predictive accuracy of dynamic predictions and suggest that using an R2-curve may help. It facilitates the evaluation of the predictive accuracy gain obtained when accumulating information on a patient's health profile over time. A nonparametric inverse probability of censoring weighted estimator is suggested to deal with censoring. Large sample results are provided, and methods to compute confidence intervals and bands are derived. A simulation study assesses the finite sample size behavior of the inference procedures and illustrates the shape of some R2-curves which can be expected in common settings. A detailed application to kidney transplant data is also presented.},
  author       = {Fournier, Marie-Cécile and Dantan, Etienne and Blanche, Paul},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/sim.7571},
  annotation   = {doi: 10.1002/sim.7571},
  date         = {2018-03},
  doi          = {10.1002/sim.7571},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Fournier, Dantan, Blanche - 2018 - An R2-curve for evaluating the accuracy of dynamic predictions.pdf:pdf},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  keywords     = {joint models,landmarking,longitudinal data,prediction modeling,predictive accuracy,survival analysis},
  number       = {7},
  pages        = {1125--1133},
  title        = {{An R2-curve for evaluating the accuracy of dynamic predictions}},
  volume       = {37},
}

@article{DurrlemanSimon1989,
  abstract     = {We describe the use of cubic splines in regression models to represent the relationship between the response variable and a vector of covariates. This simple method can help prevent the problems that result from inappropriate linearity assumptions. We compare restricted cubic spline regression to non-parametric procedures for characterizing the relationship between age and survival in the Stanford Heart Transplant data. We also provide an illustrative example in cancer therapeutics.},
  author       = {Durrleman, Sylvain and Simon, Richard},
  date         = {1989},
  doi          = {10.1002/sim.4780080504},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Durrleman, Simon - 1989 - Flexible regression models with cubic splines.pdf:pdf},
  isbn         = {0277-6715},
  issn         = {10970258},
  journaltitle = {Statistics in Medicine},
  keywords     = {Non‐parametric regression,Piecewise polynomials,Smoothing splines,classical,cox,flexible,flexible parametric,model,regression,survival},
  number       = {5},
  pages        = {551--561},
  title        = {{Flexible regression models with cubic splines}},
  volume       = {8},
}

@misc{pkgmlr3benchmark,
  author    = {Sonabend, Raphael and Pfisterer, Florian},
  publisher = {CRAN},
  url       = {https://cran.r-project.org/package=mlr3benchmark},
  date      = {2020},
  title     = {{mlr3benchmark: Benchmarking analysis for 'mlr3'}},
}

@article{dataudca1,
  abstract     = {BACKGROUND/AIMS: A double-blind, placebo-controlled trial of ursodeoxycholic acid (UDCA) was conducted in 180 patients with primary biliary cirrhosis (PBC) to define the efficacy and safety of UDCA. Efficacy was assessed by time to treatment failure defined as death; liver transplantation; histological progression; development of varices, ascites, or encephalopathy; doubling of total serum bilirubin levels; progression of fatigue or pruritus; drug toxicity; or voluntary withdrawal. METHODS: Patients with well-defined PBC underwent complete history, physical examination, liver chemistries, ultrasonography, upper endoscopy, and liver biopsy at entry as well as at 2 years. Liver chemistries were determined every 3 months. RESULTS: In patients receiving UDCA, treatment failure was delayed compared with the placebo-treated group (P = 0.0003, log rank test). Seven patients receiving UDCA died or required transplantation compared with 12 in the placebo group (P = 0.18). No patients discontinued UDCA because of side effects of toxicity. CONCLUSIONS: UDCA was extraordinarily safe and well tolerated, and its use was associated with delayed progression of the disease as defined in this study. However, the lack of effects on symptoms, histology, and the need for liver transplantation or survival indicate that further evaluation is necessary to determine the ultimate role of UDCA in the treatment of PBC.},
  author       = {Lindor, K D and Dickson, E R and Baldus, W P and Jorgensen, R A and Ludwig, J and Murtaugh, P A and Harrison, J M and Wiesner, R H and Anderson, M L and Lange, S M},
  language     = {eng},
  date         = {1994-05},
  doi          = {10.1016/0016-5085(94)90021-3},
  issn         = {0016-5085 (Print)},
  journaltitle = {Gastroenterology},
  keywords     = {Adult,Biliary,Bilirubin,Dose-Response Relationship,Double-Blind Method,Drug,Female,Humans,Liver,Liver Cirrhosis,Male,Middle Aged,Ursodeoxycholic Acid,blood,drug therapy,pathology,standards,therapeutic use},
  number       = {5},
  pages        = {1284--1290},
  title        = {{Ursodeoxycholic acid in the treatment of primary biliary cirrhosis.}},
  volume       = {106},
}

@article{Stone1977,
  abstract     = {A logarithmic assessment of the performance of a predicting density is found to lead to asymptotic equivalence of choice of model by cross-validation and Akaike's criterion, when maximum likelihood estimation is used within each model.},
  author       = {Stone, M},
  url          = {http://www.jstor.org/stable/2984877%5Cnpapers2://publication/uuid/5566B0BD-AA26-4021-9AD8-1EDD68030924},
  date         = {1977},
  doi          = {10.2307/2984877},
  eprint       = {arXiv:1001.2762v1},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Stone - 1977 - An Asymptotic Equivalence of Choice of Model by Cross-Validation and Akaike's Criterion.pdf:pdf},
  isbn         = {0035-9246},
  issn         = {00359246},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords     = {akaike,cross-,model choice,predicting density,s information criterion},
  number       = {1},
  pages        = {44--47},
  title        = {{An Asymptotic Equivalence of Choice of Model by Cross-Validation and Akaike's Criterion}},
  volume       = {39},
}

@article{Loureiro2021,
  abstract     = {Introduction: Prognostic scores are important tools in oncology to facilitate clinical decision-making based on patient characteristics. To date, classic survival analysis using Cox proportional hazards regression has been employed in the development of these prognostic scores. With the advance of analytical models, this study aimed to determine if more complex machine-learning algorithms could outperform classical survival analysis methods.},
  author       = {Loureiro, Hugo and Becker, Tim and Bauer-Mehren, Anna and Ahmidi, Narges and Weberpals, Janick},
  url          = {https://www.frontiersin.org/article/10.3389/frai.2021.625573 https://www.frontiersin.org/articles/10.3389/frai.2021.625573/full},
  date         = {2021-04},
  doi          = {10.3389/frai.2021.625573},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Loureiro et al. - 2021 - Artificial Intelligence for Prognostic Scores in Oncology a Benchmarking Study.pdf:pdf},
  isbn         = {2624-8212},
  issn         = {2624-8212},
  journaltitle = {Frontiers in Artificial Intelligence},
  pages        = {9},
  title        = {{Artificial Intelligence for Prognostic Scores in Oncology: a Benchmarking Study}},
  volume       = {4},
}

@article{Ohno-Machado1997,
  abstract     = {Modeling survival of populations and establishing prognoses for individual patients are important activities in the practice of medicine. For patients with diseases that may extend for several years, in particular, accurate assessment of survival probabilities is essential. New methods, such as neural networks, have been used increasingly to model disease progression. Their advantages and disadvantages, when compared to statistical methods such as Cox proportional hazards, have seldom been explored in real-world data. In this study, we compare the performances of a Cox model and a neural network model that are used as prognostic tools for a set of people living with AIDS. We modeled disease progression for patients who had AIDS (according to the 1993 CDC definition) in a set of 588 patients in California, using data from the ATHOS project. We divided the study population into 10 training and 10 test sets and evaluated the prognostic accuracy of a Cox proportional hazards model and of a neural network model by determining sensitivities, specificities, positive and negative predictive values for an arbitrary threshold (0.5). and the areas under the receiver operating characteristic (RGC) curves that utilized all possible thresholds for intervals of 1 yr following the diagnosis of AIDS. There was no evidence that tbe Cox model performed better than did the neural network model or vice versa, but the former method had the advantage of providing some insight on which variables were most influential for prognosis. Nevertheless, it is likely that the assumptions required by the Cox model may not be satisfied in all data sets, justifying the use of neural networks in certain cases.},
  author       = {Ohno-Machado, Lucila},
  date         = {1997},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Ohno-Machado - 1997 - A COMPARISON OF COX PROPORTIONAL HAZARDS AND ARTIFICIAL NEURAL NETWORK MODELS FOR MEDICAL PROGNOSIS The theoretica.pdf:pdf},
  journaltitle = {Comput. Biol. Med},
  keywords     = {ann,cox,cox proportional hazards,machine learning,neural networks,prognosis,survival,survival analysis},
  number       = {1},
  pages        = {55--65},
  title        = {{A COMPARISON OF COX PROPORTIONAL HAZARDS AND ARTIFICIAL NEURAL NETWORK MODELS FOR MEDICAL PROGNOSIS The theoretical advantages and disadvantages of using different methods for predicting survival have seldom been tested in real data sets [ 1 , 2 ]. Althou}},
  volume       = {27},
}

@article{Ching2018a,
  abstract     = {Author summary The increasing application of high-througput transcriptomics data to predict patient prognosis demands modern computational methods. With the re-gaining popularity of artificial neural networks, we asked if a refined neural network model could be used to predict patient survival, as an alternative to the conventional methods, such as Cox proportional hazards (Cox-PH) methods with LASSO or ridge penalization. To this end, we have developed a neural network extension of the Cox regression model, called Cox-nnet. It is optimized for survival prediction from high throughput gene expression data, with comparable or better performance than other conventional methods. More importantly, Cox-nnet reveals much richer biological information, at both the pathway and gene levels, by analyzing features represented in the hidden layer nodes in Cox-nnet. Additionally, we propose to use hidden node features as a new approach for dimension reduction during survival data analysis.},
  author       = {Ching, Travers and Zhu, Xun and Garmire, Lana X},
  publisher    = {Public Library of Science},
  url          = {https://doi.org/10.1371/journal.pcbi.1006076},
  date         = {2018-04},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Ching, Zhu, Garmire - 2018 - Cox-nnet An artificial neural network method for prognosis prediction of high-throughput omics data.pdf:pdf},
  journaltitle = {PLOS Computational Biology},
  number       = {4},
  pages        = {e1006076},
  title        = {{Cox-nnet: An artificial neural network method for prognosis prediction of high-throughput omics data}},
  volume       = {14},
}

@article{Efron1979,
  author       = {Efron, B.},
  date         = {1979},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Efron - 1979 - Bootstrap Methods Another Look at the Jackknife.pdf:pdf},
  journaltitle = {Institute of Mathematical Statistics},
  keywords     = {boostrap,confidence intervals,jackknife,standard error},
  number       = {1},
  pages        = {1--26},
  title        = {{Bootstrap Methods : Another Look at the Jackknife}},
  volume       = {7},
}

@book{Anderson2014,
  abstract  = {Every day thousands of individuals need to make critical decisions about their health based on numerical information, yet recent surveys have found that over half the population of the United States is unable to complete basic math problems. How does this lack of numerical ability (also referred to as low numeracy, quantitative illiteracy or statistical illiteracy) impact healthcare? What can be done to help people with low numeracy skills? Numerical Reasoning in Judgments and Decision Making about Health addresses these questions by examining and explaining the impact of quantitative illiteracy on healthcare and in specific healthcare contexts, and discussing what can be done to reduce these healthcare disparities. This book will be a useful resource for professionals in many health fields including academics, policy makers, physicians and other healthcare providers.},
  author    = {Anderson, Britta L. and Schulkin, Jay},
  location  = {Cambridge},
  publisher = {Cambridge University Press},
  url       = {https://www.cambridge.org/core/books/numerical-reasoning-in-judgments-and-decision-making-about-health/8D32B301749D567AC4E7C5D81950A134},
  date      = {2014},
  doi       = {DOI: 10.1017/CBO9781139644358},
  isbn      = {9781107040946},
  title     = {{Numerical Reasoning in Judgments and Decision Making about Health}},
}

@article{Jager2008,
  abstract     = {What is this patient's prognosis regarding graft rejection? Do patients using a particular drug live longer than those not using it? How does this co-morbidity affect access to transplantation? To answer this type of questions one needs to perform survival analysis. This paper focuses on the Kaplan–Meier method, the most popular method used for survival analysis. It makes it possible to calculate the incidence rate of events like recovery of renal function, myocardial infarction or death by using information from all subjects at risk for these events. It explains how the method works, how survival probabilities are calculated, survival data can be summarized and survival in groups can be compared using the logrank test for hypothesis testing. In addition, it provides some guidance regarding the presentation of survival plots. Finally, it discusses the limitations of the Kaplan–Meier method and refers to other methods that better serve additional purposes.},
  author       = {Jager, Kitty J and van Dijk, Paul C and Zoccali, Carmine and Dekker, Friedo W},
  url          = {http://www.sciencedirect.com/science/article/pii/S0085253815533681},
  date         = {2008},
  doi          = {https://doi.org/10.1038/ki.2008.217},
  issn         = {0085-2538},
  journaltitle = {Kidney International},
  keywords     = {Kaplan–Meier method,epidemiology,statistics,survival analysis},
  number       = {5},
  pages        = {560--565},
  title        = {{The analysis of survival data: the Kaplan–Meier method}},
  volume       = {74},
}

@book{Rasmussen2004,
  abstract   = {Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
  author     = {Rasmussen, C. E. and Williams, C. K. I.},
  booktitle  = {International journal of neural systems},
  date       = {2004},
  doi        = {10.1142/S0129065704001899},
  eprint     = {026218253X},
  eprinttype = {arXiv},
  file       = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Rasmussen, Williams - 2004 - Gaussian processes for machine learning.pdf:pdf},
  isbn       = {026218253X},
  issn       = {0129-0657},
  keywords   = {gaussian process,gp,machine learning},
  number     = {2},
  pages      = {69--106},
  title      = {{Gaussian processes for machine learning.}},
  volume     = {14},
}

@article{Apter2008,
  author       = {Apter, Andrea J and Paasche-Orlow, Michael K and Remillard, Janine T and Bennett, Ian M and Ben-Joseph, Elana Pearl and Batista, Rosanna M and Hyde, James and Rudd, Rima E},
  publisher    = {Springer},
  date         = {2008},
  issn         = {0884-8734},
  journaltitle = {Journal of general internal medicine},
  number       = {12},
  pages        = {2117--2124},
  title        = {{Numeracy and communication with patients: they are counting on us}},
  volume       = {23},
}

@misc{pkgdistributional,
  author     = {O'Hara-Wild, Mitchell and Hayes, Alex},
  publisher  = {CRAN},
  url        = {https://cran.r-project.org/package=distributional},
  annotation = {R package version 0.2.1},
  date       = {2020},
  title      = {{distributional: Vectorised Probability Distributions}},
}

@article{pkgrbugs,
  author       = {Sturtz, Sibylle and Ligges, Uwe and Gelman, Andrew},
  date         = {2005},
  journaltitle = {Journal of Statistical Software},
  number       = {3},
  pages        = {1--16},
  title        = {{R2WinBUGS: A Package for Running WinBUGS from R}},
  volume       = {12},
}

@article{Kiraly2021,
  abstract     = {Machine learning (ML) and AI toolboxes such as scikit-learn or Weka are workhorses of contemporary data scientific practice -- their central role being enabled by usable yet powerful designs that allow to easily specify, train and validate complex modeling pipelines. However, despite their universal success, the key design principles in their construction have never been fully analyzed. In this paper, we attempt to provide an overview of key patterns in the design of AI modeling toolboxes, taking inspiration, in equal parts, from the field of software engineering, implementation patterns found in contemporary toolboxes, and our own experience from developing ML toolboxes. In particular, we develop a conceptual model for the AI/ML domain, with a new type system, called scientific types, at its core. Scientific types capture the scientific meaning of common elements in ML workflows based on the set of operations that we usually perform with them (i.e. their interface) and their statistical properties. From our conceptual analysis, we derive a set of design principles and patterns. We illustrate that our analysis can not only explain the design of existing toolboxes, but also guide the development of new ones. We intend our contribution to be a state-of-art reference for future toolbox engineers, a summary of best practices, a collection of ML design patterns which may become useful for future research, and, potentially, the first steps towards a higher-level programming paradigm for constructing AI.},
  author       = {Király, Franz J. and Löning, Markus and Blaom, Anthony and Guecioueur, Ahmed and Sonabend, Raphael},
  url          = {http://arxiv.org/abs/2101.04938},
  date         = {2021-01},
  eprint       = {2101.04938},
  eprinttype   = {arXiv},
  journaltitle = {arXiv},
  title        = {{Designing Machine Learning Toolboxes: Concepts, Principles and Patterns}},
}

@misc{pkghash,
  author = {Brown, Christopher},
  url    = {https://cran.r-project.org/package=hash},
  date   = {2013},
  title  = {{hash: Full feature implementation of hash/associated arrays/dictionaries}},
}

@article{VolinskyRaftery2000,
  author       = {Volinsky, Chris T and Raftery, Adrian E},
  date         = {2000},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Volinsky, Raftery - 2000 - Bayesian Information Criterion for Censored Survival Models.pdf:pdf},
  journaltitle = {International Biometric Society},
  keywords     = {bayes factor,cox proportional hazards model,exponential distribution,partial likelihood},
  number       = {1},
  pages        = {256--262},
  title        = {{Bayesian Information Criterion for Censored Survival Models}},
  volume       = {56},
}

@article{Singh2011,
  abstract     = {Many clinical trials involve following patients for a long time. The primary event of interest in those studies is death, relapse, adverse drug reaction or development of a new disease. The follow-up time for the study may range from few weeks to many years. A different set of statistical procedures are employed to analyze the data, which involves time to event an analysis. It is a very useful tool in clinical research and provides invaluable information about an intervention. This article introduces the researcher to the different tools of survival analysis.},
  author       = {Singh, Ritesh and Mukhopadhyay, Keshab},
  language     = {eng},
  publisher    = {Medknow Publications & Media Pvt Ltd},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/22145125 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3227332/},
  date         = {2011-10},
  doi          = {10.4103/2229-3485.86872},
  issn         = {2229-5488},
  journaltitle = {Perspectives in clinical research},
  keywords     = {Cox proportional hazard model,hazard ratio,survival analysis},
  number       = {4},
  pages        = {145--148},
  title        = {{Survival analysis in clinical trials: Basics and must know areas}},
  volume       = {2},
}

@misc{pkgrankdeepsurv,
  author = {Jing, Bingzhong and Zhang, Tao and Wang, Zixian and Jin, Ying and Liu, Kuiyuan and Qiu, Wenze and Ke, Liangru and Sun, Ying and He, Caisheng and Hou, Dan and Tang, Linquan and Lv, Xing and Li, Chaofeng},
  url    = {https://github.com/sysucc-ailab/RankDeepSurv},
  date   = {2018},
  title  = {{RankDeepSurv}},
}

@book{Stata,
  author    = {StataCorp},
  publisher = {StataCorp LLC},
  url       = {https://www.stata.com/manuals13/st.pdf},
  date      = {2017},
  doi       = {03.2007.11},
  edition   = {15},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/StataCorp - 2017 - STATA Survival Analysis Reference Manual.pdf:pdf},
  isbn      = {978-1-59718-126-6},
  title     = {{STATA Survival Analysis Reference Manual}},
}

@misc{pkgtensorflow,
  author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jozefowicz, Rafal and Jia, Yangqing and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mané, Dan and Schuster, Mike and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viégas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  url    = {https://www.tensorflow.org/},
  date   = {2015},
  title  = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems}},
}

@book{Becker1988,
  author    = {Becker, Richard and Chambers, John M and Wilks, A R},
  publisher = {Wadsworth & Brooks/Cole},
  date      = {1998},
  isbn      = {1351091883},
  title     = {{The New S Language}},
}

@misc{pkgmlr3pipelines,
  author    = {Binder, Martin and Pfisterer, Florian and Bischl, Bernd and Lang, Michel and Dandl, Susanne},
  publisher = {CRAN},
  url       = {https://cran.r-project.org/package=mlr3pipelines},
  date      = {2019},
  title     = {{mlr3pipelines: Preprocessing Operators and Pipelines for 'mlr3'}},
}

@article{Ishwaran2008,
  author       = {Ishwaran, By Hemant and Kogalur, Udaya B and Blackstone, Eugene H and Lauer, Michael S},
  annotation   = {sum of CHF = deaths},
  date         = {2008},
  doi          = {10.1214/08-AOAS169},
  eprint       = {arXiv:0811.1645v1},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Ishwaran et al. - 2008 - Random survival forests.pdf:pdf},
  journaltitle = {The Annals of Statistics},
  keywords     = {and phrases,conservation of events,cumulative hazard function,ensemble,forests,maximal subtree,minimal depth,random forests,random survival forests,survival forests,trees,variable selection,vimp},
  number       = {3},
  pages        = {841--860},
  title        = {{Random survival forests}},
  volume       = {2},
}

@article{Lawless2010,
  abstract     = {Abstract When statistical models are used to predict the values of unobserved random variables, loss functions are often used to quantify the accuracy of a prediction. The expected loss over some specified set of occasions is called the prediction error. This paper considers the estimation of prediction error when regression models are used to predict survival times and discusses the use of these estimates. Extending the previous work, we consider both point and confidence interval estimations of prediction error, and allow for variable selection and model misspecification. Different estimators are compared in a simulation study for an absolute relative error loss function, and results indicate that cross-validation procedures typically produce reliable point estimates and confidence intervals, whereas model-based estimates are sensitive to model misspecification. Links between performance measures for point predictors and for predictive distributions of survival times are also discussed. The methodology is illustrated in a medical setting involving survival after treatment for disease. Copyright ? 2009 John Wiley & Sons, Ltd.},
  author       = {Lawless, Jerald F and Yuan, Yan},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/sim.3758},
  annotation   = {doi: 10.1002/sim.3758},
  date         = {2010-01},
  doi          = {10.1002/sim.3758},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Lawless, Yuan - 2010 - Estimation of prediction error for survival models.pdf:pdf},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  keywords     = {absolute error,confidence intervals,expected loss,inverse probability of censoring weights,misspecified models,survival time predictors},
  number       = {2},
  pages        = {262--274},
  title        = {{Estimation of prediction error for survival models}},
  volume       = {29},
}

@article{McKinney2020,
  abstract     = {Screening mammography aims to identify breast cancer at earlier stages of the disease, when treatment can be more successful1. Despite the existence of screening programmes worldwide, the interpretation of mammograms is affected by high rates of false positives and false negatives2. Here we present an artificial intelligence (AI) system that is capable of surpassing human experts in breast cancer prediction. To assess its performance in the clinical setting, we curated a large representative dataset from the UK and a large enriched dataset from the USA. We show an absolute reduction of 5.7% and 1.2% (USA and UK) in false positives and 9.4% and 2.7% in false negatives. We provide evidence of the ability of the system to generalize from the UK to the USA. In an independent study of six radiologists, the AI system outperformed all of the human readers: the area under the receiver operating characteristic curve (AUC-ROC) for the AI system was greater than the AUC-ROC for the average radiologist by an absolute margin of 11.5%. We ran a simulation in which the AI system participated in the double-reading process that is used in the UK, and found that the AI system maintained non-inferior performance and reduced the workload of the second reader by 88%. This robust assessment of the AI system paves the way for clinical trials to improve the accuracy and efficiency of breast cancer screening.},
  author       = {McKinney, Scott Mayer and Sieniek, Marcin and Godbole, Varun and Godwin, Jonathan and Antropova, Natasha and Ashrafian, Hutan and Back, Trevor and Chesus, Mary and Corrado, Greg C and Darzi, Ara and Etemadi, Mozziyar and Garcia-Vicente, Florencia and Gilbert, Fiona J and Halling-Brown, Mark and Hassabis, Demis and Jansen, Sunny and Karthikesalingam, Alan and Kelly, Christopher J and King, Dominic and Ledsam, Joseph R and Melnick, David and Mostofi, Hormuz and Peng, Lily and Reicher, Joshua Jay and Romera-Paredes, Bernardino and Sidebottom, Richard and Suleyman, Mustafa and Tse, Daniel and Young, Kenneth C and {De Fauw}, Jeffrey and Shetty, Shravya},
  url          = {https://doi.org/10.1038/s41586-019-1799-6},
  date         = {2020},
  doi          = {10.1038/s41586-019-1799-6},
  issn         = {1476-4687},
  journaltitle = {Nature},
  number       = {7788},
  pages        = {89--94},
  title        = {{International evaluation of an AI system for breast cancer screening}},
  volume       = {577},
}

@article{Rindt2022,
  author     = {Rindt, David and Hu, Robert and Steinsaltz, David and Sejdinovic, Dino},
  url        = {http://arxiv.org/abs/2103.14755},
  date       = {2022-03},
  eprint     = {2103.14755},
  eprinttype = {arXiv},
  file       = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Rindt et al. - 2022 - Survival Regression with Proper Scoring Rules and Monotonic Neural Networks.pdf:pdf},
  title      = {{Survival Regression with Proper Scoring Rules and Monotonic Neural Networks}},
}

@misc{pkgMLDR,
  author = {Charte, Francisco and Charte, David},
  date   = {2015},
  file   = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Charte, Charte - 2015 - Working with Multilabel Datasets in R The mldr Package.pdf:pdf},
  number = {December},
  pages  = {149--162},
  title  = {{Working with Multilabel Datasets in R : The mldr Package}},
  volume = {7},
}

@article{Heagerty2000,
  abstract     = {Summary. ROC curves are a popular method for displaying sensitivity and specificity of a continuous marker, X, for a binary disease variable, D. However, many disease outcomes are time dependent, D(t, and ROC curves that vary as a function of time may be mire appropriate. A common examples of a time-dependent variable is vital status, where D(t) = 1 if a patient has died prior to time t and zero otherwise. We propose summarizing the discrimination potential of a marker X, measured at baseline (t= 0), by calculating ROC Curves for cumulative disease or death incidence by time t, which we denote as ROC(t). A typical complexity with survival data is that observations may be censored. Two ROC curve estimators are proposed that can accommodate censored data. A simple estimator is based on using the Kaplan-Meier estimated for each possible subset X > c. However, this estimator does not guarantee the necessary condition that sensitivity and specificity are monotone in X. An alternative estimator that does guarantee monotonicity is based on a nearest neighbor estimator for the bivariate distribution function of (X, T), where T represents survival time (Akritas, M. J., 1994, Annals of Statistics22, 1299?1327). We present an example where ROC(t) is used to compare a standard and a modified flow cytometry measurement for predicting survival after detection of breast cancer and an example where the ROC(t) curve displays the impact of modifying eligibility criteria for sample size and power in HIV prevention trials.},
  author       = {Heagerty, Patrick J. and Lumley, Thomas and Pepe, Margaret S.},
  url          = {https://doi.org/10.1111/j.0006-341X.2000.00337.x},
  date         = {2000},
  doi          = {10.1111/j.0006-341X.2000.00337.x},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Heagerty, Lumley, Pepe - 2000 - Time-Dependent ROC Curves for Censored Survival Data and a Diagnostic Marker.pdf:pdf},
  issn         = {0006-341X},
  journaltitle = {Biometrics},
  keywords     = {Accuracy,Discrimination,Kaplan-Meier estimator,Kernel smoothing,Sensitivity,Specificity},
  number       = {2},
  pages        = {337--344},
  title        = {{Time-Dependent ROC Curves for Censored Survival Data and a Diagnostic Marker}},
  volume       = {56},
}

@article{Lobo2008,
  author       = {Lobo, Jorge M and Jiménez-valverde, Alberto and Real, Raimundo},
  date         = {2008},
  doi          = {10.1111/j.1466-8238.2007.00358.x},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Lobo, Jim{\'{e}}nez-valverde, Real - 2008 - AUC a misleading measure of the performance of predictive distribution models.pdf:pdf},
  journaltitle = {Global Ecology and Biogeography},
  keywords     = {auc,distribution models,ecological statistics,goodness-of-fit,model accuracy},
  pages        = {145--151},
  title        = {{AUC : a misleading measure of the performance of predictive distribution models}},
  volume       = {17},
}

@misc{pkgmlr3tuning,
  author    = {Lang, Michel and Richter, Jakob and Bischl, Bernd and Schalk, Daniel},
  publisher = {CRAN},
  url       = {https://cran.r-project.org/package=mlr3tuning},
  date      = {2019},
  title     = {{mlr3tuning: Tuning for 'mlr3'}},
}

@article{Blanche2019,
  abstract     = {We show that the widely used concordance index for time to event outcome is not proper when interest is in predicting a $t$-year risk of an event, for example 10-year mortality. In the situation with a fixed prediction horizon, the concordance index can be higher for a misspecified model than for a correctly specified model. Impropriety happens because the concordance index assesses the order of the event times and not the order of the event status at the prediction horizon. The time-dependent area under the receiver operating characteristic curve does not have this problem and is proper in this context.},
  author       = {Blanche, Paul and Kattan, Michael W and Gerds, Thomas A},
  url          = {https://doi.org/10.1093/biostatistics/kxy006},
  date         = {2019-04},
  doi          = {10.1093/biostatistics/kxy006},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Blanche, Kattan, Gerds - 2019 - The c-index is not proper for the evaluation of $t$-year predicted risks.pdf:pdf},
  issn         = {1465-4644},
  journaltitle = {Biostatistics},
  number       = {2},
  pages        = {347--357},
  title        = {{The c-index is not proper for the evaluation of $t$-year predicted risks}},
  volume       = {20},
}

@article{Faraggi1997,
  abstract     = {Abstract Neural networks are considered by many to be very promising tools for classification and prediction. The flexibility of the neural network models often result in over-fit. Shrinking the parameters using a penalized likelihood is often used in order to overcome such over-fit. In this paper we extend the approach proposed by FARAGGI and SIMON (1995a) to modeling censored survival data using the input-output relationship associated with a single hidden layer feed-forward neural network. Instead of estimating the neural network parameters using the method of maximum likelihood, we place normal prior distributions on the parameters and make inferences based on derived posterior distributions of the parameters. This Bayesian formulation will result in shrinking the parameters of the neural network model and will reduce the over-fit compared with the maximum likelihood estimators. We illustrate our proposed method on a simulated and a real example.},
  author       = {Faraggi, David and Simon, R and Yaskil, E and Kramar, A},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/bimj.4710390502},
  annotation   = {doi: 10.1002/bimj.4710390502},
  date         = {1997-01},
  doi          = {10.1002/bimj.4710390502},
  issn         = {0323-3847},
  journaltitle = {Biometrical Journal},
  keywords     = {Bayesian analysis,Feed-forward neural network,Maximum likelihood,Shrinkage,Sufficiency principle},
  number       = {5},
  pages        = {519--532},
  title        = {{Bayesian Neural Network Models for Censored Data}},
  volume       = {39},
}

@article{LEcuyer1999,
  abstract     = {Combining parallel multiple recursive sequences provides an efficient way of implementing random number generators with long periods and good structural properties. Such generators are statistically more robust than simple linear congruential generators that fit into a computer word. We made extensive computer searches for good parameter sets, with respect to the spectral test, for combined multiple recursive generators of different sizes. We also compare different implementations and give a specific code in C that is faster than previous implementations of similar generators.},
  author       = {L'Ecuyer, Pierre},
  publisher    = {INFORMS},
  url          = {https://pubsonline.informs.org/doi/abs/10.1287/opre.47.1.159},
  annotation   = {doi: 10.1287/opre.47.1.159},
  date         = {1999-02},
  doi          = {10.1287/opre.47.1.159},
  issn         = {0030-364X},
  journaltitle = {Operations Research},
  number       = {1},
  pages        = {159--164},
  title        = {{Good Parameters and Implementations for Combined Multiple Recursive Random Number Generators}},
  volume       = {47},
}

@article{Schemper1996,
  abstract     = {Several measures of explained variation have been suggested for the Cox proportional hazards regression model. We have categorized these measures into three classes which correspond to three different definitions of multiple R2 of the general linear model. In an empirical study we compared the performance of these measures and classified them by their adherence to a set of criteria which we think should be met by a measure of explained variation for survival data. We suggest that currently there is no uniformly superior measure, particularly as the concepts of either uncensored or censored populations may lead to different choices. For uncensored populations, a measure by Kent and O'Quigley and the squared rank correlation between survival time and the predictor from a Cox regression model appear recommendable choices. For the latter, censored survival times are terminated using a very recent data augmentation algorithm for multiple imputation under proportional hazards. With censored populations, Schemper's measure, V2, could be considered. We give an introductory example, discuss aspects of application and stress the desirability of routinely evaluating explained variation in studies of survival.},
  author       = {Schemper, Michael and Stare, Janez},
  date         = {1996},
  doi          = {10.1002/(SICI)1097-0258(19961015)15:19<1999::AID-SIM353>3.0.CO;2-D},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Schemper, Stare - 1996 - Explained variation in survival analysis.pdf:pdf},
  isbn         = {0277-6715},
  issn         = {02776715},
  journaltitle = {Statistics in Medicine},
  keywords     = {R2 measures,residuals},
  number       = {19},
  pages        = {1999--2012},
  title        = {{Explained variation in survival analysis}},
  volume       = {15},
}

@article{Lao2017,
  abstract     = {Traditional radiomics models mainly rely on explicitly-designed handcrafted features from medical images. This paper aimed to investigate if deep features extracted via transfer learning can generate radiomics signatures for prediction of overall survival (OS) in patients with Glioblastoma Multiforme (GBM). This study comprised a discovery data set of 75 patients and an independent validation data set of 37 patients. A total of 1403 handcrafted features and 98304 deep features were extracted from preoperative multi-modality MR images. After feature selection, a six-deep-feature signature was constructed by using the least absolute shrinkage and selection operator (LASSO) Cox regression model. A radiomics nomogram was further presented by combining the signature and clinical risk factors such as age and Karnofsky Performance Score. Compared with traditional risk factors, the proposed signature achieved better performance for prediction of OS (C-index = 0.710, 95% CI: 0.588, 0.932) and significant stratification of patients into prognostically distinct groups (P < 0.001, HR = 5.128, 95% CI: 2.029, 12.960). The combined model achieved improved predictive performance (C-index = 0.739). Our study demonstrates that transfer learning-based deep features are able to generate prognostic imaging signature for OS prediction and patient stratification for GBM, indicating the potential of deep imaging feature-based biomarker in preoperative care of GBM patients.},
  author       = {Lao, Jiangwei and Chen, Yinsheng and Li, Zhi-Cheng and Li, Qihua and Zhang, Ji and Liu, Jing and Zhai, Guangtao},
  url          = {https://doi.org/10.1038/s41598-017-10649-8},
  annotation   = {imaging},
  date         = {2017},
  doi          = {10.1038/s41598-017-10649-8},
  issn         = {2045-2322},
  journaltitle = {Scientific Reports},
  number       = {1},
  pages        = {10353},
  title        = {{A Deep Learning-Based Radiomics Model for Prediction of Survival in Glioblastoma Multiforme}},
  volume       = {7},
}

@article{Friedman1982,
  author       = {Friedman, Michael},
  publisher    = {Institute of Mathematical Statistics},
  date         = {1982},
  issn         = {0090-5364},
  journaltitle = {The Annals of Statistics},
  number       = {1},
  pages        = {101--113},
  title        = {{Piecewise exponential models for survival data with covariates}},
  volume       = {10},
}

@book{Blitzstein2014,
  author    = {Blitzstein, J K and Hwang, J},
  publisher = {CRC Press},
  url       = {https://books.google.co.uk/books?id=z2POBQAAQBAJ},
  date      = {2014},
  isbn      = {9781466575592},
  series    = {Chapman & Hall/CRC Texts in Statistical Science},
  title     = {{Introduction to Probability}},
}

@inproceedings{Mani1999,
  author    = {Mani, D R and Drew, James and Betz, Andrew and Datta, Piew},
  booktitle = {Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining},
  date      = {1999},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Mani et al. - 1999 - Statistics and data mining techniques for lifetime value modeling.pdf:pdf},
  pages     = {94--103},
  title     = {{Statistics and data mining techniques for lifetime value modeling}},
}

@misc{pkghashmap,
  author = {Russell, Nathan},
  url    = {https://cran.r-project.org/package=hashmap},
  date   = {2017},
  title  = {{hashmap: The Faster Hash Map}},
}

@article{Graf1999,
  abstract     = {SUMMARY Prognostic classi"cation schemes have often been used in medical applications, but rarely subjected to a rigorous examination of their adequacy. For survival data, the statistical methodology to assess such schemes consists mainly of a range of ad hoc approaches, and there is an alarming lack of commonly accepted standards in this "eld. We review these methods and develop measures of inaccuracy which may be calculated in a validation study in order to assess the usefulness of estimated patient-speci"c survival probabilities associated with a prognostic classi"cation scheme. These measures are meaningful even when the estimated probabilities are misspeci"ed, and asymptotically they are not a!ected by random censorship. In addition, they can be used to derive R-type measures of explained residual variation. A breast cancer study will serve for illustration throughout the paper.},
  author       = {Graf, Erika and Schmoor, Claudia and Sauerbrei, Willi and Schumacher, Martin},
  url          = {http://doi.wiley.com/10.1002/%28SICI%291097-0258%2819990915/30%2918%3A17/18%3C2529%3A%3AAID-SIM274%3E3.0.CO%3B2-5},
  annotation   = {Brier for Survival 14 - Paywall, 23, 27, 29},
  date         = {1999},
  doi          = {10.1002/(SICI)1097-0258(19990915/30)18:17/18<2529::AID-SIM274>3.0.CO;2-5},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Graf et al. - 1999 - Assessment and comparison of prognostic classification schemes for survival data.pdf:pdf},
  isbn         = {0277-6715 (Print)},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  keywords     = {graf score,scoring rules,survival},
  number       = {17-18},
  pages        = {2529--2545},
  title        = {{Assessment and comparison of prognostic classification schemes for survival data}},
  volume       = {18},
}

@article{Lee2018a,
  abstract     = {Survival analysis (time-to-event analysis) is widely used in economics and finance, engineering, medicine and many other areas. A fundamental problem is to understand the relationship between the covariates and the (distribution of) survival times(times-to-event). Much of the previous work has approached the problem by viewing the survival time as the first hitting time of a stochastic process, assuming a specific form for the underlying stochastic process, using available data to learn the relationship between the covariates and the parameters of the model, and then deducing the relationship between covariates and the distribution of first hitting times (the risk). However, previous models rely on strong parametric assumptions that are often violated. This paper proposes a very different approach to survival analysis, DeepHit, that uses a deep neural network to learn the distribution of survival times directly.DeepHit makes no assumptions about the underlying stochastic process and allows for the possibility that the relationship between covariates and risk(s) changes over time. Most importantly, DeepHit smoothly handles competing risks; i.e. settings in which there is more than one possible event of interest.Comparisons with previous models on the basis of real and synthetic datasets demonstrate that DeepHit achieves large and statistically significant performance improvements over previous state-of-the-art methods.},
  author       = {Lee, Changhee and Zame, William and Yoon, Jinsung and {Van der Schaar}, Mihaela},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/11842},
  date         = {2018-04},
  doi          = {10.1609/aaai.v32i1.11842},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2018 - Deephit A deep learning approach to survival analysis with competing risks.pdf:pdf},
  issn         = {2374-3468},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  number       = {1},
  title        = {{DeepHit: A Deep Learning Approach to Survival Analysis With Competing Risks}},
  volume       = {32},
}

@article{Swift2009,
  author       = {Swift, Louise and Miles, Susan and Price, Gill and Shepstone, Lee and Leinster, Sam},
  date         = {2009-07},
  doi          = {10.1002/sim.3608},
  journaltitle = {Statistics in medicine},
  pages        = {1969--1981},
  title        = {{Do doctors need statistics? Doctors' use of and attitudes to probability and statistics}},
  volume       = {28},
}

@article{Pencina2012,
  abstract     = {Cardiovascular risk prediction functions offer an important diagnostic tool for clinicians and patients themselves. They are usually constructed with the use of parametric or semi-parametric survival regression models. It is essential to be able to evaluate the performance of these models, preferably with summaries that offer natural and intuitive interpretations. The concept of discrimination, popular in the logistic regression context, has been extended to survival analysis. However, the extension is not unique. In this paper, we define discrimination in survival analysis as the model's ability to separate those with longer event-free survival from those with shorter event-free survival within some time horizon of interest. This definition remains consistent with that used in logistic regression, in the sense that it assesses how well the model-based predictions match the observed data. Practical and conceptual examples and numerical simulations are employed to examine four C statistics proposed in the literature to evaluate the performance of survival models. We observe that they differ in the numerical values and aspects of discrimination that they capture. We conclude that the index proposed by Harrell is the most appropriate to capture discrimination described by the above definition. We suggest researchers report which C statistic they are using, provide a rationale for their selection, and be aware that comparing different indices across studies may not be meaningful.},
  author       = {Pencina, Michael J. and D'Agostino, Ralph B. and Song, Linye},
  date         = {2012},
  doi          = {10.1002/sim.4508},
  eprint       = {NIHMS150003},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Pencina, D'Agostino, Song - 2012 - Quantifying discrimination of Framingham risk functions with different survival C statistics.pdf:pdf},
  isbn         = {1097-0258 (Electronic)0̊277-6715 (Linking)},
  issn         = {02776715},
  journaltitle = {Statistics in Medicine},
  keywords     = {AUC,Censoring,Concordance,Discrimination,Risk function,concordance,discrimination},
  number       = {15},
  pages        = {1543--1553},
  title        = {{Quantifying discrimination of Framingham risk functions with different survival C statistics}},
  volume       = {31},
}

@article{Knock2021,
  abstract     = {We fitted a model of SARS-CoV-2 transmission in care homes and the community to regional surveillance data for England. Among control measures implemented, only national lockdown brought the reproduction number below 1 consistently; introduced one week earlier it could have reduced first wave deaths from 36,700 to 15,700 (95%CrI: 8,900–26,800). Improved clinical care reduced the infection fatality ratio from 1.25% (95%CrI: 1.18%–1.33%) to 0.77% (95%CrI: 0.71%–0.84%). The infection fatality ratio was higher in the elderly residing in care homes (35.9%, 95%CrI: 29.1%–43.4%) than those residing in the community (10.4%, 95%CrI: 9.1%–11.5%). England is still far from herd immunity, with regional cumulative infection incidence to 1st December 2020 between 4.8% (95%CrI: 4.4%–5.1%) and 15.4% (95%CrI: 14.9%–15.9%) of the population.One-sentence summary We fit a mathematical model of SARS-CoV-2 transmission to surveillance data from England, to estimate transmissibility, severity, and the impact of interventionsCompeting Interest StatementThe authors have declared no competing interest.Funding StatementThis work was supported by the NIHR HPRU in Modelling and Health Economics, a partnership between PHE, Imperial College London and LSHTM (grant code NIHR200908). We acknowledge funding from the MRC Centre for Global Infectious Disease Analysis (reference MR/R015600/1), jointly funded by the UK Medical Research Council (MRC) and the UK Foreign, Commonwealth &amp; Development Office (FCDO), under the MRC/FCDO Concordat agreement and is also part of the EDCTP2 programme supported by the European Union.Author DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesThe details of the IRB/oversight body that provided approval or exemption for the research described are given below:The use of pillar 2 PCR testing data was made possible thanks to PHE colleagues. The use of serological data was made possible by colleagues at PHE Porton Down, Colindale, and the NHS Blood Transfusion Service.All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.YesAll code and de-identified regionally aggregated data (see supplementary materials for full details) required to reproduce this analysis are available at https://github.com/mrc-ide/sarscov2-transmission-england (https://zenodo.org/record/4384864) https://github.com/mrc-ide/sarscov2-transmission-england https://zenodo.org/record/4384864},
  author       = {Knock, Edward S and Whittles, Lilith K and Lees, John A and Perez-Guzman, Pablo N and Verity, Robert and FitzJohn, Richard G and Gaythorpe, Katy A M and Imai, Natsuko and Hinsley, Wes and Okell, Lucy C and Rosello, Alicia and Kantas, Nikolas and Walters, Caroline E and Bhatia, Sangeeta and Watson, Oliver J and Whittaker, Charlie and Cattarino, Lorenzo and Boonyasiri, Adhiratha and Djaafara, Bimandra A and Fraser, Keith and Fu, Han and Wang, Haowei and Xi, Xiaoyue and Donnelly, Christl A and Jauneikaite, Elita and Laydon, Daniel J and White, Peter J and Ghani, Azra C and Ferguson, Neil M and Cori, Anne and Baguelin, Marc},
  url          = {http://medrxiv.org/content/early/2021/01/13/2021.01.11.21249564.abstract},
  date         = {2021-01},
  doi          = {10.1101/2021.01.11.21249564},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Knock et al. - 2021 - The 2020 SARS-CoV-2 epidemic in England key epidemiological drivers and impact of interventions.pdf:pdf},
  journaltitle = {medRxiv},
  pages        = {2021.01.11.21249564},
  title        = {{The 2020 SARS-CoV-2 epidemic in England: key epidemiological drivers and impact of interventions}},
}

@article{Vinzamuri2017,
  abstract     = {Censoring is a common phenomenon that arises in many longitudinal studies where an event of interest could not be recorded within the given time frame. Censoring causes missing time-to-event labels, and this effect is compounded when dealing with datasets which have high amounts of censored instances. In addition, dependent censoring in the data, where censoring is dependent on the covariates in the data leads to bias in standard survival estimators. This motivates us to develop an approach for pre-processing censored data which calibrates the right censored (RC) times in an attempt to reduce the bias in the survival estimators. This calibration is done using an imputation method which estimates the sparse inverse covariance matrix over the dataset in an iterative convergence framework. During estimation, we apply row and column-based regularization to account for both row and column-wise correlations between different instances while imputing them. This is followed by comparing these imputed censored times with the original RC times to obtain the final calibrated RC times. These calibrated RC times can now be used in the survival dataset in place of the original RC times for more effective prediction. One of the major benefits of our calibration approach is that it is a pre-processing method for censored data which can be used in conjunction with any survival prediction algorithm and improve its performance.We evaluate the goodness of our approach using a wide array of survival prediction algorithms which are applied over crowdfunding data, electronic health records (EHRs), and synthetic censored datasets. Experimental results indicate that our calibration method improves the AUC values of survival prediction algorithms, compared to applying them directly on the original survival data},
  author       = {Vinzamuri, Bhanukiran and Li, Yan and Reddy, Chandan K.},
  annotation   = {Calibration in survival analysis to deal with censoring without losing information},
  date         = {2017},
  doi          = {10.1109/TKDE.2017.2719028},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Vinzamuri, Li, Reddy - 2017 - Pre-processing censored survival data using inverse covariance matrix based calibration.pdf:pdf},
  issn         = {10414347},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  keywords     = {Crowdfunding,Healthcare,Imputation,Pre-processing,Right censoring,Survival analysis},
  number       = {10},
  pages        = {2111--2124},
  title        = {{Pre-processing censored survival data using inverse covariance matrix based calibration}},
  volume       = {29},
}

@article{Lininger1979,
  author       = {Lininger, Lloyd and Gail, Mitchell H and Green, Sylvan B. and Byar, David P.},
  annotation   = {Referenced in Choodari-Oskooei 2012 as studies on administrative censoring},
  date         = {1979},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Lininger et al. - 1979 - Comparison of Four Tests for Equality of Survival Curves in the Presence of Stratification and Censoring.pdf:pdf},
  journaltitle = {Biometrika},
  number       = {3},
  pages        = {419--428},
  title        = {{Comparison of Four Tests for Equality of Survival Curves in the Presence of Stratification and Censoring}},
  volume       = {66},
}

@article{pkgset6,
  author       = {Sonabend, Raphael and Kiraly, Franz J.},
  url          = {https://cran.r-project.org/package=set6},
  date         = {2020-11},
  doi          = {10.21105/joss.02598},
  issn         = {2475-9066},
  journaltitle = {Journal of Open Source Software},
  keywords     = {object-oriented,sets,statistics},
  number       = {55},
  pages        = {2598},
  title        = {{set6: R6 Mathematical Sets Interface}},
  volume       = {5},
}

@article{Gandon2020,
  abstract     = {Studies have documented that traditional motor skills (i.e. motor habits) are part of the cultural way of life that characterises each society. Yet, it is still unclear to what extent motor skills are inherited through culture. Drawing on ethnology and motor behaviour, we addressed this issue through a detailed description of traditional pottery skills. Our goal was to quantify the influence of three kinds of constraints: the transcultural constraints of wheel-throwing, the cultural constraints induced via cultural transmission, and the potters' individual constraints. Five expert Nepalese potters were invited to produce three familiar pottery types, each in five specimens. A total of 31 different fashioning hand positions were identified. Most of them (14) were cross-cultural, ten positions were cultural, five positions were individual, and two positions were unique. Statistical tests indicated that the subset of positions used by the participants in this study were distinct from those of other cultural groups. Behaviours described in terms of fashioning duration, number of gestures, and hand position repertoires size highlighted both individual and cross-cultural traits. We also analysed the time series of the successive hand positions used throughout the fashioning of each vessel. Results showed, for each pottery type, strong reproducible sequences at the individual level and a clearly higher level of variability between potters. Overall, our findings confirm the existence of a cultural transmission in craft skills but also demonstrated that the skill is not fully determined by a cultural marking. We conclude that the influence of culture on craft skills should not be overstated, even if its role is significant given the fact that it reflects the socially transmitted part of the skill. Such research offers insights into archaeological problems in providing a representative view of how cultural constraints influence the motor skills implied in artefact manufacturing.},
  author       = {Gandon, Enora and Nonaka, Tetsushi and Sonabend, Raphael and Endler, John},
  publisher    = {Public Library of Science},
  url          = {https://doi.org/10.1371/journal.pone.0239139},
  date         = {2020-10},
  journaltitle = {PLOS ONE},
  number       = {10},
  pages        = {e0239139},
  title        = {{Assessing the influence of culture on craft skills: A quantitative study with expert Nepalese potters}},
  volume       = {15},
}

@misc{pkgfuture,
  author     = {Bengtsson, Henrik},
  publisher  = {CRAN},
  url        = {https://cran.r-project.org/package=future},
  annotation = {R package version 1.18.0},
  date       = {2020},
  title      = {{future: Unified Parallel and Distributed Processing in R for Everyone}},
}

@article{Kaplan1958,
  author       = {Kaplan, E. L. and Meier, Paul},
  annotation   = {The seminal paper that interested the Kaplan-Meier estimate or as they called it the product-limit estimate In this paper censored observations are referred to as 'losses', L_i},
  date         = {1958},
  doi          = {10.2307/2281868},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Kaplan, Meier - 1958 - Nonparametric Estimation from Incomplete Observations.pdf:pdf},
  isbn         = {01621459},
  issn         = {01621459},
  journaltitle = {Journal of the American Statistical Association},
  number       = {282},
  pages        = {457--481},
  title        = {{Nonparametric Estimation from Incomplete Observations}},
  volume       = {53},
}

@article{Gerds2006,
  abstract     = {Abstract In survival analysis with censored data the mean squared error of prediction can be estimated by weighted averages of time-dependent residuals. Graf et al. (1999) suggested a robust weighting scheme based on the assumption that the censoring mechanism is independent of the covariates. We show consistency of the estimator. Furthermore, we show that a modified version of this estimator is consistent even when censoring and event times are only conditionally independent given the covariates. The modified estimators are derived on the basis of regression models for the censoring distribution. A simulation study and a real data example illustrate the results. (? 2007 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim)},
  author       = {Gerds, Thomas A and Schumacher, Martin},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/bimj.200610301},
  annotation   = {doi: 10.1002/bimj.200610301},
  date         = {2006-12},
  doi          = {10.1002/bimj.200610301},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Gerds, Schumacher - 2006 - Consistent Estimation of the Expected Brier Score in General Survival Models with Right-Censored Event Times.pdf:pdf},
  issn         = {0323-3847},
  journaltitle = {Biometrical Journal},
  keywords     = {Brier Score,Censoring Bias,Inverse of Probability of Censoring Weighting,Model Validation,Survival Analysis},
  number       = {6},
  pages        = {1029--1040},
  title        = {{Consistent Estimation of the Expected Brier Score in General Survival Models with Right-Censored Event Times}},
  volume       = {48},
}

@article{Bou-Hamad2011,
  abstract     = {This paper presents a non-technical account of the developments in tree-based methods for the analysis of survival data with censoring. This review describes the initial developments, which mainly extended the existing basic tree methodologies to censored data as well as to more recent work. We also cover more complex models, more specialized methods, and more specific problems such as multivariate data, the use of time-varying covariates, discrete-scale survival data, and ensemble methods applied to survival trees. A data example is used to illustrate some methods that are implemented in R.},
  author       = {Bou-Hamad, Imad and Larocque, Denis and Ben-Ameur, Hatem},
  language     = {en},
  publisher    = {The American Statistical Association, the Bernoulli Society, the Institute of Mathematical Statistics, and the Statistical Society of Canada},
  url          = {https://projecteuclid.org:443/euclid.ssu/1315833185},
  date         = {2011},
  doi          = {10.1214/09-SS047},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Bou-Hamad, Larocque, Ben-Ameur - 2011 - A review of survival trees.pdf:pdf},
  issn         = {1935-7516},
  journaltitle = {Statist. Surv.},
  keywords     = {CART,Survival trees,bagging,discrete-time,ensemble methods,right-censored data,survival forest,time-varying covariate,time-varying effect},
  pages        = {44--71},
  title        = {{A review of survival trees}},
  volume       = {5},
}

@misc{pkgsurvauc,
  author = {Potapov, Sergej and Adler, Werner and Schmid, Matthias},
  date   = {2012},
  title  = {{survAUC: Estimators of prediction accuracy for time-to-event data.}},
}

@article{Boulesteix2013,
  abstract     = {In computational science literature including, e.g., bioinformatics, computational statistics or machine learning, most published articles are devoted to the development of “new methods”, while comparison studies are generally appreciated by readers but surprisingly given poor consideration by many journals. This paper stresses the importance of neutral comparison studies for the objective evaluation of existing methods and the establishment of standards by drawing parallels with clinical research. The goal of the paper is twofold. Firstly, we present a survey of recent computational papers on supervised classification published in seven high-ranking computational science journals. The aim is to provide an up-to-date picture of current scientific practice with respect to the comparison of methods in both articles presenting new methods and articles focusing on the comparison study itself. Secondly, based on the results of our survey we critically discuss the necessity, impact and limitations of neutral comparison studies in computational sciences. We define three reasonable criteria a comparison study has to fulfill in order to be considered as neutral, and explicate general considerations on the individual components of a “tidy neutral comparison study”. R codes for completely replicating our statistical analyses and figures are available from the companion website http://www.ibe.med.uni-muenchen.de/organisation/mitarbeiter/020_professuren/boulesteix/plea2013.},
  author       = {Boulesteix, Anne-Laure and Lauer, Sabine and Eugster, Manuel J A},
  publisher    = {Public Library of Science},
  url          = {https://doi.org/10.1371/journal.pone.0061562},
  date         = {2013-04},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Boulesteix, Lauer, Eugster - 2013 - A Plea for Neutral Comparison Studies in Computational Sciences.pdf:pdf},
  journaltitle = {PLOS ONE},
  number       = {4},
  pages        = {e61562},
  title        = {{A Plea for Neutral Comparison Studies in Computational Sciences}},
  volume       = {8},
}

@article{Kiraly2018d,
  abstract     = {Objective: To determine the completeness of argumentative steps necessary to conclude effectiveness of an algorithm in a sample of current ML/AI supervised learning literature. Data Sources: Papers published in the Neural Information Processing Systems (NeurIPS, née NIPS) journal where the official record showed a 2017 year of publication. Eligibility Criteria: Studies reporting a (semi-)supervised model, or pre-processing fused with (semi-)supervised models for tabular data. Study Appraisal: Three reviewers applied the assessment criteria to determine argumentative completeness. The criteria were split into three groups, including: experiments (e.g real and/or synthetic data), baselines (e.g uninformed and/or state-of-art) and quantitative comparison (e.g. performance quantifiers with confidence intervals and formal comparison of the algorithm against baselines). Results: Of the 121 eligible manuscripts (from the sample of 679 abstracts), 99\% used real-world data and 29\% used synthetic data. 91\% of manuscripts did not report an uninformed baseline and 55\% reported a state-of-art baseline. 32\% reported confidence intervals for performance but none provided references or exposition for how these were calculated. 3\% reported formal comparisons. Limitations: The use of one journal as the primary information source may not be representative of all ML/AI literature. However, the NeurIPS conference is recognised to be amongst the top tier concerning ML/AI studies, so it is reasonable to consider its corpus to be representative of high-quality research. Conclusion: Using the 2017 sample of the NeurIPS supervised learning corpus as an indicator for the quality and trustworthiness of current ML/AI research, it appears that complete argumentative chains in demonstrations of algorithmic effectiveness are rare.},
  author       = {Király, Franz J and Mateen, Bilal and Sonabend, Raphael},
  url          = {http://arxiv.org/abs/1812.07519},
  date         = {2018-12},
  eprint       = {1812.07519},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Kir{\'{a}}ly, Mateen, Sonabend - 2018 - NIPS - Not Even Wrong A Systematic Review of Empirically Complete Demonstrations of Algorithmic Effec.pdf:pdf},
  journaltitle = {arXiv},
  title        = {{NIPS - Not Even Wrong? A Systematic Review of Empirically Complete Demonstrations of Algorithmic Effectiveness in the Machine Learning and Artificial Intelligence Literature}},
}

@book{pkgggplot2,
  author    = {Wickham, Hadley},
  publisher = {Springer-Verlag New York},
  url       = {https://ggplot2.tidyverse.org},
  date      = {2016},
  isbn      = {978-3-319-24277-4},
  title     = {{ggplot2: Elegant Graphics for Data Analysis}},
}

@article{CortesVapnik1995,
  author       = {Cortes, Corinna and Vapnik, Vladimir},
  date         = {1995},
  doi          = {10.1007/BF00994018},
  eprint       = {arXiv:1011.1669v3},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Cortes, Vapnik - 1995 - Support-Vector Networks.pdf:pdf},
  isbn         = {0885-6125},
  issn         = {08856125},
  journaltitle = {Machine Learning},
  keywords     = {efficient learning algorithms,machine learning,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers,support vector machine,svc,svm},
  pages        = {273--297},
  title        = {{Support-Vector Networks}},
  volume       = {20},
}

@article{pkgsets,
  author       = {Meyer, David and Hornik, Kurt},
  date         = {2009},
  doi          = {10.18637/jss.v031.i02},
  journaltitle = {Journal of Statistical Software},
  number       = {2},
  pages        = {1--27},
  title        = {{Generalized and Customizable Sets in R}},
  volume       = {31},
}

@article{Demsar2006,
  author       = {Demšar, Janez},
  date         = {2006},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Dem{\v{s}}ar - 2006 - Statistical comparisons of classifiers over multiple data sets.pdf:pdf},
  journaltitle = {Journal of Machine learning research},
  number       = {1},
  pages        = {1--30},
  title        = {{Statistical comparisons of classifiers over multiple data sets}},
  volume       = {7},
}

@article{Bergmeir2012,
  author       = {Bergmeir, Christoph and Benítez, José M},
  publisher    = {Elsevier Inc.},
  url          = {http://dx.doi.org/10.1016/j.ins.2011.12.028},
  date         = {2012},
  doi          = {10.1016/j.ins.2011.12.028},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Bergmeir, Ben{\'{i}}tez - 2012 - On the use of cross-validation for time series predictor evaluation.pdf:pdf},
  issn         = {0020-0255},
  journaltitle = {Information Sciences},
  pages        = {192--213},
  title        = {{On the use of cross-validation for time series predictor evaluation}},
  volume       = {191},
}

@article{LeBlanc1993,
  abstract     = {[A tree-based method for censored survival data is developed, based on maximizing the difference in survival between groups of patients represented by nodes in a binary tree. The method includes a pruning algorithm with optimal properties analogous to the classification and regression tree (CART) pruning algorithm. Uniform convergence of the estimates of the conditional cumulative hazard and survival functions is discussed, and an example is given to show the utility of the algorithm for developing prognostic classifications for patients.]},
  author       = {LeBlanc, Michael and Crowley, John},
  publisher    = {[American Statistical Association, Taylor & Francis, Ltd.]},
  url          = {http://www.jstor.org/stable/2290325},
  date         = {1993-04},
  doi          = {10.2307/2290325},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/LeBlanc, Crowley - 1993 - Survival Trees by Goodness of Split.pdf:pdf},
  issn         = {01621459},
  journaltitle = {Journal of the American Statistical Association},
  number       = {422},
  pages        = {457--467},
  title        = {{Survival Trees by Goodness of Split}},
  volume       = {88},
}

@article{Dawid2014,
  author       = {Dawid, A Philip and Musio, Monica},
  date         = {2014},
  eprint       = {arXiv:1401.0398v1},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Dawid, Musio - 2014 - Theory and Applications of Proper Scoring Rules.pdf:pdf},
  journaltitle = {Metron},
  number       = {2},
  pages        = {169--183},
  title        = {{Theory and Applications of Proper Scoring Rules}},
  volume       = {72},
}

@article{Chen2014,
  abstract     = {Numerous cancer studies have combined gene expression experiments and clinical survival data to predict the prognosis of patients of specific gene types. However, most results of these studies were data dependent and were not suitable for other data sets. This study performed cross-laboratory validations for the cancer patient data from 4 hospitals. We investigated the feasibility of survival risk predictions using high-throughput gene expression data and clinical data. We analyzed multiple data sets for prognostic applications in lung cancer diagnosis. After building tens of thousands of various ANN architectures using the training data, five survival-time correlated genes were identified from 4 microarray gene expression data sets by examining the correlation between gene signatures and patient survival time. The experimental results showed that gene expression data can be used for valid predictions of cancer patient survival classification with an overall accuracy of 83.0% based on survival time trusted data. The results show the prediction model yielded excellent predictions given that patients in the high-risk group obtained a lower median overall survival compared with low-risk patients (log-rank test P-value<0.00001). This study provides a foundation for further clinical studies and research into other types of cancer. We hope these findings will improve the prognostic methods of cancer patients.},
  author       = {Chen, Yen-Chen and Ke, Wan-Chi and Chiu, Hung-Wen},
  url          = {http://www.sciencedirect.com/science/article/pii/S0010482514000377},
  date         = {2014},
  doi          = {https://doi.org/10.1016/j.compbiomed.2014.02.006},
  issn         = {0010-4825},
  journaltitle = {Computers in Biology and Medicine},
  keywords     = {Gene expression,Lung cancer,Machine learning,Microarray,Neural network,Outcome prediction,Survival analysis},
  pages        = {1--7},
  title        = {{Risk classification of cancer survival using ANN with gene expression data from multiple laboratories}},
  volume       = {48},
}

@article{Mittelstadt2017,
  abstract     = {The conjunction of wireless computing, ubiquitous Internet access, and the miniaturisation of sensors have opened the door for technological applications that can monitor health and well-being outside of formal healthcare systems. The health-related Internet of Things (H-IoT) increasingly plays a key role in health management by providing real-time tele-monitoring of patients, testing of treatments, actuation of medical devices, and fitness and well-being monitoring. Given its numerous applications and proposed benefits, adoption by medical and social care institutions and consumers may be rapid. However, a host of ethical concerns are also raised that must be addressed. The inherent sensitivity of health-related data being generated and latent risks of Internet-enabled devices pose serious challenges. Users, already in a vulnerable position as patients, face a seemingly impossible task to retain control over their data due to the scale, scope and complexity of systems that create, aggregate, and analyse personal health data. In response, the H-IoT must be designed to be technologically robust and scientifically reliable, while also remaining ethically responsible, trustworthy, and respectful of user rights and interests. To assist developers of the H-IoT, this paper describes nine principles and nine guidelines for ethical design of H-IoT devices and data protocols.},
  author       = {Mittelstadt, Brent},
  date         = {2017},
  doi          = {10.3390/info8030077},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Mittelstadt - 2017 - Designing the health-related internet of things Ethical principles and guidelines.pdf:pdf},
  isbn         = {0141-9889},
  issn         = {20782489},
  journaltitle = {Information (Switzerland)},
  keywords     = {Data analytics,Data ethics,Health,Internet of things,Responsible research and innovation},
  number       = {3},
  pages        = {1--25},
  title        = {{Designing the health-related internet of things: Ethical principles and guidelines}},
  volume       = {8},
}

@article{Harrell1982,
  abstract     = {A method is presented for evaluating the amount of information a medical test provides about individual patients. Emphasis is placed on the role of a test in the evaluation of patients with a chronic disease. In this context, the yield of a test is best interpreted by analyzing the prognostic information it furnishes. Information from the history, physical examination, and routine procedures should be used in assessing the yield of a new test. As an example, the method is applied to the use of the treadmill exercise test in evaluating the prognosis of patients with suspected coronary artery disease. The treadmill test is shown to provide surprisingly little prognostic information beyond that obtained from basic clinical measurements.(JAMA 1982;247:2543-2546)},
  author       = {Harrell, Frank E. and Califf, Robert M. and Pryor, David B.},
  url          = {http://dx.doi.org/10.1001/jama.1982.03320430047030},
  annotation   = {10.1001/jama.1982.03320430047030},
  date         = {1982-05},
  issn         = {0098-7484},
  journaltitle = {JAMA},
  number       = {18},
  pages        = {2543--2546},
  title        = {{Evaluating the yield of medical tests}},
  volume       = {247},
}

@misc{DSG2020,
  author    = {{Data Study Group team}},
  publisher = {Zenodo},
  url       = {http://doi.org/10.5281/zenodo.3670726},
  date      = {2020},
  title     = {{Data Study Group Final Report: Great Ormond Street Hospital.}},
}

@inproceedings{Ling1998,
  abstract  = {Direct marketing is a process of identifying likely buyers of certain products and promoting the products accordingly.},
  author    = {Ling, Charles X. and Li, Chenghui},
  booktitle = {Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining},
  date      = {1998},
  doi       = {10.1126/science.48.1235.216},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Ling, Li - 1998 - Data Mining for Direct Marketing Problems and Solutions.pdf:pdf},
  issn      = {00368075},
  pages     = {73--79},
  title     = {{Data Mining for Direct Marketing: Problems and Solutions}},
}

@article{Jing2019,
  abstract     = {Survival analyses of populations and the establishment of prognoses for individual patients are important activities in the practice of medicine. Standard survival models, such as the Cox proportional hazards model, require extensive feature engineering or prior knowledge to model at an individual level. Some survival analysis models can avoid these problems by using machine learning extended the CPH model, and higher performance has been reported. In this paper, we propose an innovative loss function that is defined as the sum of an extended mean squared error loss and a pairwise ranking loss based on ranking information on survival data. We apply this loss function to optimize a deep feed-forward neural network (RankDeepSurv), which can be used to model survival data. We demonstrate that the performance of our model, RankDeepSurv, is superior to that of other state-of-the-art survival models based on an analysis of 4 public medical clinical datasets. When modelling the prognosis of nasopharyngeal carcinoma (NPC), RankDeepSurv achieved better prognostic accuracy than the CPH established by clinical experts. The difference between high and low risk groups in the RankDeepSurv model is greater than the difference in the CPH. The results show that our method has considerable potential to model survival data in medical settings.},
  author       = {Jing, Bingzhong and Zhang, Tao and Wang, Zixian and Jin, Ying and Liu, Kuiyuan and Qiu, Wenze and Ke, Liangru and Sun, Ying and He, Caisheng and Hou, Dan and Tang, Linquan and Lv, Xing and Li, Chaofeng},
  url          = {http://www.sciencedirect.com/science/article/pii/S0933365718305992},
  date         = {2019},
  doi          = {https://doi.org/10.1016/j.artmed.2019.06.001},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Jing et al. - 2019 - A deep survival analysis method based on ranking.pdf:pdf},
  issn         = {0933-3657},
  journaltitle = {Artificial Intelligence in Medicine},
  keywords     = {Nasopharyngeal carcinoma,Neural networks,Prognosis,Survival analysis},
  pages        = {1--9},
  title        = {{A deep survival analysis method based on ranking}},
  volume       = {98},
}

@article{Zhu2017,
  author       = {Zhu, Bing and Baesens, Bart and Seppe, K L M},
  publisher    = {Elsevier Inc.},
  date         = {2017},
  doi          = {10.1016/j.ins.2017.04.015},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Zhu, Baesens, Seppe - 2017 - An empirical comparison of techniques for the class imbalance problem in churn prediction.pdf:pdf},
  journaltitle = {Information Sciences},
  keywords     = {Churn prediction,Class imbalance,Benchmark experim},
  pages        = {84--99},
  title        = {{An empirical comparison of techniques for the class imbalance problem in churn prediction}},
  volume       = {408},
}

@article{Manson2007,
  author       = {Manson, N C and O'Neill, O},
  date         = {2007-01},
  doi          = {10.1017/CBO9780511814600},
  journaltitle = {Rethinking Informed Consent in Bioethics},
  pages        = {1--212},
  title        = {{Rethinking Informed Consent in Bioethics}},
}

@article{Mariani1997,
  abstract     = {The purpose of the present study was toassess prognostic factors for metachronous contralateral recurrence ofbreast cancer (CBC). Two factors were of particularinterest, namely estrogen (ER) and progesterone (PgR) receptorsassayed with the biochemical method in primary tumortissue. Information was obtained from a prospective clinicaldatabase for 1763 axillary node-negative women who hadreceived curative surgery, mostly of the conservative type,and followed-up for a median of 82 months.The analysis was performed based on both astandard (linear) Cox model and an artificial neuralnetwork (ANN) extension of this model proposed byFaraggi and Simon [9]. Furthermore, to assess theprognostic importance of the factors considered, model predictiveability was computed.},
  author       = {Mariani, L and Coradini, D and Biganzoli, E and Boracchi, P and Marubini, E and Pilotti, S and Salvadori, B and Silvestrini, R and Veronesi, U and Zucali, R and Rilke, F},
  url          = {https://doi.org/10.1023/A:1005765403093},
  date         = {1997},
  doi          = {10.1023/A:1005765403093},
  issn         = {1573-7217},
  journaltitle = {Breast Cancer Research and Treatment},
  number       = {2},
  pages        = {167--178},
  title        = {{Prognostic factors for metachronous contralateral breast cancer: A comparison of the linear Cox regression model and its artificial neural network extension}},
  volume       = {44},
}

@article{Nadeau2003,
  abstract     = {In order to compare learning algorithms, experimental results reported in the machine learning literature often use statistical tests of significance to support the claim that a new learning algorithm generalizes better. Such tests should take into account the variability due to the choice of training set and not only that due to the test examples, as is often the case. This could lead to gross underestimation of the variance of the cross-validation estimator, and to the wrong conclusion that the new algorithm is significantly better when it is not. We perform a theoretical investigation of the variance of a variant of the cross-validation estimator of the generalization error that takes into account the variability due to the randomness of the training set as well as test examples. Our analysis shows that all the variance estimators that are based only on the results of the cross-validation experiment must be biased. This analysis allows us to propose new estimators of this variance. We show, via simulations, that tests of hypothesis about the generalization error using those new variance estimators have better properties than tests involving variance estimators currently in use and listed in Dietterich (1998). In particular, the new tests have correct size and good power. That is, the new tests do not reject the null hypothesis too often when the hypothesis is true, but they tend to frequently reject the null hypothesis when the latter is false.},
  author       = {Nadeau, Claude and Bengio, Yoshua},
  url          = {https://doi.org/10.1023/A:1024068626366},
  date         = {2003},
  doi          = {10.1023/A:1024068626366},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Nadeau, Bengio - 2003 - Inference for the Generalization Error.pdf:pdf},
  issn         = {1573-0565},
  journaltitle = {Machine Learning},
  number       = {3},
  pages        = {239--281},
  title        = {{Inference for the Generalization Error}},
  volume       = {52},
}

@article{LeBlanc1992,
  abstract     = {[A method is developed for obtaining tree-structured relative risk estimates for censored survival data. The first step of a full likelihood estimation procedure is used in a recursive partitioning algorithm that adopts most aspects of the widely used Classification and Regression Tree (CART) algorithm of Breiman et al. (1984, Classification and Regression Trees, Belmont, California: Wadsworth). The performance of the technique is investigated through simulation and compared to the tree-structured survival methods proposed by Davis and Anderson (1989, Statistics in Medicine 8, 947-961) and Therneau, Grambsch, and Fleming (1990, Biometrika 77, 147-160).]},
  author       = {LeBlanc, Michael and Crowley, John},
  publisher    = {[Wiley, International Biometric Society]},
  url          = {http://www.jstor.org/stable/2532300},
  date         = {1992},
  doi          = {10.2307/2532300},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/LeBlanc, Crowley - 1992 - Relative Risk Trees for Censored Survival Data.pdf:pdf},
  issn         = {0006341X, 15410420},
  journaltitle = {Biometrics},
  keywords     = {PH,machine learning,random forests,survival,survival forests,survival trees},
  number       = {2},
  pages        = {411--425},
  title        = {{Relative Risk Trees for Censored Survival Data}},
  volume       = {48},
}

@book{datamelanoma,
  author    = {Andersen, Per K and Borgan, Ornulf and Gill, Richard D and Keiding, Niels},
  publisher = {Springer Science & Business Media},
  date      = {2012},
  isbn      = {1461243483},
  title     = {{Statistical models based on counting processes}},
}

@article{Crombe2021,
  abstract     = {PURPOSENeoadjuvant chemotherapy (NAC) has been increasingly used in patients with locally advanced high-risk soft tissue sarcomas in the past decade, but definition and prognostic impact of a good histologic response (GHR) are lacking. Our aim was to investigate which histologic feature from the post-NAC surgical specimen independently correlated with metastatic relapse-free survival (MFS) in combination with clinical, radiologic, and pathologic features using a machine learning approach.METHODSThis retrospective study included 175 consecutive patients (median age: 59 years, 75 women) with resectable disease, treated with anthracycline-based NAC between 1989 and 2015 in our sarcoma reference center, and with quantitative histopathologic analysis of the surgical specimen. The outcome of interest was the MFS. A multimodel, multivariate survival analysis was used to define GHR. The added prognostic value of GHR was investigated through the comparisons with the standard model (including histologic grade, size, and depth) and SARCULATOR nomogram using concordance indices (c-index) and Monte-Carlo cross-validation.RESULTSSeventy-two patients (72 of 175, 41.1%) had a metastatic relapse. Stepwise Cox regression, random survival forests, and least absolute shrinkage and selection operator?penalized Cox regression all converged toward the same definition for GHR, ie, < 5% stainable tumor cells. The five-year MFS probability was 1 (95% CI, 1 to 1) in patients with GHR versus 0.73 (95% CI, 0.65 to 0.81) in patients without GHR (log-rank P = .0122). The final prognostic model incorporating the GHR was significantly better than the standard model and SARCULATOR (average c-index in testing sets = 0.72 [95% CI, 0.61 to 0.82] v 0.57 [95% CI, 0.44 to 0.70] and 0.54 [95% CI, 0.45 to 0.64], respectively; P = .0414 and .0091).CONCLUSIONHistologic response to NAC improves the prediction of MFS in patients with soft tissue sarcoma and represents a possible end point in future studies exploring innovative regimens in the neoadjuvant setting.},
  author       = {Crombé, Amandine and Cousin, Sophie and Spalato-Ceruso, Mariella and {Le Loarer}, François and Toulmonde, Maud and Michot, Audrey and Kind, Michèle and Stoeckle, Eberhard and Italiano, Antoine},
  publisher    = {Wolters Kluwer},
  url          = {https://doi.org/10.1200/CCI.21.00062},
  annotation   = {doi: 10.1200/CCI.21.00062},
  date         = {2021-09},
  doi          = {10.1200/CCI.21.00062},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Cromb{\'{e}} et al. - 2021 - Implementing a Machine Learning Strategy to Predict Pathologic Response in Patients With Soft Tissue Sarcomas Tr.pdf:pdf},
  journaltitle = {JCO Clinical Cancer Informatics},
  number       = {5},
  pages        = {958--972},
  title        = {{Implementing a Machine Learning Strategy to Predict Pathologic Response in Patients With Soft Tissue Sarcomas Treated With Neoadjuvant Chemotherapy}},
}

@article{Binder2008,
  abstract     = {When predictive survival models are built from high-dimensional data, there are often additional covariates, such as clinical scores, that by all means have to be included into the final model. While there are several techniques for the fitting of sparse high-dimensional survival models by penalized parameter estimation, none allows for explicit consideration of such mandatory covariates.},
  author       = {Binder, Harald and Schumacher, Martin},
  url          = {https://doi.org/10.1186/1471-2105-9-14},
  date         = {2008},
  doi          = {10.1186/1471-2105-9-14},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Binder, Schumacher - 2008 - Allowing for mandatory covariates in boosting estimation of sparse high-dimensional survival models.pdf:pdf},
  issn         = {1471-2105},
  journaltitle = {BMC Bioinformatics},
  keywords     = {boosting,cox,ensemble,likelihood,ml,ph,survival},
  number       = {1},
  pages        = {14},
  title        = {{Allowing for mandatory covariates in boosting estimation of sparse high-dimensional survival models}},
  volume       = {9},
}

@article{Gneiting2007,
  author       = {Gneiting, Tilmann and Raftery, Adrian E},
  url          = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
  date         = {2007-03},
  doi          = {10.1198/016214506000001437},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Gneiting, Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Estimation.pdf:pdf},
  issn         = {0162-1459},
  journaltitle = {Journal of the American Statistical Association},
  keywords     = {bayes factor,bregman divergence,brier score,coherent,continuous ranked probability score,cross-validation,distribution,entropy,kernel score,loss function,minimum contrast estimation,negative definite function,prediction interval,predictive,quantile forecast,scoring rule,skill score,strictly proper,utility function},
  number       = {477},
  pages        = {359--378},
  title        = {{Strictly Proper Scoring Rules, Prediction, and Estimation}},
  volume       = {102},
}

@book{Urquizo2019,
  author = {Urquizo, Javier and Calderon, Carlos and James, Philip},
  date   = {2019-05},
  doi    = {10.18687/LACCEI2019.1.1.1},
  file   = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Urquizo, Calderon, James - 2019 - A Taxonomy of key uncertainties using high level frameworks for energy modelling.pdf:pdf},
  title  = {{A Taxonomy of key uncertainties using high level frameworks for energy modelling}},
}

@article{Rafi2017,
  author       = {Rafi, Imran and Freeman, Alexandra and Marshall, Martin and Treadwell, Julian and Spiegelhalter, David},
  url          = {http://www.embase.com/search/results?subaction=viewrecord&from=export&id=L617643810%0Ahttp://dx.doi.org/10.3399/bjgp17X691793},
  date         = {2017},
  doi          = {10.3399/bjgp17X691793},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Rafi et al. - 2017 - Communicating risk in primary care What the Academy of Medical Sciences' report means in practice.pdf:pdf},
  issn         = {0960-1643},
  journaltitle = {British Journal of General Practice},
  keywords     = {cardiovascular risk,clinical research,editorial,general practice,health care,health care personnel,health care policy,health practitioner,human,medical education,medical school,medicine,multiple chronic conditions,patient information,practice guideline,primary medical care,shared decision making},
  number       = {661},
  pages        = {346--347},
  title        = {{Communicating risk in primary care: What the Academy of Medical Sciences' report means in practice}},
  volume       = {67},
}

@article{Sashegyi2017,
  abstract     = {This brief communication will clarify the difference between a relative hazard and a relative risk. We highlight the importance of this difference, and demonstrate in practical terms that 1 minus the hazard ratio should not be interpreted as a risk reduction in the commonly understood sense of the term. This article aims to provide a better understanding of the type of risk reduction that a hazard ratio implies, thereby clarifying the intent in the communication among practitioners and researchers and establishing an accurate and realistic foundation for communicating with patients. The Oncologist 2017;22:484-486.},
  author       = {Sashegyi, Andreas and Ferry, David},
  language     = {eng},
  publisher    = {AlphaMed Press},
  url          = {https://pubmed.ncbi.nlm.nih.gov/28314839 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5388384/},
  date         = {2017-04},
  doi          = {10.1634/theoncologist.2016-0198},
  edition      = {2017/03/17},
  issn         = {1549-490X},
  journaltitle = {The oncologist},
  keywords     = {*Proportional Hazards Models,Communication,Humans,Neoplasms/*epidemiology,Risk},
  number       = {4},
  pages        = {484--486},
  title        = {{On the Interpretation of the Hazard Ratio and Communication of Survival Benefit}},
  volume       = {22},
}

@article{Bewick2021,
  abstract     = {As we close in on one year since the COVID-19 pandemic began, hope has been placed on bringing the virus under control through mass administration of recently developed vaccines. Unfortunately, newly emerged, fast-spreading strains of COVID-19 threaten to undermine progress by interfering with vaccine efficacy. While a long-term solution to this challenge would be to develop vaccines that simultaneously target multiple different COVID-19 variants, this approach faces both developmental and regulatory hurdles. A simpler option would be to switch the target of the current vaccine to better match the newest viral variant. I use a stochastic simulation to determine when it is better to target a newly emerged viral variant and when it is better to target the dominant but potentially less transmissible strain. My simulation results suggest that it is almost always better to target the faster spreading strain, even when the initial prevalence of this variant is much lower. In scenarios where targeting the slower spreading variant is best, all vaccination strategies perform relatively well, meaning that the choice of vaccination strategy has a small effect on public health outcomes. In scenarios where targeting the faster spreading variant is best, use of vaccines against the faster spreading viral variant can save many lives. My results provide ‘rule of thumb' guidance for those making critical decisions about vaccine formulation over the coming months.Competing Interest StatementThe authors have declared no competing interest.Funding StatementFunding for this work was provided as part of Sharon Bewick&#039;s start-up package by The Biological Sciences Department at Clemson UniversityAuthor DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesThe details of the IRB/oversight body that provided approval or exemption for the research described are given below:n/aAll necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.YesPython code for model simulations is provided at: https://github.com/bewicklab/COVID-Vaccination-Strategies https://github.com/bewicklab/COVID-Vaccination-Strategies},
  author       = {Bewick, Sharon},
  url          = {http://medrxiv.org/content/early/2021/01/06/2021.01.05.21249255.abstract},
  date         = {2021-01},
  doi          = {10.1101/2021.01.05.21249255},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Bewick - 2021 - Viral Variants and Vaccinations If We Can Change the COVID-19 Vaccine{\ldots} Should We.pdf:pdf},
  journaltitle = {medRxiv},
  pages        = {2021.01.05.21249255},
  title        = {{Viral Variants and Vaccinations: If We Can Change the COVID-19 Vaccine{\ldots} Should We?}},
}

@article{Buhlmann2006,
  abstract     = {We prove that boosting with the squared error loss, L<sub>2</sub>Boosting, is consistent for very high-dimensional linear models, where the number of predictor variables is allowed to grow essentially as fast as O(exp(sample size)), assuming that the true underlying regression function is sparse in terms of the <sub>1</sub>-norm of the regression coefficients. In the language of signal processing, this means consistency for de-noising using a strongly overcomplete dictionary if the underlying signal is sparse in terms of the <sub>1</sub>-norm. We also propose here an AIC-based method for tuning, namely for choosing the number of boosting iterations. This makes L<sub>2</sub>Boosting computationally attractive since it is not required to run the algorithm multiple times for cross-validation as commonly used so far. We demonstrate L<sub>2</sub>Boosting for simulated data, in particular where the predictor dimension is large in comparison to sample size, and for a difficult tumor-classification problem with gene expression microarray data.},
  author       = {Buhlmann, Peter},
  language     = {en},
  publisher    = {The Institute of Mathematical Statistics},
  url          = {https://projecteuclid.org:443/euclid.aos/1151418234},
  date         = {2006},
  doi          = {10.1214/009053606000000092},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Buhlmann - 2006 - Boosting for high-dimensional linear models.pdf:pdf},
  issn         = {0090-5364},
  journaltitle = {Ann. Statist.},
  keywords     = {Binary classification,Lasso,gene expression,matching pursuit,overcomplete dictionary,sparsity,variable selection,weak greedy algorithm},
  number       = {2},
  pages        = {559--583},
  title        = {{Boosting for high-dimensional linear models}},
  volume       = {34},
}

@incollection{Ripley2001,
  author    = {Ripley, Brian D and Ripley, Ruth M},
  editor    = {Dybowski, Richard and Gant, Vanya},
  location  = {Cambridge},
  publisher = {Cambridge University Press},
  url       = {https://www.cambridge.org/core/books/clinical-applications-of-artificial-neural-networks/neural-networks-as-statistical-methods-in-survival-analysis/6AC01B644586FE2EF1D34B6A59CC183E},
  booktitle = {Clinical Applications of Artificial Neural Networks},
  date      = {2001},
  doi       = {DOI: 10.1017/CBO9780511543494.011},
  isbn      = {9780521662710},
  pages     = {237--255},
  title     = {{Neural networks as statistical methods in survival analysis}},
}

@article{Sutherland2012,
  abstract     = {The need for policy makers to understand science and for scientists to understand policy processes is widely recognised. However, the science-policy relationship is sometimes difficult and occasionally dysfunctional; it is also increasingly visible, because it must deal with contentious issues, or itself becomes a matter of public controversy, or both. We suggest that identifying key unanswered questions on the relationship between science and policy will catalyse and focus research in this field. To identify these questions, a collaborative procedure was employed with 52 participants selected to cover a wide range of experience in both science and policy, including people from government, non-governmental organisations, academia and industry. These participants consulted with colleagues and submitted 239 questions. An initial round of voting was followed by a workshop in which 40 of the most important questions were identified by further discussion and voting. The resulting list includes questions about the effectiveness of science-based decision-making structures; the nature and legitimacy of expertise; the consequences of changes such as increasing transparency; choices among different sources of evidence; the implications of new means of characterising and representing uncertainties; and ways in which policy and political processes affect what counts as authoritative evidence. We expect this exercise to identify important theoretical questions and to help improve the mutual understanding and effectiveness of those working at the interface of science and policy.},
  author       = {Sutherland, William J. and Bellingan, Laura and Bellingham, Jim R. and Blackstock, Jason J. and Bloomfield, Robert M. and Bravo, Michael and Cadman, Victoria M. and Cleevely, David D. and Clements, Andy and Cohen, Anthony S. and Cope, David R. and Daemmrich, Arthur A. and Devecchi, Cristina and Anadon, Laura Diaz and Denegri, Simon and Doubleday, Robert and Dusic, Nicholas R. and Evans, Robert J. and Feng, Wai Y. and Godfray, H. Charles J. and Harris, Paul and Hartley, Sue E. and Hester, Alison J. and Holmes, John and Hughes, Alan and Hulme, Mike and Irwin, Colin and Jennings, Richard C. and Kass, Gary S. and Littlejohns, Peter and Marteau, Theresa M. and McKee, Glenn and Millstone, Erik P. and Nuttall, William J. and Owens, Susan and Parker, Miles M. and Pearson, Sarah and Petts, Judith and Ploszek, Richard and Pullin, Andrew S. and Reid, Graeme and Richards, Keith S. and Robinson, John G. and Shaxson, Louise and Sierra, Leonor and Smith, Beck G. and Spiegelhalter, David J. and Stilgoe, Jack and Stirling, Andy and Tyler, Christopher P. and Winickoff, David E. and Zimmern, Ron L.},
  date         = {2012},
  doi          = {10.1371/journal.pone.0031824},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Sutherland et al. - 2012 - A collaboratively-derived science-policy research agenda.pdf:pdf},
  isbn         = {1932-6203 (Electronic)1̊932-6203 (Linking)},
  issn         = {19326203},
  journaltitle = {PLoS ONE},
  number       = {3},
  pages        = {3--7},
  title        = {{A collaboratively-derived science-policy research agenda}},
  volume       = {7},
}

@inproceedings{Fernandez2016a,
  abstract   = {We introduce a semi-parametric Bayesian model for survival analysis. The model is centred on a parametric baseline hazard, and uses a Gaussian process to model variations away from it nonparametrically, as well as dependence on covariates. As opposed to many other methods in survival analysis, our framework does not impose unnecessary constraints in the hazard rate or in the survival function. Furthermore, our model handles left, right and interval censoring mechanisms common in survival analysis. We propose a MCMC algorithm to perform inference and an approximation scheme based on random Fourier features to make computations faster. We report experimental results on synthetic and real data, showing that our model performs better than competing models such as Cox proportional hazards, ANOVA-DDP and random survival forests.},
  author     = {Fernández, Tamara and Rivera, Nicolas Nicolás and Teh, Yee Whye},
  publisher  = {Curran Associates, Inc.},
  url        = {http://arxiv.org/abs/1611.00817 https://proceedings.neurips.cc/paper/2016/file/ef1e491a766ce3127556063d49bc2f98-Paper.pdf},
  booktitle  = {Advances in Neural Information Processing Systems},
  date       = {2016},
  eprint     = {1611.00817},
  eprinttype = {arXiv},
  file       = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Fern{\'{a}}ndez, Rivera, Teh - 2016 - Gaussian Processes for Survival Analysis.pdf:pdf},
  isbn       = {1053-0509},
  issn       = {10495258},
  keywords   = {gaussian process,gp,machine learning,survival},
  number     = {Nips},
  title      = {{Gaussian Processes for Survival Analysis}},
  volume     = {29},
}

@misc{KassRaftery1995,
  author    = {Kass, R. E. and Raftery, A. E.},
  booktitle = {Journal of the American Statistical Association},
  date      = {1995},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Kass, Raftery - 1995 - Bayes Factors.pdf:pdf},
  pages     = {773--795},
  title     = {{Bayes Factors}},
  volume    = {90},
}

@article{Rietschel2018,
  author       = {Rietschel, Carl and Yoon, Jinsung and van der Schaar, Mihaela},
  date         = {2018},
  journaltitle = {arXiv preprint arXiv:1811.09317},
  title        = {{Feature Selection for Survival Analysis with Competing Risks using Deep Learning}},
}

@article{Tibshirani1996,
  author       = {Tibshirani, Robert},
  date         = {1996},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Tibshirani - 1996 - Regression shrinkage and selection via the lasso.pdf:pdf},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords     = {autobiographical memory,depression,executive control,lasso,machine learning,overgeneral memory,regularization,shrinkage,working},
  number       = {1},
  pages        = {267--288},
  title        = {{Regression shrinkage and selection via the lasso}},
  volume       = {58},
}

@article{Mateen2018,
  abstract   = {Machine learning (ML), artificial intelligence (AI) and other modern statistical methods are providing new opportunities to operationalize previously untapped and rapidly growing sources of data for patient benefit. Whilst there is a lot of promising research currently being undertaken, the literature as a whole lacks: transparency; clear reporting to facilitate replicability; exploration for potential ethical concerns; and, clear demonstrations of effectiveness. There are many reasons for why these issues exist, but one of the most important that we provide a preliminary solution for here is the current lack of ML/AI- specific best practice guidance. Although there is no consensus on what best practice looks in this field, we believe that interdisciplinary groups pursuing research and impact projects in the ML/AI for health domain would benefit from answering a series of questions based on the important issues that exist when undertaking work of this nature. Here we present 20 questions that span the entire project life cycle, from inception, data analysis, and model evaluation, to implementation, as a means to facilitate project planning and post-hoc (structured) independent evaluation. By beginning to answer these questions in different settings, we can start to understand what constitutes a good answer, and we expect that the resulting discussion will be central to developing an international consensus framework for transparent, replicable, ethical and effective research in artificial intelligence (AI-TREE) for health.},
  author     = {Vollmer, Sebastian and Mateen, Bilal A. and Bohner, Gergo and Király, Franz J. and Ghani, Rayid and Jonsson, Pall and Cumbers, Sarah and Jonas, Adrian and McAllister, Katherine S. L. and Myles, Puja and Granger, David and Birse, Mark and Branson, Richard and Moons, Karel GM and Collins, Gary S and Ioannidis, John P. A. and Holmes, Chris and Hemingway, Harry},
  date       = {2018},
  eprint     = {1812.10404},
  eprinttype = {arXiv},
  file       = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Vollmer et al. - 2018 - Machine learning and AI research for Patient Benefit 20 Critical Questions on Transparency, Replicability, Ethic.pdf:pdf},
  title      = {{Machine learning and AI research for Patient Benefit: 20 Critical Questions on Transparency, Replicability, Ethics and Effectiveness}},
}

@article{Brostrom2008,
  author       = {Broström, Göran and Lindkvist, Marie},
  date         = {2008},
  doi          = {10.1080/03610910701884021},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Brostr{\"{o}}m, Lindkvist - 2008 - Partial partial likelihood.pdf:pdf},
  isbn         = {00063444},
  issn         = {03610918},
  journaltitle = {Communications in Statistics: Simulation and Computation},
  keywords     = {Breslow's method,Cox regression,Efron's method,Partial likelihood,Profile likelihood,Survival analysis,Tied data},
  number       = {4},
  pages        = {679--686},
  title        = {{Partial partial likelihood}},
  volume       = {37},
}

@book{Bishop2006,
  author    = {Bishop, Christopher M},
  publisher = {springer},
  date      = {2006},
  isbn      = {1493938436},
  title     = {{Pattern recognition and machine learning}},
}

@report{Therneau1994,
  author      = {Therneau, Terry and Sicks, JoRean and Bergstralh, Erik and Offord, Jan},
  institution = {Department of Health Sciences Research, Mayo Clinic},
  url         = {https://www.mayo.edu/research/documents/biostat-52pdf/doc-10027807},
  date        = {1994},
  title       = {{Technical Report Series No. 52: Expected Survival Based on Hazard Rates.}},
  type        = {techreport},
}

@article{Bakker2004,
  abstract     = {Abstract In this article we show that traditional Cox survival analysis can be improved upon when supplemented with sensible priors and analysed within a neural Bayesian framework. We demonstrate that the Bayesian method gives more reliable predictions, in particular for relatively small data sets. The obtained posterior (the probability distribution of network parameters given the data) which in itself is intractable, can be made accessible by several approximations. We review approximations by Hybrid Markov Chain Monte Carlo sampling, a variational method and the Laplace approximation. We argue that although each Bayesian approach circumvents the shortcomings of the original Cox analysis, and therefore yields better predictive results, in practice the use of variational methods or Laplace is preferable. Since Cox survival analysis is infamous for its poor results with (too) many inputs, we use the Bayesian posterior to estimate p-values on the inputs and to formulate an algorithm for backward elimination. We show that after removal of irrelevant inputs Bayesian methods still achieve significantly better results than classical Cox. Copyright ? 2004 John Wiley & Sons, Ltd.},
  author       = {Bakker, Bart and Heskes, Tom and Neijt, Jan and Kappen, Bert},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/sim.1904},
  annotation   = {doi: 10.1002/sim.1904},
  date         = {2004-10},
  doi          = {10.1002/sim.1904},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  number       = {19},
  pages        = {2989--3012},
  title        = {{Improving Cox survival analysis with a neural-Bayesian approach}},
  volume       = {23},
}

@article{Ciampi1986,
  abstract     = {The problem of defining prognostic groups on the basis of censored survival times and covariates is central in medical biostatistics. Several methods have been proposed, but little is known about their relative advantages. Here three methods are discussed: Stepwise Regression, Correspondence Analysis and Recursive Partition. The approach is empirical in that the focus is on the performance on real data sets. Our example is discussed at length. We find that Stepwise Regression has the advantage of flexibility and economy of description, but is limited in discovering interactions and other complex features of the data. Correspondence Analysis is equally flexible, though less economical, and is very powerful in revealing unexpected features of the data: it is recommended as an exploratory tool. Recursive Partition is efficient in discovering interactions within large data sets and has the advantage of being the only method that produces clear descriptions in direct clinical terms; its flexibility, however, is limited, especially when the number of covariates is large relative to the number of individuals. Since no method is universally preferable, their joint use is recommended. A variety of criteria for ranking stratifications are proposed when a choice to be made. {©} 1986.},
  author       = {Ciampi, Antonio and Thiffault, Johanne and Nakache, Jean Pierre and Asselain, Bernard},
  annotation   = {Survival trees},
  date         = {1986},
  doi          = {10.1016/0167-9473(86)90033-2},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Ciampi et al. - 1986 - Stratification by stepwise regression, correspondence analysis and recursive partition a comparison of three meth.pdf:pdf},
  issn         = {01679473},
  journaltitle = {Computational Statistics and Data Analysis},
  keywords     = {"$\beta$-Barycentric' representation,Akaike criterion,Amalgamation algorithm,Censored data,Log-rank,Survival curve,decision trees,machine learning,random forests,survival forests,survival trees},
  number       = {3},
  pages        = {185--204},
  title        = {{Stratification by stepwise regression, correspondence analysis and recursive partition: a comparison of three methods of analysis for survival data with covariates}},
  volume       = {4},
}

@article{dataflchain,
  abstract     = {OBJECTIVE: To determine whether the free light chain (FLC) assay provides prognostic information relevant to the general population. METHODS: After excluding persons with a known plasma cell disorder, we studied 15,859 Olmsted County, Minnesota, residents 50 years or older in whom unmasked data and samples for FLC testing were available. Baseline information was obtained between March 13, 1995, and November 21, 2003, and follow-up status and cause of death were identified through June 30, 2009. The $\kappa$ and $\lambda$ FLC sum ($\Sigma$ FLC) was evaluated for its ability to predict overall survival. Specific causes of death were also investigated. RESULTS: In 158,003 person-years of follow-up, 4348 individuals died. A high $\Sigma$ FLC was significantly predictive of worse overall survival; the risk ratio for death for those with the highest decile of $\Sigma$ FLC (ie, ≥ 4.72 mg/dL) was 4.4 (95% confidence interval, 4.1-4.7) relative to the remaining study participants. Multivariate analyses demonstrated that this excess risk of death was independent of age, sex, and renal insufficiency, with a corrected risk ratio of 2.1 (95% confidence interval, 1.9-2.2). The increased mortality was not restricted to any particular cause of death because the observed-to-expected risk of death from most causes was significantly higher among those individuals with an antecedent $\Sigma$ FLC of 4.72 mg/dL or higher, which is near the upper limit of normal for the test. CONCLUSION: A nonclonal elevation of $\Sigma$ FLC is a significant predictor of worse overall survival in the general population of persons without plasma cell disorders.},
  author       = {Dispenzieri, Angela and Katzmann, Jerry A and Kyle, Robert A and Larson, Dirk R and Therneau, Terry M and Colby, Colin L and Clark, Raynell J and Mead, Graham P and Kumar, Shaji and {Melton 3rd}, L Joseph and Rajkumar, S Vincent},
  language     = {eng},
  publisher    = {Mayo Foundation},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/22677072 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3538473/},
  date         = {2012-06},
  doi          = {10.1016/j.mayocp.2012.03.009},
  issn         = {1942-5546},
  journaltitle = {Mayo Clinic proceedings},
  keywords     = {*Mortality,80 and over,Aged,Cause of Death,Female,Humans,Immunoglobulin kappa-Chains/*blood,Immunoglobulin lambda-Chains/*blood,Male,Middle Aged,Minnesota/epidemiology,Multivariate Analysis,Prognosis},
  number       = {6},
  pages        = {517--523},
  title        = {{Use of nonclonal serum immunoglobulin free light chains to predict overall survival in the general population}},
  volume       = {87},
}

@article{datanki,
  author       = {{Van De Vijver}, Marc J and He, Yudong D and {Van't Veer}, Laura J and Dai, Hongyue and Hart, Augustinus A M and Voskuil, Dorien W and Schreiber, George J and Peterse, Johannes L and Roberts, Chris and Marton, Matthew J},
  publisher    = {Mass Medical Soc},
  date         = {2002},
  issn         = {0028-4793},
  journaltitle = {New England Journal of Medicine},
  number       = {25},
  pages        = {1999--2009},
  title        = {{A gene-expression signature as a predictor of survival in breast cancer}},
  volume       = {347},
}

@article{VanHouwelingen2000,
  abstract     = {The problem of assessing the validity and value of prognostic survival models presented in the literature for a particular population for which some data has been collected is discussed. Methods are sketched to perform validation through 'calibration', that is by embedding the literature model in a larger calibration model. This general approach is exemplified for x-year survival probabilities, Cox regression and general non-proportional hazards models. Some comments are made on basic structural changes to the model, described as 'revision'. Finally, general methods are discussed to combine models from different sources. The methods are illustrated with a model for non-Hodgkin's lymphoma validated on a Dutch data set.},
  author       = {{Van Houwelingen}, Hans C.},
  annotation   = {Referenced in Royston/Altman 2013. Discuss the calibration slope This seems like a messy approach and what's more he's then using a calibrated model after validation, which isn't validated in itself. This doesn't really seem advisable...},
  date         = {2000},
  doi          = {10.1002/1097-0258(20001230)19:24<3401::AID-SIM554>3.0.CO;2-2},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Van Houwelingen - 2000 - Validation, calibration, revision and combination of prognostic survival models.pdf:pdf},
  isbn         = {0277-6715 (Print)0̊277-6715 (Linking)},
  issn         = {02776715},
  journaltitle = {Statistics in Medicine},
  number       = {24},
  pages        = {3401--3415},
  title        = {{Validation, calibration, revision and combination of prognostic survival models}},
  volume       = {19},
}

@article{dataunemployment,
  author       = {McCall, B. P.},
  date         = {1996},
  journaltitle = {Econometrica},
  pages        = {647--682},
  title        = {{Unemployment Insurance Rules, Joblessness, and Part-time Work}},
  volume       = {64},
}

@article{EfronTibshirani1986,
  author       = {Efron, B. and Tibshirani, Robert},
  date         = {1986},
  doi          = {10.1214/ss/1177013815},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Efron, Tibshirani - 1986 - Boostrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy.pdf:pdf},
  issn         = {0883-4237},
  journaltitle = {Statistical Science},
  keywords     = {boostrap,confidence intervals,jackknife,standard error},
  number       = {1},
  pages        = {54--77},
  title        = {{Boostrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy}},
  volume       = {1},
}

@article{Han2018,
  abstract     = {Synovial sarcoma is a rare disease with diverse progression characteristics. We developed a novel deep-learning-based prediction algorithm for survival rates of synovial sarcoma patients. The purpose of this study is to evaluate the performance of the proposed prediction model and demonstrate its clinical usage. The study involved 242 patients who were diagnosed with synovial sarcoma in three institutions between March 2001 and February 2013. The patients were randomly divided into a training set (80%) and a testing set (20%). Fivefold cross validation was performed utilizing the training set. The test set was retained for the final testing. A Cox proportional hazard model, simple neural network, and the proposed survival neural network were all trained utilizing the same training set, and fivefold cross validation was performed. The final testing was performed utilizing the isolated test data to determine the best prediction model. The multivariate Cox proportional hazard regression analysis revealed that size, initial metastasis, and margin were independent prognostic factors. In fivefold cross validation, the median value of the receiver-operating characteristic curve (area under the curve) was 0.87 in the survival neural network, which is significantly higher compared to the area under the curve of 0.792 for the simple neural network (p?=?0.043). In the final test, survival neural network model showed the better performance (area under the curve: 0.814) compared to the Cox proportional hazard model (area under the curve: 0.629; p?=?0.0001). The survival neural network model predicted survival of synovial sarcoma patients more accurately compared to Cox proportional hazard model.},
  author       = {Han, Ilkyu and Kim, June Hyuk and Park, Heeseol and Kim, Han-Soo and Seo, Sung Wook},
  publisher    = {SAGE Publications Ltd STM},
  url          = {https://doi.org/10.1177/1010428318799264},
  annotation   = {classification},
  date         = {2018-09},
  doi          = {10.1177/1010428318799264},
  issn         = {1010-4283},
  journaltitle = {Tumor Biology},
  number       = {9},
  pages        = {1010428318799264},
  title        = {{Deep learning approach for survival prediction for patients with synovial sarcoma}},
  volume       = {40},
}

@article{Kohavi1995,
  author       = {Kohavi, Ron},
  date         = {1995},
  journaltitle = {Ijcai},
  number       = {2},
  pages        = {1137--1145},
  title        = {{A study of cross-validation and bootstrap for accuracy estimation and model selection}},
  volume       = {14},
}

@article{pkgsksurvival,
  author       = {Pölsterl, Sebastian},
  url          = {http://jmlr.org/papers/v21/20-729.html},
  date         = {2020},
  journaltitle = {Journal of Machine Learning Research},
  number       = {212},
  pages        = {1--6},
  title        = {{scikit-survival: A Library for Time-to-Event Analysis Built on Top of scikit-learn}},
  volume       = {21},
}

@article{RoystonParmar2002,
  abstract     = {Modelling of censored survival data is almost always done by Cox proportional-hazards regression. However, use of parametric models for such data may have some advantages. For example, nonproportional hazards, a potential difficulty with Cox models, may sometimes be handled in a simple way, and visualization of the hazard function is much easier. Extensions of the Weibull and log-logistic models are proposed in which natural cubic splines are used to smooth the baseline log cumulative hazard and log cumulative odds of failure functions. Further extensions to allow non-proportional effects of some or all of the covariates are introduced. A hypothesis test of the appropriateness of the scale chosen for covariate effects (such as of treatment) is proposed. The new models are applied to two data sets in cancer. The results throw interesting light on the behaviour of both the hazard function and the hazard ratio over time. The tools described here may be a step towards providing greater insight into the natural history of the disease and into possible underlying causes of clinical events. We illustrate these aspects by using the two examples in cancer. Copyright (C) 2002 John Wiley Sons, Ltd.},
  author       = {Royston, Patrick and Parmar, Mahesh K.B.},
  date         = {2002},
  doi          = {10.1002/sim.1203},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Royston, Parmar - 2002 - Flexible parametric proportional-hazards and proportional-odds models for censored survival data, with applicat.pdf:pdf},
  isbn         = {0277-6715},
  issn         = {02776715},
  journaltitle = {Statistics in Medicine},
  keywords     = {Parametric models,Proportional hazards,Proportional odds,Splines,Survival analysis},
  number       = {15},
  pages        = {2175--2197},
  title        = {{Flexible parametric proportional-hazards and proportional-odds models for censored survival data, with application to prognostic modelling and estimation of treatment effects}},
  volume       = {21},
}

@article{Ravdin1992,
  abstract     = {It has been previously shown that Neural Networks can be trained to recognize individual breast cancer patients at high and low risk for recurrent disease and death. This paper expands on the initial investigation and shows that by coding time as one of the prognostic variables, a Neural Network can use censored survival data to predict patient outcome over time. In this demonstration a Neural Network was trained, tested, and validated using censored survival data from a group of 1373 patients with node-positive breast cancer. The Neural Network method predicted patient outcome as accurately as Cox Regression modeling. The final Neural Network model can be presented with a patient's prognostic information and make a series of predictions about probability of relapse at different times of follow-up, allowing one to draw survival probability curves for individual patients.},
  author       = {Ravdin, Peter M. and Clark, Gary M.},
  date         = {1992},
  doi          = {10.1007/BF01840841},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Ravdin, Clark - 1992 - A practical application of neural network analysis for predicting outcome of individual breast cancer patients.pdf:pdf},
  issn         = {01676806},
  journaltitle = {Breast Cancer Research and Treatment},
  keywords     = {artificial intelligence,breast cancer,neural networks,prognostic factors},
  number       = {3},
  pages        = {285--293},
  title        = {{A practical application of neural network analysis for predicting outcome of individual breast cancer patients}},
  volume       = {22},
}

@article{Koziol2009,
  abstract     = {Harrell's c-index or concordance C has been widely used as a measure of separation of two survival distributions. In the absence of censored data, the c-index estimates the Mann-Whitney parameter Pr(X>Y), which has been repeatedly utilized in various statistical contexts. In the presence of randomly censored data, the c-index no longer estimates Pr(X>Y); rather, a parameter that involves the underlying censoring distributions. This is in contrast to Efron's maximum likelihood estimator of the Mann-Whitney parameter, which is recommended in the setting of random censorship.},
  author       = {Koziol, James A. and Jia, Zhenyu},
  annotation   = {So instead of using a c-statistic for discrimination perhaps this estimate for the Mann-Whitney parameter is preffered. In which case we need not CIs for AUC/C but for Efron concordance indices, so as not to rely on bootstrap.},
  date         = {2009},
  doi          = {10.1002/bimj.200800228},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Koziol, Jia - 2009 - The concordance index C and the Mann-Whitney parameter Pr(XY) with randomly censored data.pdf:pdf},
  isbn         = {1019498242},
  issn         = {03233847},
  journaltitle = {Biometrical Journal},
  keywords     = {Random censorship model,Survival analysis,auc,c-Index,c-index,concordance,discrimination,evaluation,mann0,measure,survival},
  number       = {3},
  pages        = {467--474},
  title        = {{The concordance index C and the Mann-Whitney parameter Pr(X>Y) with randomly censored data}},
  volume       = {51},
}

@article{Guo2008,
  author       = {Guo, Xinjian and Yin, Yilong and Dong, Cailing and Yang, Gongping and Zhou, Guangtong},
  date         = {2008},
  doi          = {10.1109/ICNC.2008.871},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Guo et al. - 2008 - On the Class Imbalance Problem.pdf:pdf},
  isbn         = {9780769533049},
  journaltitle = {IEEE Computer Society},
  pages        = {192--201},
  title        = {{On the Class Imbalance Problem}},
}

@article{Newson1983,
  author       = {Newson, Roger B},
  date         = {1983},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Newson - 1983 - Comparing the predictive power of survival models using Harrell's c or Somers' D.pdf:pdf},
  journaltitle = {The Stata Journal},
  keywords     = {cox,prediction,somers,stata},
  number       = {ii},
  pages        = {1--19},
  title        = {{Comparing the predictive power of survival models using Harrell's c or Somers' D}},
}

@article{VanBelle2010,
  abstract     = {Abstract This work studies a new survival modeling technique based on least-squares support vector machines. We propose the use of a least-squares support vector machine combining ranking and regression. The advantage of this kernel-based model is threefold: (i) the problem formulation is convex and can be solved conveniently by a linear system; (ii) non-linearity is introduced by using kernels, componentwise kernels in particular are useful to obtain interpretable results; and (iii) introduction of ranking constraints makes it possible to handle censored data. In an experimental setup, the model is used as a preprocessing step for the standard Cox proportional hazard regression by estimating the functional forms of the covariates. The proposed model was compared with different survival models from the literature on the clinical German Breast Cancer Study Group data and on the high-dimensional Norway/Stanford Breast Cancer Data set. Copyright ? 2009 John Wiley & Sons, Ltd.},
  author       = {{Van Belle}, V and Pelckmans, K and Suykens, J A K and {Van Huffel}, S},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/sim.3743},
  annotation   = {doi: 10.1002/sim.3743},
  date         = {2010-01},
  doi          = {10.1002/sim.3743},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Van Belle et al. - 2010 - Additive survival least-squares support vector machines.pdf:pdf},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  keywords     = {LS-SVM,kernel-based learning,survival data},
  number       = {2},
  pages        = {296--308},
  title        = {{Additive survival least-squares support vector machines}},
  volume       = {29},
}

@inproceedings{Buhlmann2002,
  author    = {Bühlmann, Peter Lukas},
  publisher = {Seminar für Statistik, Eidgenössische Technische Hochschule (ETH)},
  booktitle = {Research report/Seminar für Statistik, Eidgenössische Technische Hochschule (ETH)},
  date      = {2002},
  title     = {{Consistency for L2boosting and matching pursuit with trees and tree-type basis functions}},
  volume    = {109},
}

@misc{pkgpycox,
  author = {Kvamme, H{å}vard},
  url    = {https://pypi.org/project/pycox/},
  date   = {2018},
  title  = {pycox},
}

@article{Jackson2014,
  abstract     = {The Cox proportional hazards model is frequently used in medical statistics. The standard methods for fitting this model rely on the assumption of independent censoring. Although this is sometimes plausible, we often wish to explore how robust our inferences are as this untestable assumption is relaxed. We describe how this can be carried out in a way that makes the assumptions accessible to all those involved in a research project. Estimation proceeds via multiple imputation, where censored failure times are imputed under user-specified departures from independent censoring. A novel aspect of our method is the use of bootstrapping to generate proper imputations from the Cox model. We illustrate our approach using data from an HIV-prevention trial and discuss how it can be readily adapted and applied in other settings.},
  author       = {Jackson, Dan and White, Ian R. and Seaman, Shaun and Evans, Hannah and Baisley, Kathy and Carpenter, James},
  publisher    = {John Wiley and Sons Ltd},
  date         = {2014-11},
  doi          = {10.1002/sim.6274},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Jackson et al. - 2014 - Relaxing the independent censoring assumption in the Cox proportional hazards model using multiple imputation.pdf:pdf},
  issn         = {10970258},
  journaltitle = {Statistics in Medicine},
  keywords     = {Bootstrapping,Informative censoring,Multiple imputation,Schoenfeld residuals,Sensitivity analysis,Survival analysis,censoring,imputation,informative censoring,reduction},
  number       = {27},
  pages        = {4681--4694},
  title        = {{Relaxing the independent censoring assumption in the Cox proportional hazards model using multiple imputation}},
  volume       = {33},
}

@misc{pkgfda,
  author = {Ramsay, J. O. and Wickham, Hadley and Graves, Spencer and Hooker, Giles},
  url    = {https://cran.r-project.org/package=fda},
  title  = {{fda: Functional Data Analysis}},
}

@misc{pkgrstan,
  author = {{Stan Development Team}},
  url    = {http://mc-stan.org/},
  date   = {2018},
  title  = {{RStan: The R interface to Stan}},
}

@article{Bender2005,
  abstract     = {Abstract Simulation studies present an important statistical tool to investigate the performance, properties and adequacy of statistical models in pre-specified situations. One of the most important statistical models in medical research is the proportional hazards model of Cox. In this paper, techniques to generate survival times for simulation studies regarding Cox proportional hazards models are presented. A general formula describing the relation between the hazard and the corresponding survival time of the Cox model is derived, which is useful in simulation studies. It is shown how the exponential, the Weibull and the Gompertz distribution can be applied to generate appropriate survival times for simulation studies. Additionally, the general relation between hazard and survival time can be used to develop own distributions for special situations and to handle flexibly parameterized proportional hazards models. The use of distributions other than the exponential distribution is indispensable to investigate the characteristics of the Cox proportional hazards model, especially in non-standard situations, where the partial likelihood depends on the baseline hazard. A simulation study investigating the effect of measurement errors in the German Uranium Miners Cohort Study is considered to illustrate the proposed simulation techniques and to emphasize the importance of a careful modelling of the baseline hazard in Cox models. Copyright ? 2005 John Wiley & Sons, Ltd.},
  author       = {Bender, Ralf and Augustin, Thomas and Blettner, Maria},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/sim.2059},
  annotation   = {doi: 10.1002/sim.2059},
  date         = {2005-06},
  doi          = {10.1002/sim.2059},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Bender, Augustin, Blettner - 2005 - Generating survival times to simulate Cox proportional hazards models.pdf:pdf},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  keywords     = {Cox model,Gompertz distribution,Weibull distribution,exponential distribution,proportional hazards model,simulation,survival times},
  number       = {11},
  pages        = {1713--1723},
  title        = {{Generating survival times to simulate Cox proportional hazards models}},
  volume       = {24},
}

@article{pkgflexsurv,
  author       = {Jackson, Christopher},
  date         = {2016},
  journaltitle = {Journal of Statistical Software},
  number       = {8},
  pages        = {1--33},
  title        = {{flexsurv: A Platform for Parametric Survival Modeling in R}},
  volume       = {70},
}

@article{Peto1972,
  author       = {Peto, Richard and Peto, Julian},
  publisher    = {Wiley Online Library},
  date         = {1972},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Peto, Peto - 1972 - Asymptotically efficient rank invariant test procedures.pdf:pdf},
  issn         = {0035-9238},
  journaltitle = {Journal of the Royal Statistical Society: Series A (General)},
  number       = {2},
  pages        = {185--198},
  title        = {{Asymptotically efficient rank invariant test procedures}},
  volume       = {135},
}

@article{Hand2014,
  author       = {Hand, D J and Anagnostopoulos, C},
  date         = {2014},
  eprint       = {arXiv:1202.2564v2},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Hand, Anagnostopoulos - 2014 - A better Beta for the H measure of classification performance performance.pdf:pdf},
  journaltitle = {Pattern Recognition Letters},
  title        = {{A better Beta for the H measure of classification performance performance}},
  volume       = {40},
}

@article{Miller1991,
  abstract     = {Abstract This paper presents a comprehensive approach to the validation of logistic prediction models. It reviews measures of overall goodness-of-fit, and indices of calibration and refinement. Using a model-based approach developed by Cox, we adapt logistic regression diagnostic techniques for use in model validation. This allows identification of problematic predictor variables in the prediction model as well as influential observations in the validation data that adversely affect the fit of the model. In appropriate situations, recommendations are made for correction of models that provide poor fit.},
  author       = {Miller, Michael E and Hui, Siu L and Tierney, William M},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/sim.4780100805},
  annotation   = {doi: 10.1002/sim.4780100805},
  date         = {1991-08},
  doi          = {10.1002/sim.4780100805},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Miller, Hui, Tierney - 1991 - Validation techniques for logistic regression models.pdf:pdf},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  number       = {8},
  pages        = {1213--1226},
  title        = {{Validation techniques for logistic regression models}},
  volume       = {10},
}

@article{Choodari2012b,
  abstract     = {Measures of predictive ability play an important role in quantifying the clinical significance of prognostic factors. Several measures have been proposed to evaluate the predictive ability of survival models in the last two decades, but no single measure is consistently used. The proposed measures can be classified into the following categories: explained variation, explained randomness, and predictive accuracy. The three categories are conceptually different and are based on different principles. Several new measures have been proposed since Schemper and Stare's study in 1996 on some of the existing measures. This paper is the first of two papers that study the proposed measures systematically by applying a set of criteria that a measure of predictive ability should possess in the context of survival analysis. The present paper focuses on the explained variation category, and part II studies the proposed measures in the other categories. Simulation studies are used to examine the performance of five explained variation measures with respect to these criteria, discussing their strengths and shortcomings. Our simulation studies show that the measures proposed by Kent and O'Quigley, R, and Royston and Sauerbrei, R, appear to be the best overall at quantifying predictive ability. However, it should be noted that neither measure is perfect; R is sensitive to outliers and R to (marked) non-normality of the distribution of the prognostic index. The results show that the other measures perform poorly, primarily because they are adversely affected by censoring. Copyright {©} 2011 John Wiley & Sons, Ltd.},
  author       = {Choodari-Oskooei, Babak and Royston, Patrick and Parmar, Mahesh K.B.},
  annotation   = {In machine learning we are generally interested in what the authors term 'Predictive Accuracy' measures. The main validation of interest is in the ability of the model to correctly predict the outcome of interest on an individual level.The ability oft he prognostic factors in predicting an individuals' status at different time-points.},
  date         = {2012},
  doi          = {10.1002/sim.4242},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Choodari-Oskooei, Royston, Parmar - 2012 - A simulation study of predictive ability measures in a survival model II explained randomness.pdf:pdf},
  isbn         = {1097-0258 (Electronic)0̊277-6715 (Linking)},
  issn         = {02776715},
  journaltitle = {Statistics in Medicine},
  keywords     = {Breast cancer,Cox proportional hazards model,Explained variation,Predictive ability,Survival analysis,comparison,concordance,discrimination,evaluation},
  number       = {23},
  pages        = {2644--2659},
  title        = {{A simulation study of predictive ability measures in a survival model II: explained randomness and predictive accuracy}},
  volume       = {31},
}

@article{Ishwaran2004,
  author       = {Ishwaran, Hemant and Blackstone, Eugene H and Pothier, Claire E and Lauer, Michael S},
  publisher    = {Taylor & Francis},
  url          = {https://doi.org/10.1198/016214504000000638},
  annotation   = {doi: 10.1198/016214504000000638},
  date         = {2004-09},
  doi          = {10.1198/016214504000000638},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Ishwaran et al. - 2004 - Relative Risk Forests for Exercise Heart Rate Recovery as a Predictor of Mortality.pdf:pdf},
  issn         = {0162-1459},
  journaltitle = {Journal of the American Statistical Association},
  number       = {467},
  pages        = {591--600},
  title        = {{Relative Risk Forests for Exercise Heart Rate Recovery as a Predictor of Mortality}},
  volume       = {99},
}

@article{Tsiatis1975,
  abstract     = {For an experimental animal exposed to k greater than 1 possible risks of death R1, R2, ..., Rk, the term i-th potential survival time designates a random variable Yi supposed to represent the age at death of the animal in hypothetical conditions in which Ri is the only possible risk. The probability that Yi will exceed a preassigned t is called the i-th net survival probability. The results of a survival experiment are represented by k "crude" survival functions, empirical counterparts of the probabilities Qi(t) that an animal will survive at least up to the age t and eventually die from Ri. The analysis of a survival experiment aims at estimating the k net survival probabilities using the empirical data on those termed crude. Therorems 1 and 2 establish the relationship between the net and the crude probabilities of survival. In particular, Theorem 2 shows that, without the not directly verifiable assumption that in their joint distribution the variables Y1, Y2, ..., Yk are mutually independent, a given set of crude survival probabilities Qi(t) does not identify the corresponding net probabilities. An example shows that the results of a customary method of analysis, based on the assumption that Y1, Y2, ..., Yk are independent, may have no resemblance to reality.},
  author       = {Tsiatis, A},
  language     = {eng},
  url          = {https://pubmed.ncbi.nlm.nih.gov/1054494 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC432231/},
  date         = {1975-01},
  doi          = {10.1073/pnas.72.1.20},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Tsiatis - 1975 - A nonidentifiability aspect of the problem of competing risks.pdf:pdf},
  issn         = {0027-8424},
  journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
  keywords     = {*Models,*Mortality,*Probability,*Risk,Animals,Biological,Laboratory,Mathematics,non-identifiability,survival},
  number       = {1},
  pages        = {20--22},
  title        = {{A nonidentifiability aspect of the problem of competing risks}},
  volume       = {72},
}

@article{Friedman2001,
  abstract     = {[Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.]},
  author       = {Friedman, Jerome H},
  publisher    = {Institute of Mathematical Statistics},
  url          = {http://www.jstor.org/stable/2699986},
  date         = {2001-04},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Friedman - 2001 - Greedy Function Approximation A Gradient Boosting Machine.pdf:pdf},
  issn         = {00905364},
  journaltitle = {The Annals of Statistics},
  number       = {5},
  pages        = {1189--1232},
  title        = {{Greedy Function Approximation: A Gradient Boosting Machine}},
  volume       = {29},
}

@article{HURVICH1989,
  abstract     = {A bias correction to the Akaike information criterion, AIC, is derived for regression and autoregressive time series models. The correction is of particular use when the sample size is small, or when the number of fitted parameters is a moderate to large fraction of the sample size. The corrected method, called AICC, is asymptotically efficient if the true model is infinite dimensional. Furthermore, when the true model is of finite dimension, AICC is found to provide better model order choices than any other asymptotically efficient method. Applications to nonstationary autoregressive and mixed autoregressive moving average time series models are also discussed.},
  author       = {HURVICH, CLIFFORD M and TSAI, CHIH-LING},
  url          = {https://doi.org/10.1093/biomet/76.2.297},
  date         = {1989-06},
  doi          = {10.1093/biomet/76.2.297},
  issn         = {0006-3444},
  journaltitle = {Biometrika},
  number       = {2},
  pages        = {297--307},
  title        = {{Regression and time series model selection in small samples}},
  volume       = {76},
}

@article{Friedman1999,
  author       = {Friedman, Jerome},
  date         = {1999-03},
  doi          = {10.1016/S0167-9473(01)00065-2},
  journaltitle = {Computational Statistics & Data Analysis},
  pages        = {367--378},
  title        = {{Stochastic Gradient Boosting}},
  volume       = {38},
}

@article{Japkowicz2000,
  author       = {Japkowicz, Nathalie},
  date         = {2000},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Japkowicz - 2000 - Learning from Imbalanced Data Sets A Comparison of Various Strategies.pdf:pdf},
  journaltitle = {AAAI Technical Report},
  title        = {{Learning from Imbalanced Data Sets: A Comparison of Various Strategies}},
}

@article{Kirmani2001,
  abstract     = {The proportional odds (PO) model with its property of convergent hazard functions is of considerable value in modeling survival data with non-proportional hazards. This paper explores the structure, implications, and properties of the PO model. Results proved include connections with geometric minima and maxima, ageing characteristics, and bounds on mean and variance of survival times.},
  author       = {Kirmani, S N U A and Gupta, Ramesh C},
  url          = {https://doi.org/10.1023/A:1012458303498},
  date         = {2001},
  doi          = {10.1023/A:1012458303498},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Kirmani, Gupta - 2001 - On the Proportional Odds Model in Survival Analysis.pdf:pdf},
  issn         = {1572-9052},
  journaltitle = {Annals of the Institute of Statistical Mathematics},
  number       = {2},
  pages        = {203--216},
  title        = {{On the Proportional Odds Model in Survival Analysis}},
  volume       = {53},
}

@article{Collins2014,
  abstract     = {BACKGROUND: Before considering whether to use a multivariable (diagnostic or prognostic) prediction model, it is essential that its performance be evaluated in data that were not used to develop the model (referred to as external validation). We critically appraised the methodological conduct and reporting of external validation studies of multivariable prediction models.\n\nMETHODS: We conducted a systematic review of articles describing some form of external validation of one or more multivariable prediction models indexed in PubMed core clinical journals published in 2010. Study data were extracted in duplicate on design, sample size, handling of missing data, reference to the original study developing the prediction models and predictive performance measures.\n\nRESULTS: 11,826 articles were identified and 78 were included for full review, which described the evaluation of 120 prediction models. in participant data that were not used to develop the model. Thirty-three articles described both the development of a prediction model and an evaluation of its performance on a separate dataset, and 45 articles described only the evaluation of an existing published prediction model on another dataset. Fifty-seven percent of the prediction models were presented and evaluated as simplified scoring systems. Sixteen percent of articles failed to report the number of outcome events in the validation datasets. Fifty-four percent of studies made no explicit mention of missing data. Sixty-seven percent did not report evaluating model calibration whilst most studies evaluated model discrimination. It was often unclear whether the reported performance measures were for the full regression model or for the simplified models.\n\nCONCLUSIONS: The vast majority of studies describing some form of external validation of a multivariable prediction model were poorly reported with key details frequently not presented. The validation studies were characterised by poor design, inappropriate handling and acknowledgement of missing data and one of the most key performance measures of prediction models i.e. calibration often omitted from the publication. It may therefore not be surprising that an overwhelming majority of developed prediction models are not used in practice, when there is a dearth of well-conducted and clearly reported (external validation) studies describing their performance on independent participant data.},
  author       = {Collins, Gary S. and {De Groot}, Joris A. and Dutton, Susan and Omar, Omar and Shanyinde, Milensu and Tajar, Abdelouahid and Voysey, Merryn and Wharton, Rose and Yu, Ly Mee and Moons, Karel G. and Altman, Douglas G.},
  publisher    = {BMC Medical Research Methodology},
  url          = {BMC Medical Research Methodology},
  date         = {2014},
  doi          = {10.1186/1471-2288-14-40},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Collins et al. - 2014 - External validation of multivariable prediction models A systematic review of methodological conduct and reporti.pdf:pdf},
  isbn         = {1471-2288},
  issn         = {14712288},
  journaltitle = {BMC Medical Research Methodology},
  keywords     = {concordance,harrells c},
  number       = {1},
  pages        = {1--11},
  title        = {{External validation of multivariable prediction models: A systematic review of methodological conduct and reporting}},
  volume       = {14},
}

@article{Bland2004,
  author       = {Bland, J Martin and Altman, Douglas G.},
  language     = {eng},
  publisher    = {BMJ Publishing Group Ltd.},
  url          = {https://pubmed.ncbi.nlm.nih.gov/15117797 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC403858/},
  date         = {2004-05},
  doi          = {10.1136/bmj.328.7447.1073},
  issn         = {1756-1833},
  journaltitle = {BMJ (Clinical research ed.)},
  keywords     = {*Survival Analysis,Astrocytoma/mortality,Glioma/mortality,Humans,Life Tables,Proportional Hazards Models,Survival Rate,censoring,log-rank,logrank,survival,theory},
  number       = {7447},
  pages        = {1073},
  title        = {{The logrank test}},
  volume       = {328},
}

@article{Dennis2020,
  abstract     = {OBJECTIVE To describe the relationship between type 2 diabetes and all-cause mortality among adults with coronavirus disease 2019 (COVID-19) in the critical care setting.RESEARCH DESIGN AND METHODS This was a nationwide retrospective cohort study in people admitted to hospital in England with COVID-19 requiring admission to a high dependency unit (HDU) or intensive care unit (ICU) between 1 March 2020 and 27 July 2020. Cox proportional hazards models were used to estimate 30-day in-hospital all-cause mortality associated with type 2 diabetes, with adjustment for age, sex, ethnicity, obesity, and other major comorbidities (chronic respiratory disease, asthma, chronic heart disease, hypertension, immunosuppression, chronic neurological disease, chronic renal disease, and chronic liver disease).RESULTS A total of 19,256 COVID-19–related HDU and ICU admissions were included in the primary analysis, including 13,809 HDU (mean age 70 years) and 5,447 ICU (mean age 58 years) admissions. Of those admitted, 3,524 (18.3%) had type 2 diabetes and 5,077 (26.4%) died during the study period. Patients with type 2 diabetes were at increased risk of death (adjusted hazard ratio [aHR] 1.23 [95% CI 1.14, 1.32]), and this result was consistent in HDU and ICU subsets. The relative mortality risk associated with type 2 diabetes decreased with higher age (age 18–49 years aHR 1.50 [95% CI 1.05, 2.15], age 50–64 years 1.29 [1.10, 1.51], and age ≥65 years 1.18 [1.09, 1.29], P value for age–type 2 diabetes interaction = 0.002).CONCLUSIONS Type 2 diabetes may be an independent prognostic factor for survival in people with severe COVID-19 requiring critical care treatment, and in this setting the risk increase associated with type 2 diabetes is greatest in younger people.},
  author       = {Dennis, John M and Mateen, Bilal A and Sonabend, Raphael and Thomas, Nicholas J and Patel, Kashyap A and Hattersley, Andrew T and Denaxas, Spiros and McGovern, Andrew P and Vollmer, Sebastian J},
  url          = {http://care.diabetesjournals.org/content/early/2020/10/23/dc20-1444.abstract},
  date         = {2020-10},
  doi          = {10.2337/dc20-1444},
  journaltitle = {Diabetes Care},
  pages        = {dc201444},
  title        = {{Type 2 Diabetes and COVID-19–Related Mortality in the Critical Care Setting: A National Cohort Study in England, March–July 2020}},
}

@article{Zikmund-Fisher2012,
  abstract     = {While patients often receive risk information, exactly what constitutes being ?informed? about health risks is often unclear. Patients have specific needs, such as avoiding being surprised by a possible outcome and making complex risk trade-off decisions. Yet all risk information is not equally informative for those needs. In this article, I present a taxonomy of seven risk concepts that vary in their inherent precision and evaluability. Congruent with the ?less is more? concept, I argue that risk communications should use formats that are tailored to message recipients? specific informational needs. Simpler formats can be used when patients only need to order risks, while more complex numerical probability statements will be necessary when patients need to assess differences in risk magnitude and put those differences into meaningful context. Selecting need-congruent formats when designing communications about risks to patients is a novel approach that may better support patients? health care decision making.},
  author       = {Zikmund-Fisher, Brian J},
  publisher    = {SAGE Publications Inc},
  url          = {https://doi.org/10.1177/1077558712458541},
  annotation   = {doi: 10.1177/1077558712458541},
  date         = {2012-09},
  doi          = {10.1177/1077558712458541},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Zikmund-Fisher - 2012 - The Right Tool Is What They Need, Not What We Have A Taxonomy of Appropriate Levels of Precision in Patient Risk.pdf:pdf},
  issn         = {1077-5587},
  journaltitle = {Medical Care Research and Review},
  number       = {1_suppl},
  pages        = {37S--49S},
  title        = {{The Right Tool Is What They Need, Not What We Have: A Taxonomy of Appropriate Levels of Precision in Patient Risk Communication}},
  volume       = {70},
}

@article{Breiman1992,
  abstract     = {Often, in a regression situation with many variables, a sequence of submodels is generated containing fewer variables by using such methods as stepwise addition or deletion of variables, or 'best subsets'. The question is which of this sequence of submodels is 'best', and how can submodel performance be evaluated. This was explored in Breiman (1988) for a fixed X-design. This is a sequel exploring the case of random X-designs. Analytical results are difficult, if not impossible. This study involved an extensive simulation. The basis of the study is the theoretical definition of prediction error (PE) as the expected squared error produced by applying a prediction equation to the distributional universe of (y, x) values. This definition is used throughout to compare various submodels. There can be startling differences between the x-fixed and x-random situations and different PE estimates are appropriate. Non-resampling estimates such as CP, adjusted R2, etc. turn out to be highly biased methods for submodel selection. The two best methods are cross-validation and bootstrap. One surprise is that 5 fold cross-validation (leave out 20% of the data) is better at submodel selection and evaluation than leave-one-out cross-validation. There are a number of other surprises. /// Dans l'analyse de probl&#xe8;mes de r&#xe9;gression &#xe0; plusieurs variables (ind&#xe9;pendantes), on produit souvent une s&#xe9;rie de sous-mod&#xe8;les constitu&#xe9;s d'un sous-ensemble des variables par des m&#xe9;thodes telles que l'addition par &#xe9;tape, le retrait par &#xe9;tape et la m&#xe9;thode du meilleur sous-ensemble. Le probl&#xe8;me est de d&#xe9;terminer lequel de ces sous-mod&#xe8;les est le meilleur et d'&#xe9;valuer sa performance. Ce probl&#xe8;me fut explor&#xe9; dans Breiman (1988) pour le cas d'une matrice X fixe. Dans ce qui suit on consid&#xe8;re le cas o&#xf9; la matrice X est al&#xe9;atoire. La d&#xe9;termination de r&#xe9;sultats analytiques est difficile, sinon impossible. Notre &#xe9;tude a utilis&#xe9; des simulations de grande envergure. Elle se base sur la d&#xe9;finition th&#xe9;orique de l'erreur de pr&#xe9;diction (EP) comme &#xe9;tant l'esp&#xe9;rance du carr&#xe9; de l'erreur produite en applicant une &#xe9;quation de pr&#xe9;diction &#xe0; l'univers distributional des valeurs (y, x). La d&#xe9;finition est utilis&#xe9;e dans toute l'&#xe9;tude &#xe0; fin de comparer divers sous-mod&#xe8;les. Il y a une diff&#xe9;rence &#xe9;tonnante entre le cas o&#xf9; la matrice X est fix&#xe9;e et celui o&#xf9; elle est al&#xe9;atoire. Diff&#xe9;rents estimateurs de la EP sont &#xe0; propos. Les estimateurs n'utilisant pas de r&#xe9;-&#xe9;chantillonage, tels que le Cp et le R2 ajust&#xe9;, produisent des m&#xe9;thodes de s&#xe9;lection ayant grand biais. Les deux meilleures m&#xe9;thodes sont la validation crois&#xe9;e et l'autoamor&#xe7;age. Une surprise est que la validation crois&#xe9;e quintuple est meilleure que la validation crois&#xe9;e tous sauf un. Il y a plusieurs autres r&#xe9;sultats surprenants.},
  author       = {Breiman, Leo and Spector, Philip},
  publisher    = {[Wiley, International Statistical Institute (ISI)]},
  url          = {http://www.jstor.org/stable/1403680},
  date         = {1992},
  doi          = {10.2307/1403680},
  issn         = {03067734, 17515823},
  journaltitle = {International Statistical Review / Revue Internationale de Statistique},
  number       = {3},
  pages        = {291--319},
  title        = {{Submodel Selection and Evaluation in Regression. The X-Random Case}},
  volume       = {60},
}

@article{Schmid2008b,
  author       = {Schmid, Matthias and Hothorn, Torsten},
  date         = {2008-02},
  doi          = {10.1186/1471-2105-9-269},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Schmid, Hothorn - 2008 - Flexible boosting of accelerated failure time models.pdf:pdf},
  journaltitle = {BMC bioinformatics},
  keywords     = {aft,boosting,ensemble,ml,survival},
  pages        = {269},
  title        = {{Flexible boosting of accelerated failure time models}},
  volume       = {9},
}

@inproceedings{Nair2010,
  author    = {Nair, Vinod and Hinton, Geoffrey E},
  booktitle = {Proceedings of the 27th international conference on machine learning (ICML-10)},
  date      = {2010},
  pages     = {807--814},
  title     = {{Rectified linear units improve restricted boltzmann machines}},
}

@article{Lee2019,
  abstract     = {Survival analysis mainly deals with the time to event, including death, onset of disease, and bankruptcy. The common characteristic of survival analysis is that it contains "censored" data, in which the time to event cannot be completely observed, but instead represents the lower bound of the time to event. Only the occurrence of either time to event or censoring time is observed. Many traditional statistical methods have been effectively used for analyzing survival data with censored observations. However, with the development of high-throughput technologies for producing "omics" data, more advanced statistical methods, such as regularization, should be required to construct the predictive survival model with high-dimensional genomic data. Furthermore, machine learning approaches have been adapted for survival analysis, to fit nonlinear and complex interaction effects between predictors, and achieve more accurate prediction of individual survival probability. Presently, since most clinicians and medical researchers can easily assess statistical programs for analyzing survival data, a review article is helpful for understanding statistical methods used in survival analysis. We review traditional survival methods and regularization methods, with various penalty functions, for the analysis of high-dimensional genomics, and describe machine learning techniques that have been adapted to survival analysis.},
  author       = {Lee, Seungyeoun and Lim, Heeju},
  language     = {eng},
  publisher    = {Korea Genome Organization},
  url          = {https://pubmed.ncbi.nlm.nih.gov/31896241 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6944043/},
  date         = {2019-12},
  doi          = {10.5808/GI.2019.17.4.e41},
  edition      = {2019/12/20},
  issn         = {1598-866X},
  journaltitle = {Genomics & informatics},
  keywords     = {Cox model,Kaplan-Meier curve,censoring,literature review,machine learning,regularization,review,survival,survival time},
  number       = {4},
  pages        = {e41--e41},
  title        = {{Review of statistical methods for survival analysis using genomic data}},
  volume       = {17},
}

@article{Colosimo2002,
  author       = {Colosimo, Enrico and Ferreira, Fla´vio and Oliveira, Maristela and Sousa, Cleide},
  publisher    = {Taylor & Francis},
  url          = {https://doi.org/10.1080/00949650212847},
  annotation   = {doi: 10.1080/00949650212847},
  date         = {2002-01},
  doi          = {10.1080/00949650212847},
  issn         = {0094-9655},
  journaltitle = {Journal of Statistical Computation and Simulation},
  number       = {4},
  pages        = {299--308},
  title        = {{Empirical comparisons between Kaplan-Meier and Nelson-Aalen survival function estimators}},
  volume       = {72},
}

@article{Wang2017,
  abstract     = {<p> Survival analysis is a subfield of statistics where the goal is to analyze and model data where the outcome is the time until an event of interest occurs. One of the main challenges in this context is the presence of instances whose event outcomes become unobservable after a certain time point or when some instances do not experience any event during the monitoring period. This so-called <italic>censoring</italic> can be handled most effectively using survival analysis techniques. Traditionally, statistical approaches have been widely developed in the literature to overcome the issue of censoring. In addition, many machine learning algorithms have been adapted to deal with such censored data and tackle other challenging problems that arise in real-world data. In this survey, we provide a comprehensive and structured review of the statistical methods typically used and the machine learning techniques developed for survival analysis, along with a detailed taxonomy of the existing methods. We also discuss several topics that are closely related to survival analysis and describe several successful applications in a variety of real-world application domains. We hope that this article will give readers a more comprehensive understanding of recent advances in survival analysis and offer some guidelines for applying these approaches to solve new problems arising in applications involving censored data. </p>},
  author       = {Wang, Ping and Li, Yan and Reddy, Chandan K.},
  url          = {https://dl.acm.org/doi/10.1145/3214306},
  annotation   = {It's a good paper for getting references but it doesn't actually provide any comparisons.... It lists many different methods but doesn't describe any in enough detail or give reasons why some may be preferred over others. However it has provided some references, graphs and tables.},
  date         = {2019-11},
  doi          = {10.1145/3214306},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Li, Reddy - 2017 - Machine Learning for Survival Analysis A Survey.pdf:pdf},
  issn         = {0360-0300},
  journaltitle = {ACM Computing Surveys},
  keywords     = {Concordance index,Cox model,Survival data,censoring,hazard rate,machine learning,regression,review,survival,survival analysis},
  number       = {6},
  pages        = {1--36},
  title        = {{Machine Learning for Survival Analysis}},
  volume       = {51},
}

@article{Chambless2006,
  abstract     = {Abstract Sensitivity, specificity, and area under the ROC curve (AUC) are often used to measure the ability of survival models to predict future risk. Estimation of these parameters is complicated by the fact that these parameters are time-dependent and by the fact that censoring affects their estimation just as it affects estimation of survival curves or coefficients of survival regression models. The authors present several estimators that overcome these complications. One approach is a recursive calculation over the ordered times of events, analogous to the Kaplan?Meier approach to survival function estimation. Another is to first apply Bayes' theorem to write the parameters of interest in terms of conditional survival functions that are then estimated by survival analysis methods. Simulation studies demonstrate that the proposed estimators perform well in practical situations, when compared with an estimator (c-statistic, from logistic regression) that ignores time. An illustration with data from a cardiovascular follow-up study is provided. Copyright ? 2005 John Wiley & Sons, Ltd.},
  author       = {Chambless, Lloyd E and Diao, Guoqing},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/sim.2299},
  annotation   = {doi: 10.1002/sim.2299},
  date         = {2006-10},
  doi          = {10.1002/sim.2299},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  keywords     = {Kaplan–Meier estimator,ROC curves,area under the curve,risk prediction,sensitivity,specificity},
  number       = {20},
  pages        = {3474--3486},
  title        = {{Estimation of time-dependent area under the ROC curve for long-term risk prediction}},
  volume       = {25},
}

@article{Zhong2019,
  abstract   = {In this paper, we explore a method for treating survival analysis as a classification problem. The method uses a "stacking" idea that collects the features and outcomes of the survival data in a large data frame, and then treats it as a classification problem. In this framework, various statistical learning algorithms (including logistic regression, random forests, gradient boosting machines and neural networks) can be applied to estimate the parameters and make predictions. For stacking with logistic regression, we show that this approach is approximately equivalent to the Cox proportional hazards model with both theoretical analysis and simulation studies. For stacking with other machine learning algorithms, we show through simulation studies that our method can outperform Cox proportional hazards model in terms of estimated survival curves. This idea is not new, but we believe that it should be better known by statistiicians and other data scientists.},
  author     = {Zhong, Chenyang and Tibshirani, Robert},
  url        = {http://arxiv.org/abs/1909.11171},
  date       = {2019-09},
  eprint     = {1909.11171},
  eprinttype = {arXiv},
  file       = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Zhong, Tibshirani - 2019 - Survival analysis as a classification problem.pdf:pdf},
  title      = {{Survival analysis as a classification problem}},
}

@article{Mason2002,
  author       = {Mason, S. J. and Graham, N. E.},
  date         = {2002},
  doi          = {10.1016/B978-1-4377-0795-3.00025-9},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Mason, Graham - 2002 - Areas beneath the relative operating characteristics (ROC) and relative operating levels (ROL) curves Statistical.pdf:pdf},
  isbn         = {9781437707953},
  journaltitle = {Q. J. R. Meteorol. Soc},
  pages        = {2145--2166},
  title        = {{Areas beneath the relative operating characteristics (ROC) and relative operating levels (ROL) curves: Statistical signi?cance and interpretation}},
  volume       = {128},
}

@article{Efron1988,
  abstract     = {[We discuss the use of standard logistic regression techniques to estimate hazard rates and survival curves from censored data. These techniques allow the statistician to use parametric regression modeling on censored data in a flexible way that provides both estimates and standard errors. An example is given that demonstrates the increased structure that can be seen in a parametric analysis, as compared with the nonparametric Kaplan-Meier survival curves. In fact, the logistic regression estimates are closely related to Kaplan-Meier curves, and approach the Kaplan-Meier estimate as the number of parameters grows large.]},
  author       = {Efron, Bradley},
  publisher    = {[American Statistical Association, Taylor & Francis, Ltd.]},
  url          = {http://www.jstor.org/stable/2288857},
  date         = {1988},
  doi          = {10.2307/2288857},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Efron - 1988 - Logistic Regression, Survival Analysis, and the Kaplan-Meier Curve.pdf:pdf},
  issn         = {01621459},
  journaltitle = {Journal of the American Statistical Association},
  number       = {402},
  pages        = {414--425},
  title        = {{Logistic Regression, Survival Analysis, and the Kaplan-Meier Curve}},
  volume       = {83},
}

@article{Graf2003,
  author       = {Graf, E and Gerds, Thomas},
  date         = {2003-02},
  doi          = {10.1267/METH03050564},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Graf, Gerds - 2003 - How to Assess Prognostic Models for Survival Data A Case Study in Oncology.pdf:pdf},
  journaltitle = {Methods of information in medicine},
  pages        = {564--571},
  title        = {{How to Assess Prognostic Models for Survival Data: A Case Study in Oncology}},
  volume       = {42},
}

@article{Tsoumakas2007,
  abstract     = {Multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization, and semantic scene classification. This article introduces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs comparative experimental results of certain multi-label classification methods. It also contributes the definition of concepts for the quantification of the multi-label nature of a data set.},
  author       = {Tsoumakas, Grigorios and Katakis, Ioannis},
  url          = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/jdwm.2007070101},
  date         = {2007},
  doi          = {10.4018/jdwm.2007070101},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Tsoumakas, Katakis - 2007 - Multi-Label Classification An Overview.pdf:pdf},
  isbn         = {978-1-4244-1065-1},
  issn         = {1548-3924},
  journaltitle = {International Journal of Data Warehousing and Mining},
  number       = {3},
  pages        = {1--13},
  title        = {{Multi-Label Classification: An Overview}},
  volume       = {3},
}

@book{Breiman1984,
  author    = {Breiman, L and Friedman, J and Stone, C J and Olshen, R A},
  publisher = {Taylor & Francis},
  url       = {https://books.google.co.uk/books?id=JwQx-WOmSyQC},
  date      = {1984},
  isbn      = {9780412048418},
  series    = {The Wadsworth and Brooks-Cole statistics-probability series},
  title     = {{Classification and Regression Trees}},
}

@article{Zhu2020,
  abstract     = {Deep learning has been applied to many areas in health care, including imaging diagnosis, digital pathology, prediction of hospital admission, drug design, classification of cancer and stromal cells, doctor assistance, etc. Cancer prognosis is to estimate the fate of cancer, probabilities of cancer recurrence and progression, and to provide survival estimation to the patients. The accuracy of cancer prognosis prediction will greatly benefit clinical management of cancer patients. The improvement of biomedical translational research and the application of advanced statistical analysis and machine learning methods are the driving forces to improve cancer prognosis prediction. Recent years, there is a significant increase of computational power and rapid advancement in the technology of artificial intelligence, particularly in deep learning. In addition, the cost reduction in large scale next-generation sequencing, and the availability of such data through open source databases (e.g., TCGA and GEO databases) offer us opportunities to possibly build more powerful and accurate models to predict cancer prognosis more accurately. In this review, we reviewed the most recent published works that used deep learning to build models for cancer prognosis prediction. Deep learning has been suggested to be a more generic model, requires less data engineering, and achieves more accurate prediction when working with large amounts of data. The application of deep learning in cancer prognosis has been shown to be equivalent or better than current approaches, such as Cox-PH. With the burst of multi-omics data, including genomics data, transcriptomics data and clinical information in cancer studies, we believe that deep learning would potentially improve cancer prognosis.},
  author       = {Zhu, Wan and Xie, Longxiang and Han, Jianye and Guo, Xiangqian},
  language     = {eng},
  publisher    = {MDPI},
  url          = {https://pubmed.ncbi.nlm.nih.gov/32150991 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7139576/},
  date         = {2020-03},
  doi          = {10.3390/cancers12030603},
  issn         = {2072-6694},
  journaltitle = {Cancers},
  keywords     = {cancer prognosis,deep learning,machine learning,multi-omics,prognosis prediction},
  number       = {3},
  pages        = {603},
  title        = {{The Application of Deep Learning in Cancer Prognosis Prediction}},
  volume       = {12},
}

@article{RoystonLambert2011,
  abstract     = {Explores the use and applications of flexible parametric survival models, programmed in Stata, in order to go beyond standard models such as the Cox model. Discusses using the stset and stsplit commands; a graphical introduction to the principal datasets; Poisson models; Royston-Parmar models; prognostic models; time-dependent effects; relative survival; and further topics. Royston is with the United Kingdom's Medical Research Council Clinical Trials Unit. Lambert is with the Department of Health Sciences at the University of Leicester and is with Medical Epidemiology and Biostatistics at the Karolinska Institute. Name and subject indexes.},
  author       = {Royston, Patrick and Lambert, P C},
  date         = {2011},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Royston, Lambert - 2011 - Flexible Parametric Survival Analysis Using Stata Beyond the Cox Model.pdf:pdf},
  isbn         = {9781597180795},
  journaltitle = {Stata Journal},
  number       = {July},
  pages        = {1--25},
  title        = {{Flexible Parametric Survival Analysis Using Stata: Beyond the Cox Model}},
}

@misc{pkggamlssadd,
  author     = {Stasinopoulos, Mikis and Rigby, Bob and Voudouris, Vlasios and Kiose, Daniil},
  publisher  = {CRAN},
  url        = {https://cran.r-project.org/package=gamlss.add},
  annotation = {R package version 5.1-6},
  date       = {2020},
  title      = {{gamlss.add: Extra Additive Terms for Generalized Additive Models for Location Scale and Shape}},
}

@article{Shen2012,
  author       = {Shen, Hui and Welch, William and Hughes-Oliver, Jacqueline},
  date         = {2012-02},
  doi          = {10.1214/11-AOAS491},
  journaltitle = {Annals of Applied Statistics - ANN APPL STAT},
  keywords     = {futility,machine learning,optimisation,toolboxes,tuning},
  title        = {{Efficient, adaptive cross-validation for tuning and comparing models, with application to drug discovery}},
  volume       = {5},
}

@article{Frost2000,
  abstract     = {[In an epidemiological study the regression slope between a response and predictor variable is underestimated when the predictor variable is measured imprecisely. Repeat measurements of the predictor in individuals in a subset of the study or in a separate study can be used to estimate a multiplicative factor to correct for this 'regression dilution bias'. In applied statistics publications various methods have been used to estimate this correction factor. Here we compare six different estimation methods and explain how they fall into two categories, namely regression and correlation-based methods. We provide new asymptotic variance formulae for the optimal correction factors in each category, when these are estimated from the repeat measurements subset alone, and show analytically and by simulation that the correlation method of choice gives uniformly lower variance. The simulations also show that, when the correction factor is not much greater than 1, this correlation method gives a correction factor which is closer to the true value than that from the best regression method on up to 80% of occasions. We also provide a variance formula for a modified correlation method which uses the standard deviation of the predictor variable in the main study; this shows further improved performance provided that the correction factor is not too extreme. A confidence interval for a corrected regression slope in an epidemiological study should reflect the imprecision of both the uncorrected slope and the estimated correction factor. We provide formulae for this and show that, particularly when the correction factor is large and the size of the subset of repeat measures is small, the effect of allowing for imprecision in the estimated correction factor can be substantial.]},
  author       = {Frost, Chris and Thompson, Simon G},
  publisher    = {[Wiley, Royal Statistical Society]},
  url          = {http://www.jstor.org/stable/2680496},
  date         = {2000-12},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Frost, Thompson - 2000 - Correcting for Regression Dilution Bias Comparison of Methods for a Single Predictor Variable.pdf:pdf},
  issn         = {09641998, 1467985X},
  journaltitle = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
  number       = {2},
  pages        = {173--189},
  title        = {{Correcting for Regression Dilution Bias: Comparison of Methods for a Single Predictor Variable}},
  volume       = {163},
}

@inproceedings{Shiao2013,
  author    = {Shiao, Han-Tai and Cherkassky, Vladimir},
  publisher = {The Steering Committee of The World Congress in Computer Science, Computer {\ldots}},
  booktitle = {Proceedings of the International Conference on Data Mining (DMIN)},
  date      = {2013},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Shiao, Cherkassky - 2013 - SVM-based approaches for predictive modeling of survival data.pdf:pdf},
  pages     = {1},
  title     = {{SVM-based approaches for predictive modeling of survival data}},
}

@article{Parast2011,
  abstract     = {Abstract In disease screening and prognosis studies, an important task is to determine useful markers for identifying high-risk subgroups. Once such markers are established, they can be incorporated into public health practice to provide appropriate strategies for treatment or disease monitoring based on each individual's predicted risk. In the recent years, genetic and biological markers have been examined extensively for their potential to signal progression or risk of disease. In addition to these markers, it has often been argued that short-term outcomes may be helpful in making a better prediction of disease outcomes in clinical practice. In this paper we propose model-free non-parametric procedures to incorporate short-term event information to improve the prediction of a long-term terminal event. We include the optional availability of a single discrete marker measurement and assess the additional information gained by including the short-term outcome. We focus on the semi-competing risk setting where the short-term event is an intermediate event that may be censored by the terminal event while the terminal event is only subject to administrative censoring. Simulation studies suggest that the proposed procedures perform well in finite samples. Our procedures are illustrated using a data set of post-dialysis patients with end-stage renal disease.},
  author       = {Parast, Layla and Cheng, Su-Chun and Cai, Tianxi},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/bimj.201000150},
  annotation   = {doi: 10.1002/bimj.201000150},
  date         = {2011-03},
  doi          = {10.1002/bimj.201000150},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Parast, Cheng, Cai - 2011 - Incorporating short-term outcome information to predict long-term survival with discrete markers.pdf:pdf},
  issn         = {0323-3847},
  journaltitle = {Biometrical Journal},
  keywords     = {Biomarkers,Disease prognosis,Risk prediction,Survival analysis},
  number       = {2},
  pages        = {294--307},
  title        = {{Incorporating short-term outcome information to predict long-term survival with discrete markers}},
  volume       = {53},
}

@article{pkgmlaut,
  author = {Király, Franz J. and Kazakov, V},
  url    = {https://openreview.net/forum?id=HkeqbksAtm},
  date   = {2018},
  file   = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Kiraly, Kazakov - 2018 - Machine Learning Automation Toolbox (MLAUT).pdf:pdf},
  title  = {{Machine Learning Automation Toolbox (MLAUT)}},
}

@report{FSA2020,
  author      = {{Food Standards Agency}},
  institution = {Food Standards Agency},
  url         = {https://www.food.gov.uk/research/research-projects/the-fsa-risk-communication-toolkit},
  date        = {2020},
  file        = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Food Standards Agency - 2020 - The FSA Risk Communication Toolkit.pdf:pdf},
  title       = {{The FSA Risk Communication Toolkit}},
  type        = {techreport},
}

@article{Evers2008,
  author       = {Evers, Ludger and Messow, Claudia-Martina},
  publisher    = {Oxford University Press},
  date         = {2008},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Evers, Messow - 2008 - Sparse kernel methods for high-dimensional survival data.pdf:pdf},
  issn         = {1460-2059},
  journaltitle = {Bioinformatics},
  keywords     = {machine learning,ranking,ssvm,support vector machine,survival,svm},
  number       = {14},
  pages        = {1632--1638},
  title        = {{Sparse kernel methods for high-dimensional survival data}},
  volume       = {24},
}

@unpublished{Lee2018,
  author     = {Lee, Donald K K and Chen, Ningyuan and Ishwaran, Hemant},
  date       = {2019},
  eprint     = {arXiv:1701.07926v6},
  eprinttype = {arXiv},
  file       = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Chen, Ishwaran - 2019 - Boosted nonparametric hazards with time-dependent covariates.pdf:pdf},
  keywords   = {functional data,gradient boosting,like-,regression trees,step-size shrinkage,survival analysis},
  title      = {{Boosted nonparametric hazards with time-dependent covariates}},
}

@article{pkgcheckmate,
  author       = {Lang, Michel},
  date         = {2017},
  journaltitle = {The R Journal},
  number       = {1},
  pages        = {437--445},
  title        = {{checkmate: Fast Argument Checks for Defensive R Programming}},
  volume       = {9},
}

@article{Johnson2003,
  author       = {Johnson, Eric J and Goldstein, Daniel},
  url          = {http://science.sciencemag.org/content/302/5649/1338.abstract},
  date         = {2003-11},
  doi          = {10.1126/science.1091721},
  journaltitle = {Science},
  number       = {5649},
  pages        = {1338 LP -- 1339},
  title        = {{Do Defaults Save Lives?}},
  volume       = {302},
}

@article{Austin2017,
  abstract     = {Predicting outcomes that occur over time is important in clinical, population health, and health services research. We compared changes in different measures of performance when a novel risk factor or marker was added to an existing Cox proportional hazards regression model. We performed Monte Carlo simulations for common measures of performance: concordance indices (c, including various extensions to survival outcomes), Royston's D index, R(2)-type measures, and Chambless' adaptation of the integrated discrimination improvement to survival outcomes. We found that the increase in performance due to the inclusion of a risk factor tended to decrease as the performance of the reference model increased. Moreover, the increase in performance increased as the hazard ratio or the prevalence of a binary risk factor increased. Finally, for the concordance indices and R(2)-type measures, the absolute increase in predictive accuracy due to the inclusion of a risk factor was greater when the observed event rate was higher (low censoring). Amongst the different concordance indices, Chambless and Diao's c-statistic exhibited the greatest increase in predictive accuracy when a novel risk factor was added to an existing model. Amongst the different R(2)-type measures, O'Quigley et al.'s modification of Nagelkerke's R(2) index and Kent and O'Quigley's [Formula: see text] displayed the greatest sensitivity to the addition of a novel risk factor or marker. These methods were then applied to a cohort of 8635 patients hospitalized with heart failure to examine the added benefit of a point-based scoring system for predicting mortality after initial adjustment with patient age alone.},
  author       = {Austin, Peter C. and Pencinca, Michael J. and Steyerberg, Ewout W.},
  date         = {2017},
  doi          = {10.1177/0962280214567141},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Austin, Pencinca, Steyerberg - 2017 - Predictive accuracy of novel risk factors and markers A simulation study of the sensitivity of dif.pdf:pdf},
  isbn         = {1097-0258 (Electronic)0̊277-6715 (Linking)},
  issn         = {14770334},
  journaltitle = {Statistical Methods in Medical Research},
  keywords     = {Cox proportional hazards model,Monte Carlo simulations,Survival analysis,discrimination,model performance,predictive accuracy,predictive models,risk factors},
  number       = {3},
  pages        = {1053--1077},
  title        = {{Predictive accuracy of novel risk factors and markers: A simulation study of the sensitivity of different performance measures for the Cox proportional hazards regression model}},
  volume       = {26},
}

@article{Thai2010,
  author       = {Thai-nghe, Nguyen and Gantner, Zeno and Schmidt-thieme, Lars},
  date         = {2010},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Thai-nghe, Gantner, Schmidt-thieme - 2010 - Cost-Sensitive Learning Methods for Imbalanced Data.pdf:pdf},
  isbn         = {9781424481262},
  journaltitle = {IEEE},
  title        = {{Cost-Sensitive Learning Methods for Imbalanced Data}},
}

@article{Sonabend2022b,
  author     = {Sonabend, Raphael and Zobolas, John and Kopper, Philipp and Burk, Lukas and Bender, Andreas},
  url        = {http://arxiv.org/abs/2212.05260},
  date       = {2024-12},
  eprint     = {2212.05260},
  eprinttype = {arXiv},
  title      = {{Examining properness in the external validation of survival models with squared and logarithmic losses}},
}

@unpublished{Turing2020,
  author    = {{Data Study Group Team}},
  url       = {https://www.turing.ac.uk/research/publications/data-study-group-final-report-great-ormond-street-hospital},
  booktitle = {Zenodo},
  date      = {2020},
  doi       = {10.5281/zenodo.3670726},
  title     = {{Data Study Group Final Report: Great Ormond Street Hospital.}},
}

@article{Hand2009,
  author       = {Hand, David J},
  date         = {2009},
  doi          = {10.1007/s10994-009-5119-5},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Hand - 2009 - Measuring classifier performance a coherent alternative to the area under the ROC curve.pdf:pdf},
  journaltitle = {Mach Learn},
  keywords     = {auc,classification,cost,error rate,loss,misclassification,rate,roc curves,sensitivity,specificity},
  pages        = {103--123},
  title        = {{Measuring classifier performance : a coherent alternative to the area under the ROC curve}},
  volume       = {77},
}

@article{Yuan2018,
  abstract     = {Prediction performance of a risk scoring system needs to be carefully assessed before its adoption in clinical practice. Clinical preventive care often uses risk scores to screen asymptomatic population. The primary clinical interest is to predict the risk of having an event by a prespecified future time t0. Accuracy measures such as positive predictive values have been recommended for evaluating the predictive performance. However, for commonly used continuous or ordinal risk score systems, these measures require a subjective cutoff threshold value that dichotomizes the risk scores. The need for a cutoff value created barriers for practitioners and researchers. In this paper, we propose a threshold-free summary index of positive predictive values that accommodates time-dependent event status and competing risks. We develop a nonparametric estimator and provide an inference procedure for comparing this summary measure between 2 risk scores for censored time to event data. We conduct a simulation study to examine the finite-sample performance of the proposed estimation and inference procedures. Lastly, we illustrate the use of this measure on a real data example, comparing 2 risk score systems for predicting heart failure in childhood cancer survivors.},
  author       = {Yuan, Yan and Zhou, Qian M and Li, Bingying and Cai, Hengrui and Chow, Eric J and Armstrong, Gregory T},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/sim.7606},
  annotation   = {doi: 10.1002/sim.7606},
  date         = {2018-05},
  doi          = {10.1002/sim.7606},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Yuan et al. - 2018 - A threshold-free summary index of prediction accuracy for censored time to event data.pdf:pdf},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  keywords     = {censored event time,positive predictive value,precision-recall curve,risk prediction,screening,time-dependent prediction accuracy},
  number       = {10},
  pages        = {1671--1681},
  title        = {{A threshold-free summary index of prediction accuracy for censored time to event data}},
  volume       = {37},
}

@article{Cheng1995,
  author       = {Cheng, S. C. and Wei, L. J. and Ying, Z.},
  date         = {1995},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Cheng, Wei, Ying - 1995 - Analysis of Transformation Models with Censored Data.pdf:pdf},
  journaltitle = {Biometrika},
  number       = {4},
  pages        = {835--845},
  title        = {{Analysis of Transformation Models with Censored Data}},
  volume       = {82},
}

@article{Nasejje2017,
  abstract     = {BACKGROUND: Random survival forest (RSF) models have been identified as alternative methods to the Cox proportional hazards model in analysing time-to-event data. These methods, however, have been criticised for the bias that results from favouring covariates with many split-points and hence conditional inference forests for time-to-event data have been suggested. Conditional inference forests (CIF) are known to correct the bias in RSF models by separating the procedure for the best covariate to split on from that of the best split point search for the selected covariate. METHODS: In this study, we compare the random survival forest model to the conditional inference model (CIF) using twenty-two simulated time-to-event datasets. We also analysed two real time-to-event datasets. The first dataset is based on the survival of children under-five years of age in Uganda and it consists of categorical covariates with most of them having more than two levels (many split-points). The second dataset is based on the survival of patients with extremely drug resistant tuberculosis (XDR TB) which consists of mainly categorical covariates with two levels (few split-points). RESULTS: The study findings indicate that the conditional inference forest model is superior to random survival forest models in analysing time-to-event data that consists of covariates with many split-points based on the values of the bootstrap cross-validated estimates for integrated Brier scores. However, conditional inference forests perform comparably similar to random survival forests models in analysing time-to-event data consisting of covariates with fewer split-points. CONCLUSION: Although survival forests are promising methods in analysing time-to-event data, it is important to identify the best forest model for analysis based on the nature of covariates of the dataset in question.},
  author       = {Nasejje, Justine B and Mwambi, Henry and Dheda, Keertan and Lesosky, Maia},
  language     = {eng},
  publisher    = {BioMed Central},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/28754093 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5534080/},
  date         = {2017-07},
  doi          = {10.1186/s12874-017-0383-8},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Nasejje et al. - 2017 - A comparison of the conditional inference survival forest model to random survival forests based on a simulation.pdf:pdf},
  issn         = {1471-2288},
  journaltitle = {BMC medical research methodology},
  keywords     = {*Algorithms,*Models,*Proportional Hazards Models,*Survival Analysis,Child,Conditional inference forests,Drug Resistance,Humans,Random survival forests,Split-points,Statistical,Survival analysis,Survival trees,Tuberculosis/drug therapy,Uganda,logrank,random forests,splitting rules,survival forests,survival trees},
  number       = {1},
  pages        = {115},
  title        = {{A comparison of the conditional inference survival forest model to random survival forests based on a simulation study as well as on two applications with time-to-event data}},
  volume       = {17},
}

@article{Biganzoli1998,
  abstract     = {Abstract Flexible modelling in survival analysis can be useful both for exploratory and predictive purposes. Feed forward neural networks were recently considered for flexible non-linear modelling of censored survival data through the generalization of both discrete and continuous time models. We show that by treating the time interval as an input variable in a standard feed forward network with logistic activation and entropy error function, it is possible to estimate smoothed discrete hazards as conditional probabilities of failure. We considered an easily implementable approach with a fast selection criteria of the best configurations. Examples on data sets from two clinical trials are provided. The proposed artificial neural network (ANN) approach can be applied for the estimation of the functional relationships between covariates and time in survival data to improve model predictivity in the presence of complex prognostic relationships. ? 1998 John Wiley & Sons, Ltd.},
  author       = {Biganzoli, Elia and Boracchi, Patrizia and Mariani, Luigi and Marubini, Ettore},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/(SICI)1097-0258(19980530)17:10%3C1169::AID-SIM796%3E3.0.CO http://2-d},
  annotation   = {doi: 10.1002/(SICI)1097-0258(19980530)17:103.0.CO;2-D},
  date         = {1998-05},
  doi          = {10.1002/(SICI)1097-0258(19980530)17:10<1169::AID-SIM796>3.0.CO;2-D},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Biganzoli et al. - 1998 - Feed forward neural networks for the analysis of censored survival data a partial logistic regression approach.pdf:pdf},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  number       = {10},
  pages        = {1169--1186},
  title        = {{Feed forward neural networks for the analysis of censored survival data: a partial logistic regression approach}},
  volume       = {17},
}

@article{KalbfleischPrentice1973,
  abstract     = {Marginal likelihoods are obtained for the regression parameters in the model presented by Cox (1972). If no ties occur in the recording of failure time data the results of Cox are given a straightforward justification. If ties occur in the data, results different from those suggested by Cox are obtained. Some Monte-Carlo comparisons of these competing results are made. A discrete model is developed for grouped data from Cox's model and estimates of the survivor function are given for both continuous and grouped data.},
  author       = {Kalbfleisch, J. D. and Prentice, R. L.},
  date         = {1973},
  doi          = {10.1093/biomet/60.2.267},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Kalbfleisch, Prentice - 1973 - Marginal likelihoods based on Cox's regression and life model.pdf:pdf},
  isbn         = {00063444},
  issn         = {00063444},
  journaltitle = {Biometrika},
  keywords     = {Censored data,Group invariance,Marginal likelihood,Marginal sufficiency,Ranks,Regression,Survivals with covariates,Tied data},
  number       = {2},
  pages        = {267--278},
  title        = {{Marginal likelihoods based on Cox's regression and life model}},
  volume       = {60},
}

@article{Tutz2007,
  author       = {Tutz, Gerhard and Binder, Harald},
  date         = {2007-02},
  doi          = {10.1016/j.csda.2006.11.041},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Tutz, Binder - 2007 - Boosting Ridge Regression.pdf:pdf},
  journaltitle = {Computational Statistics & Data Analysis},
  pages        = {6044--6059},
  title        = {{Boosting Ridge Regression}},
  volume       = {51},
}

@misc{Therneau2020,
  author  = {Therneau, Terry M. and Atkinson, Elizabeth},
  url     = {https://cran.r-project.org/web/packages/survival/vignettes/concordance.pdf},
  date    = {2020},
  file    = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Therneau, Atkinson - 2020 - Concordance.pdf:pdf},
  title   = {{Concordance}},
  urldate = {2020-06-02},
}

@article{Schmid2011,
  abstract     = {In clinical applications, the prediction error of survival models has to be taken into consideration to assess the practical suitability of conclusions drawn from these models. Different approaches to evaluate the predictive performance of survival models have been suggested in the literature. In this article, we analyze the properties of the estimator of prediction error developed by Schemper and Henderson (2000, Biometrics 56, 249-255), which quantifies the absolute distance between predicted and observed survival functions. We provide a formal proof that the estimator proposed by Schemper and Henderson is not robust against misspecification of the survival model, that is, the estimator will only be meaningful if the model family used for deriving predictions has been specified correctly. To remedy this problem, we construct a new estimator of the absolute distance between predicted and observed survival functions. We show that this modified Schemper-Henderson estimator is robust against model misspecification, allowing its practical application to a wide class of survival models. The properties of the Schemper-Henderson estimator and its new modification are illustrated by means of a simulation study and the analysis of two clinical data sets.},
  author       = {Schmid, Matthias and Hielscher, Thomas and Augustin, Thomas and Gefeller, Olaf},
  date         = {2011},
  doi          = {10.1111/j.1541-0420.2010.01459.x},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Schmid et al. - 2011 - A Robust Alternative to the Schemper-Henderson Estimator of Prediction Error.pdf:pdf},
  issn         = {0006341X},
  journaltitle = {Biometrics},
  keywords     = {Consistency,Model misspecification,Prediction error,Prognostic performance,Survival analysis},
  number       = {2},
  pages        = {524--535},
  title        = {{A Robust Alternative to the Schemper-Henderson Estimator of Prediction Error}},
  volume       = {67},
}

@article{Chawla2002,
  abstract     = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
  author       = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, Philip W.},
  date         = {2002},
  doi          = {10.1613/jair.953},
  eprint       = {1106.1813},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Chawla et al. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf:pdf},
  isbn         = {013805326X},
  issn         = {10769757},
  journaltitle = {Journal of Artifical Intelligence Research},
  pages        = {321--357},
  title        = {{SMOTE: Synthetic Minority Over-sampling Technique}},
  volume       = {16},
}

@article{pkgdistr6,
  abstract     = {distr6 is an object-oriented (OO) probability distributions interface leveraging the extensibil ity and scalability of R6 and the speed and efficiency of Rcpp. Over 50 probability distributions are currently implemented in the package with ‘core' methods, including density, distribution, and gener ating functions, and more ‘exotic' ones, including hazards and distribution function anti-derivatives. In addition to simple distributions, distr6 supports compositions such as truncation, mixtures, and product distributions. This paper presents the core functionality of the package and demonstrates examples for key use-cases. In addition, this paper provides a critical review of the object-oriented programming paradigms in R and describes some novel implementations for design patterns and core object-oriented features introduced by the package for supporting distr6 components.},
  author       = {Sonabend, Raphael and Kiraly, Franz J.},
  url          = {https://journal.r-project.org/archive/2021/RJ-2021-055/index.html https://cran.r-project.org/package=distr6},
  date         = {2021},
  doi          = {10.32614/RJ-2021-055},
  eprint       = {2009.02993},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Sonabend, Kiraly - 2021 - distr6 R6 Object-Oriented Probability Distributions Interface in R.pdf:pdf},
  journaltitle = {The R Journal},
  number       = {1},
  pages        = {444--466},
  title        = {{distr6: R6 Object-Oriented Probability Distributions Interface in R}},
  volume       = {13},
}

@misc{Saul2016,
  author    = {Saul, Alan D},
  publisher = {University of Sheffield},
  date      = {2016},
  keywords  = {gp,survey,survival},
  title     = {{Gaussian Process Based Approaches for Survival Analysis}},
}

@misc{pkgpysurvival,
  author = {Fotso, Stephane},
  url    = {https://www.pysurvival.io/},
  date   = {2019},
  title  = {{PySurvival: Open source package for Survival Analysis modeling}},
}

@article{pkgpartykit,
  author       = {Hothorn, Torsten and Zeileis, Achim},
  url          = {http://jmlr.org/papers/v16/hothorn15a.html},
  date         = {2015},
  journaltitle = {Journal of Machine Learning Research},
  pages        = {3905--3909},
  title        = {{partykit: A Modular Toolkit for Recursive Partytioning in R.}},
  volume       = {16},
}

@misc{pkgdistributions,
  author = {Lin, Dahua and White, John Myles and Byrne, Simon and Bates, Douglas and Noack, Andreas and Pearson, John and Arslan, Alex and Squire, Kevin and Anthoff, David and Papamarkou, Theodore and Besançon, Mathieu and Drugowitsch, Jan and Schauer, Moritz},
  date   = {2019},
  doi    = {10.5281/zenodo.2647458},
  title  = {{JuliaStats/Distributions.jl: a Julia package for probability distributions and associated functions}},
}

@book{pkgnnet,
  author     = {{N. Venables}, W and {D. Ripley}, B},
  publisher  = {Springer},
  url        = {http://www.stats.ox.ac.uk/pub/MASS4},
  annotation = {ISBN 0-387-95457-0},
  date       = {2002},
  title      = {{Modern Applied Statistics with S}},
}

@misc{pkgdiscsurv,
  author     = {Welchowski, Thomas and Schmid, Matthias},
  publisher  = {CRAN},
  url        = {https://cran.r-project.org/package=discSurv},
  annotation = {R package version 1.4.1},
  date       = {2019},
  title      = {{discSurv: Discrete Time Survival Analysis}},
}

@article{Chen2012,
  abstract     = {Cancer survival studies are commonly analyzed using survival-time prediction models for cancer prognosis. A number of different performance metrics are used to ascertain the concordance between the predicted risk score of each patient and the actual survival time, but these metrics can sometimes conflict. Alternatively, patients are sometimes divided into two classes according to a survival-time threshold, and binary classifiers are applied to predict each patient's class. Although this approach has several drawbacks, it does provide natural performance metrics such as positive and negative predictive values to enable unambiguous assessments. We compare the survival-time prediction and survival-time threshold approaches to analyzing cancer survival studies. We review and compare common performance metrics for the two approaches. We present new randomization tests and cross-validation methods to enable unambiguous statistical inferences for several performance metrics used with the survival-time prediction approach. We consider five survival prediction models consisting of one clinical model, two gene expression models, and two models from combinations of clinical and gene expression models. A public breast cancer dataset was used to compare several performance metrics using five prediction models. 1) For some prediction models, the hazard ratio from fitting a Cox proportional hazards model was significant, but the two-group comparison was insignificant, and vice versa. 2) The randomization test and cross-validation were generally consistent with the p-values obtained from the standard performance metrics. 3) Binary classifiers highly depended on how the risk groups were defined; a slight change of the survival threshold for assignment of classes led to very different prediction results. 1) Different performance metrics for evaluation of a survival prediction model may give different conclusions in its discriminatory ability. 2) Evaluation using a high-risk versus low-risk group comparison depends on the selected risk-score threshold; a plot of p-values from all possible thresholds can show the sensitivity of the threshold selection. 3) A randomization test of the significance of Somers' rank correlation can be used for further evaluation of performance of a prediction model. 4) The cross-validated power of survival prediction models decreases as the training and test sets become less balanced.},
  author       = {Chen, Hung Chia and Kodell, Ralph L. and Cheng, Kuang Fu and Chen, James J.},
  date         = {2012},
  doi          = {10.1186/1471-2288-12-102},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2012 - Assessment of performance of survival prediction models for cancer prognosis.pdf:pdf},
  isbn         = {1471-2288 (Electronic) 1471-2288 (Linking)},
  issn         = {14712288},
  journaltitle = {BMC Medical Research Methodology},
  title        = {{Assessment of performance of survival prediction models for cancer prognosis}},
  volume       = {12},
}

@report{Sonabend2021d,
  author      = {Sonabend, Raphael and Imai, Natsuko and Knock, Edward S and Perez-Guzman, Pablo N and Whittles, Lilith K and Rawson, Thomas and Mangal, Tara and Volz, Erik M and Ferguson, Neil M and Baguelin, Marc and Cori, Anne},
  institution = {Imperial College COVID-19 Response Team},
  url         = {https://www.gov.uk/government/publications/imperial-college-london-evaluating-the-roadmap-out-of-lockdown-for-england-modelling-the-delayed-step-4-of-the-roadmap-in-the-context-of-the-delta-v},
  date        = {2021},
  title       = {{Evaluating the Roadmap out of Lockdown for England: modelling the delayed step 4 of the roadmap in the context of the Delta variant}},
  type        = {techreport},
}

@article{Royston2008,
  abstract     = {Because of censoring, standard methods of plotting individual survival times are invalid. Therefore, graphic display of time-to-event data usually takes the form of a Kaplan–Meier survival plot. Kaplan–Meier plots, however, make differences between groups seem larger than they really are. To overcome these limitations, we developed a technique for producing scatter plots with survival data and applied it to data from a randomized trial of patients with renal cancer. As of June 21, 2001, 25 of the 347 patients with kidney cancer in the Medical Research Council RE01 randomized treatment trial for whom data were available had been censored, and the remainder had died. Values of the censored survival times were imputed by assuming a log-normal distribution in survival times and by drawing a random sample given that that each patient with censored data survived at least to the point of censoring. The combined original and imputed data were then examined by use of dot plots and scatter plots. In the RE01 trial, median survival of patients treated with interferon was 3.0 months (95% confidence interval = 0.3 to 5.5 months) longer than that in patients treated with medroxyprogesterone acetate. The Kaplan–Meier analysis showed clear separation between treatment groups and between prognostic groups. In contrast, comparisons of individual observed and imputed survival times between groups of patients showed considerable overlap and gave a more realistic idea of the modest between-group differences than Kaplan–Meier comparisons. These graphs of the distribution of survival times for individuals in each study group, which are simple to produce, may usefully complement Kaplan–Meier plots.},
  author       = {Royston, Patrick and Parmar, Mahesh K B and Altman, Douglas G},
  url          = {https://doi.org/10.1093/jnci/djm265},
  date         = {2008-01},
  doi          = {10.1093/jnci/djm265},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Royston, Parmar, Altman - 2008 - Visualizing Length of Survival in Time-to-Event Studies A Complement to Kaplan–Meier Plots.pdf:pdf},
  issn         = {0027-8874},
  journaltitle = {JNCI: Journal of the National Cancer Institute},
  number       = {2},
  pages        = {92--97},
  title        = {{Visualizing Length of Survival in Time-to-Event Studies: A Complement to Kaplan–Meier Plots}},
  volume       = {100},
}

@article{Reyna2004,
  abstract     = {Many health and safety problems, including war and terrorism, are by-products of how people reason about risk. I describe a new approach to reasoning about risk that implements a modern dual-process model of memory called fuzzy-trace theory. This approach posits encoding of both verbatim and gist representations, with reliance on the latter whenever possible; dependence of reasoning on retrieval cues that access stored values and principles; and vulnerability of reasoning to processing interference from overlapping classes of events, which causes denominator neglect in risk or probability judgments. These simple principles explain classic and new findings, for example, the finding that people overestimate small risks but ignore very small risks. Fuzzy-trace theory differs from other dual-process approaches to reasoning in that it places intuition at the apex of development, considering fuzzy intuitive processing more advanced than precise computational processing (e.g., trading off risks and rewards). The theory supplies a conception of rationality that distinguishes degrees of severity of errors in reasoning. It also includes a mechanism for achieving consistency in reasoning, a hallmark of rationality, by explaining how a person can treat superficially different reasoning problems in the same way if the problems share an underlying gist.},
  author       = {Reyna, Valerie F},
  publisher    = {SAGE Publications Inc},
  url          = {https://doi.org/10.1111/j.0963-7214.2004.00275.x},
  annotation   = {doi: 10.1111/j.0963-7214.2004.00275.x},
  date         = {2004-04},
  doi          = {10.1111/j.0963-7214.2004.00275.x},
  issn         = {0963-7214},
  journaltitle = {Current Directions in Psychological Science},
  number       = {2},
  pages        = {60--66},
  title        = {{How People Make Decisions That Involve Risk: A Dual-Processes Approach}},
  volume       = {13},
}

@article{pkgmlr,
  author       = {Bischl, Bernd and Lang, Michel and Kotthoff, Lars and Schiffner, Julia and Richter, Jakob and Studerus, Erich and Casalicchio, Giuseppe and Jones, Zachary M},
  url          = {http://jmlr.org/papers/v17/15-066.html https://cran.r-project.org/package=mlr},
  date         = {2016},
  journaltitle = {Journal of Machine Learning Research},
  number       = {170},
  pages        = {1--5},
  title        = {{mlr: Machine Learning in R}},
  volume       = {17},
}

@article{datanafld1,
  abstract     = {Recent population-based data on nonalcoholic fatty liver disease (NAFLD) epidemiology in general, and incidence in particular, are lacking. We examined trends in NAFLD incidence in a U.S. community and the impact of NAFLD on incident metabolic comorbidities (MCs), cardiovascular (CV) events, and mortality. A community cohort of all adults diagnosed with NAFLD in Olmsted County, Minnesota, between 1997 and 2014 was constructed using the Rochester Epidemiology Project database. The yearly incidence rate were calculated. The impact of NAFLD on incident MCs, CV events, and mortality was studied using a multistate model, with a 4:1 age- and sex-matched general population as a reference. We identified 3,869 NAFLD subjects (median age, 53; 52% women) and 15,209 controls; median follow-up was 7 (1-20) years. NAFLD incidence increased 5-fold, from 62 to 329 in 100,000 person-years. The increase was highest (7-fold) in young adults, aged 18-39 years. The 10-year mortality was higher in NAFLD subjects (10.2%) than controls (7.6%; P < 0.0001). NAFLD was an independent risk factor for incident MCs and death. Mortality risk decreased as the number of incident MCs increased: relative risk (RR) = 2.16 (95% confidence [CI], 1.41-3.31), 1.99 (95% CI, 1.48-2.66), 1.75 (95% CI, 1.42-2.14), and 1.08 (95% CI, 0.89-1.30) when 0, 1, 2, or 3 MCs were present, respectively. The NAFLD impact on CV events was significant only in subjects without MCs (RR = 1.96; 95% CI = 1.35-2.86). NAFLD reduced life expectancy by 4 years, with more time spent in high metabolic burden. CONCLUSION: Incidence of NAFLD diagnosis in the community has increased 5-fold, particularly in young adults. NAFLD is a consequence, but also a precursor of MC. Incident MC attenuates the impact of NAFLD on death and annuls its impact on CV disease. (Hepatology 2018;67:1726-1736).},
  author       = {Allen, Alina M and Therneau, Terry M and Larson, Joseph J and Coward, Alexandra and Somers, Virend K and Kamath, Patrick S},
  language     = {eng},
  date         = {2018-05},
  doi          = {10.1002/hep.29546},
  issn         = {1527-3350 (Electronic)},
  journaltitle = {Hepatology (Baltimore, Md.)},
  keywords     = {Adolescent,Adult,Cardiovascular Diseases,Comorbidity,Databases,Factual,Female,Follow-Up Studies,Humans,Incidence,Male,Metabolic Syndrome,Minnesota,Non-alcoholic Fatty Liver Disease,Residence Characteristics,Risk Factors,Survival Rate,Young Adult,complications,epidemiology,etiology,mortality},
  number       = {5},
  pages        = {1726--1736},
  title        = {{Nonalcoholic fatty liver disease incidence and impact on metabolic burden and death: A 20 year-community study.}},
  volume       = {67},
}

@article{Lynam2020,
  abstract     = {There is much interest in the use of prognostic and diagnostic prediction models in all areas of clinical medicine. The use of machine learning to improve prognostic and diagnostic accuracy in this area has been increasing at the expense of classic statistical models. Previous studies have compared performance between these two approaches but their findings are inconsistent and many have limitations. We aimed to compare the discrimination and calibration of seven models built using logistic regression and optimised machine learning algorithms in a clinical setting, where the number of potential predictors is often limited, and externally validate the models.},
  author       = {Lynam, Anita L and Dennis, John M and Owen, Katharine R and Oram, Richard A and Jones, Angus G and Shields, Beverley M and Ferrat, Lauric A},
  url          = {https://doi.org/10.1186/s41512-020-00075-2},
  date         = {2020},
  doi          = {10.1186/s41512-020-00075-2},
  issn         = {2397-7523},
  journaltitle = {Diagnostic and Prognostic Research},
  number       = {1},
  pages        = {6},
  title        = {{Logistic regression has similar performance to optimised machine learning algorithms in a clinical setting: application to the discrimination between type 1 and type 2 diabetes in young adults}},
  volume       = {4},
}

@article{Ciampi1981,
  abstract     = {The survival experience of 982 non-Hodgkin's lymphoma patients registered at Princess Margaret Hospital, Toronto, between 1967 and 1975, was studied. Prognostic groups were obtained by means of a classification procedure based on standard statistical techniques; the variables utilized in the classification were ones of known reproducibility which could be measured with little inconvenience to the patient. The results show that these prognostic groups give better information than the Ann Arbor staging classification in the sense that the survival curves are clearly separated and "good prognosis" and "poor prognosis" groups are clearly identified. Implications for therapy planning are briefly discussed},
  author       = {Ciampi, A. and Bush, R. S. and Gospodarowicz, M. and Till, J. E.},
  annotation   = {Survival trees},
  date         = {1981},
  doi          = {10.1002/1097-0142(19810201)47:3<621::AID-CNCR2820470333>3.0.CO;2-0},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Ciampi et al. - 1981 - An approach to classifying prognostic factors related to survival experience for nonHodgkin's lymphoma patients B.pdf:pdf},
  issn         = {10970142},
  journaltitle = {Cancer},
  keywords     = {decision trees,machine learning,random forests,survival forests,survival trees},
  number       = {3},
  pages        = {621--627},
  title        = {{An approach to classifying prognostic factors related to survival experience for non?Hodgkin's lymphoma patients: Based on a series of 982 patients: 1967?1975}},
  volume       = {47},
}

@article{pkgIrace,
  author       = {López-Ibáñez, Manuel and Dubois-Lacoste, Jérémie and Cáceres, Leslie Pérez and Stützle, Thomas and Birattari, Mauro},
  date         = {2016},
  doi          = {10.1016/j.orp.2016.09.002},
  journaltitle = {Operations Research Perspectives},
  pages        = {43--58},
  title        = {{The irace package: Iterated Racing for Automatic Algorithm Configuration}},
  volume       = {3},
}

@misc{pkgxgboost,
  author     = {Chen, Tianqi and He, Tong and Benesty, Michael and Khotilovich, Vadim and Tang, Yuan and Cho, Hyunsu and Chen, Kailong and Mitchell, Rory and Cano, Ignacio and Zhou, Tianyi and Li, Mu and Xie, Junyuan and Lin, Min and Geng, Yifeng and Li, Yutian},
  publisher  = {CRAN},
  url        = {https://cran.r-project.org/package=xgboost},
  annotation = {R package version 1.0.0.2},
  date       = {2020},
  title      = {{xgboost: Extreme Gradient Boosting}},
}

@article{Royston2013,
  abstract     = {BACKGROUND: A prognostic model should not enter clinical practice unless it has been demonstrated that it performs a useful role. External validation denotes evaluation of model performance in a sample independent of that used to develop the model. Unlike for logistic regression models, external validation of Cox models is sparsely treated in the literature. Successful validation of a model means achieving satisfactory discrimination and calibration (prediction accuracy) in the validation sample. Validating Cox models is not straightforward because event probabilities are estimated relative to an unspecified baseline function.\n\nMETHODS: We describe statistical approaches to external validation of a published Cox model according to the level of published information, specifically (1) the prognostic index only, (2) the prognostic index together with Kaplan-Meier curves for risk groups, and (3) the first two plus the baseline survival curve (the estimated survival function at the mean prognostic index across the sample). The most challenging task, requiring level 3 information, is assessing calibration, for which we suggest a method of approximating the baseline survival function.\n\nRESULTS: We apply the methods to two comparable datasets in primary breast cancer, treating one as derivation and the other as validation sample. Results are presented for discrimination and calibration. We demonstrate plots of survival probabilities that can assist model evaluation.\n\nCONCLUSIONS: Our validation methods are applicable to a wide range of prognostic studies and provide researchers with a toolkit for external validation of a published Cox model.},
  author       = {Royston, Patrick and Altman, Douglas G.},
  annotation   = {Basically there are no equivalents to PSRs that can just yield a result + SE that is meaningful in itself and there are no baselines to be beaten. The best we can hope for at the moment is to run a wide number of discrimination tests on training and test data and see how they compare.},
  date         = {2013},
  doi          = {10.1186/1471-2288-13-33},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Royston, Altman - 2013 - External validation of a Cox prognostic model Principles and methods.pdf:pdf},
  isbn         = {1471-2288 (Electronic)1̊471-2288 (Linking)},
  issn         = {14712288},
  journaltitle = {BMC Medical Research Methodology},
  keywords     = {Calibration,Cox proportional hazards model,Discrimination,External validation,Prognostic models,Time to event data},
  number       = {1},
  title        = {{External validation of a Cox prognostic model: Principles and methods}},
  volume       = {13},
}

@article{Hadanny2022,
  author       = {Hadanny, Amir and Shouval, Roni and Wu, Jianhua and Gale, Chris P and Unger, Ron and Zahger, Doron and Gottlieb, Shmuel and Matetzky, Shlomi and Goldenberg, Ilan and Beigel, Roy and Iakobishvili, Zaza},
  publisher    = {Elsevier},
  url          = {https://doi.org/10.1016/j.jjcc.2021.11.006},
  annotation   = {doi: 10.1016/j.jjcc.2021.11.006},
  date         = {2022-01},
  doi          = {10.1016/j.jjcc.2021.11.006},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Hadanny et al. - 2022 - Machine learning-based prediction of 1-year mortality for acute coronary syndrome.pdf:pdf},
  issn         = {0914-5087},
  journaltitle = {Journal of Cardiology},
  title        = {{Machine learning-based prediction of 1-year mortality for acute coronary syndrome}},
}

@article{Schmid2008a,
  author       = {Schmid, Matthias and Hothorn, Torsten},
  publisher    = {Elsevier},
  date         = {2008},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Schmid, Hothorn - 2008 - Boosting additive models using component-wise P-splines.pdf:pdf},
  issn         = {0167-9473},
  journaltitle = {Computational Statistics & Data Analysis},
  number       = {2},
  pages        = {298--311},
  title        = {{Boosting additive models using component-wise P-splines}},
  volume       = {53},
}

@article{Cutler1958,
  author       = {Cutler, Sidney J. and Ederer, Fred},
  annotation   = {Non-parametric survival: life-table method},
  date         = {1958},
  journaltitle = {Journal of Chronic Diseases},
  number       = {6},
  pages        = {699--712},
  title        = {{Maximum utilization of the life table method in analyzing survival}},
  volume       = {8},
}

@article{Peters2007,
  author       = {Peters, Ellen and Hibbard, Judith and Slovic, Paul and Dieckmann, Nathan},
  publisher    = {Project HOPE-The People-to-People Health Foundation, Inc.},
  date         = {2007},
  issn         = {0278-2715},
  journaltitle = {Health Affairs},
  number       = {3},
  pages        = {741--748},
  title        = {{Numeracy skill and the communication, comprehension, and use of risk-benefit information}},
  volume       = {26},
}

@inproceedings{Biganzoli2009,
  abstract  = {Linear and non-linear flexible regression analysis techniques, such as those based on splines and feed forward artificial neural networks (FFANN), have been proposed for the statistical analysis of censored survival time data, to account for the presence of non linear effects of predictors. Among survival functions, the hazard has a biological interest for the study of the disease dynamics, moreover it allows for the estimation of cumulative incidence functions for predicting outcome probabilities over follow-up. Therefore, specific error functions and data representation have been introduced for FFANN extensions of generalized linear models, in the perspective of modelling the hazard function of censored survival data. These techniques can be applied to account for the prognostic contribution of new biomarkers in addition to the traditional ones.},
  author    = {Biganzoli, E M and Ambrogi, F and Boracchi, P},
  booktitle = {2009 International Joint Conference on Neural Networks},
  date      = {2009},
  doi       = {10.1109/IJCNN.2009.5178824},
  isbn      = {2161-4407 VO -},
  keywords  = {Artificial neural networks,Biological system modeling,Biomarkers,Diseases,Feeds,Hazards,Logistics,Probability,Regression analysis,Statistical analysis,biological interest,censored survival data,data handling,data representation,data structures,feed forward artificial neural networks,feedforward neural nets,linear flexible regression analysis,medical computing,nonlinear flexible regression analysis,partial logistic artificial neural networks,regression analysis,specific error functions,statistical analysis},
  pages     = {340--346},
  title     = {{Partial logistic artificial neural networks (PLANN) for flexible modeling of censored survival data}},
}

@article{Hanley1982,
  abstract     = {A representation and interpretation of the area under a receiver operating characteristic (ROC) curve obtained by the "rating" method, or by mathematical predictions based on patient characteristics, is presented. It is shown that in such a setting the area represents the probability that a randomly chosen diseased subject is (correctly) rated or ranked with greater suspicion than a randomly chosen non-diseased subject. Moreover, this probability of a correct ranking is the same quantity that is estimated by the already well-studied nonparametric Wilcoxon statistic. These two relationships are exploited to (a) provide rapid closed-form expressions for the approximate magnitude of the sampling variability, i.e., standard error that one uses to accompany the area under a smoothed ROC curve, (b) guide in determining the size of the sample required to provide a sufficiently reliable estimate of this area, and (c) determine how large sample sizes should be to ensure that one can statistically detect differences in the accuracy of diagnostic techniques.},
  author       = {Hanley, James A. and McNeil, Barbara J.},
  date         = {1982},
  doi          = {10.2196/jmir.9160},
  eprint       = {NIHMS150003},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Hanley, McNeil - 1982 - The Meaning and Use of the Area under a Receiver Operating Characteristic (ROC) Curve.pdf:pdf},
  isbn         = {0033-8419 (Print) 0033-8419 (Linking)},
  issn         = {14388871},
  journaltitle = {Radiology},
  keywords     = {Consumer ratings,Digital health,Online ratings,Patient satisfaction,Telemedicine,auc,evaluation},
  number       = {1},
  pages        = {29--36},
  title        = {{The Meaning and Use of the Area under a Receiver Operating Characteristic (ROC) Curve}},
  volume       = {143},
}

@article{Kusner2017,
  abstract   = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
  author     = {Kusner, Matt J. and Loftus, Joshua R. and Russell, Chris and Silva, Ricardo},
  url        = {http://arxiv.org/abs/1703.06856},
  date       = {2017},
  doi        = {10.1644/06-MAMM-A-187.1},
  eprint     = {1703.06856},
  eprinttype = {arXiv},
  file       = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Kusner et al. - 2017 - Counterfactual Fairness.pdf:pdf},
  isbn       = {0022-2372},
  issn       = {0022-2372},
  number     = {Nips},
  title      = {{Counterfactual Fairness}},
}

@article{Fleming1980,
  abstract     = {[The classical Kolmogorov one-sample goodness-of-fit and Smirnov two-sample test procedures are modified in an attempt to obtain increased power when applied to uncensored data. Both are then generalized for use with arbitrarily right-censored data. Motivation for the formulation of the generalized procedures is provided heuristically by appealing to the basic concept upon which the procedures for uncensored data are based; a theoretical motivation is provided by asymptotic weak convergence results. The size and power of the generalized Smirnov two-sample procedure are evaluated for small and moderate sample sizes using Monte Carlo simulations. Results of the simulations are also used to make comparisons with the Gehan-Wilcoxon and logrank two-sample test procedures. The generalized Smirnov procedure is found to maintain the size of the test in nearly all the cases studied, although it is conservative in small samples of heavily censored data. It is found to be more powerful than both the Gehan-Wilcoxon and the logrank procedures for the non-Lehmann alternatives considered.]},
  author       = {Fleming, Thomas R and O'Fallon, Judith R and O'Brien, Peter C and Harrington, David P},
  publisher    = {[Wiley, International Biometric Society]},
  url          = {http://www.jstor.org/stable/2556114},
  date         = {1980-04},
  doi          = {10.2307/2556114},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Fleming et al. - 1980 - Modified Kolmogorov-Smirnov Test Procedures with Application to Arbitrarily Right-Censored Data.pdf:pdf},
  issn         = {0006341X, 15410420},
  journaltitle = {Biometrics},
  number       = {4},
  pages        = {607--625},
  title        = {{Modified Kolmogorov-Smirnov Test Procedures with Application to Arbitrarily Right-Censored Data}},
  volume       = {36},
}

@article{pkgtorch,
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  date   = {2017},
  title  = {{Automatic differentiation in pytorch}},
}

@article{Korn1990,
  abstract     = {The predictive power of a set of prognostic variables in a survival time model is a concept distinct from the statistical significance of the variables or the adequacy of the model fit. In this paper we discuss the importance of quantifying the predictive power of a prognostic model, and suggest measures of explained variation as a possible quantification. The important features of our approach are that (1) the measures are completely model-based; (2) a specification of the time range of interest is easily incorporated; and (3) the null models used for comparison are derived as mixtures of the predicted distributions.},
  author       = {Korn, Edward L. and Simon, Richard},
  date         = {1990},
  doi          = {10.1002/sim.4780090503},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Korn, Simon - 1990 - Measures of explained variation for survival data.pdf:pdf},
  issn         = {10970258},
  journaltitle = {Statistics in Medicine},
  number       = {5},
  pages        = {487--503},
  title        = {{Measures of explained variation for survival data}},
  volume       = {9},
}

@misc{Vehtari2013,
  author  = {Vehtari, Aki and Joensuu, Heikki},
  url     = {https://users.aalto.fi/$\sim$ave/VehtariJoensuu_GIST_CT_timing_poster_2013.pdf},
  date    = {2013},
  title   = {{A Gaussian processes model for survival analysis with time dependent covariates and interval censoring}},
  urldate = {2020-04-20},
}

@article{Mateen2019,
  abstract     = {Bilal Mateen and Raphael Sonabend are searching for Santa Claus, armed only with a statistical model to predict ear size based on age. The plan sounds ridiculous, but there's a serious lesson here about the limits of predictive models},
  author       = {Mateen, Bilal and Sonabend, Raphael},
  publisher    = {John Wiley & Sons, Ltd (10.1111)},
  url          = {https://doi.org/10.1111/j.1740-9713.2019.01336.x},
  annotation   = {doi: 10.1111/j.1740-9713.2019.01336.x},
  date         = {2019-12},
  doi          = {10.1111/j.1740-9713.2019.01336.x},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Mateen, Sonabend - 2019 - All I want for Christmas is{\ldots}Rigorous validation of predictive models to prevent hasty generalisations.pdf:pdf},
  issn         = {1740-9705},
  journaltitle = {Significance},
  number       = {6},
  pages        = {20--24},
  title        = {{All I want for Christmas is{\ldots}Rigorous validation of predictive models to prevent hasty generalisations}},
  volume       = {16},
}

@article{Gelfand2000,
  abstract     = {We propose a novel semiparametric version of the widely used proportional hazards survival model. Features include an arbitrarily rich class of continuous base-line hazards, an attractive epidemiological interpretation of the hazard as a latent competing risk model and trivial handling of censoring. Models are fitted by using a data augmentation scheme. The methodology is applied to a data set recording times to first hospitalization following clinical diagnosis of acquired immune deficiency syndrome for a sample of 169 patients.},
  author       = {Gelfand, Alan E and Ghosh, Sujit K and Christiansen, Cindy and Soumerai, Stephen B and McLaughlin, Thomas J},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1111/1467-9876.00199},
  annotation   = {https://doi.org/10.1111/1467-9876.00199},
  date         = {2000-01},
  doi          = {https://doi.org/10.1111/1467-9876.00199},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Gelfand et al. - 2000 - Proportional hazards models a latent competing risk approach.pdf:pdf},
  issn         = {0035-9254},
  journaltitle = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  keywords     = {Censoring,Competing risk,Data augmentation,Semiparametric regression},
  number       = {3},
  pages        = {385--397},
  title        = {{Proportional hazards models: a latent competing risk approach}},
  volume       = {49},
}

@report{Perez-Guzman2021,
  author      = {Perez-Guzman, Pablo N and Imai, Natsuko and Knock, Edward S and Rawson, Thomas and Sonabend, Raphael and Kanapram, Divya Thekke and Whittles, Lilith K and Volz, Erik M and Ferguson, Neil M and Cori, Anne and Baguelin, Marc},
  institution = {Imperial College COVID-19 Response Team},
  url         = {https://www.gov.uk/government/publications/imperial-college-london-autumn-and-winter-2021-to-2022-potential-covid-19-epidemic-trajectories-13-october-2021},
  date        = {2021},
  title       = {{Autumn and Winter 2021-2022: potential COVID-19 epidemic trajectories}},
  type        = {techreport},
}

@article{Hess1995,
  abstract     = {A major assumption of the Cox proportional hazards model is that the effect of a given covariate does not change over time. If this assumption is violated, the simple Cox model is invalid, and more sophisticated analyses are required. This paper describes eight graphical methods for detecting violations of the propor- tional hazards assumption and demonstrates each on three published datasets with a single binary covariate. 1 discuss the relative merits of these methods. Smoothed plots of the scaled Schoenfeld residuals are recommended for assessing PH violations because they provide precise usable information about the time dependence of the covariate effects. INTRODUCTION},
  author       = {Hess, Kenneth R.},
  date         = {1995},
  doi          = {10.1002/sim.4780141510},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Hess - 1995 - Graphical methods for assessing violations of the proportional hazards assumption in cox regression.pdf:pdf},
  isbn         = {0277-6715 (Print)0̊277-6715 (Linking)},
  issn         = {10970258},
  journaltitle = {Statistics in Medicine},
  number       = {15},
  pages        = {1707--1723},
  title        = {{Graphical methods for assessing violations of the proportional hazards assumption in cox regression}},
  volume       = {14},
}

@article{Zhao2020,
  author       = {Zhao, Lili and Feng, Dai},
  url          = {https://arxiv.org/abs/1908.02337 https://ieeexplore.ieee.org/document/9034100/},
  date         = {2020-11},
  doi          = {10.1109/JBHI.2020.2980204},
  eprint       = {1908.02337},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Zhao, Feng - 2020 - DNNSurv Deep Neural Networks for Survival Analysis Using Pseudo Values.pdf:pdf},
  issn         = {2168-2194},
  journaltitle = {IEEE Journal of Biomedical and Health Informatics},
  number       = {11},
  pages        = {3308--3314},
  title        = {{Deep Neural Networks for Survival Analysis Using Pseudo Values}},
  volume       = {24},
}

@article{CoxSnell1968,
  author       = {R., Cox and J., Snell},
  date         = {1968},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/R., J. - 1968 - A General Definition of Residuals.pdf:pdf},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords     = {accelerated life tests,age-specific failure rate,asymptotic theory,censored data,conditional inference,hazard function,life table,limit estimate,medical applications,product,regression,reliability,theory,two-sample rank tests},
  number       = {2},
  pages        = {248--275},
  title        = {{A General Definition of Residuals}},
  volume       = {30},
}

@article{Stare2011,
  abstract     = {Summary There is no shortage of proposed measures of prognostic value of survival models in the statistical literature. They come under different names, including explained variation, correlation, explained randomness, and information gain, but their goal is common: to define something analogous to the coefficient of determination R(2)  in linear regression. None however have been uniformly accepted, none have been extended to general event history data, including recurrent events, and many cannot incorporate time-varying effects or covariates. We present here a measure specifically tailored for use with general dynamic event history regression models. The measure is applicable and interpretable in discrete or continuous time; with tied data or otherwise; with time-varying, time-fixed, or dynamic covariates; with time-varying or time-constant effects; with single or multiple event times; with parametric or semiparametric models; and under general independent censoring/observation. For single-event survival data with neither censoring nor time dependency it reduces to the concordance index. We give expressions for its population value and the variance of the estimator and explore its use in simulations and applications. A web link to R software is provided.},
  author       = {Stare, Janez and Perme, Maja Pohar and Henderson, Robin},
  date         = {2011},
  doi          = {10.1111/j.1541-0420.2010.01526.x},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Stare, Perme, Henderson - 2011 - A Measure of Explained Variation for Event History Data.pdf:pdf},
  isbn         = {1541-0420},
  issn         = {0006341X},
  journaltitle = {Biometrics},
  keywords     = {C-index,Dynamic models,Explained variation,Rank correlation,Recurrent events},
  number       = {3},
  pages        = {750--759},
  title        = {{A Measure of Explained Variation for Event History Data}},
  volume       = {67},
}

@article{Graf1995,
  abstract     = {[Two measures of explained variation for survival data have been proposed by Korn and Simon and by Schemper. In this paper, we demonstrate how these measures compare and work out the conceptual differences between them. Both compare the variance when a covariate is accounted for with the variance when it is ignored, but only the second incorporates differences between observed and fitted outcomes. First, the relationship between both measures is studied for the situation without censoring. It turns out that considering the explained variation as a process in time is helpful in this context as well as in its own right. Censoring is incorporated quite differently for the two measures. We illustrate the points made by examples of fictitious data and a study on the treatment of breast cancer.]},
  author       = {Graf, Erika and Schumacher, Martin},
  publisher    = {[Royal Statistical Society, Wiley]},
  url          = {http://www.jstor.org/stable/2348898},
  date         = {1995-06},
  doi          = {10.2307/2348898},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Graf, Schumacher - 1995 - An Investigation on Measures of Explained Variation in Survival Analysis.pdf:pdf},
  issn         = {00390526, 14679884},
  journaltitle = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  number       = {4},
  pages        = {497--507},
  title        = {{An Investigation on Measures of Explained Variation in Survival Analysis}},
  volume       = {44},
}

@misc{pkgrjags,
  author    = {Plummer, Martyn},
  publisher = {CRAN},
  url       = {https://cran.r-project.org/package=rjags},
  date      = {2018},
  title     = {{rjags: Bayesian Graphical Models using MCMC}},
}

@article{Xiang2000,
  abstract     = {Strategies that have been developed to extend NN prediction methods to accommodate right-censored data include methods due to Faraggi–Simon, Liestol–Andersen–Andersen, and a modification of the Buckley–James method. In a Monte Carlo simulation study, we evaluated the performance of all three NN methods with that of Cox regression models which included main effects and interactions, when interactions exist. Using the EPILOG PLUS{®} PROC NEURAL utility, feed-forward back-propagation networks were examined under nine designs representing a variety of experimental conditions which varied (a) the number of inputs and interactions, (b) the degree of censoring, (c) proportional vs. non-proportional hazards, and (d) sample size. Minimization methods were implemented that efficiently determined optimal parameters. The C-index was used as a measure of performance. For the testing phase of the study, none of the NN methods outperformed Cox regression. Compared to Cox regression, the Faraggi–Simon, Buckley–James, and Liestol–Andersen–Andersen methods performed as well as Cox regression for 7,5 and 1 of the nine designs, respectively. The effect on performance of modeling interactions in Cox regression, varying the number of intervals in the Liestol–Andersen–Andersen method, and varying the NN architecture are also presented. The results of our study suggest that NNs can serve as effective methods for modeling right-censored data. However, the performance of the NN is somewhat variable, depending on the underlying data structure.},
  author       = {Xiang, Anny and Lapuerta, Pablo and Ryutov, Alex and Buckley, Jonathan and Azen, Stanley},
  url          = {http://www.sciencedirect.com/science/article/pii/S0167947399000985},
  date         = {2000},
  doi          = {https://doi.org/10.1016/S0167-9473(99)00098-5},
  issn         = {0167-9473},
  journaltitle = {Computational Statistics & Data Analysis},
  keywords     = {Cox regression,Monte Carlo simulation,Neural network},
  number       = {2},
  pages        = {243--257},
  title        = {{Comparison of the performance of neural network methods and Cox regression for censored survival data}},
  volume       = {34},
}

@article{datamyeloid,
  abstract     = {Background/aims The goal of this article is to illustrate the utility of multi-state models in cancer clinical trials. Our specific aims are to describe multi-state models and how they differ from standard survival methods, to illustrate how multi-state models can facilitate deeper understanding of the treatment effect on multiple paths along the disease process that patients could experience in cancer clinical trials, to explain the differences between multi-state models and time-dependent Cox models, and to briefly describe available software to conduct such analyses. Methods Data from 717 newly diagnosed acute myeloid leukemia patients who enrolled in the CALGB 10603 trial were used as an illustrative example. The current probability-in-state was estimated using the Aalen-Johansen estimator. The restricted mean time in state was calculated as the area under the probability-in-state curves. Cox-type regression was used to evaluate the effect of midostaurin on the various clinical paths. Simulation was conducted using a newly constructed shiny application. All analyses were performed using the R software. Results Multi-state model analyses of CALGB 10603 suggested that the overall improvement in survival with midostaurin seen in the primary analysis possibly resulted from a higher complete remission rate in combination with a lower risk of relapse and of death after complete remission in patients treated with midostaurin. Simulation results, in a three-state illness-death without recovery model, demonstrate that multi-state models and time-dependent Cox models evaluate treatment effects from different frameworks. Conclusion Multi-state models allow detailed evaluation of treatment effects in complex clinical trial settings where patients can experience multiple paths between study enrollment and the final outcome. Multi-state models can be used as a complementary tool to standard survival analyses to provide deeper insights to the effects of treatment in trial settings with complex disease process.},
  author       = {Le-Rademacher, Jennifer G and Peterson, Ryan A and Therneau, Terry M and Sanford, Ben L and Stone, Richard M and Mandrekar, Sumithra J},
  language     = {eng},
  date         = {2018-10},
  doi          = {10.1177/1740774518789098},
  issn         = {1740-7753 (Electronic)},
  journaltitle = {Clinical trials (London, England)},
  keywords     = {Acute,Antineoplastic Agents,Disease-Free Survival,Humans,Leukemia,Models,Myeloid,Nonparametric,Proportional Hazards Models,Randomized Controlled Trials as Topic,Remission Induction,Statistical,Statistics,Staurosporine,Survival Analysis,analogs & derivatives,drug therapy,mortality,therapeutic use},
  number       = {5},
  pages        = {489--498},
  title        = {{Application of multi-state models in cancer clinical trials.}},
  volume       = {15},
}

@misc{pkgmboost,
  author    = {Hothorn, Torsten and Buehlmann, Peter and Kneib, Thomas and Schmid, Matthias and Hofner, Benjamin},
  publisher = {CRAN},
  url       = {https://cran.r-project.org/package=mboost},
  date      = {2020},
  title     = {{mboost: Model-Based Boosting}},
}

@article{Han2016,
  abstract     = {Disclosure of prognosis in end-of-life care is a practice that is widely and increasingly recommended. However, prognostic disclosure is known to be resisted by many dying persons and by physicians, who instead engage in a "collusion of silence"-discussing prognosis either not at all or in vague, indirect terms. Debates about the ethics of prognostic disclosure and non-disclosure have tended to focus on their relative benefits and harms, or on the psychological acceptability of prognostic information to dying persons. Unaddressed, however, is a more fundamental assumption upon which the practice of prognostic disclosure depends: that prognostic certainty is what dying persons ultimately need. In this essay I question this assumption. Reflecting on the experience of my father's recent death, I argue that prognostic certainty is not only unattainable but existentially irrelevant to many dying persons, and that prognostic uncertainty can be a greater need. Respect for individuals' existential need for uncertainty justifies prognostic silence and enables dying persons-as well as the loved ones and clinicians who care for them-to be open to new possibilities of finding meaning at the end of life.},
  author       = {Han, Paul K},
  language     = {eng},
  url          = {https://pubmed.ncbi.nlm.nih.gov/28690246 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5560765/},
  date         = {2016},
  doi          = {10.1353/pbm.2016.0049},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Han - 2016 - The Need for Uncertainty A Case for Prognostic Silence.pdf:pdf},
  issn         = {1529-8795},
  journaltitle = {Perspectives in biology and medicine},
  number       = {4},
  pages        = {567--575},
  title        = {{The Need for Uncertainty: A Case for Prognostic Silence}},
  volume       = {59},
}

@article{Baruch2014,
  author       = {Baruch, Fischhoff and Davis, Alex L.},
  publisher    = {Proceedings of the National Academy of Sciences},
  url          = {https://doi.org/10.1073/pnas.1317504111},
  annotation   = {doi: 10.1073/pnas.1317504111},
  date         = {2014-09},
  doi          = {10.1073/pnas.1317504111},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Baruch, L. - 2014 - Communicating scientific uncertainty.pdf:pdf},
  journaltitle = {Proceedings of the National Academy of Sciences},
  number       = {supplement_4},
  pages        = {13664--13671},
  title        = {{Communicating scientific uncertainty}},
  volume       = {111},
}

@article{Bello2019,
  abstract     = {Motion analysis is used in computer vision to understand the behaviour of moving objects in sequences of images. Optimizing the interpretation of dynamic biological systems requires accurate and precise motion tracking as well as efficient representations of high-dimensional motion trajectories so that these can be used for prediction tasks. Here we use image sequences of the heart, acquired using cardiac magnetic resonance imaging, to create time-resolved three-dimensional segmentations using a fully convolutional network trained on anatomical shape priors. This dense motion model formed the input to a supervised denoising autoencoder (4Dsurvival), which is a hybrid network consisting of an autoencoder that learns a task-specific latent code representation trained on observed outcome data, yielding a latent representation optimized for survival prediction. To handle right-censored survival outcomes, our network used a Cox partial likelihood loss function. In a study of 302 patients, the predictive accuracy (quantified by Harrell's C-index) was significantly higher (P = 0.0012) for our model C = 0.75 (95% CI: 0.70–0.79) than the human benchmark of C = 0.59 (95% CI: 0.53–0.65). This work demonstrates how a complex computer vision task using high-dimensional medical image data can efficiently predict human survival.},
  author       = {Bello, Ghalib A and Dawes, Timothy J W and Duan, Jinming and Biffi, Carlo and de Marvao, Antonio and Howard, Luke S G E and Gibbs, J Simon R and Wilkins, Martin R and Cook, Stuart A and Rueckert, Daniel and O'Regan, Declan P},
  url          = {https://doi.org/10.1038/s42256-019-0019-2},
  date         = {2019},
  doi          = {10.1038/s42256-019-0019-2},
  issn         = {2522-5839},
  journaltitle = {Nature Machine Intelligence},
  number       = {2},
  pages        = {95--104},
  title        = {{Deep-learning cardiac motion analysis for human survival prediction}},
  volume       = {1},
}

@article{Buhlmann2007,
  abstract     = {We present a statistical perspective on boosting. Special emphasis is given to estimating potentially complex parametric or nonparametric models, including generalized linear and additive models as well as regression models for survival analysis. Concepts of degrees of freedom and corresponding Akaike or Bayesian information criteria, particularly useful for regularization and variable selection in high-dimensional covariate spaces, are discussed as well. The practical aspects of boosting procedures for fitting statistical models are illustrated by means of the dedicated open-source software package mboost. This package implements functions which can be used for model fitting, prediction and variable selection. It is flexible, allowing for the implementation of new boosting algorithms optimizing user-specified loss functions.},
  author       = {Buhlmann, Peter and Hothorn, Torsten},
  language     = {en},
  publisher    = {The Institute of Mathematical Statistics},
  url          = {https://projecteuclid.org:443/euclid.ss/1207580163},
  annotation   = {Set step size to 0.1},
  date         = {2007},
  doi          = {10.1214/07-STS242},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Buhlmann, Hothorn - 2007 - Boosting Algorithms Regularization, Prediction and Model Fitting.pdf:pdf},
  issn         = {0883-4237},
  journaltitle = {Statist. Sci.},
  keywords     = {Generalized linear models,generalized additive models,gradient boosting,software,survival analysis,variable selection},
  number       = {4},
  pages        = {477--505},
  title        = {{Boosting Algorithms: Regularization, Prediction and Model Fitting}},
  volume       = {22},
}

@inproceedings{Avati2020,
  abstract   = {Probabilistic survival predictions from models trained with Maximum Likelihood Estimation (MLE) can have high, and sometimes unacceptably high variance. The field of meteorology, where the paradigm of maximizing sharpness subject to calibration is popular, has addressed this problem by using scoring rules beyond MLE, such as the Continuous Ranked Probability Score (CRPS). In this paper we present the \emph{Survival-CRPS}, a generalization of the CRPS to the survival prediction setting, with right-censored and interval-censored variants. We evaluate our ideas on the mortality prediction task using two different Electronic Health Record (EHR) data sets (STARR and MIMIC-III) covering millions of patients, with suitable deep neural network architectures: a Recurrent Neural Network (RNN) for STARR and a Fully Connected Network (FCN) for MIMIC-III. We compare results between the two scoring rules while keeping the network architecture and data fixed, and show that models trained with Survival-CRPS result in sharper predictive distributions compared to those trained by MLE, while still maintaining calibration.},
  author     = {Avati, Anand and Duan, Tony and Zhou, Sharon and Jung, Kenneth and Shah, Nigam H. and Ng, Andrew},
  url        = {https://proceedings.mlr.press/v115/avati20a.html http://arxiv.org/abs/1806.08324},
  annotation = {* 'defines' survival CRPS * claims strictly proper without proof * isn't proper},
  booktitle  = {Proceedings of Machine Learning Research},
  date       = {2020-06},
  eprint     = {1806.08324},
  eprinttype = {arXiv},
  file       = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Avati et al. - 2018 - Countdown Regression Sharp and Calibrated Survival Predictions.pdf:pdf},
  keywords   = {measure,scoring rule,survival},
  pages      = {145--155},
  title      = {{Countdown Regression: Sharp and Calibrated Survival Predictions}},
}

@article{Puddu2012,
  abstract     = {BACKGROUND: Projection pursuit regression, multilayer feed-forward networks, multivariate adaptive regression splines and trees (including survival trees) have challenged classic multivariable models such as the multiple logistic function, the proportional hazards life table Cox model (Cox), the Poisson's model, and the Weibull's life table model to perform multivariable predictions. However, only artificial neural networks (NN) have become popular in medical applications.\n\nRESULTS: We compared several Cox versus NN models in predicting 45-year all-cause mortality (45-ACM) by 18 risk factors selected a priori: age; father life status; mother life status; family history of cardiovascular diseases; job-related physical activity; cigarette smoking; body mass index (linear and quadratic terms); arm circumference; mean blood pressure; heart rate; forced expiratory volume; serum cholesterol; corneal arcus; diagnoses of cardiovascular diseases, cancer and diabetes; minor ECG abnormalities at rest. Two Italian rural cohorts of the Seven Countries Study, made up of men aged 40 to 59 years, enrolled and first examined in 1960 in Italy. Cox models were estimated by: a) forcing all factors; b) a forward-; and c) a backward-stepwise procedure. Observed cases of deaths and of survivors were computed in decile classes of estimated risk. Forced and stepwise NN were run and compared by C-statistics (ROC analysis) with the Cox models. Out of 1591 men, 1447 died. Model global accuracies were extremely high by all methods (ROCs > 0.810) but there was no clear-cut superiority of any model to predict 45-ACM. The highest ROCs (> 0.838) were observed by NN. There were inter-model variations to select predictive covariates: whereas all models concurred to define the role of 10 covariates (mainly cardiovascular risk factors), family history, heart rate and minor ECG abnormalities were not contributors by Cox models but were so by forced NN. Forced expiratory volume and arm circumference (two protectors), were not selected by stepwise NN but were so by the Cox models.\n\nCONCLUSIONS: There were similar global accuracies of NN versus Cox models to predict 45-ACM. NN detected specific predictive covariates having a common thread with physical fitness as related to job physical activity such as arm circumference and forced expiratory volume. Future attention should be concentrated on why NN versus Cox models detect different predictors.},
  author       = {Puddu, Paolo Emilio and Menotti, Alessandro},
  url          = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-12-100},
  date         = {2012-12},
  doi          = {10.1186/1471-2288-12-100},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Puddu, Menotti - 2012 - Artificial neural networks versus proportional hazards Cox models to predict 45-year all-cause mortality in the.pdf:pdf},
  isbn         = {1471-2288},
  issn         = {1471-2288},
  journaltitle = {BMC Medical Research Methodology},
  keywords     = {45-year follow-up,All-cause mortality,Cox models,Epidemiology,Neural networks,Prediction,Seven countries study,ann,cox,machine learning,neural networks,survival},
  number       = {1},
  pages        = {100},
  title        = {{Artificial neural networks versus proportional hazards Cox models to predict 45-year all-cause mortality in the Italian Rural Areas of the Seven Countries Study}},
  volume       = {12},
}

@article{Song2008,
  author       = {Song, Xiao and Zhou, Xiao-Hua},
  date         = {2008-07},
  journaltitle = {Statistica Sinica},
  pages        = {947--965},
  title        = {{A semiparametric approach for the covariate specific ROC curve with survival outcome}},
  volume       = {18},
}

@report{Brizzi2021,
  author      = {Brizzi, A and Whittaker, C and Servo, LMS and Hawryluk, I and {Prete Jr}, CA and {De Souza}, WM and Aguiar, RS and Araujo, LJT and Bastos, LS and Blenkinsop, A and Buss, LF and Candido, D and Castro, MC and Costa, SF and Croda, J and {De Souza Santos}, A and Dye, C and Flaxman, S and Fonseca, PLC and Geddes, VEV and Gutierrez, B and Lemey, P and Levin, AS and Mellan, T and Bonfim, DM and Miscouridou, X and Mishra, S and Monod, M and Moreira, FRR and Nelson, B and Pereira, RHM and Ranzani, O and Schnekenberg, RP and Semenova, E and Sonnabend, R and Souza, RP and Xi, X and Sabino, EC and Faria, NR and Bhatt, S and Ratmann, O},
  institution = {Imperial College London},
  url         = {http://hdl.handle.net/10044/1/91875},
  date        = {2021},
  doi         = {10.25561/91875},
  title       = {{Report 46: Factors driving extensive spatial and temporal fluctuations in COVID‐19 fatality rates in Brazilian hospitals}},
  type        = {techreport},
}

@book{Yasodhara2018,
  author = {Yasodhara, Angeline and Bhat, Mamatha and Goldenberg, Anna},
  date   = {2018-12},
  title  = {{Prediction of New Onset Diabetes after Liver Transplant}},
}

@article{Gordon1989,
  author     = {Gordon, Louis and Olshen, Richard A and Butler, Jeffrey H and Gilpin, Elizabeth A},
  annotation = {Original paper on the subject in 1985 unfound},
  date       = {1989},
  file       = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Gordon et al. - 1989 - Tree-Structured Survival Analyis, II.pdf:pdf},
  title      = {{Tree-Structured Survival Analyis, II}},
}

@article{pkgglmnetb,
  author       = {Simon, Noah and Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  date         = {2011},
  journaltitle = {Journal of Statistical Software},
  number       = {5},
  pages        = {1--13},
  title        = {{Regularization Paths for Cox's Proportional Hazards Model via Coordinate Descent.}},
  volume       = {39},
}

@inproceedings{Giunchiglia2018,
  author    = {Giunchiglia, Eleonora and Nemchenko, Anton and van der Schaar, Mihaela},
  publisher = {Springer},
  booktitle = {International Conference on Artificial Neural Networks},
  date      = {2018},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Giunchiglia, Nemchenko, van der Schaar - 2018 - Rnn-surv A deep recurrent model for survival analysis.pdf:pdf},
  pages     = {23--32},
  title     = {{Rnn-surv: A deep recurrent model for survival analysis}},
}

@article{Kantidakis2020,
  abstract     = {Predicting survival of recipients after liver transplantation is regarded as one of the most important challenges in contemporary medicine. Hence, improving on current prediction models is of great interest.Nowadays, there is a strong discussion in the medical field about machine learning (ML) and whether it has greater potential than traditional regression models when dealing with complex data. Criticism to ML is related to unsuitable performance measures and lack of interpretability which is important for clinicians.},
  author       = {Kantidakis, Georgios and Putter, Hein and Lancia, Carlo and de Boer, Jacob and Braat, Andries E and Fiocco, Marta},
  url          = {https://doi.org/10.1186/s12874-020-01153-1},
  date         = {2020},
  doi          = {10.1186/s12874-020-01153-1},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Kantidakis et al. - 2020 - Survival prediction models since liver transplantation - comparisons between Cox models and machine learning.pdf:pdf},
  issn         = {1471-2288},
  journaltitle = {BMC Medical Research Methodology},
  number       = {1},
  pages        = {277},
  title        = {{Survival prediction models since liver transplantation - comparisons between Cox models and machine learning techniques}},
  volume       = {20},
}

@article{Hart2019,
  author       = {Hart, Andrew and Maxim, Laura and Siegrist, Michael and {Von Goetz}, Natalie and da Cruz, Cristina and Merten, Caroline and Mosbach‐Schulz, Olaf and Lahaniatis, Majlinda and Smith, Anthony and Hardy, Anthony},
  institution  = {European Food Safety Authority},
  url          = {http://doi.wiley.com/10.2903/j.efsa.2019.5520},
  date         = {2019-01},
  doi          = {10.2903/j.efsa.2019.5520},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Hart et al. - 2019 - Guidance on Communication of Uncertainty in Scientific Assessments.pdf:pdf},
  issn         = {18314732},
  journaltitle = {EFSA Journal},
  number       = {1},
  title        = {{Guidance on Communication of Uncertainty in Scientific Assessments}},
  volume       = {17},
}

@article{Zhang2020,
  abstract     = {Cox proportional hazard model (CPH) is commonly used in clinical research for survival analysis. In quantitative medical imaging (radiomics) studies, CPH plays an important role in feature reduction and modeling. However, the underlying linear assumption of CPH model limits the prognostic performance. In this work, using transfer learning, a convolutional neural network (CNN) based survival model was built and tested on preoperative CT images of resectable Pancreatic Ductal Adenocarcinoma (PDAC) patients.},
  author       = {Zhang, Yucheng and Lobo-Mueller, Edrise M and Karanicolas, Paul and Gallinger, Steven and Haider, Masoom A and Khalvati, Farzad},
  url          = {https://doi.org/10.1186/s12880-020-0418-1},
  annotation   = {imaging},
  date         = {2020},
  doi          = {10.1186/s12880-020-0418-1},
  issn         = {1471-2342},
  journaltitle = {BMC Medical Imaging},
  number       = {1},
  pages        = {11},
  title        = {{CNN-based survival model for pancreatic ductal adenocarcinoma in medical imaging}},
  volume       = {20},
}

@thesis{Yang2010,
  abstract    = {In this study, we analyzed a data set from real commercial data on the purchase behaviors of 168 customers to predict the next purchase time. The data were grouped to the training set and test set, and analyzed by a piecewise standard Cox PH model, a piecewise marginal Cox model and the PLANN neural network approach. The effects of the following five factors were studied: the previous purchase interval, the type of a customer, the region and size of the city where a customer lives, and the season of the last purchase. The three models (two Cox's PH models and the ANN model) were used to predict the survival of the test set. In total eight subgroups of the test set were selected and their predicted survivals were compared to the KM survival estimates. The comparison shows that the ANN methods displayed similar predictability performance with that of the piecewise standard Cox PH model. Thus, the hypothesis that the ANN method is superior to the conventional Cox's PH models does not valid. The study reveals the following patterns in the purchase behaviors: 1) the next purchase interval approximately proportional to previous interval， while the output of the marginal Cox model indicates that for a customer the marginal effect of the previous interval on the next purchase interval is not significant. 2) The purchase interval of a customer living in big or medium city is not significant different with that of a customer in small or tiny city. 3) The customers whose types are ‘catering' or ‘horeca' have similar purchase trends and have shorter purchase periods than that of other type customers. The ‘particulier' customers tend to do their next purchase later. For the retail customer, no distinct results were shown. 4) The customers in Waals Brabant or Vlaams Braban tend to do their purchases after 7 days, while the customers in West Vlaanderen, Luxemburg and Limburg would like to purchase earlier than those in the other customers. 5) Customers tend to do the next purchases earlier during Apr. to Aug., and postpone the next purchases in Dec.},
  author      = {Yang, Yanying},
  institution = {Universiteit Gent},
  date        = {2010},
  file        = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Yang - 2010 - Neural Network Survival Analysis.pdf:pdf},
  isbn        = {9781617796326},
  keywords    = {ann,cox,machine learning,neural networks,survival},
  pages       = {57},
  title       = {{Neural Network Survival Analysis}},
  type        = {phdthesis},
}

@article{Breiman2001,
  author       = {Breiman, Leo},
  url          = {papers://5e3e5e59-48a2-47c1-b6b1-a778137d3ec1/Paper/p1789},
  date         = {2001},
  doi          = {10.1017/CBO9781107415324.004},
  eprint       = {arXiv:1011.1669v3},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Breiman - 2001 - Random Forests.pdf:pdf},
  isbn         = {9788578110796},
  issn         = {1098-6596},
  journaltitle = {Machine Learning},
  keywords     = {cart,classification,machine learning,model,random forests,regression},
  number       = {1},
  pages        = {5--32},
  title        = {{Random Forests}},
  volume       = {45},
}

@misc{pkggbm,
  author    = {Greenwell, Brandon and Boehmke, Bradley and Cunningham, Jay and {GBM Developers}, .},
  publisher = {CRAN},
  url       = {https://cran.r-project.org/package=gbm},
  date      = {2019},
  title     = {{gbm: Generalized Boosted Regression Models}},
}

@article{Korn1991,
  abstract     = {[A loss function approach is used to define the concepts of explained residual variation and explained risk for general regression models. Explained risk measures the ability of the covariates in a correctly specified model to distinguish differing outcomes. Explained residual variation, which is R2 for a linear model, estimates the explained risk with a penalty for poorly fitting models. Application of the general definitions to linear regression, logistic regression, and survival analysis is given. The importance of distinguishing the concepts of explained residual variation, explained risk, and goodness of fit is discussed.]},
  author       = {Korn, Edward L and Simon, Richard},
  publisher    = {[American Statistical Association, Taylor & Francis, Ltd.]},
  url          = {http://www.jstor.org/stable/2684290},
  date         = {1991-12},
  doi          = {10.2307/2684290},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Korn, Simon - 1991 - Explained Residual Variation, Explained Risk, and Goodness of Fit.pdf:pdf},
  issn         = {00031305},
  journaltitle = {The American Statistician},
  number       = {3},
  pages        = {201--206},
  title        = {{Explained Residual Variation, Explained Risk, and Goodness of Fit}},
  volume       = {45},
}

@article{Schunemann2011,
  abstract     = {This article describes how the Grading of Recommendations Assessment, Development and Evaluation (GRADE) approach to grading the quality of evidence and strength of recommendations considers the Bradford Hill criteria for causation and how GRADE may relate to questions in public health. A primary concern in public health is that evidence from non-randomised studies may provide a more adequate or best available measure of a public health strategy's impact, but that such evidence might be graded as lower quality in the GRADE framework. GRADE, however, presents a framework that describes both criteria for assessing the quality of research evidence and the strength of recommendations that includes considerations arising from the Bradford Hill criteria. GRADE places emphasis on recommendations and in assessing quality of evidence; GRADE notes that randomisation is only one of many relevant factors. This article describes how causation may relate to developing recommendations and how the Bradford Hill criteria are considered in GRADE, using examples from the public health literature with a focus on immunisation.},
  author       = {Schünemann, Holger and Hill, Suzanne and Guyatt, Gordon and Akl, Elie A. and Ahmed, Faruque},
  date         = {2011},
  doi          = {10.1136/jech.2010.119933},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Sch{\"{u}}nemann et al. - 2011 - The GRADE approach and Bradford Hill's criteria for causation.pdf:pdf},
  issn         = {0143005X},
  journaltitle = {Journal of Epidemiology and Community Health},
  number       = {5},
  pages        = {392--395},
  title        = {{The GRADE approach and Bradford Hill's criteria for causation}},
  volume       = {65},
}

@article{Reyna2021,
  abstract     = {A framework is presented for understanding how misinformation shapes decision-making, which has cognitive representations of gist at its core. I discuss how the framework goes beyond prior work, and how it can be implemented so that valid scientific messages are more likely to be effective, remembered, and shared through social media, while misinformation is resisted. The distinction between mental representations of the rote facts of a message—its verbatim representation—and its gist explains several paradoxes, including the frequent disconnect between knowing facts and, yet, making decisions that seem contrary to those facts. Decision makers can falsely remember the gist as seen or heard even when they remember verbatim facts. Indeed, misinformation can be more compelling than information when it provides an interpretation of reality that makes better sense than the facts. Consequently, for many issues, scientific information and misinformation are in a battle for the gist. A fuzzy-processing preference for simple gist explains expectations for antibiotics, the spread of misinformation about vaccination, and responses to messages about global warming, nuclear proliferation, and natural disasters. The gist, which reflects knowledge and experience, induces emotions and brings to mind social values. However, changing mental representations is not sufficient by itself; gist representations must be connected to values. The policy choice is not simply between constraining behavior or persuasion—there is another option. Science communication needs to shift from an emphasis on disseminating rote facts to achieving insight, retaining its integrity but without shying away from emotions and values.There are no relevant data associated with the paper.},
  author       = {Reyna, Valerie F},
  url          = {http://www.pnas.org/content/118/15/e1912441117.abstract},
  date         = {2021-04},
  doi          = {10.1073/pnas.1912441117},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Reyna - 2021 - A scientific theory of gist communication and misinformation resistance, with implications for health, education, and pol.pdf:pdf},
  journaltitle = {Proceedings of the National Academy of Sciences},
  number       = {15},
  pages        = {e1912441117},
  title        = {{A scientific theory of gist communication and misinformation resistance, with implications for health, education, and policy}},
  volume       = {118},
}

@article{Breslow1974,
  author       = {Breslow, N. and Crowley, J.},
  url          = {https://projecteuclid.org/download/pdf_1/euclid.aos/1176342705},
  date         = {1974},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Breslow, Crowley - 1974 - A large sample study of the life table and product limit estimates under random censorsnhip.pdf:pdf},
  journaltitle = {The Annals of Statistics},
  number       = {3},
  pages        = {437--453},
  title        = {{A large sample study of the life table and product limit estimates under random censorsnhip}},
  volume       = {2},
}

@misc{pkgdnnsurv,
  author = {Zhao, Lili and Feng, Dai},
  url    = {https://github.com/lilizhaoUM/DNNSurv},
  date   = {2020},
  title  = {{DNNSurv}},
}

@article{Hothorn2005,
  abstract     = {We propose a unified and flexible framework for ensemble learning in the presence of censoring. For right-censored data, we introduce a random forest algorithm and a generic gradient boosting algorithm for the construction of prognostic and diagnostic models. The methodology is utilized for predicting the survival time of patients suffering from acute myeloid leukemia based on clinical and genetic covariates. Furthermore, we compare the diagnostic capabilities of the proposed censored data random forest and boosting methods, applied to the recurrence-free survival time of node-positive breast cancer patients, with previously published findings.},
  author       = {Hothorn, Torsten and Bühlmann, Peter and Dudoit, Sandrine and Molinaro, Annette and {Van Der Laan}, Mark J},
  url          = {https://doi.org/10.1093/biostatistics/kxj011},
  date         = {2005-12},
  doi          = {10.1093/biostatistics/kxj011},
  issn         = {1465-4644},
  journaltitle = {Biostatistics},
  number       = {3},
  pages        = {355--373},
  title        = {{Survival ensembles}},
  volume       = {7},
}

@article{pkgsurvivalsvm,
  abstract     = {This article introduces the R package survivalsvm, implementing support vector machines for survival analysis. Three approaches are available in the package: The regression approach takes censoring into account when formulating the inequality constraints of the support vector problem. In the ranking approach, the inequality constraints set the objective to maximize the concordance index for comparable pairs of observations. The hybrid approach combines the regression and ranking constraints in a single model. We describe survival support vector machines and their implementation, provide examples and compare the prediction performance with the Cox proportional hazards model, random survival forests and gradient boosting using several real datasets. On these datasets, survival support vector machines perform on par with the reference methods.},
  author       = {Fouodo, Cesaire J K and Konig, I and Weihs, C and Ziegler, A and Wright, M},
  date         = {2018},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Fouodo et al. - 2018 - Support vector machines for survival analysis with R.pdf:pdf},
  issn         = {20734859},
  journaltitle = {The R Journal},
  keywords     = {concordance index,kernel methods,machine learning,model,port vector machines,software,sup-,support vector machine,survival,survival analysis,svm},
  number       = {July},
  pages        = {412--423},
  title        = {{Support vector machines for survival analysis with R}},
  volume       = {10},
}

@article{Knock2020,
  author       = {Knock, E S and Whittles, L K and Perez-Guzman, P N and Bhatia, S and Guntoro, F and Watson, O J and Whittaker, C and Ferguson, N M and Cori, A and Baguelin, M and FitzJohn, R G and Lees, J A},
  url          = {https://wellcomeopenresearch.org/articles/5-288/v1},
  date         = {2020},
  doi          = {10.12688/wellcomeopenres.16466.1},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Knock et al. - 2020 - Reproducible parallel inference and simulation of stochastic state space models using odin, dust, and mcstate vers.pdf:pdf},
  journaltitle = {Wellcome Open Research},
  number       = {288},
  title        = {{Reproducible parallel inference and simulation of stochastic state space models using odin, dust, and mcstate [version 1; peer review: 1 approved, 1 approved with reservations]}},
  volume       = {5},
}

@article{Du2011,
  abstract     = {The proposed techniques investigate the strength of support vector regression (SVR) in cancer prognosis using imaging features. Cancer image features were extracted from patients and recorded into censored data. To employ censored data for prognosis, SVR methods are needed to be adapted to uncertain targets. The effectiveness of two principle breast features, tumor size and lymph node status, was demonstrated by the combination of sampling and feature selection methods. In sampling, breast data were stratified according to tumor size and lymph node status. Three types of feature selection methods comprised of no selection, individual feature selection, and feature subset forward selection, were employed. The prognosis results were evaluated by comparative study using the following performance metrics: concordance index (CI) and Brier score (BS). Cox regression was employed to compare the results. The support vector regression method (SVCR) performs similarly to Cox regression in three feature selection methods and better than Cox regression in non-feature selection methods measured by CI and BS. Feature selection methods can improve the performance of Cox regression measured by CI. Among all cross validation results, stratified sampling of tumor size achieves the best regression results for both feature selection and non-feature selection methods. The SVCR regression results, perform better than Cox regression when the techniques are used with either CI or BS. The best CI value in the validation results is 0.6845. The best CI value corresponds to the best BS value 0.2065, which were obtained in the combination of SVCR, individual feature selection, and stratified sampling of the number of positive lymph nodes. In addition, we also observe that SVCR performs more consistently than Cox regression in all prognosis studies. The feature selection method does not have a significant impact on the metric values, especially on CI. We conclude that the combinational methods of SVCR, feature selection, and sampling can improve cancer prognosis, but more significant features may further enhance cancer prognosis accuracy.},
  author       = {Du, Xian and Dua, Sumeet},
  language     = {eng},
  publisher    = {Baishideng Publishing Group Co., Limited},
  url          = {https://pubmed.ncbi.nlm.nih.gov/21603313 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3095462/},
  date         = {2011-01},
  doi          = {10.5306/wjco.v2.i1.44},
  issn         = {2218-4333},
  journaltitle = {World journal of clinical oncology},
  keywords     = {Breast cancer imaging,Cancer prognosis,SVCR,SVR,SVRc,Sampling,Support vector regression,benchmark experiment,censoring,comparison,machine learning,survey,survival,svm},
  number       = {1},
  pages        = {44--49},
  title        = {{Cancer prognosis using support vector regression in imaging modality}},
  volume       = {2},
}

@article{Brier1950,
  author       = {Brier, Glenn},
  date         = {1950},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Brier - 1950 - Verification of forecasts expressed in terms of probability.pdf:pdf},
  journaltitle = {Monthly Weather Review},
  keywords     = {classification,measure,probability,scoring rule},
  number       = {1},
  pages        = {1--3},
  title        = {{Verification of forecasts expressed in terms of probability}},
  volume       = {78},
}

@article{pkglubridate,
  author       = {Grolemund, Garrett and Wickham, Hadley},
  url          = {http://www.jstatsoft.org/v40/i03/},
  date         = {2011},
  journaltitle = {Journal of Statistical Software},
  number       = {3},
  pages        = {1--25},
  title        = {{Dates and Times Made Easy with {lubridate}}},
  volume       = {40},
}

@article{dataovarian,
  abstract     = {Treatment of patients with advanced ovarian carcinoma (stages IIIB and IV) using either cyclophosphamide alone (1 g/m2) or cyclophosphamide (500 mg/m2) plus adriamycin (40 mg/m2) by iv injection every 3 weeks each produced partial regression in approximately one third of the patients. Survival curves and time-to-progression curves for the two regimens were nearly identical in these patients with advanced disease. These same regimens produced different results when used monthly in patients who had minimal residual disease (stages II and IIIA). In patients with minimal residual disease the therapeutic index of the combination regimen was superior to that of cyclophosphamide alone. Prognosis was better overall among patients with minimal residual disease than among patients with advanced disease. Within the minimal-disease group grossly complete excision of tumor prior to chemotherapy was associated with still better prognosis. Among patients with advanced disease, prognosis was significantly better for older patients despite their generally less favorable performance scores. Much of this prognostic superiority appeared to be related to menopausal status and presumably to the depletion of endogenous estrogens in the older patients.},
  author       = {Edmonson, J H and Fleming, T R and Decker, D G and Malkasian, G D and Jorgensen, E O and Jefferies, J A and Webb, M J and Kvols, L K},
  language     = {eng},
  date         = {1979-02},
  issn         = {0361-5960 (Print)},
  journaltitle = {Cancer treatment reports},
  keywords     = {Combination,Cyclophosphamide,Doxorubicin,Drug Therapy,Female,Humans,Menopause,Middle Aged,Neoplasm Staging,Ovarian Neoplasms,Prognosis,Remission,Spontaneous,Time Factors,administration & dosage,drug therapy,pathology,surgery},
  number       = {2},
  pages        = {241--247},
  title        = {{Different chemotherapeutic sensitivities and host factors affecting prognosis in advanced ovarian carcinoma versus minimal residual disease.}},
  volume       = {63},
}

@article{datastanford,
  author       = {Crowley, J and Hu, M},
  date         = {1977},
  journaltitle = {Journal of the American Statistical Association},
  pages        = {27--36},
  title        = {{Covariance analysis of heart transplant survival data.}},
  volume       = {72},
}

@article{Huang2020,
  abstract     = {Cancer is an aggressive disease with a low median survival rate. Ironically, the treatment process is long and very costly due to its high recurrence and mortality rates. Accurate early diagnosis and prognosis prediction of cancer are essential to enhance the patient's survival rate. Developments in statistics and computer engineering over the years have encouraged many scientists to apply computational methods such as multivariate statistical analysis to analyze the prognosis of the disease, and the accuracy of such analyses is significantly higher than that of empirical predictions. Furthermore, as artificial intelligence (AI), especially machine learning and deep learning, has found popular applications in clinical cancer research in recent years, cancer prediction performance has reached new heights. This article reviews the literature on the application of AI to cancer diagnosis and prognosis, and summarizes its advantages. We explore how AI assists cancer diagnosis and prognosis, specifically with regard to its unprecedented accuracy, which is even higher than that of general statistical applications in oncology. We also demonstrate ways in which these methods are advancing the field. Finally, opportunities and challenges in the clinical implementation of AI are discussed. Hence, this article provides a new perspective on how AI technology can help improve cancer diagnosis and prognosis, and continue improving human health in the future.},
  author       = {Huang, Shigao and Yang, Jie and Fong, Simon and Zhao, Qi},
  url          = {http://www.sciencedirect.com/science/article/pii/S0304383519306135},
  date         = {2020},
  doi          = {https://doi.org/10.1016/j.canlet.2019.12.007},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Huang et al. - 2020 - Artificial intelligence in cancer diagnosis and prognosis Opportunities and challenges.pdf:pdf},
  issn         = {0304-3835},
  journaltitle = {Cancer Letters},
  keywords     = {Cancer diagnosis,Deep learning,Deep neural network,Machine learning,Prognosis prediction},
  pages        = {61--71},
  title        = {{Artificial intelligence in cancer diagnosis and prognosis: Opportunities and challenges}},
  volume       = {471},
}

@article{pkgpec,
  author       = {Mogensen, Ulla B and Ishwaran, Hemant and Gerds, Thomas A},
  url          = {http://www.jstatsoft.org/v50/i11/},
  date         = {2012},
  doi          = {10.18637/jss.v050.i11},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Mogensen, Ishwaran, Gerds - 2014 - Evaluating Random Forests for Survival Analysis using Prediction Error Curves.pdf:pdf},
  issn         = {1548-7660},
  journaltitle = {Journal of Statistical Software},
  keywords     = {prediction error curves,r,random survival forest,survival prediction},
  number       = {11},
  title        = {{Evaluating Random Forests for Survival Analysis Using Prediction Error Curves}},
  volume       = {50},
}

@article{datamgus,
  abstract     = {All 241 patients with an apparently benign monoclonal gammopathy who were examined at the Mayo Clinic before Jan. 1, 1971, underwent prospective follow-up for 20 to 35 years (median, 22 years). Electrophoresis and immunoelectrophoresis of serum and urine specimens were performed periodically in an effort to determine the frequency of development of multiple myeloma, primary amyloidosis, macroglobulinemia, or other lymphoproliferative diseases. At follow-up, the patients were categorized into one of four groups: group 1 (benign)--46 patients (19%) who were alive and had a benign monoclonal gammopathy; group 2--23 patients (10%) who had a serum monoclonal protein value of 3 g/dl or more but did not require chemotherapy; group 3--113 patients (47%) who died without evidence of myeloma or related disorders; and group 4-59 patients (24%) in whom multiple myeloma (39), systemic amyloidosis (8), macroglobulinemia (7), or a malignant lymphoproliferative disease (5) developed at a median of 10, 9, 8, and 10 1/2 years, respectively, after detection of the monoclonal protein. Thus, in patients with an apparently benign monoclonal gammopathy, follow-up must be continued indefinitely because multiple myeloma, amyloidosis, macroglobulinemia, or related disorders occur in approximately a fourth of them.},
  author       = {Kyle, R A},
  language     = {eng},
  date         = {1993-01},
  doi          = {10.1016/s0025-6196(12)60015-9},
  issn         = {0025-6196 (Print)},
  journaltitle = {Mayo Clinic proceedings},
  keywords     = {Adult,Aged,Amyloidosis,Antibodies,Electrophoresis,Female,Follow-Up Studies,Humans,Immunoglobulins,Lymphoproliferative Disorders,Male,Middle Aged,Monoclonal,Monoclonal Gammopathy of Undetermined Significance,Multiple Myeloma,Prospective Studies,Waldenstrom Macroglobulinemia,blood,complications,immunology,urine},
  number       = {1},
  pages        = {26--36},
  title        = {{"Benign" monoclonal gammopathy--after 20 to 35 years of follow-up.}},
  volume       = {68},
}

@article{Zhou2005,
  abstract     = {The authors compared five methods of studying survival bias associated with time-to-treatment initiation in a drug effectiveness study using medical administrative databases (1996–2002) from Quebec, Canada. The first two methods illustrated how survival bias could be introduced. Three additional methods were considered to control for this bias. Methods were compared in the context of evaluating statins for secondary prevention in elderly patients post-acute myocardial infarction who initiated statins within 90 days after discharge and those who did not. Method 1 that classified patients into users and nonusers at discharge resulted in an overestimation of the benefit (38% relative risk reduction at 1 year). In method 2, following users from the time of the first prescription and nonusers from a randomly selected time between 0 and 90 days attenuated the effect toward the null (10% relative risk reduction). Method 3 controlled for survival bias by following patients from the end of the 90-day time window; however, it suffered a major loss of statistical efficiency and precision. Method 4 matched prescription time distribution between users and nonusers at cohort entry. Method 5 used a time-dependent variable for treatment initiation. Methods 4 and 5 better controlled for survival bias and yielded similar results, suggesting a 20% risk reduction of recurrent myocardial infarction or death events.},
  author       = {Zhou, Zheng and Rahme, Elham and Abrahamowicz, Michal and Pilote, Louise},
  url          = {https://doi.org/10.1093/aje/kwi307},
  date         = {2005-11},
  doi          = {10.1093/aje/kwi307},
  issn         = {0002-9262},
  journaltitle = {American Journal of Epidemiology},
  keywords     = {bias,censoring bias,reduction,survival},
  number       = {10},
  pages        = {1016--1023},
  title        = {{Survival Bias Associated with Time-to-Treatment Initiation in Drug Effectiveness Evaluation: A Comparison of Methods}},
  volume       = {162},
}

@article{Gensheimer2019,
  author       = {Gensheimer, Michael F and Narasimhan, Balasubramanian},
  publisher    = {PeerJ Inc.},
  date         = {2019},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Gensheimer, Narasimhan - 2019 - A scalable discrete-time survival model for neural networks.pdf:pdf},
  issn         = {2167-8359},
  journaltitle = {PeerJ},
  pages        = {e6257},
  title        = {{A scalable discrete-time survival model for neural networks}},
  volume       = {7},
}

@report{Sonabend2021a,
  author      = {Sonabend, Raphael and Whittles, Lilith K and Imai, Natsuko and Knock, Edward S and Perez-Guzman, Pablo N and Mangal, Tara and Hogan, Alexandra B and Volz, Erik M and Ghani, Azra and Ferguson, Neil M and Baguelin, Marc and Cori, Anne},
  institution = {Imperial College COVID-19 Response Team},
  url         = {https://www.gov.uk/government/publications/imperial-college-london-evaluating-the-roadmap-out-of-lockdown-step-3-5-may-2021},
  date        = {2021},
  title       = {{Evaluating the Roadmap out of Lockdown: Step 3}},
  type        = {techreport},
}

@book{Kalbfleisch2011,
  author    = {Kalbfleisch, John D and Prentice, Ross L},
  publisher = {John Wiley & Sons},
  date      = {2011},
  isbn      = {1118031237},
  title     = {{The statistical analysis of failure time data}},
  volume    = {360},
}

@article{Wei1992,
  author       = {Wei, L J},
  date         = {1992},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Wei - 1992 - The Accelerated Failure Time Model A Useful Alternative to the Cox Regression Model in Survival Analysis.pdf:pdf},
  journaltitle = {Statistics in Medicine},
  keywords     = {accelerated failure time,aft,classical,model,ph,proportional hazards,survival},
  pages        = {1871--1879},
  title        = {{The Accelerated Failure Time Model: A Useful Alternative to the Cox Regression Model in Survival Analysis}},
  volume       = {11},
}

@article{pkgmlr3proba,
  abstract     = {As machine learning has become increasingly popular over the last few decades, so too has the number of machine learning interfaces for implementing these models. Whilst many R libraries exist for machine learning, very few offer extended support for survival analysis. This is problematic considering its importance in fields like medicine, bioinformatics, economics, engineering, and more. mlr3proba provides a comprehensive machine learning interface for survival analysis and connects with mlr3's general model tuning and benchmarking facilities to provide a systematic infrastructure for survival modeling and evaluation.mlr3proba is available under an LGPL-3 license on CRAN and at https://github.com/mlr-org/mlr3proba, with further documentation at https://mlr3book.mlr-org.com/survival.html.},
  author       = {Sonabend, Raphael and Király, Franz J and Bender, Andreas and Bischl, Bernd and Lang, Michel},
  editor       = {Wren, Jonathan},
  url          = {https://academic.oup.com/bioinformatics/article/37/17/2789/6125361 https://cran.r-project.org/package=mlr3proba},
  date         = {2021-09},
  doi          = {10.1093/bioinformatics/btab039},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Sonabend et al. - 2021 - mlr3proba An R Package for Machine Learning in Survival Analysis.pdf:pdf},
  issn         = {1367-4803},
  journaltitle = {Bioinformatics},
  number       = {17},
  pages        = {2789--2791},
  title        = {{mlr3proba: an R package for machine learning in survival analysis}},
  volume       = {37},
}

@article{Colosimo2012,
  abstract     = {In practice, data are often measured repeatedly on the same individual at several points in time. Main interest often relies in characterizing the way the response changes in time, and the predictors of that change. Marginal, mixed and transition are frequently considered to be the main models for continuous longitudinal data analysis. These approaches are proposed primarily for balanced longitudinal design. However, in clinic studies, data are usually not balanced and some restrictions are necessary in order to use these models. This paper was motivated by a data set related to longitudinal height measurements in children of HIV-infected mothers that was recorded at the university hospital of the Federal University in Minas Gerais, Brazil. This data set is severely unbalanced. The goal of this paper is to assess the application of continuous longitudinal models for the analysis of unbalanced data set.},
  author       = {Colosimo, Enrico A. and Fausto, Maria Arlene and Freitas, Marta Afonso and Pinto, Jorge Andrade},
  date         = {2012},
  doi          = {10.1080/02664763.2012.699954},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Colosimo et al. - 2012 - Practical modeling strategies for unbalanced longitudinal data analysis.pdf:pdf},
  issn         = {02664763},
  journaltitle = {Journal of Applied Statistics},
  keywords     = {BIC,GEE,marginal models,mixed model,restricted likelihood},
  number       = {9},
  pages        = {2005--2013},
  title        = {{Practical modeling strategies for unbalanced longitudinal data analysis}},
  volume       = {39},
}

@misc{pkgsimsurv,
  author    = {Brilleman, Sam},
  publisher = {CRAN},
  url       = {https://cran.r-project.org/package=simsurv},
  date      = {2019},
  title     = {{simsurv: Simulate Survival Data}},
}

@misc{LinWei1991,
  abstract  = {In this article, we extend the information matrix tests proposed by White (1982) for detecting parametric model misspecification to the partial likelihood setting with particular interest in the Cox semi-parametric regression model. First we identify two model-based consistent estimators for the inverse of the asymptotic covariance matrix of the maximum partial likelihood estimator in the Cox model. We then show that under the assumed model the difference between these two estimators is asymptotically normal with mean zero and with a covariance matrix which can be consistently estimated. Goodness-of-fit tests for the Cox model are constructed based on these asymptotic results. Extensive Monte Carlo studies indicate that the large-sample approximation is appropriate for practical use. In addition, we demonstrate that the proposed tests tend to be more powerful than other numerical methods in the literature. Two examples are provided for illustrations.},
  author    = {Lin, D Y and Wei, L J},
  booktitle = {Statistica Sinica},
  date      = {1991},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Lin, Wei - 1991 - Goodness-of-Fit Tests for the General Cox Regression-Model.pdf:pdf},
  isbn      = {1017-0405},
  keywords  = {assumption,information matrix,large sample,likelihood,martingale,model misspecification,parameter,partial likelihood,proportional hazards,proportional hazards model,score tests,survival data,survival-data},
  number    = {1},
  pages     = {1--17},
  title     = {{Goodness-of-Fit Tests for the General Cox Regression-Model}},
  volume    = {1},
}

@incollection{Zikmund-Fisher2014,
  author    = {Zikmund-Fisher, Brian J and Mayman, Gillian and Fagerlin, Angela},
  editor    = {Anderson, Britta L and Schulkin, Jay},
  location  = {Cambridge},
  publisher = {Cambridge University Press},
  url       = {https://www.cambridge.org/core/books/numerical-reasoning-in-judgments-and-decision-making-about-health/patient-numeracy-what-do-patients-need-to-recognize-think-or-do-with-health-numbers/30D5C10B10EADE270C4D8BE164C02093},
  booktitle = {Numerical Reasoning in Judgments and Decision Making about Health},
  date      = {2014},
  doi       = {DOI: 10.1017/CBO9781139644358.005},
  isbn      = {9781107040946},
  pages     = {80--104},
  title     = {{Patient numeracy: what do patients need to recognize, think, or do with health numbers?}},
}

@article{Floridi2015,
  author       = {Floridi, Luciano},
  date         = {2015},
  doi          = {10.1007/s13347-015-0192-0},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Floridi - 2015 - The Politics of Uncertainty.pdf:pdf},
  journaltitle = {Philosophy & Technology},
  pages        = {1--4},
  title        = {{The Politics of Uncertainty}},
  volume       = {28},
}

@inproceedings{Sloma2021,
  author    = {Sloma, Michael and Syed, Fayeq and Nemati, Mohammedreza and Xu, Kevin S.},
  publisher = {PMLR},
  url       = {http://proceedings.mlr.press/v146/sloma21a.html},
  booktitle = {Proceedings of AAAI Spring Symposium on Survival Prediction - Algorithms, Challenges, and Applications 2021},
  date      = {2021},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Sloma et al. - 2021 - Empirical Comparison of Continuous and Discrete-time Representations for Survival Prediction.pdf:pdf},
  pages     = {118--131},
  title     = {{Empirical Comparison of Continuous and Discrete-time Representations for Survival Prediction}},
}

@misc{pkgmlr3extralearners,
  author    = {Sonabend, Raphael and Schratz, Patrick},
  publisher = {CRAN},
  url       = {https://github.com/mlr-org/mlr3extralearners},
  date      = {2020},
  title     = {{mlr3extralearners: Extra Learners For mlr3}},
}

@misc{pkgdynpred,
  author    = {Putter, Hein},
  publisher = {CRAN},
  url       = {https://cran.r-project.org/package=dynpred},
  date      = {2015},
  title     = {{dynpred: Companion Package to "Dynamic Prediction in Clinical Survival Analysis"}},
}

@article{Wallace2011,
  author       = {Wallace, Byron C and Small, Kevin and Brodley, Carla E and Trikalinos, Thomas A},
  date         = {2011},
  doi          = {10.1109/ICDM.2011.33},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Wallace et al. - 2011 - Class Imbalance , Redux.pdf:pdf},
  journaltitle = {IEEE Computer Society},
  keywords     = {-classification,class imbalance,class imbalance refers to,classification,from each,i,i ntroduction and m,in the context of,otivation,the number of instances,the scenario in which},
  title        = {{Class Imbalance , Redux}},
}

@incollection{Walker2013,
  author    = {Walker, Warren E and Lempert, Robert J and Kwakkel, Jan H},
  location  = {Boston, MA},
  publisher = {Springer US},
  url       = {http://link.springer.com/10.1007/978-1-4419-1153-7_1140},
  booktitle = {Encyclopedia of Operations Research and Management Science},
  date      = {2013},
  doi       = {10.1007/978-1-4419-1153-7_1140},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Walker, Lempert, Kwakkel - 2013 - Deep Uncertainty.docx:docx},
  pages     = {395--402},
  title     = {{Deep Uncertainty}},
  volume    = {1},
}

@article{Hothorn2004,
  abstract     = {Abstract Predicted survival probability functions of censored event free survival are improved by bagging survival trees. We suggest a new method to aggregate survival trees in order to obtain better predictions for breast cancer and lymphoma patients. A set of survival trees based on B bootstrap samples is computed. We define the aggregated Kaplan?Meier curve of a new observation by the Kaplan?Meier curve of all observations identified by the B leaves containing the new observation. The integrated Brier score is used for the evaluation of predictive models. We analyse data of a large trial on node positive breast cancer patients conducted by the German Breast Cancer Study Group and a smaller ?pilot? study on diffuse large B-cell lymphoma, where prognostic factors are derived from microarray expression values. In addition, simulation experiments underline the predictive power of our proposal. Copyright ? 2004 John Wiley & Sons, Ltd.},
  author       = {Hothorn, Torsten and Lausen, Berthold and Benner, Axel and Radespiel-Tröger, Martin},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/sim.1593},
  annotation   = {doi: 10.1002/sim.1593},
  date         = {2004-01},
  doi          = {10.1002/sim.1593},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  keywords     = {Brier score,bootstrap aggregation,censored data,prognostic factors},
  number       = {1},
  pages        = {77--91},
  title        = {{Bagging survival trees}},
  volume       = {23},
}

@incollection{Anderson2014C3,
  author    = {Anderson, Britta L and Schulkin, Jay},
  editor    = {Anderson, Britta L and Schulkin, Jay},
  location  = {Cambridge},
  publisher = {Cambridge University Press},
  url       = {https://www.cambridge.org/core/books/numerical-reasoning-in-judgments-and-decision-making-about-health/physicians-understanding-and-use-of-numeric-information/F43F17FF804A76302D108047C66F9CE8},
  booktitle = {Numerical Reasoning in Judgments and Decision Making about Health},
  date      = {2014},
  doi       = {DOI: 10.1017/CBO9781139644358.004},
  isbn      = {9781107040946},
  pages     = {59--79},
  title     = {{Physicians' understanding and use of numeric information}},
}

@incollection{Harrell2015,
  author    = {Harrell, Frank E.},
  publisher = {Springer},
  booktitle = {Regression modeling strategies: with applications to linear models, logistic and ordinal regression, and survival analysis},
  chapter   = {5},
  date      = {2015},
  isbn      = {3319194259},
  title     = {{Regression modeling strategies: with applications to linear models, logistic and ordinal regression, and survival analysis}},
}

@article{Heagerty2005,
  abstract     = {The predictive accuracy of a survival model can be summarized using extensions of the proportion of variation explained by the model, or R 2 , commonly used for continuous response models, or using extensions of sensitivity and specificity, which are commonly used for binary response models. In this article we propose new time-dependent accuracy summaries based on time-specific versions of sensitivity and specificity calculated over risk sets. We connect the accuracy summaries to a previously proposed global concordance measure, which is a variant of Kendall's tau. In addition, we show how standard Cox regression output can be used to obtain estimates of time-dependent sensitivity and specificity, and time-dependent receiver operating characteristic (ROC) curves. Semiparametric estimation methods appropriate for both proportional and nonproportional hazards data are introduced, evaluated in simulations, and illustrated using two familiar survival data sets.},
  author       = {Heagerty, Patrick J and Zheng, Yingye},
  url          = {https://onlinelibrary.wiley.com/doi/10.1111/j.0006-341X.2005.030814.x},
  annotation   = {R2 and survival. C-index and AUC for survival},
  date         = {2005-03},
  doi          = {10.1111/j.0006-341X.2005.030814.x},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Heagerty, Zheng - 2005 - Survival Model Predictive Accuracy and ROC Curves Patrick.pdf:pdf},
  issn         = {0006-341X},
  journaltitle = {Biometrics},
  keywords     = {cox regression,discrimination,prediction,sensitivity,specificity},
  number       = {1},
  pages        = {92--105},
  title        = {{Survival Model Predictive Accuracy and ROC Curves}},
  volume       = {61},
}

@article{Wolpert1992,
  abstract     = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.},
  author       = {Wolpert, David H},
  url          = {http://www.sciencedirect.com/science/article/pii/S0893608005800231},
  date         = {1992},
  doi          = {https://doi.org/10.1016/S0893-6080(05)80023-1},
  issn         = {0893-6080},
  journaltitle = {Neural Networks},
  keywords     = {Combining generalizers,Error estimation and correction,Generalization and induction,Learning set preprocessing,cross-validation},
  number       = {2},
  pages        = {241--259},
  title        = {{Stacked generalization}},
  volume       = {5},
}

@report{Whittles2021,
  author      = {Whittles, Lilith K and Knock, Edward S and Perez-Guzman, Pablo N and Sonabend, Raphael and Ghani, Azra and Ferguson, Neil M and Baguelin, Marc and Cori, Anne},
  institution = {Imperial College COVID-19 Response Team},
  url         = {https://www.gov.uk/government/publications/imperial-college-london-unlocking-roadmap-scenarios-for-england-18-february-2021},
  date        = {2021},
  title       = {{“Unlocking” Roadmap Scenarios for England v2}},
  type        = {techreport},
}

@article{pkgcaret,
  abstract     = {The caret package, short for classification and regression training, contains numerous tools for developing predictive models using the rich set of models available in R. The package focuses on simplifying model training and tuning across a wide variety of modeling techniques. It also includes methods for pre-processing training data, calculating variable importance, and model visualizations. An example from computational chemistry is used to illustrate the functionality on a real data set and to benchmark the benefits of parallel processing with several types of models.},
  author       = {Kuhn, Max},
  url          = {https://www.jstatsoft.org/v028/i05 http://dx.doi.org/10.18637/jss.v028.i05},
  date         = {2008-11},
  journaltitle = {Journal of Statistical Software; Vol 1, Issue 5 (2008)},
  title        = {{Building Predictive Models in R Using the caret Package}},
}

@book{Hastie2001,
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  publisher = {Springer New York Inc.},
  date      = {2001},
  title     = {{The Elements of Statistical Learning}},
}

@article{datadiabetic,
  abstract     = {Analyses of visual acuity and visual field results in the Diabetic Retinopathy Study provide evidence that photocoagulation treatment as carried out according to the study protocol (extensive "scatter" photocoagulation and focal treatment of new vessels) is of benefit in preventing severe visual loss, over a two-year follow-up period, in eyes with proliferative retinopathy. Location of new vessels relative to the disk, severity on new vessels, and the presence of hemorrhage (vitreous or preretinal) all proved to be important prognostic factors. On the basis of these findings, these steps have been taken: All patients in the study have been informed of results to date and given an explanation of their implications. Photocoagulation treatment will be considered for the initially untreated eyes which now or in the future fulfill any one of the following criteria: (a) moderate or severe new vessels on or within 1-disk diameter of the optic disk; (b) mild new vessels on or within 1-disk diameter of the optic disk if fresh hemorrhage is present; and (c) moderate or severe new vessels elsewhere, if fresh hemorrhage is present. Follow-up of all patients will continue to allow long-term comparison between the argon- and xenon-treatment techniques employed. Further analyses of accumulating data will be performed to evaluate more completely the efficacy of photocoagulation therapy.},
  author       = {{The Diabetic Retinopathy Study Research Group}},
  language     = {eng},
  date         = {1976-04},
  doi          = {10.1016/0002-9394(76)90292-0},
  issn         = {0002-9394 (Print)},
  journaltitle = {American journal of ophthalmology},
  keywords     = {Argon,Diabetic Retinopathy,Fluorescein Angiography,Follow-Up Studies,Humans,Laser Therapy,Light Coagulation,Prognosis,Vision Disorders,Visual Acuity,Visual Fields,Xenon,prevention & control,surgery},
  number       = {4},
  pages        = {383--396},
  title        = {{Preliminary report on effects of photocoagulation therapy.}},
  volume       = {81},
}

@article{Garcia-Beltran2021,
  abstract     = {Vaccination elicits immune responses capable of potently neutralizing SARS-CoV-2. However, ongoing surveillance has revealed the emergence of variants harboring mutations in spike, the main target of neutralizing antibodies. To understand the impact of these variants, we evaluated the neutralization potency of 99 individuals that received one or two doses of either BNT162b2 or mRNA-1273 vaccines against pseudoviruses representing 10 globally circulating strains of SARS-CoV-2. Five of the 10 pseudoviruses, harboring receptor-binding domain mutations, including K417N/T, E484K, and N501Y, were highly resistant to neutralization. Cross- neutralization of B.1.351 variants was comparable to SARS-CoV and bat-derived WIV1-CoV, suggesting that a relatively small number of mutations can mediate potent escape from vaccine responses. While the clinical impact of neutralization resistance remains uncertain, these results highlight the potential for variants to escape from neutralizing humoral immunity and emphasize the need to develop broadly protective interventions against the evolving pandemic.Competing Interest StatementThe authors have declared no competing interest.Funding StatementB.M.H. is supported by award Number T32GM007753 from the National Institute of General Medical Sciences. J.F. is supported by T32AI007245. D.J.G., M.C.P., and M.N.P. were supported by the VIC Innovation fund. A.S was supported by the Bill and Melinda Gates Investment INV-018944 (AS) and by the South African Medical Research Council and the Department of Science and Innovation (TdO), A.G.S. was supported by NIH R01 AI146779 and a Massachusetts Consortium on Pathogenesis Readiness (MassCPR) grant. A.J.I. is supported by the Lambertus Family Foundation. A.B.B. was supported by the National Institutes for Drug Abuse (NIDA) Avenir New Innovator Award DP2DA040254, the MGH Transformative Scholars Program as well as funding from the Charles H. Hood Foundation. This independent research was supported by the Gilead Sciences Research Scholars Program in HIV.Author DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesThe details of the IRB/oversight body that provided approval or exemption for the research described are given below:Use of human samples was approved by Partners Institutional Review Board (protocol 2020P002274)All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).Yes I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.YesThis study assessed the neutralization potency of sera obtained from vaccinated individuals and was not a clinical trial. De-identified raw data of the results of serological assays can be provided upon request.},
  author       = {Garcia-Beltran, Wilfredo F and Lam, Evan C and {St. Denis}, Kerri and Nitido, Adam D and Garcia, Zeidy H and Hauser, Blake M and Feldman, Jared and Pavlovic, Maia N and Gregory, David J and Poznansky, Mark C and Sigal, Alex and Schmidt, Aaron G and Iafrate, A John and Naranbhai, Vivek and Balazs, Alejandro B},
  url          = {http://medrxiv.org/content/early/2021/03/12/2021.02.14.21251704.abstract},
  date         = {2021-01},
  doi          = {10.1101/2021.02.14.21251704},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Garcia-Beltran et al. - 2021 - Multiple SARS-CoV-2 variants escape neutralization by vaccine-induced humoral immunity.pdf:pdf},
  journaltitle = {medRxiv},
  pages        = {2021.02.14.21251704},
  title        = {{Multiple SARS-CoV-2 variants escape neutralization by vaccine-induced humoral immunity}},
}

@article{Efron1977,
  author       = {Efron, Bradley},
  publisher    = {Taylor & Francis},
  url          = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1977.10480613},
  annotation   = {doi: 10.1080/01621459.1977.10480613},
  date         = {1977-09},
  doi          = {10.1080/01621459.1977.10480613},
  issn         = {0162-1459},
  journaltitle = {Journal of the American Statistical Association},
  number       = {359},
  pages        = {557--565},
  title        = {{The Efficiency of Cox's Likelihood Function for Censored Data}},
  volume       = {72},
}

@article{Luxhoj1997,
  abstract     = {Because of increased manufacturing competitiveness, new methods for\nreliability estimation are being developed. Intelligent manufacturing\nrelies upon accurate component and product reliability estimates for\ndetermining warranty costs, as well as optimal maintenance, inspection,\nand replacement schedules. Accelerated life testing is one approach that\nis used for shortening the life of products or components or hastening\ntheir performance degradation with the purpose of obtaining data that\nmay be used to predict device life or performance under normal operating\nconditions. The proportional hazards (PH) model is a non-parametric\nmultiple regression approach for reliability estimation, in which a\nbaseline hazard function is modified multiplicatively by covariates\n(i.e. applied stresses). While the PH model is a distribution-free\napproach, specific assumptions need to be made about the time behavior\nof the hazard rates. A neural network (NN) is particularly useful in\npattern recognition problems that involve capturing and learning complex\nunderlying (but consistent) trends in the data. Neural networks are\nhighly non-linear, and in some cases are capable of producing better\napproximations than multiple regression. This paper reports on the\ncomparison of PH and MN models for the analysis of time-dependent\ndielectric breakdown data for a metal-oxide-semiconductor integrated\ncircuit. In this case, the NN model results in a better fit to the data\nbased upon minimizing the mean square error of the predictions when\nusing failure data from an elevated temperature and voltage to predict\nreliability at a lower temperature and voltage.},
  author       = {Luxhoj, James T. and Shyur, Huan Jyh},
  date         = {1997},
  doi          = {10.1023/A:1018525308809},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Luxhoj, Shyur - 1997 - Comparison of proportional hazards models and neural networks for reliability estimation.pdf:pdf},
  isbn         = {0956-5515},
  issn         = {09565515},
  journaltitle = {Journal of Intelligent Manufacturing},
  keywords     = {Accelerated life testing,Neural networks,Proportional hazards models},
  number       = {3},
  pages        = {227--234},
  title        = {{Comparison of proportional hazards models and neural networks for reliability estimation}},
  volume       = {8},
}

@misc{pkgriskregression,
  author    = {Gerds, Thomas Alexander and Ozenne, Brice},
  publisher = {CRAN},
  url       = {https://cran.r-project.org/package=riskRegression},
  date      = {2019},
  title     = {{riskRegression: Risk Regression Models and Prediction Scores for Survival Analysis with Competing Risks}},
}

@article{Fotso2018,
  abstract     = {Survival analysis/time-to-event models are extremely useful as they can help companies predict when a customer will buy a product, churn or default on a loan, and therefore help them improve their ROI. In this paper, we introduce a new method to calculate survival functions using the Multi-Task Logistic Regression (MTLR) model as its base and a deep learning architecture as its core. Based on the Concordance index (C-index) and Brier score, this method outperforms the MTLR in all the experiments disclosed in this paper as well as the Cox Proportional Hazard (CoxPH) model when nonlinear dependencies are found.},
  author       = {Fotso, Stephane},
  url          = {http://arxiv.org/abs/1801.05512},
  date         = {2018-01},
  eprint       = {1801.05512},
  eprinttype   = {arXiv},
  journaltitle = {arXiv preprint arXiv:1801.05512},
  title        = {{Deep Neural Networks for Survival Analysis Based on a Multi-Task Framework}},
}

@article{pkgglmnet,
  author       = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  url          = {http://www.jstatsoft.org/v33/i01/},
  date         = {2010},
  journaltitle = {Journal of Statistical Software},
  number       = {1},
  pages        = {1--22},
  title        = {{Regularization Paths for Generalized Linear Models via Coordinate Descent.}},
  volume       = {33},
}

@article{Kvamme2019a,
  author       = {Kvamme, H{å}vard and Borgan, {Ø}rnulf and Scheel, Ida},
  date         = {2019},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Kvamme, Borgan, Scheel - 2019 - Time-to-event prediction with neural networks and Cox regression.pdf:pdf},
  issn         = {1533-7928},
  journaltitle = {Journal of Machine Learning Research},
  number       = {129},
  pages        = {1--30},
  title        = {{Time-to-event prediction with neural networks and Cox regression}},
  volume       = {20},
}

@article{Wegwarth2010,
  author       = {Wegwarth, Odette and Gaissmaier, Wolfgang and Gigerenzer, Gerd},
  date         = {2010-12},
  doi          = {10.1177/0272989X10391469},
  journaltitle = {Medical decision making : an international journal of the Society for Medical Decision Making},
  pages        = {386--394},
  title        = {{Deceiving Numbers: Survival Rates and Their Impact on Doctors' Risk Communication}},
  volume       = {31},
}

@article{Hanczar2010,
  author       = {Hanczar, Blaise and Hua, Jianping and Sima, Chao and Weinstein, John and Bittner, Michael and Dougherty, Edward R},
  date         = {2010},
  doi          = {10.1093/bioinformatics/btq037},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Hanczar et al. - 2010 - Small-sample precision of ROC-related estimates.pdf:pdf},
  journaltitle = {Bioinformatics},
  number       = {6},
  pages        = {822--830},
  title        = {{Small-sample precision of ROC-related estimates}},
  volume       = {26},
}

@thesis{Qi2009,
  author = {Qi, Jiezhi},
  date   = {2009},
  file   = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Qi - 2009 - Comparison of Proportional Hazards and Accelerated Failure Time Models.pdf:pdf},
  title  = {{Comparison of Proportional Hazards and Accelerated Failure Time Models}},
  type   = {phdthesis},
}

@misc{pkgdeepsurv,
  author = {Katzman, Jared},
  url    = {https://pypi.org/project/deepsurv/},
  date   = {2016},
  title  = {{DeepSurv}},
}

@article{Polley2010,
  author    = {Polley, Eric C and {Van Der Laan}, Mark J},
  publisher = {bepress},
  date      = {2010},
  title     = {{Super learner in prediction}},
}

@inproceedings{Sonabend2022a,
  abstract   = {Algorithmic fairness is an increasingly important field concerned with detecting and mitigating biases in machine learning models. There has been a wealth of literature for algorithmic fairness in regression and classification however there has been little exploration of the field for survival analysis. Survival analysis is the prediction task in which one attempts to predict the probability of an event occurring over time. Survival predictions are particularly important in sensitive settings such as when utilising machine learning for diagnosis and prognosis of patients. In this paper we explore how to utilise existing survival metrics to measure bias with group fairness metrics. We explore this in an empirical experiment with 29 survival datasets and 8 measures. We find that measures of discrimination are able to capture bias well whereas there is less clarity with measures of calibration and scoring rules. We suggest further areas for research including prediction-based fairness metrics for distribution predictions.},
  author     = {Sonabend, Raphael and Pfisterer, Florian and Mishler, Alan and Schauer, Moritz and Burk, Lukas and Mukherjee, Sumantrak and Vollmer, Sebastian},
  url        = {http://arxiv.org/abs/2206.03256},
  booktitle  = {DSHealth 2022 Workshop on Applied Data Science for Healthcare at KDD2022},
  date       = {2022},
  eprint     = {2206.03256},
  eprinttype = {arXiv},
  file       = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Sonabend et al. - 2022 - Flexible Group Fairness Metrics for Survival Analysis.pdf:pdf},
  title      = {{Flexible Group Fairness Metrics for Survival Analysis}},
}

@article{Schmid2012,
  abstract     = {Discrimination measures for continuous time-to-event outcomes have become an important tool in medical decision making. The idea behind discrimination measures is to evaluate the performance of a prediction model by measuring its ability to distinguish between observations having an event and those having no event. Researchers proposed a variety of approaches to estimate discrimination measures from a set of right-censored data. These approaches rely on different regularity assumptions that are needed to ensure consistency of the respective estimators. Typical examples of regularity assumptions include the proportional hazards assumption in Cox regression and the random censoring assumption. Because regularity assumptions are often violated in practice, conducting a sensitivity analysis of the estimators is of considerable interest. The aim of the paper is to analyze and to compare the most popular estimators of discrimination measures for event time outcomes. On the basis of the results of an extensive simulation study and the analysis of molecular data, we investigate the behavior of the estimators in situations where the underlying regularity assumptions do not hold. We show that violations of the regularity assumptions may induce a nonignorable bias and may therefore result in biased medical decision making.},
  author       = {Schmid, Matthias and Potapov, Sergej},
  date         = {2012},
  doi          = {10.1002/sim.5464},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Schmid, Potapov - 2012 - A comparison of estimators to evaluate the discriminatory power of time-to-event models.pdf:pdf},
  isbn         = {1097-0258; 0277-6715},
  issn         = {02776715},
  journaltitle = {Statistics in Medicine},
  keywords     = {Concordance probability,Cox regression,Discrimination measures,Molecular markers,Risk prediction,Survival analysis},
  number       = {23},
  pages        = {2588--2609},
  title        = {{A comparison of estimators to evaluate the discriminatory power of time-to-event models}},
  volume       = {31},
}

@article{Meinshausen2010,
  abstract     = {Summary.? Estimation of structure, such as in variable selection, graphical modelling or cluster analysis, is notoriously difficult, especially for high dimensional data. We introduce stability selection. It is based on subsampling in combination with (high dimensional) selection algorithms. As such, the method is extremely general and has a very wide range of applicability. Stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularization for structure estimation. Variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied. We prove for the randomized lasso that stability selection will be variable selection consistent even if the necessary conditions for consistency of the original lasso method are violated. We demonstrate stability selection for variable selection and Gaussian graphical modelling, using real and simulated data.},
  author       = {Meinshausen, Nicolai and Bühlmann, Peter},
  url          = {https://doi.org/10.1111/j.1467-9868.2010.00740.x},
  annotation   = {doi: 10.1111/j.1467-9868.2010.00740.x},
  date         = {2010-09},
  doi          = {10.1111/j.1467-9868.2010.00740.x},
  issn         = {1369-7412},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords     = {High dimensional data,Resampling,Stability selection,Structure estimation},
  number       = {4},
  pages        = {417--473},
  title        = {{Stability selection}},
  volume       = {72},
}

@article{pkgranger,
  author       = {Wright, Marvin N. and Ziegler, Andreas},
  date         = {2017},
  journaltitle = {Journal of Statistical Software},
  number       = {1},
  pages        = {1--17},
  title        = {{ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R}},
  volume       = {77},
}

@article{Windish2007,
  author       = {Windish, Donna M and Huot, Stephen J and Green, Michael L},
  publisher    = {American Medical Association},
  date         = {2007},
  issn         = {0098-7484},
  journaltitle = {Jama},
  number       = {9},
  pages        = {1010--1022},
  title        = {{Medicine residents' understanding of the biostatistics and results in the medical literature}},
  volume       = {298},
}

@misc{pkgdistributions3,
  author    = {Hayes, Alex and Moller-Trane, Ralph},
  publisher = {CRAN},
  url       = {https://cran.r-project.org/package=distributions3},
  date      = {2019},
  title     = {{distributions3: Probability Distributions as S3 Objects}},
}

@article{Reeves2021,
  abstract     = {The goals of SARS-CoV-2 vaccination programs are to maximally reduce cases and deaths, and to limit the amount of time required under lockdown. Using a mathematical model calibrated to data from King County Washington but generalizable across states, we simulated multiple scenarios with different vaccine efficacy profiles, vaccination rates, and case thresholds for triggering and relaxing partial lockdowns. We assumed that a contagious variant is currently present at low levels. In all scenarios, it rapidly becomes dominant by early summer. Low case thresholds for triggering partial lockdowns during current and future waves of infection strongly predict lower total numbers of COVID-19 infections, hospitalizations and deaths in 2021. However, in regions with relatively higher current seroprevalence, there is a predicted delay in onset of a subsequent surge in new variant infections. For all vaccine efficacy profiles, increasing vaccination rate lowers the total number of infections and deaths, as well as the total number of days under partial lockdown. Due to variable current estimates of emerging variant infectiousness, vaccine efficacy against these variants, vaccine refusal, and future adherence to masking and physical distancing, we project considerable uncertainty regarding the timing and intensity of subsequent waves of infection. Nevertheless, under all plausible scenarios, rapid vaccination and early implementation of partial lockdown are the two most critical variables to save the greatest number of lives.Competing Interest StatementThe authors have declared no competing interest.Funding StatementDrs. Schiffer and Dimitrov receive funding via a COVID-19 Forecasting and Modeling grant through the Council of State and Territorial Epidemiologists.Author DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesThe details of the IRB/oversight body that provided approval or exemption for the research described are given below:IRB approval was not required for this study.All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.YesAll code is available as described in the supplementary information.},
  author       = {Reeves, Daniel B and Bracis, Chloe and Swan, David A and Moore, Mia and Dimitrov, Dobromir and Schiffer, Joshua T},
  url          = {http://medrxiv.org/content/early/2021/02/03/2021.02.02.21250985.abstract},
  date         = {2021-01},
  doi          = {10.1101/2021.02.02.21250985},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Reeves et al. - 2021 - Rapid vaccination and early reactive partial lockdown will minimize deaths from emerging highly contagious SARS-C.pdf:pdf},
  journaltitle = {medRxiv},
  pages        = {2021.02.02.21250985},
  title        = {{Rapid vaccination and early reactive partial lockdown will minimize deaths from emerging highly contagious SARS-CoV-2 variants}},
}

@inproceedings{VanBelle2007,
  author    = {{Van Belle}, Vanya and Pelckmans, Kristiaan and Suykens, Johan A.K. and {Van Huffel}, Sabine},
  booktitle = {In Proceedings of the Third International Conference on Computational Intelligence in Medicine and Healthcare},
  date      = {2007},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Van Belle et al. - 2007 - Support Vector Machines for Survival Analysis.pdf:pdf},
  keywords  = {concordance index,kernel methods,machine learning,port vector machines,sup-,support vector machine,survival,survival analysis,svm,svr},
  number    = {1},
  title     = {{Support Vector Machines for Survival Analysis}},
}

@article{Cook2007,
  author       = {Cook, Nancy R.},
  publisher    = {American Heart Association},
  url          = {https://doi.org/10.1161/CIRCULATIONAHA.106.672402},
  annotation   = {doi: 10.1161/CIRCULATIONAHA.106.672402},
  date         = {2007-02},
  doi          = {10.1161/CIRCULATIONAHA.106.672402},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Cook - 2007 - Use and Misuse of the Receiver Operating Characteristic Curve in Risk Prediction.pdf:pdf},
  journaltitle = {Circulation},
  number       = {7},
  pages        = {928--935},
  title        = {{Use and Misuse of the Receiver Operating Characteristic Curve in Risk Prediction}},
  volume       = {115},
}

@article{Harrell1996,
  abstract     = {Multivariable regression models are powerful tools that are used frequently in studies of clinical outcomes. These models can use a mixture of categorical and continuous variables and can handle partially observed (censored) responses. However, uncritical application of modelling techniques can result in models that poorly fit the dataset at hand, or, even more likely, inaccurately predict outcomes on new subjects. One must know how to measure qualities of a model's fit in order to avoid poorly fitted or overfitted models. Measurement of predictive accuracy can be difficult for survival time data in the presence of censoring. We discuss an easily interpretable index of predictive discrimination as well as methods for assessing calibration of predicted survival probabilities. Both types of predictive accuracy should be unbiasedly validated using bootstrapping or cross-validation, before using predictions in a new data series. We discuss some of the hazards of poorly fitted and overfitted regression models and present one modelling strategy that avoids many of the problems discussed. The methods described are applicable to all regression models, but are particularly needed for binary, ordinal, and time-to-event outcomes. Methods are illustrated with a survival analysis in prostate cancer using Cox regression.},
  author       = {Harrell, Frank E. and Lee, Kerry L. and Mark, Daniel B.},
  date         = {1996},
  doi          = {10.1002/0470023678.ch2b(i)},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Harrell, Lee, Mark - 1996 - Multivariable Prognostic Models Issues in Developing Models, Evaluating Assumptions and Adequacy, and Measur.pdf:pdf},
  isbn         = {9780470023679},
  issn         = {02776715},
  journaltitle = {Statistics in Medicine},
  pages        = {361--387},
  title        = {{Multivariable Prognostic Models: Issues in Developing Models, Evaluating Assumptions and Adequacy, and Measuring and Reducing Errors}},
  volume       = {15},
}

@misc{pkgsurvpack,
  author = {Evers, Ludger},
  url    = {http://www.stats.gla.ac.uk/$\sim$levers/software.html},
  date   = {2009},
  title  = {{survpack: Methods for Fitting High-Dimensional Survival Models}},
}

@article{Cortes2005,
  abstract     = {Inmany applications, good ranking is a highly desirable performance for a classifier. The criterion commonly used to measure the ranking quality of a classification algorithm is the area under the ROC curve (AUC). To report it properly, it is crucial to determine an interval of confidence for its value. This paper provides confidence intervals for the AUC based on a statistical and combinatorial analysis using only simple parameters such as the error rate and the number of positive and negative examples. The analysis is distribution-independent, it makes no assumption about the distribution of the scores of negative or positive examples. The results are of practical use and can be viewed as the equivalent for AUC of the standard confidence intervals given in the case of the error rate. They are comparedwith previous approaches in several standard classification tasks demonstrating the benefits of our analysis. 1},
  author       = {Cortes, Corinna and Mohri, Mehryar},
  url          = {http://books.google.com/books?hl=en&lr=&id=etp-l5VrbHsC&oi=fnd&pg=PA305&dq=Confidence+Intervals+for+the+Area+under+the+ROC+Curve&ots=_K6x0GtGxG&sig=_clX-1y-IV17gIDSI5c63gBewSg},
  date         = {2005},
  doi          = {10.1016/0006-8993(94)91632-2},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Cortes, Mohri - 2005 - Confidence intervals for the area under the ROC curve.pdf:pdf},
  isbn         = {978-0-262-19534-8},
  issn         = {0262195348},
  journaltitle = {Advances in Neural Information Processing Systems {\ldots}},
  pages        = {305--312},
  title        = {{Confidence intervals for the area under the ROC curve}},
  volume       = {17},
}

@article{Pearce2017,
  author       = {Pearce, Neil and Lawlor, Debbie A},
  url          = {https://doi.org/10.1093/ije/dyw328},
  date         = {2017-03},
  doi          = {10.1093/ije/dyw328},
  issn         = {0300-5771},
  journaltitle = {International Journal of Epidemiology},
  number       = {6},
  pages        = {1895--1903},
  title        = {{Causal inference—so much more than statistics}},
  volume       = {45},
}

@article{Anderson2011,
  author       = {Anderson, Britta L and Schulkin, Jay},
  publisher    = {Karger Publishers},
  date         = {2011},
  issn         = {1661-3791},
  journaltitle = {Breast care},
  number       = {4},
  pages        = {285--288},
  title        = {{Physicians' perceptions of patients' knowledge and opinions regarding breast cancer: Associations with patient education and physician numeracy}},
  volume       = {6},
}

@book{Hastie2013,
  author    = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  location  = {New York},
  publisher = {Springer},
  date      = {2013},
  title     = {{An introduction to statistical learning}},
  volume    = {112},
}

@article{KATTAN2003,
  abstract     = {ABSTRACT Purpose There is controversy as to whether artificial neural networks and other machine learning methods provide predictions that are more accurate than those provided by traditional statistical models when applied to censored data. Materials and Methods Several machine learning prediction methods are compared with Cox proportional hazards regression using 3 large urological datasets. As a measure of predictive ability, discrimination that is similar to an area under the receiver operating characteristic curve is computed for each. Results In all 3 datasets Cox regression provided comparable or superior predictions compared with neural networks and other machine learning techniques. In general, this finding is consistent with the literature. Conclusions Although theoretically attractive, artificial neural networks and other machine learning techniques do not often provide an improvement in predictive accuracy over Cox regression.},
  author       = {KATTAN, MICHAEL W},
  url          = {http://www.sciencedirect.com/science/article/pii/S0022534701682508 http://www.jurology.com/doi/10.1097/01.ju.0000094764.56269.2d},
  date         = {2003-12},
  doi          = {10.1097/01.ju.0000094764.56269.2d},
  issn         = {0022-5347},
  journaltitle = {Journal of Urology},
  keywords     = {machine learning,neural network models,predictive value of tests,statistical model},
  number       = {6S},
  pages        = {S6--S10},
  title        = {{Comparison of Cox Regression With Other Methods for Determining Prediction Models and Nomograms}},
  volume       = {170},
}

@article{Han2019,
  abstract     = {Objective To promote a more systematic approach to research on uncertainty in health care, and to explore promising starting points and future directions for this research. Methods We examine three fundamental aspects of medical uncertainty that a systematic research program should ideally address: its nature, effects, and communication. We summarize key insights from past empirical research and explore existing conceptual models that can help guide future research. Results A diverse body of past research on medical uncertainty has produced valuable empirical insights and conceptual models that provide useful starting points for future empirical and theoretical work. However, these insights need to be more fully developed and integrated to answer remaining questions about what uncertainty is, how it affects people, and how and why it should be communicated. Conclusion Uncertainty in health care is an extremely important but incompletely understood phenomenon. Improving our understanding of the many important aspects of uncertainty in health care will require a more systematic program of research based upon shared, integrative conceptual models and active, collaborative engagement of the broader research community. Practice Implications A more systematic approach to investigating uncertainty in health care can help elucidate how the clinical communication of uncertainty might be improved.},
  author       = {Han, Paul K J and Babrow, Austin and Hillen, Marij A and Gulbrandsen, P{å}l and Smets, Ellen M and Ofstad, Eirik H},
  url          = {https://www.sciencedirect.com/science/article/pii/S0738399119302447},
  date         = {2019},
  doi          = {https://doi.org/10.1016/j.pec.2019.06.012},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Han et al. - 2019 - Uncertainty in health care Towards a more systematic program of research.pdf:pdf},
  issn         = {0738-3991},
  journaltitle = {Patient Education and Counseling},
  keywords     = {Medical uncertainty,Uncertainty},
  number       = {10},
  pages        = {1756--1766},
  title        = {{Uncertainty in health care: Towards a more systematic program of research}},
  volume       = {102},
}

@misc{pkgmlr3misc,
  author    = {Lang, Michel and Schratz, Patrick},
  publisher = {CRAN},
  url       = {https://cran.r-project.org/package=mlr3misc},
  date      = {2020},
  title     = {{mlr3misc: Helper Functions for 'mlr3'}},
}

@article{Guyatt2008,
  abstract     = {Guidelines are inconsistent in how they rate the quality of evidence and the strength of recommendations. This article explores the advantages of the GRADE system, which is increasingly being adopted by organisations worldwide.},
  author       = {Guyatt, Gordon H and Oxman, Andrew D and Vist, Gunn E and Kunz, Regina and Falck-, Yngve and Alonso-coello, Pablo and Schünemann, Holger J},
  date         = {2008},
  doi          = {10.1136/bmj.39489.470347.AD},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Guyatt et al. - 2008 - Rating Quality of Evidence and Strength of Recommendations GRADE An Emerging Consensus on Rating Quality of Evi.pdf:pdf},
  isbn         = {1756-1833 (Electronic)\n0959-535X (Linking)},
  issn         = {1756-1833},
  journaltitle = {British Medical Journal},
  number       = {April},
  pages        = {924--926},
  title        = {{Rating Quality of Evidence and Strength of Recommendations : GRADE : An Emerging Consensus on Rating Quality of Evidence and Strength of Recommendations RATING QUALITY OF EVIDENCE OF RECOMMENDATIONS GRADE : of evidence an emerging and consensus of on rati}},
  volume       = {336},
}

@article{dataprostate,
  author       = {Lu-Yao, Grace L and Albertsen, Peter C and Moore, Dirk F and Shih, Weichung and Lin, Yong and DiPaola, Robert S and Barry, Michael J and Zietman, Anthony and O'Leary, Michael and Walker-Corkery, Elizabeth},
  publisher    = {American Medical Association},
  date         = {2009},
  issn         = {0098-7484},
  journaltitle = {Jama},
  number       = {11},
  pages        = {1202--1209},
  title        = {{Outcomes of localized prostate cancer following conservative management}},
  volume       = {302},
}

@article{Volz2021,
  abstract     = {The SARS-CoV-2 lineage B.1.1.7, now designated Variant of Concern 202012/01 (VOC) by Public Health England, originated in the UK in late Summer to early Autumn 2020. We examine epidemiological evidence for this VOC having a transmission advantage from several perspectives. First, whole genome sequence data collected from community-based diagnostic testing provides an indication of changing prevalence of different genetic variants through time. Phylodynamic modelling additionally indicates that genetic diversity of this lineage has changed in a manner consistent with exponential growth. Second, we find that changes in VOC frequency inferred from genetic data correspond closely to changes inferred by S-gene target failures (SGTF) in community-based diagnostic PCR testing. Third, we examine growth trends in SGTF and non-SGTF case numbers at local area level across England, and show that the VOC has higher transmissibility than non-VOC lineages, even if the VOC has a different latent period or generation time. Available SGTF data indicate a shift in the age composition of reported cases, with a larger share of under 20 year olds among reported VOC than non-VOC cases. Fourth, we assess the association of VOC frequency with independent estimates of the overall SARS-CoV-2 reproduction number through time. Finally, we fit a semi-mechanistic model directly to local VOC and non-VOC case incidence to estimate the reproduction numbers over time for each. There is a consensus among all analyses that the VOC has a substantial transmission advantage, with the estimated difference in reproduction numbers between VOC and non-VOC ranging between 0.4 and 0.7, and the ratio of reproduction numbers varying between 1.4 and 1.8. We note that these estimates of transmission advantage apply to a period where high levels of social distancing were in place in England; extrapolation to other transmission contexts therefore requires caution.Competing Interest StatementThe authors have declared no competing interest.Funding StatementCOG-UK is supported by funding from the Medical Research Council (MRC) part of UK Research &amp; Innovation (UKRI), the National Institute of Health Research (NIHR) and Genome Research Limited, operating as the Wellcome Sanger Institute. The Imperial College COVID-19 Research Fund, UKRI (MR/V038109/1), The Academy of Medical Sciences (SBF004/1080), Bill &amp; Melinda Gates Foundation (OPP1197730, OPP1175094), the European Commission (CoroNAb 101003653), the NIHR BRC Imperial College NHS Trust Infection and COVID themes (RDA02), Amazon AWS and Microsoft AI for Health, the EPSRC, The Medical Research Council (MR/R015600/1), the NIHR Health Protection Research Unit for Modelling and Health Economics, NIHR VEEPD project funding. Wellcome core funding to the Wellcome Sanger Institute (206194).Author DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesThe details of the IRB/oversight body that provided approval or exemption for the research described are given below:NAAll necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.YesAll aggregated data to reproduce analysis will be provided in the url below. https://github.com/mrc-ide/covid19-variant-N501Y},
  author       = {Volz, Erik and Mishra, Swapnil and Chand, Meera and Barrett, Jeffrey C and Johnson, Robert and Geidelberg, Lily and Hinsley, Wes R and Laydon, Daniel J and Dabrera, Gavin and O'Toole, Áine and Amato, Roberto and Ragonnet-Cronin, Manon and Harrison, Ian and Jackson, Ben and Ariani, Cristina V and Boyd, Olivia and Loman, Nicholas J and McCrone, John T and Gonçalves, Sónia and Jorgensen, David and Myers, Richard and Hill, Verity and Jackson, David K and Gaythorpe, Katy and Groves, Natalie and Sillitoe, John and Kwiatkowski, Dominic P and Flaxman, Seth and Ratmann, Oliver and Bhatt, Samir and Hopkins, Susan and Gandy, Axel and Rambaut, Andrew and Ferguson, Neil M},
  url          = {http://medrxiv.org/content/early/2021/01/04/2020.12.30.20249034.1.abstract},
  date         = {2021-01},
  doi          = {10.1101/2020.12.30.20249034},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Volz et al. - 2021 - Transmission of SARS-CoV-2 Lineage B.1.1.7 in England Insights from linking epidemiological and genetic data.pdf:pdf},
  journaltitle = {medRxiv},
  pages        = {2020.12.30.20249034},
  title        = {{Transmission of SARS-CoV-2 Lineage B.1.1.7 in England: Insights from linking epidemiological and genetic data}},
}

@article{Royston2004,
  abstract     = {Multivariable prognostic models are widely used in cancer and other disease areas, and have a range of applications in clinical medicine, clinical trials and allocation of health services resources. A well-founded and reliable measure of the prognostic ability of a model would be valuable to help define the separation between patients or prognostic groups that the model could provide, and to act as a benchmark of model performance in a validation setting. We propose such a measure for models of survival data. Its motivation derives originally from the idea of separation between Kaplan-Meier curves. We define the criteria for a successful measure and discuss them with respect to our approach. Adjustments for 'optimism', the tendency for a model to predict better on the data on which it was derived than on new data, are suggested. We study the properties of the measure by simulation and by example in three substantial data sets. We believe that our new measure will prove useful as a tool to evaluate the separation available-with a prognostic model.},
  author       = {Royston, Patrick and Sauerbrei, Willi},
  annotation   = {Referenced in Royston/Altman 2013},
  date         = {2004},
  doi          = {10.1002/sim.1621},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Royston, Sauerbrei - 2004 - A new measure of prognostic separation in survival data.pdf:pdf},
  isbn         = {0277-6715 (Print)0̊277-6715 (Linking)},
  issn         = {02776715},
  journaltitle = {Statistics in Medicine},
  keywords     = {Measure of separation,Multivariable regression models,Prognostic modelling,Survival data},
  number       = {5},
  pages        = {723--748},
  title        = {{A new measure of prognostic separation in survival data}},
  volume       = {23},
}

@misc{pkgchron,
  author = {James, David and Hornik, Kurt},
  url    = {https://cran.r-project.org/package=chron},
  date   = {2018},
  title  = {{chron: Chronological Objects which Can Handle Dates and Times}},
}

@article{Liang2008,
  abstract     = {In survival analysis, it is of interest to appropriately select significant predictors. In this paper, we extend the AIC(C) selection procedure of Hurvich and Tsai to survival models to improve the traditional AIC for small sample sizes. A theoretical verification under a special case of the exponential distribution is provided. Simulation studies illustrate that the proposed method substantially outperforms its counterpart: AIC, in small samples, and competes it in moderate and large samples. Two real data sets are also analyzed.},
  author       = {Liang, Hua and Zou, Guohua},
  language     = {eng},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/19158943 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2344147/},
  date         = {2008-01},
  doi          = {10.1016/j.csda.2007.09.003},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Liang, Zou - 2008 - Improved AIC Selection Strategy for Survival Analysis.pdf:pdf},
  issn         = {0167-9473},
  journaltitle = {Computational statistics & data analysis},
  number       = {5},
  pages        = {2538--2548},
  title        = {{Improved AIC Selection Strategy for Survival Analysis}},
  volume       = {52},
}

@article{Schoop2011,
  abstract     = {Abstract Prediction of future events using longitudinally collected patient measurements is increasingly popular as technical and methodological advances allow the construction of more and more complex prognostic models. We aim to give an overview of existing approaches to measure the prediction error of such dynamic predictions and link these to a measure proposed in a preceding paper (Schoop et al.), the conditional prediction error. We present theoretical results of the conditional prediction error, especially regarding the comparison of different prediction rules and its behavior in the presence of misspecification of the link between longitudinal covariates and survival time. A simulation study investigating the performance of its estimator in finite sample sizes rounds off this paper.},
  author       = {Schoop, Rotraut and Schumacher, Martin and Graf, Erika},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/bimj.201000145},
  annotation   = {doi: 10.1002/bimj.201000145},
  date         = {2011-03},
  doi          = {10.1002/bimj.201000145},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Schoop, Schumacher, Graf - 2011 - Measures of prediction error for survival data with longitudinal covariates.pdf:pdf},
  issn         = {0323-3847},
  journaltitle = {Biometrical Journal},
  keywords     = {Brier score,IPCW,Prediction error,Quadratic loss,Time-dependent covariates},
  number       = {2},
  pages        = {275--293},
  title        = {{Measures of prediction error for survival data with longitudinal covariates}},
  volume       = {53},
}

@misc{pkgkmsurv,
  author     = {Klein, John P and Moeschberger, Melvin L},
  publisher  = {CRAN},
  url        = {https://cran.r-project.org/package=KMsurv},
  annotation = {R package version 0.1-5},
  date       = {2012},
  title      = {{KMsurv: Data sets from Klein and Moeschberger (1997), Survival Analysis}},
}

@article{Johnson2011,
  abstract     = {Lung cancer is among the most common cancers in the United States, in terms of incidence and mortality. In 2009, it is estimated that more than 150,000 deaths will result from lung cancer alone. Genetic information is an extremely valuable data source in characterizing the personal nature of cancer. Over the past several years, investigators have conducted numerous association studies where intensive genetic data is collected on relatively few patients compared to the numbers of gene predictors, with one scientific goal being to identify genetic features associated with cancer recurrence or survival. In this note, we propose high-dimensional survival analysis through a new application of boosting, a powerful tool in machine learning. Our approach is based on an accelerated lifetime model and minimizing the sum of pairwise differences in residuals. We apply our method to a recent microarray study of lung adenocarcinoma and find that our ensemble is composed of 19 genes, while a proportional hazards (PH) ensemble is composed of nine genes, a proper subset of the 19-gene panel. In one of our simulation scenarios, we demonstrate that PH boosting in a misspecified model tends to underfit and ignore moderately-sized covariate effects, on average. Diagnostic analyses suggest that the PH assumption is not satisfied in the microarray data and may explain, in part, the discrepancy in the sets of active coefficients. Our simulation studies and comparative data analyses demonstrate how statistical learning by PH models alone is insufficient.},
  author       = {Johnson, Brent A and Long, Qi},
  language     = {en},
  publisher    = {The Institute of Mathematical Statistics},
  url          = {https://projecteuclid.org:443/euclid.aoas/1310562217},
  date         = {2011},
  doi          = {10.1214/10-AOAS426},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Johnson, Long - 2011 - Survival ensembles by the sum of pairwise differences with application to lung cancer microarray studies.pdf:pdf},
  issn         = {1932-6157},
  journaltitle = {Ann. Appl. Stat.},
  keywords     = {Accelerated failure time,aft,boosting,concordance,ensemble,gehan,lasso,ml,proportional hazards regression,ranking,survival,survival analysis},
  number       = {2A},
  pages        = {1081--1101},
  title        = {{Survival ensembles by the sum of pairwise differences with application to lung cancer microarray studies}},
  volume       = {5},
}

@article{Lundberg2017,
  author       = {Lundberg, Scott M and Lee, Su-In},
  date         = {2017},
  journaltitle = {Advances in Neural Information Processing Systems},
  title        = {{A Unified Approach to Interpreting Model Predictions}},
  volume       = {30},
}

@article{pkgsklearn,
  author       = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  date         = {2011},
  journaltitle = {Journal of Machine Learning Research},
  pages        = {2825--2830},
  title        = {{Scikit-learn: Machine Learning in Python}},
  volume       = {12},
}

@article{Langford2016,
  author       = {Langford, John and Mineiro, Paul and Beygelzimer, Alina and Daume, Hal},
  date         = {2016},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Langford et al. - 2016 - Learning Reductions that Really Work.pdf:pdf},
  journaltitle = {Proceedings of the IEEE},
  number       = {1},
  title        = {{Learning Reductions that Really Work}},
  volume       = {104},
}

@book{Collett2014,
  author    = {Collett, David},
  publisher = {CRC},
  date      = {2014},
  edition   = {3},
  isbn      = {978-1584883258},
  title     = {{Modelling Survival Data in Medical Research}},
}

@article{Akritas1994,
  abstract     = {We consider the problem of estimating the bivariate distribution of the random vector $(X, Y)$ when $Y$ may be subject to random censoring. The censoring variable $C$ is allowed to depend on $X$ but it is assumed that $Y$ and $C$ are conditionally independent given $X = x$. The estimate of the bivariate distribution is obtained by averaging estimates of the conditional distribution of $Y$ given $X = x$ over a range of values of $x$. The weak convergence of the centered estimator multiplied by $n^{1/2}$ is obtained, and a closed-form expression for the covariance function of the limiting process is given. It is shown that the proposed estimator is optimal in the Beran sense. This is similar to an optimality property the product-limit estimator enjoys. Using the proposed estimator of the bivariate distribution, an extension of the least squares estimator to censored data polynomial regression is obtained and its asymptotic normality established.},
  author       = {Akritas, Michael G},
  language     = {en},
  publisher    = {The Institute of Mathematical Statistics},
  url          = {https://projecteuclid.org:443/euclid.aos/1176325630},
  date         = {1994},
  doi          = {10.1214/aos/1176325630},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Akritas - 1994 - Nearest Neighbor Estimation of a Bivariate Distribution Under Random Censoring.pdf:pdf},
  issn         = {0090-5364},
  journaltitle = {Ann. Statist.},
  keywords     = {Beran optimality,Conditional empirical processes,conditional Kaplan-Meier estimator,polynomial regression,weak convergence},
  number       = {3},
  pages        = {1299--1327},
  title        = {{Nearest Neighbor Estimation of a Bivariate Distribution Under Random Censoring}},
  volume       = {22},
}

@article{Christodoulou2019,
  author       = {Christodoulou, Evangelia and Ma, Jie and Collins, Gary S and Steyerberg, Ewout W and Verbakel, Jan Y and {Van Calster}, Ben},
  publisher    = {Elsevier},
  url          = {https://doi.org/10.1016/j.jclinepi.2019.02.004},
  annotation   = {doi: 10.1016/j.jclinepi.2019.02.004},
  date         = {2019-06},
  doi          = {10.1016/j.jclinepi.2019.02.004},
  issn         = {0895-4356},
  journaltitle = {Journal of Clinical Epidemiology},
  pages        = {12--22},
  title        = {{A systematic review shows no performance benefit of machine learning over logistic regression for clinical prediction models}},
  volume       = {110},
}

@misc{pkgnnetsurvival,
  author = {Gensheimer, Michael F. and Narasimhan, Balasubramanian},
  url    = {https://github.com/MGensheimer/nnet-survival},
  date   = {2019},
  title  = {nnet-survival},
}

@inproceedings{Seker2002a,
  abstract   = {This study aims to identify the most and least significant prognostic factors for breast cancer survival analysis by means of feature evaluation indices derived from multilayer feedforward backpropagation neural networks (MLFFBPNN), fuzzy k-nearest neighbour classifier (FK-NN) and a logistic regression-based backward stepwise method (ER). The data used for the survival analysis were collected from 100 women who had been clinically diagnosed with breast disease in the form of carcinoma or benign conditions. The data set consists of seven different histological and cytological prognostic factors and two corresponding outputs to be predicted (whether the patient is alive or dead within 5 years of diagnosis). The MLFFBPNN, FK-NN and LR based indices identified different subsets of the factors as the most significant sets. We therefore suggest that it could be dangerous to rely on one method's outcome for assessment of such factors. It should also be noted that "S-phase fraction" (SPF) is the common cytological factor identified by all three methods while none of the three methods identified another cytological factor, namely "minimum (start) nuclear pleomorphism index" (NPI/sub min/). We, therefore, conclude that "S-phase fraction" and "minimum (start) nuclear pleomorphism index" appear to be the most and least important prognostic factors, respectively, for survival analysis in breast cancer patients, and should be investigated thoroughly in future clinical studies in oncology.},
  author     = {Seker, H and Odetayo, M O and Petrovic, D and Naguib, R N G and Bartoli, C and Alasio, L and Lakshmi, M S and Sherbet, G V and Hinton, O R},
  annotation = {feature selection},
  booktitle  = {IEEE CCECE2002. Canadian Conference on Electrical and Computer Engineering. Conference Proceedings (Cat. No.02CH37373)},
  date       = {2002},
  doi        = {10.1109/CCECE.2002.1013121},
  isbn       = {0840-7789 VO - 2},
  keywords   = {Artificial neural networks,Backpropagation,Breast cancer,Diseases,Erbium,Feedforward neural networks,Fuzzy neural networks,Logistics,Multi-layer neural network,Neural networks,S-phase fraction,artificial neural network based feature evaluation,backpropagation,benign conditions,breast cancer survival analysis,breast disease,cancer,carcinoma,cellular biophysics,clinical factor assessment,cytological prognostic factors,feedforward neural nets,fuzzy k-nearest neighbour classifier,histological prognostic factors,logistic regression-based backward stepwise method,medical computing,minimum nuclear pleomorphism index,multilayer feedforward backpropagation neural netw,multilayer perceptrons,oncology,pattern classification,statistical analysis,women},
  pages      = {1211--1215 vol.2},
  title      = {{An artificial neural network based feature evaluation index for the assessment of clinical factors in breast cancer survival analysis}},
  volume     = {2},
}

@article{Ridgeway1999,
  author       = {Ridgeway, Greg},
  date         = {1999},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Ridgeway - 1999 - The state of boosting.pdf:pdf},
  journaltitle = {Computing Science and Statistics},
  keywords     = {boosting,cox,ensemble,likelihood,ml,ph,survival},
  pages        = {172--181},
  title        = {{The state of boosting}},
  volume       = {31},
}

@book{Vapnik1998,
  author = {Vapnik, Vladimir},
  date   = {1998},
  isbn   = {978-0-387-94559-0},
  title  = {{The Nature of Statistical Learning Theory}},
}

@article{pkgpammtools,
  author       = {Bender, Andreas and Scheipl, Fabian},
  publisher    = {CRAN},
  url          = {http://arxiv.org/abs/1806.01042},
  date         = {2018},
  journaltitle = {arXiv:1806.01042 [stat]},
  title        = {{pammtools: Piece-wise exponential Additive Mixed Modeling tools}},
}

@incollection{Djulbegovic2011,
  abstract  = {Publisher Summary This chapter review and classify uncertainties in clinical medicine. Epistemic uncertainty is intimately linked to the relationship between theory, evidence, and knowledge. The relationships among observed, observable, and unobservable realities express uncertainties that can be characterized as a lack of knowledge about what is known (unknown knowns), what is known to be unknown (known unknowns), and not knowing what is unknown (unknown unknowns). Intimately linked with this classiﬁcation of uncertainty is the psychological taxonomy that categorizes uncertainty based on knowledge of the external world and on our own state of knowledge. It is suggested that any attempt to develop a comprehensive treatise of uncertainty in clinical medicine must take into account the insights obtained from psychological research on uncertainty. Uncertainty can be effectively managed by explicitly recognizing its many sources, improving the quality of medical evidence, using better information technology tools, searching for sources of bias, and applying probability and decision theory to decisions under uncertainty.},
  author    = {Djulbegovic, Benjamin and Hozo, Iztok and Greenland, Sander},
  editor    = {Gifford, Fred B T - Philosophy of Medicine},
  location  = {Amsterdam},
  publisher = {North-Holland},
  url       = {https://www.sciencedirect.com/science/article/pii/B9780444517876500118},
  booktitle = {Handbook of the Philosophy of Science},
  date      = {2011},
  doi       = {https://doi.org/10.1016/B978-0-444-51787-6.50011-8},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Djulbegovic, Hozo, Greenland - 2011 - Uncertainty in Clinical Medicine.pdf:pdf},
  isbn      = {18789846},
  pages     = {299--356},
  title     = {{Uncertainty in Clinical Medicine}},
  volume    = {16},
}

@article{Ma2006,
  author       = {Ma, Shuangge and Huang, Jian},
  date         = {2006-01},
  doi          = {10.1093/bioinformatics/bti724},
  journaltitle = {Bioinformatics (Oxford, England)},
  pages        = {4356--4362},
  title        = {{Regularized ROC method for disease classification and biomarker selection with microarray data}},
  volume       = {21},
}

@article{Imai2023,
  author       = {Imai, Natsuko and Rawson, Thomas and Knock, Edward S and Sonabend, Raphael and Elmaci, Yasin and Perez-Guzman, Pablo N and Whittles, Lilith K and Kanapram, Divya Thekke and Gaythorpe, Katy A M and Hinsley, Wes and Djaafara, Bimandra A and Wang, Haowei and Fraser, Keith and FitzJohn, Richard G and Hogan, Alexandra B and Doohan, Patrick and Ghani, Azra C and Ferguson, Neil M and Baguelin, Marc and Cori, Anne},
  url          = {https://linkinghub.elsevier.com/retrieve/pii/S2468266722003371},
  date         = {2023-02},
  doi          = {10.1016/S2468-2667(22)00337-1},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Imai et al. - 2023 - Quantifying the effect of delaying the second COVID-19 vaccine dose in England a mathematical modelling study.pdf:pdf},
  issn         = {24682667},
  journaltitle = {The Lancet Public Health},
  title        = {{Quantifying the effect of delaying the second COVID-19 vaccine dose in England: a mathematical modelling study}},
}

@misc{Primo2010,
  author    = {Primo, Walquiria Quida Salles Pereira and Garrafa, Volnei},
  publisher = {scielo},
  booktitle = {Revista da Associação Médica Brasileira},
  date      = {2010},
  isbn      = {0104-4230 UL - http://www.scielo.br/scielo.php?script=sci_arttext&pid=S0104-42302010000400010&nrm=iso},
  pages     = {397--402},
  title     = {{Ethical issues of diagnosis disclosure and treatment in patients with genital or breast cancer}},
  volume    = {56},
}

@article{Cox1968,
  author       = {R., Cox and J., Snell},
  date         = {1968},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/R., J. - 1968 - A General Definition of Residuals.pdf:pdf;:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/R., J. - 1968 - A General Definition of Residuals(2).pdf:pdf},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords     = {accelerated life tests,age-specific failure rate,asymptotic theory,censored data,conditional inference,hazard function,life table,limit estimate,medical applications,product,regression,reliability,theory,two-sample rank tests},
  number       = {2},
  pages        = {248--275},
  title        = {{A General Definition of Residuals}},
  volume       = {30},
}

@article{Burton2006,
  author       = {Burton, Andrea and Altman, Douglas G. and Royston, Patrick and Holder, Roger L.},
  url          = {http://doi.wiley.com/10.1002/sim.2673},
  annotation   = {Guidelines for simulating survival data},
  date         = {2006-12},
  doi          = {10.1002/sim.2673},
  eprint       = {NIHMS150003},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Burton et al. - 2006 - The design of simulation studies in medical statistics.pdf:pdf},
  isbn         = {2007090091480},
  issn         = {02776715},
  journaltitle = {Statistics in Medicine},
  number       = {24},
  pages        = {4279--4292},
  title        = {{The design of simulation studies in medical statistics}},
  volume       = {25},
}

@inproceedings{Steck2008,
  author    = {Steck, Harald and Krishnapuram, Balaji and Dehing-Oberije, Cary and Lambin, Philippe and Raykar, Vikas C},
  booktitle = {Advances in neural information processing systems},
  date      = {2008},
  pages     = {1209--1216},
  title     = {{On ranking in survival analysis: Bounds on the concordance index}},
}

@article{Mittelstadt2016,
  abstract     = {The capacity to collect and analyse data is growing exponentially. Referred to as ‘Big Data', this scientific, social and technological trend has helped create destabilising amounts of information, which can challenge accepted social and ethical norms. Big Data remains a fuzzy idea, emerging across social, scientific, and business contexts sometimes seemingly related only by the gigantic size of the datasets being considered. As is often the case with the cutting edge of scientific and technological progress, understanding of the ethical implications of Big Data lags behind. In order to bridge such a gap, this article systematically and comprehensively analyses academic literature concerning the ethical implications of Big Data, providing a watershed for future ethical investigations and regulations. Particular attention is paid to biomedical Big Data due to the inherent sensitivity of medical information. By means of a meta-analysis of the literature, a thematic narrative is provided to guide ethicists, data scientists, regulators and other stakeholders through what is already known or hypothesised about the ethical risks of this emerging and innovative phenomenon. Five key areas of concern are identified: (1) informed consent, (2) privacy (including anonymisation and data protection), (3) ownership, (4) epistemology and objectivity, and (5) ‘Big Data Divides' created between those who have or lack the necessary resources to analyse increasingly large datasets. Critical gaps in the treatment of these themes are identified with suggestions for future research. Six additional areas of concern are then suggested which, although related have not yet attracted extensive debate in the existing literature. It is argued that they will require much closer scrutiny in the immediate future: (6) the dangers of ignoring group-level ethical harms; (7) the importance of epistemology in assessing the ethics of Big Data; (8) the changing nature of fiduciary relationships that become increasingly data saturated; (9) the need to distinguish between ‘academic' and ‘commercial' Big Data practices in terms of potential harm to data subjects; (10) future problems with ownership of intellectual property generated from analysis of aggregated datasets; and (11) the difficulty of providing meaningful access rights to individual data subjects that lack necessary resources. Considered together, these eleven themes provide a thorough critical framework to guide ethical assessment and governance of emerging Big Data practices.},
  author       = {Mittelstadt, Brent Daniel and Floridi, Luciano},
  publisher    = {Springer Netherlands},
  date         = {2016},
  doi          = {10.1007/s11948-015-9652-2},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Mittelstadt, Floridi - 2016 - The Ethics of Big Data Current and Foreseeable Issues in Biomedical Contexts.pdf:pdf},
  isbn         = {1471-5546 (Electronic)1̊353-3452 (Linking)},
  issn         = {14715546},
  journaltitle = {Science and Engineering Ethics},
  keywords     = {Big data,Bioethics,Ethical foresight,Ethics,Information ethics,Medical ethics},
  number       = {2},
  pages        = {303--341},
  title        = {{The Ethics of Big Data: Current and Foreseeable Issues in Biomedical Contexts}},
  volume       = {22},
}

@article{Dehbi2017,
  abstract     = {The hazard ratio (HR) is the most common measure of treatment effect in clinical trials that use time-to-event outcomes such as survival. When survival curves cross over or separate only after a considerable time, the proportional hazards assumption of the Cox model is violated, and HR can be misleading. We present two measures of treatment effects for situations where the HR changes over time: the life expectancy difference (LED) and life expectancy ratio (LER). LED is the difference between mean survival times in the intervention and control arms. LER is the ratio of these two times. LED and LER can be calculated for at least two time intervals during the trial, allowing for curves where the treatment effect changes over time. The two measures are readily interpretable as absolute and relative gains or losses in life expectancy. ### Summary In randomised controlled trials (RCTs), time-to-event endpoints, such as overall survival or time to disease occurrence, are shown as Kaplan-Meier curves. The effect of an intervention compared with a control is {\ldots}},
  author       = {Dehbi, Hakim Moulay and Royston, Patrick and Hackshaw, Allan},
  url          = {http://dx.doi.org/doi:10.1136/bmj.j2250},
  date         = {2017},
  doi          = {10.1136/bmj.j2250},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Dehbi, Royston, Hackshaw - 2017 - Life expectancy difference and life expectancy ratio Two measures of treatment effects in randomised t.pdf:pdf},
  issn         = {17561833},
  journaltitle = {BMJ (Online)},
  number       = {5},
  pages        = {1--7},
  title        = {{Life expectancy difference and life expectancy ratio: Two measures of treatment effects in randomised trials with non-proportional hazards}},
  volume       = {357},
}

@article{Nezhad2019,
  abstract     = {Survival analysis has been developed and applied in the number of areas including manufacturing, finance, economics and healthcare. In healthcare domain, usually clinical data are high-dimensional, sparse and complex and sometimes there exists few amount of time-to-event (labeled) instances. Therefore building an accurate survival model from electronic health records is challenging. With this motivation, we address this issue and provide a new survival analysis framework using deep learning and active learning with a novel sampling strategy. First, our approach provides better representation with lower dimensions from clinical features using labeled (time-to-event) and unlabeled (censored) instances and then actively trains the survival model by labeling the censored data using an oracle. As a clinical assistive tool, we introduce a simple effective treatment recommendation approach based on our survival model. In the experimental study, we apply our approach on SEER-Medicare data related to prostate cancer among African–Americans and white patients. The results indicate that our approach outperforms significantly than baseline models.},
  author       = {Nezhad, Milad Zafar and Sadati, Najibesadat and Yang, Kai and Zhu, Dongxiao},
  url          = {http://www.sciencedirect.com/science/article/pii/S0957417418304949},
  date         = {2019},
  doi          = {https://doi.org/10.1016/j.eswa.2018.07.070},
  issn         = {0957-4174},
  journaltitle = {Expert Systems with Applications},
  keywords     = {Active learning,Deep learning,Electronic health records,Prostate cancer,Survival analysis,Treatment recommendation},
  pages        = {16--26},
  title        = {{A Deep Active Survival Analysis approach for precision treatment recommendations: Application of prostate cancer}},
  volume       = {115},
}

@article{Liestol1994,
  abstract     = {Abstract We consider feed-forward neural nets and their relation to regression models for survival data. We show how the back-propagation algorithm may be used to obtain maximum likelihood estimates in certain standard regression models for survival data, as well as in various generalizations of these. Examples concerning malignant melanoma and post-partum amenorrhoea during lactation are used as illustration. We conclude that although problems with the substantial number of parameters and their interpretation remain, the feed-forward neural network models are flexible extensions to the standard regression models and thereby candidates for use in prediction and exploratory analyses in larger data sets.},
  author       = {Liestol, Knut and Andersen, Per Kragh and Andersen, Ulrich},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/sim.4780131202},
  annotation   = {doi: 10.1002/sim.4780131202},
  date         = {1994-06},
  doi          = {10.1002/sim.4780131202},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Liestol, Andersen, Andersen - 1994 - Survival analysis and neural nets.pdf:pdf},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  number       = {12},
  pages        = {1189--1200},
  title        = {{Survival analysis and neural nets}},
  volume       = {13},
}

@report{ICF2019,
  author      = {{ICF Consulting Services Limited}},
  institution = {ICF Consulting Services Limited},
  url         = {https://www.food.gov.uk/sites/default/files/media/document/communicating-risk_final-report-no-front-page-table-002.pdf},
  date        = {2019},
  file        = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/ICF Consulting Services Limited - 2019 - Communicating Risk A review of guidance and academic literature on communicating risk in relati.pdf:pdf},
  title       = {{Communicating Risk: A review of guidance and academic literature on communicating risk in relation to food}},
  type        = {techreport},
}

@misc{pkgdistr,
  author    = {Ruckdeschel, Peter and Kohl, Matthias and Stabla, Thomas and Camphausen, Florian},
  publisher = {R News},
  url       = {https://cran.r-project.org/package=distr},
  date      = {2006},
  pages     = {2--6},
  title     = {{S4 Classes for Distributions}},
}

@article{Ciampi1988,
  abstract     = {The methodology of recursive partition and amalgamation in biostatistics is presented and a FORTRAN program for its implementation, RECPAM, is described. RECPAM can be used to obtain classifications of patients according to several criteria commonly occurring in clinical biostatistics: an example in prognostic classification based on survival data. Classes are defined by simple statements, expressed in clinical terms, about predictor variables (e.g. prognostic factors). Special features of RECPAM are: the possibility of implementing a variety of classification criteria, the integration of recursive partition and amalgamation, and the availability of several strategies for constructing classification trees. A simple example to illustrate input and output features is given. The scope and flexibility of RECPAM will be illustrated in greater detail in a subsequent paper.},
  author       = {Ciampi, Antonio and Hogg, Sheilah A and McKinney, Steve and Thiffault, Johanne},
  url          = {http://www.sciencedirect.com/science/article/pii/0169260788900041},
  date         = {1988},
  doi          = {https://doi.org/10.1016/0169-2607(88)90004-1},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Ciampi et al. - 1988 - RECPAM a computer program for recursive partition and amalgamation for censored survival data and other situation.pdf:pdf},
  issn         = {0169-2607},
  journaltitle = {Computer Methods and Programs in Biomedicine},
  keywords     = {Categorical response,Classification,Growing trees,Treatment-response},
  number       = {3},
  pages        = {239--256},
  title        = {{RECPAM: a computer program for recursive partition and amalgamation for censored survival data and other situations frequently occurring in biostatistics. I. Methods and program features}},
  volume       = {26},
}

@article{Moghimi-dehkordi2008,
  abstract     = {BACKGROUND: The Cox Proportional Hazard model is the most popular technique to analysis the effects of covariates on survival time but under certain circumstances parametric models may offer advantages over Cox's model. In this study we use Cox regression and alternative parametric models such as: Weibull, Exponential and Lognormal models to evaluate prognostic factors affecting survival of patients with stomach cancer. Comparisons were made to find the best model. METHODS: To determine independent prognostic factors reducing survival time for stomach cancer, we compared parametric and semi-parametric methods applied to patients who registered in one cancer registry center located in southern Iran using the Akaike Information Criterion. RESULTS: Of a total of 442 patients, 266 (60.2%) died. The results of data analysis using Cox and parametric models were approximately similar. Patients with ages 60-75 and >75 years at diagnosis had an increased risk for death followed by those with poor differentiated grade and presence of distant metastasis (P<0.05). CONCLUSION: Although the Hazard Ratios in the Cox model and parametric ones are approximately similar, according to Akaike Information Criterion, the Weibull and Exponential models are the most favorable for survival analysis.},
  author       = {Moghimi-dehkordi, Bijan and Safaee, Azadeh and Pourhoseingholi, Mohamad Amin and Fatemi, Reza and Tabeie, Ziaoddin and Zali, Mohammad Reza},
  date         = {2008},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Moghimi-dehkordi et al. - 2008 - Statistical Comparison of Survival Models for Analysis of Cancer Data.pdf:pdf},
  isbn         = {1513-7368 (Print)1̊513-7368 (Linking)},
  issn         = {1513-7368},
  journaltitle = {Asian Pacific Journal of Cancer Prevention},
  pages        = {417--420},
  title        = {{Statistical Comparison of Survival Models for Analysis of Cancer Data}},
  volume       = {9},
}

@article{Spooner2020,
  author       = {Spooner, Annette and Chen, Emily and Sowmya, Arcot and Sachdev, Perminder and Kochan, Nicole A and Trollor, Julian and Brodaty, Henry},
  url          = {https://doi.org/10.1038/s41598-020-77220-w},
  date         = {2020},
  doi          = {10.1038/s41598-020-77220-w},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Spooner et al. - 2020 - A comparison of machine learning methods for survival analysis of high-dimensional clinical data for dementia pr.pdf:pdf},
  issn         = {2045-2322},
  journaltitle = {Scientific Reports},
  number       = {1},
  pages        = {20410},
  title        = {{A comparison of machine learning methods for survival analysis of high-dimensional clinical data for dementia prediction}},
  volume       = {10},
}

@article{Gompertz1825,
  author       = {Gompertz, Benjamin},
  date         = {1825},
  journaltitle = {Philosophical Transactions of the Royal Society of London},
  pages        = {513--583},
  title        = {{On the Nature of the Function Expressive of the Law of Human Mortality, and on a New Mode of Determining the Value of Life Contingencies}},
  volume       = {115},
}

@article{Jiao2016,
  author       = {Jiao, Yasen and Du, Pufeng},
  publisher    = {Springer},
  date         = {2016},
  issn         = {2095-4689},
  journaltitle = {Quantitative Biology},
  keywords     = {cross-validation,evaluation,evaluation guarantees,performance guarantees,validation},
  number       = {4},
  pages        = {320--330},
  title        = {{Performance measures in evaluating machine learning based bioinformatics predictors for classifications}},
  volume       = {4},
}

@article{BarlowPrentice1988,
  abstract     = {Several possible definitions of residuals are given for relative risk regression with time-varying covariates. Each such residual has a representation as an estimator of a stochastic integral with respect to the martingale arising from a subject's failure time counting process. Previously proposed residuals for individual study subjects and for specific time points are shown to be special cases of this definition, as are previously derived regression diagnostics. An illustration and various generalizations are also given.},
  author       = {Barlow, William E. and Prentice, Ross L.},
  date         = {1988},
  doi          = {10.1093/biomet/75.1.65},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Barlow, Prentice - 1988 - Residuals for relative risk regression.pdf:pdf},
  isbn         = {1464-3510},
  issn         = {00063444},
  journaltitle = {Biometrika},
  keywords     = {Case-control study,Censoring,Failure time data,Influence function,Proportional hazards,Regression diagnostic,Relative risk regression,Residual},
  number       = {1},
  pages        = {65--74},
  title        = {{Residuals for relative risk regression}},
  volume       = {75},
}

@misc{Gamma1996,
  abstract  = {Design Patterns is a modern classic in the literature of object-oriented development, offering timeless and elegant solutions to common problems in software design. It describes patterns for managing object creation, composing objects into larger structures, and coordinating control flow between objects. The book provides numerous examples where using composition rather than inheritance can improve the reusability and flexibility of code. Note, though, that it's not a tutorial but a catalog that you can use to find an object-oriented design pattern that's appropriate for the needs of your particular application--a selection for virtuoso programmers who appreciate (or require) consistent, well-engineered object-oriented designs. Now on {CD}, this internationally acclaimed bestseller is more valuable than ever! Use the contents of the {CD} to create your own design documents and reusable components. The {CD} contains: 23 patterns you can cut and paste into your own design documents; sample code demonstrating pattern implementation; complete Design Patterns content in standard {HTML} format, with numerous hyperlinked cross-references; accessed through a standard web browser; Java-based dynamic search mechanism, enhancing online seach capabilities; graphical user environment, allowing ease of navigation. First published in 1995, this landmark work on object-oriented software design presents a catalog of simple and succinct solutions to common design problems. Created by four experienced designers, the 23 patterns contained herein have become an essential resource for anyone developing reusable object-oriented software. In response to reader demand, the complete text and pattern catalog are now available on {CD}-{ROM}. This electronic version of Design Patterns enables programmers to install the book directly onto a computer or network for use as an online reference for creating reusable object-oriented software. The authors first describe what patterns are and how they can help you in the design process. They then systematically name, explain, evaluate, and catalog recurring designs in object-oriented systems. All patterns are compiled from real-world examples and include code that demonstrates how they may be implemented in object-oriented programming languages such as C++ and Smalltalk. Readers who already own the book will want the {CD} to take advantage of its dynamic search mechanism and ready-to-install patterns.},
  author    = {Gamma, E and Helm, R and Johnson, R and Vlissides, J},
  booktitle = {Addison-Wesley Professional Computing Series},
  date      = {1996},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Gamma et al. - 1996 - Design Patterns Elements of Reusable Software.pdf:pdf},
  isbn      = {020163361-2},
  issn      = {ISBN: 0-201-63361-2},
  keywords  = {design patterns,oop,software},
  pages     = {395},
  title     = {{Design Patterns: Elements of Reusable Software}},
}

@article{Herrmann2020,
  abstract     = {Multi-omics data, that is, datasets containing different types of high-dimensional molecular variables, are increasingly often generated for the investigation of various diseases. Nevertheless, questions remain regarding the usefulness of multi-omics data for the prediction of disease outcomes such as survival time. It is also unclear which methods are most appropriate to derive such prediction models. We aim to give some answers to these questions through a large-scale benchmark study using real data. Different prediction methods from machine learning and statistics were applied on 18 multi-omics cancer datasets (35 to 1000 observations, up to 100 000 variables) from the database ‘The Cancer Genome Atlas' (TCGA). The considered outcome was the (censored) survival time. Eleven methods based on boosting, penalized regression and random forest were compared, comprising both methods that do and that do not take the group structure of the omics variables into account. The Kaplan–Meier estimate and a Cox model using only clinical variables were used as reference methods. The methods were compared using several repetitions of 5-fold cross-validation. Uno's C-index and the integrated Brier score served as performance metrics. The results indicate that methods taking into account the multi-omics structure have a slightly better prediction performance. Taking this structure into account can protect the predictive information in low-dimensional groups—especially clinical variables—from not being exploited during prediction. Moreover, only the block forest method outperformed the Cox model on average, and only slightly. This indicates, as a by-product of our study, that in the considered TCGA studies the utility of multi-omics data for prediction purposes was limited. Contact: moritz.herrmann@stat.uni-muenchen.de, +49 89 2180 3198 Supplementary information: Supplementary data are available at Briefings in Bioinformatics online. All analyses are reproducible using R code freely available on Github.},
  author       = {Herrmann, Moritz and Probst, Philipp and Hornung, Roman and Jurinovic, Vindi and Boulesteix, Anne-Laure},
  url          = {https://academic.oup.com/bib/article/doi/10.1093/bib/bbaa167/5895463},
  date         = {2021-05},
  doi          = {10.1093/bib/bbaa167},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Herrmann et al. - 2020 - Large-scale benchmark study of survival prediction methods using multi-omics data.pdf:pdf},
  issn         = {1467-5463},
  journaltitle = {Briefings in Bioinformatics},
  number       = {3},
  title        = {{Large-scale benchmark study of survival prediction methods using multi-omics data}},
  volume       = {22},
}

@article{VanHouwelingen2007,
  abstract     = {This article advocates the landmarking approach that dynamically adjusts predictive models for survival data during the follow up. This updating is achieved by directly fitting models for the individuals still at risk at the landmark point. Using this approach, simple proportional hazards models are able to catch the development over time for models with time-varying effects of covari- ates or data with time-dependent covariates (biomarkers). To smooth the effect of the landmarking, sequences of models are considered with parametric effects of the landmark time point and fitted by maximizing appropriate pseudo log-likelihoods that extend the partial log-likelihood to cover the landmarking approach.},
  author       = {{Van Houwelingen}, Hans C.},
  date         = {2007},
  doi          = {10.1111/j.1467-9469.2006.00529.x},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Van Houwelingen - 2007 - Dynamic prediction by landmarking in event history analysis.pdf:pdf},
  issn         = {03036898},
  journaltitle = {Scandinavian Journal of Statistics},
  keywords     = {Landmark analysis,Landmarking,Pseudo-partial likelihood,Survival analysis,Time-dependent covariates,Time-varying effects},
  number       = {1},
  pages        = {70--85},
  title        = {{Dynamic prediction by landmarking in event history analysis}},
  volume       = {34},
}

@article{Schumacher2007,
  abstract     = {Motivation: In the process of developing risk prediction models, various steps of model building and model selection are involved. If this process is not adequately controlled, overfitting may result in serious overoptimism leading to potentially erroneous conclusions.Methods: For right censored time-to-event data, we estimate the prediction error for assessing the performance of a risk prediction model (Gerds and Schumacher, 2006; Graf et al., 1999). Furthermore, resampling methods are used to detect overfitting and resulting overoptimism and to adjust the estimates of prediction error (Gerds and Schumacher, 2007).Results: We show how and to what extent the methodology can be used in situations characterized by a large number of potential predictor variables where overfitting may be expected to be overwhelming. This is illustrated by estimating the prediction error of some recently proposed techniques for fitting a multivariate Cox regression model applied to the data of a prognostic study in patients with diffuse large-B-cell lymphoma (DLBCL).Availability: Resampling-based estimation of prediction error curves is implemented in an R package called pec available from the authors.Contact:sec@imbi.uni-freiburg.de},
  author       = {Schumacher, Martin and Binder, Harald and Gerds, Thomas},
  url          = {https://doi.org/10.1093/bioinformatics/btm232},
  date         = {2007-05},
  doi          = {10.1093/bioinformatics/btm232},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Schumacher, Binder, Gerds - 2007 - Assessment of survival prediction models based on microarray data.pdf:pdf},
  issn         = {1367-4803},
  journaltitle = {Bioinformatics},
  number       = {14},
  pages        = {1768--1774},
  title        = {{Assessment of survival prediction models based on microarray data}},
  volume       = {23},
}

@inproceedings{VanBelle2008,
  author    = {{Van Belle}, Vanya and Pelckmans, Kristiaan and Suykens, Johan A K and {Van Huffel}, Sabine},
  booktitle = {Proceedings of the 16th European Symposium on Artificial Neural Networks (ESANN)},
  date      = {2008},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Van Belle et al. - 2008 - Survival SVM a practical scalable algorithm.pdf:pdf},
  keywords  = {concordance,machine learning,ranking,support vector machine,survival,svm},
  pages     = {89--94},
  title     = {{Survival SVM: a practical scalable algorithm}},
}

@article{pkgmlr3,
  author       = {Lang, Michel and Binder, Martin and Richter, Jakob and Schratz, Patrick and Pfisterer, Florian and Coors, Stefan and Au, Quay and Casalicchio, Giuseppe and Kotthoff, Lars and Bischl, Bernd},
  url          = {https://joss.theoj.org/papers/10.21105/joss.01903 https://cran.r-project.org/package=mlr3},
  date         = {2019},
  doi          = {10.21105/joss.01903},
  journaltitle = {Journal of Open Source Software},
  number       = {44},
  pages        = {1903},
  title        = {{mlr3: A modern object-oriented machine learning framework in R}},
  volume       = {4},
}

@book{Sonabend2023,
  author = {Sonabend, Raphael and Bender, Andreas},
  url    = {https://www.mlsabook.com},
  date   = {2023},
  title  = {{Machine Learning in Survival Analysis}},
}

@article{BraddockIII1999,
  author       = {{Braddock III}, Clarence H and Edwards, Kelly A and Hasenberg, Nicole M and Laidley, Tracy L and Levinson, Wendy},
  publisher    = {American Medical Association},
  date         = {1999},
  issn         = {0098-7484},
  journaltitle = {Jama},
  number       = {24},
  pages        = {2313--2320},
  title        = {{Informed decision making in outpatient practice: time to get back to basics}},
  volume       = {282},
}

@inproceedings{Hielscher2010,
  abstract  = {As part of the validation of any statistical model, it is good statistical practice to quantify the amount of prognostic information represented by the model; this includes gene expression signatures derived from high-dimensional microarray data. Several approaches exist for right-censored survival data that measure the gain in prognostic information compared to established clinical parameters or biomarkers in terms of explained variation or explained randomness. They are either model-based or use estimates of the prediction accuracy.},
  author    = {Hielscher, Thomas and Zucknick, Manuela and Werft, Wiebke and Benner, Axel},
  editor    = {Fink, Andreas and Lausen, Berthold and Seidel, Wilfried and Ultsch, Alfred},
  location  = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  date      = {2010},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Hielscher et al. - 2010 - On the prognostic value of gene expression signatures for censored data.pdf:pdf},
  isbn      = {978-3-642-01044-6},
  pages     = {663--673},
  title     = {{On the Prognostic Value of Gene Expression Signatures for Censored Data BT - Advances in Data Analysis, Data Handling and Business Intelligence}},
}

@book{datapbcseq,
  author    = {Therneau, Terry M. and Grambsch, Patricia M.},
  location  = {New York},
  booktitle = {Springer-Verlag},
  date      = {2000},
  isbn      = {0-387-98784-3},
  title     = {{Modeling Survival Data: Extending the Cox Model}},
}

@article{Schemper2000,
  author       = {Schemper, Michael and Henderson, Robin},
  date         = {2000},
  doi          = {10.1002/sim.1486},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Schemper, Henderson - 2000 - Predictive Accuracy and Explained Variation in Cox Regression.pdf:pdf},
  isbn         = {0277-6715 (Print)0̊277-6715 (Linking)},
  issn         = {02776715},
  journaltitle = {Biometrics},
  pages        = {249--255},
  title        = {{Predictive Accuracy and Explained Variation in Cox Regression}},
  volume       = {56},
}

@article{Pencina2008,
  abstract     = {Abstract Identification of key factors associated with the risk of developing cardiovascular disease and quantification of this risk using multivariable prediction algorithms are among the major advances made in preventive cardiology and cardiovascular epidemiology in the 20th century. The ongoing discovery of new risk markers by scientists presents opportunities and challenges for statisticians and clinicians to evaluate these biomarkers and to develop new risk formulations that incorporate them. One of the key questions is how best to assess and quantify the improvement in risk prediction offered by these new models. Demonstration of a statistically significant association of a new biomarker with cardiovascular risk is not enough. Some researchers have advanced that the improvement in the area under the receiver-operating-characteristic curve (AUC) should be the main criterion, whereas others argue that better measures of performance of prediction models are needed. In this paper, we address this question by introducing two new measures, one based on integrated sensitivity and specificity and the other on reclassification tables. These new measures offer incremental information over the AUC. We discuss the properties of these new measures and contrast them with the AUC. We also develop simple asymptotic tests of significance. We illustrate the use of these measures with an example from the Framingham Heart Study. We propose that scientists consider these types of measures in addition to the AUC when assessing the performance of newer biomarkers. Copyright ? 2007 John Wiley & Sons, Ltd.},
  author       = {Pencina, Michael J and {D' Agostino Sr}, Ralph B and {D' Agostino Jr}, Ralph B and Vasan, Ramachandran S},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/sim.2929},
  annotation   = {doi: 10.1002/sim.2929},
  date         = {2008-01},
  doi          = {10.1002/sim.2929},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  keywords     = {AUC,biomarker,discrimination,model performance,risk prediction},
  number       = {2},
  pages        = {157--172},
  title        = {{Evaluating the added predictive ability of a new marker: From area under the ROC curve to reclassification and beyond}},
  volume       = {27},
}

@article{datahepato,
  abstract     = {CXC ligand 17 (CXCL17) is a novel CXC chemokine whose clinical significance remains largely unknown. In the present study, we characterized the prognostic value of CXCL17 in patients with hepatocellular carcinoma (HCC) and evaluated the association of CXCL17 with immune infiltration. We examined CXCL17 expression in 227 HCC tissue specimens by immunohistochemical staining, and correlated CXCL17 expression patterns with clinicopathological features, prognosis, and immune infiltrate density (CD4 T cells, CD8 T cells, B cells, natural killer cells, neutrophils, macrophages). Kaplan-Meier survival analysis showed that both increased intratumoral CXCL17 (P = 0.015 for overall survival [OS], P = 0.003 for recurrence-free survival [RFS]) and peritumoral CXCL17 (P = 0.002 for OS, P<0.001 for RFS) were associated with shorter OS and RFS. Patients in the CXCL17low group had significantly lower 5-year recurrence rate compared with patients in the CXCL17high group (peritumoral: 53.1% vs. 77.7%, P<0.001, intratumoral: 58.6% vs. 73.0%, P = 0.001, respectively). Multivariate Cox proportional hazards analysis identified peritumoral CXCL17 as an independent prognostic factor for both OS (hazard ratio [HR] = 2.066, 95% confidence interval [CI] = 1.296–3.292, P = 0.002) and RFS (HR = 1.844, 95% CI = 1.218–2.793, P = 0.004). Moreover, CXCL17 expression was associated with more CD68 and less CD4 cell infiltration (both P<0.05). The combination of CXCL17 density and immune infiltration could be used to further classify patients into subsets with different prognosis for RFS. Our results provide the first evidence that tumor-infiltrating CXCL17+ cell density is an independent prognostic factor that predicts both OS and RFS in HCC. CXCL17 production correlated with adverse immune infiltration and might be an important target for anti-HCC therapies.},
  author       = {Li, Li and Yan, Jing and Xu, Jing and Liu, Chao-Qun and Zhen, Zuo-Jun and Chen, Huan-Wei and Ji, Yong and Wu, Zhi-Peng and Hu, Jian-Yuan and Zheng, Limin and Lau, Wan Yee},
  publisher    = {Public Library of Science},
  url          = {https://doi.org/10.1371/journal.pone.0110064},
  date         = {2014-10},
  journaltitle = {PLOS ONE},
  number       = {10},
  pages        = {e110064},
  title        = {{CXCL17 Expression Predicts Poor Prognosis and Correlates with Adverse Immune Infiltration in Hepatocellular Carcinoma}},
  volume       = {9},
}

@article{Reid1994,
  author       = {Reid, Nancy},
  url          = {http://www.jstor.org/stable/2238700%5Cnhttp://projecteuclid.org/euclid.aoms/1177705148},
  date         = {1994},
  doi          = {10.1214/aos/1176348654},
  eprint       = {arXiv:1011.1669v3},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Reid - 1994 - A Conversation with Sir David Cox.pdf:pdf},
  isbn         = {0412343908},
  issn         = {00905364},
  journaltitle = {Statistical Science},
  number       = {3},
  pages        = {439--455},
  title        = {{A Conversation with Sir David Cox}},
  volume       = {9},
}

@article{Wachter2017,
  author       = {Wachter, Sandra and Mittelstadt, Brent and Floridi, Luciano},
  date         = {2017},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Wachter, Mittelstadt, Floridi - 2017 - Why a right to explanation of automated decision-making does not exist in the General Data Protec.pdf:pdf},
  journaltitle = {International Data Privacy Law},
  title        = {{Why a right to explanation of automated decision-making does not exist in the General Data Protection Regulation}},
}

@article{Ng2018,
  abstract     = {Prognostic models incorporating survival analysis predict the risk (i.e., probability) of experiencing a future event over a specific time period. In 2002, Royston and Parmar described a type of flexible parametric survival model called the Royston-Parmar model in Statistics in Medicine, a model which fits a restricted cubic spline to flexibly model the baseline log cumulative hazard on the proportional hazards scale. This feature permits absolute measures of effect (e.g., hazard rates) to be estimated at all time points, an important feature when using the model. The Royston-Parmar model can also incorporate time-dependent effects and be used on different scales (e.g., proportional odds, probit). These features make the Royston-Parmar model attractive for prediction, yet their current uptake for prognostic modeling is unknown. Thus, the objectives were to conduct a scoping review of how the Royston-Parmar model has been applied to prognostic models in health research, to raise awareness of the model, to identify gaps in current reporting, and to offer model building considerations and reporting suggestions for other researchers. Five electronic databases and gray literature indexed in web sources from 2001 to 2016 were searched to identify articles for inclusion in the scoping review. Two reviewers independently screened 1429 articles, and after applying exclusion criteria through a two-step screening process, data from 12 studies were abstracted. Since 2001, only 12 studies were identified that used the Royston-Parmar model in some capacity for prognostic modeling, 10 of which used the model as the basis for their prognostic model. The restricted cubic spline varied across studies in the number of interior knots (range 1 to 6), and only three studies reported knot placement. Three studies provided details about the baseline function, with two studies using a figure and the third providing coefficients. However, no studies provided adequate information on their restricted cubic spline to permit others to validate or completely use the model. Despite the advantages of the Royston-Parmar model for prognostic models, they are not widely used in health research. Better reporting of details about the restricted cubic spline is needed, so the prognostic model can be used and validated by others. The protocol was registered with Open Science Framework ( https://osf.io/r3232/ ).},
  author       = {Ng, Ryan and Kornas, Kathy and Sutradhar, Rinku and Wodchis, Walter P. and Rosella, Laura C.},
  publisher    = {Diagnostic and Prognostic Research},
  url          = {https://diagnprognres.biomedcentral.com/articles/10.1186/s41512-018-0026-5},
  date         = {2018},
  doi          = {10.1186/s41512-018-0026-5},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Ng et al. - 2018 - The current application of the Royston-Parmar model for prognostic modeling in health research a scoping review.pdf:pdf},
  issn         = {2397-7523},
  journaltitle = {Diagnostic and Prognostic Research},
  keywords     = {Biomedicine,Health Sciences,Medicine,Medicine/Public Health,Statistics for Life Sciences,general},
  number       = {1},
  pages        = {4},
  title        = {{The current application of the Royston-Parmar model for prognostic modeling in health research: a scoping review}},
  volume       = {2},
}

@article{Buckley1979,
  abstract     = {[We give a method of estimating parameters in the linear regression model which allows the dependent variable to be censored and the residual distribution to be unspecified. The method differs from that of Miller (1976) in that the normal equations rather than the sum of squares of residuals are modified and this appears to overcome the inconsistency problems in Miller's approach. Large sample properties of the estimator of slope are derived heuristically and substantiated by simulations. Some of the heart transplant data reported and analysed by Miller are reanalysed using the present method.]},
  author       = {Buckley, Jonathan and James, Ian},
  publisher    = {[Oxford University Press, Biometrika Trust]},
  url          = {http://www.jstor.org/stable/2335161},
  date         = {1979-04},
  doi          = {10.2307/2335161},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Buckley, James - 1979 - Linear Regression with Censored Data.pdf:pdf},
  issn         = {00063444},
  journaltitle = {Biometrika},
  keywords     = {aft,classical,imputation,linear regression,reduction,regression,survival},
  number       = {3},
  pages        = {429--436},
  title        = {{Linear Regression with Censored Data}},
  volume       = {66},
}

@article{datatransplant,
  abstract     = {The usual method of estimating survival probabilities, namely the Kaplan-Meier method, is suboptimal in the analysis of deaths on the transplant waiting list. Death, transplantation, and withdrawal from list must all be considered. In this analysis, we applied the competing risk analysis method, which allows evaluating these end points individually and simultaneously, to compare the risk of waiting list death across era, blood types, liver disease diagnosis, and severity (Model for End-stage Liver Disease; MELD). Of 861 patients registered on the waiting list at Mayo Clinic Rochester between 1990 and 1999, 657 (76%) patients underwent transplantation, 82 (10%) died while waiting, 41 (5%) withdrew from the list, and 81 (9%) patients were still waiting as of February 2002. The risk of death at 3 years was 10% by the competing risk analysis. During the study period, the median time to transplantation increased from 45 to 517 days. In univariate analyses, there was no significant difference in the risk of death by era of listing (P = .25) or blood type (P = .31), whereas the risk of death was significantly higher in patients with alcohol-induced liver disease and those with higher MELD score (P < .01). A multivariable analysis showed that after adjusting for MELD, blood type, and diagnosis, patients listed in the latter era had higher mortality. In conclusion, the competing risk analysis method is useful in estimating the risk of death among patients awaiting liver transplantation.},
  author       = {Kim, W Ray and Therneau, Terry M and Benson, Joanne T and Kremers, Walter K and Rosen, Charles B and Gores, Gregory J and Dickson, E Rolland},
  language     = {eng},
  date         = {2006-02},
  doi          = {10.1002/hep.21025},
  issn         = {0270-9139 (Print)},
  journaltitle = {Hepatology (Baltimore, Md.)},
  keywords     = {Adult,Female,Humans,Liver Failure,Liver Transplantation,Male,Middle Aged,Multivariate Analysis,Risk Assessment,Severity of Illness Index,Survival Analysis,Waiting Lists,mortality,statistics & numerical data,surgery},
  number       = {2},
  pages        = {345--351},
  title        = {{Deaths on the liver transplant waiting list: an analysis of competing risks.}},
  volume       = {43},
}

@article{Mayr2014,
  abstract     = {The development of molecular signatures for the prediction of time-to-event outcomes is a methodologically challenging task in bioinformatics and biostatistics. Although there are numerous approaches for the derivation of marker combinations and their evaluation, the underlying methodology often suffers from the problem that different optimization criteria are mixed during the feature selection, estimation and evaluation steps. This might result in marker combinations that are suboptimal regarding the evaluation criterion of interest. To address this issue, we propose a unified framework to derive and evaluate biomarker combinations. Our approach is based on the concordance index for time-to-event data, which is a non-parametric measure to quantify the discriminatory power of a prediction rule. Specifically, we propose a gradient boosting algorithm that results in linear biomarker combinations that are optimal with respect to a smoothed version of the concordance index. We investigate the performance of our algorithm in a large-scale simulation study and in two molecular data sets for the prediction of survival in breast cancer patients. Our numerical results show that the new approach is not only methodologically sound but can also lead to a higher discriminatory power than traditional approaches for the derivation of gene signatures.},
  author       = {Mayr, Andreas and Schmid, Matthias},
  language     = {eng},
  publisher    = {Public Library of Science},
  url          = {https://pubmed.ncbi.nlm.nih.gov/24400093 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3882229/},
  date         = {2014-01},
  doi          = {10.1371/journal.pone.0084483},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Mayr, Schmid - 2014 - Boosting the concordance index for survival data--a unified framework to derive and evaluate biomarker combination.pdf:pdf},
  issn         = {1932-6203},
  journaltitle = {PloS one},
  keywords     = {*Biomarkers,*Models,*Survival Analysis,Algorithms,Biological,Breast Neoplasms/genetics/mortality/pathology,Computational Biology/methods,Computer Simulation,Female,Gene Expression Profiling,Humans,Neoplasm Metastasis,Prognosis,Statistical,boosting,concordance,ensemble,ml,survival},
  number       = {1},
  pages        = {e84483--e84483},
  title        = {{Boosting the concordance index for survival data--a unified framework to derive and evaluate biomarker combinations}},
  volume       = {9},
}

@article{Gensheimer2018,
  abstract   = {There is currently great interest in applying neural networks to prediction tasks in medicine. It is important for predictive models to be able to use survival data, where each patient has a known follow-up time and event/censoring indicator. This avoids information loss when training the model and enables generation of predicted survival curves. In this paper, we describe a discrete-time survival model that is designed to be used with neural networks. The model is trained with the maximum likelihood method using minibatch stochastic gradient descent (SGD). The use of SGD enables rapid training speed. The model is flexible, so that the baseline hazard rate and the effect of the input data can vary with follow-up time. It has been implemented in the Keras deep learning framework, and source code for the model and several examples is available online. We demonstrated the high performance of the model by using it as part of a convolutional neural network to predict survival for over 10,000 patients with metastatic cancer, using the full text of 1,137,317 provider notes. The model's C-index on the validation set was 0.71, which was superior to a linear baseline model (C-index 0.69).},
  author     = {Gensheimer, Michael F. and Narasimhan, Balasubramanian},
  url        = {http://arxiv.org/abs/1805.00917},
  date       = {2018},
  doi        = {arXiv:1805.00917v3},
  eprint     = {1805.00917},
  eprinttype = {arXiv},
  file       = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Gensheimer, Narasimhan - 2018 - A Simple Discrete-Time Survival Model for Neural Networks.pdf:pdf},
  pages      = {1--17},
  title      = {{A Simple Discrete-Time Survival Model for Neural Networks}},
}

@article{Uno2007,
  abstract     = {[Suppose that we are interested in establishing simple but reliable rules for predicting future t-year survivors through censored regression models. In this article we present inference procedures for evaluating such binary classification rules based on various prediction precision measures quantified by the overall misclassification rate, sensitivity and specificity, and positive and negative predictive values. Specifically, under various working models, we derive consistent estimators for the above measures through substitution and cross-validation estimation procedures. Furthermore, we provide large-sample approximations to the distributions of these nonsmooth estimators without assuming that the working model is correctly specified. Confidence intervals, for example, for the difference of the precision measures between two competing rules can then be constructed. All of the proposals are illustrated with real examples, and their finite-sample properties are evaluated through a simulation study.]},
  author       = {Uno, Hajime and Cai, Tianxi and Tian, Lu and Wei, L J},
  publisher    = {[American Statistical Association, Taylor & Francis, Ltd.]},
  url          = {http://www.jstor.org/stable/27639883},
  date         = {2007},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Uno et al. - 2007 - Evaluating Prediction Rules for t-Year Survivors with Censored Regression Models.pdf:pdf},
  issn         = {01621459},
  journaltitle = {Journal of the American Statistical Association},
  number       = {478},
  pages        = {527--537},
  title        = {{Evaluating Prediction Rules for t-Year Survivors with Censored Regression Models}},
  volume       = {102},
}

@article{Dawid1986,
  author       = {Dawid, A Philip},
  date         = {1986},
  journaltitle = {Encyclopedia of Statistical Sciences},
  pages        = {210--218},
  title        = {{Probability Forecasting}},
  volume       = {7},
}

@article{Alotaibi2015,
  abstract     = {Although there are many suggested measures of explained variation for single-event survival data, there has been little attention to explained variation for recurrent event data. We describe an existing rank-based measure and we investigate a new statistic based on observed and expected event count processes. Both methods can be used for all models. Adjustments for missing data are proposed and demonstrated through simulation to be effective. We compare the population values of the two statistics and illustrate their use in comparing an array of non-nested models for data on recurrent episodes of infant diarrhoea.},
  author       = {Alotaibi, Refah and Fiaccone, Rosemeire and Henderson, Robin and Stare, Janez},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/bimj.201300143},
  annotation   = {doi: 10.1002/bimj.201300143},
  date         = {2015-07},
  doi          = {10.1002/bimj.201300143},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Alotaibi et al. - 2015 - Explained variation for recurrent event data.pdf:pdf},
  issn         = {0323-3847},
  journaltitle = {Biometrical Journal},
  keywords     = {C-index,Counting process,Covariates,Event history,Survival data},
  number       = {4},
  pages        = {571--591},
  title        = {{Explained variation for recurrent event data}},
  volume       = {57},
}

@misc{pkgextradistr,
  author    = {Wolodzko, Tymoteusz},
  publisher = {CRAN},
  url       = {https://cran.r-project.org/package=extraDistr},
  date      = {2019},
  title     = {{extraDistr: Additional Univariate and Multivariate Distributions}},
}

@misc{pkgr62s3,
  abstract  = {After defining an R6 class, R62S3 is used to automatically generate optional S3/S4 generics and methods for dispatch. This additionally allows piping for R6 objects.},
  author    = {Sonabend, Raphael},
  publisher = {CRAN},
  url       = {https://cran.r-project.org/package=R62S3},
  date      = {2019-05},
  title     = {{R62S3: Automatic Method Generation from R6}},
}

@article{datanwtco,
  abstract     = {Two-phase stratified sampling is used to select subjects for the collection of additional data, e.g. validation data in measurement error problems. Stratification jointly by outcome and covariates, with sampling fractions chosen to achieve approximately equal numbers per stratum at the second phase of sampling, enhances efficiency compared with stratification based on the outcome or covariates alone. Nonparametric maximum likelihood may result in substantially more efficient estimates of logistic regression coefficients than weighted or pseudolikelihood procedures. Software to implement all three procedures is available. We demonstrate the practical importance of these design and analysis principles by an analysis of, and simulations based on, data from the US National Wilms Tumor Study.},
  author       = {Breslow, N E and Chatterjee, N},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1111/1467-9876.00165},
  annotation   = {doi: 10.1111/1467-9876.00165},
  date         = {1999-01},
  doi          = {10.1111/1467-9876.00165},
  issn         = {0035-9254},
  journaltitle = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  keywords     = {Design efficiency,Logistic regression,Nonparametric maximum likelihood,Stratified sampling},
  number       = {4},
  pages        = {457--468},
  title        = {{Design and analysis of two-phase studies with binary outcome applied to Wilms tumour prognosis}},
  volume       = {48},
}

@article{Dietterich1998,
  abstract     = {This article reviews five approximate statistical tests for determining whether one learning algorithm outperforms another on a particular learning task. These test sare compared experimentally to determine their probability of incorrectly detecting a difference when no difference exists (type I error). Two widely used statistical tests are shown to have high probability of type I error in certain situations and should never be used: a test for the difference of two proportions and a paired-differences t test based on taking several random train-test splits. A third test, a paired-differences t test based on 10-fold cross-validation, exhibits somewhat elevated probability of type I error. A fourth test, McNemar's test, is shown to have low type I error. The fifth test is a new test, 5 × 2 cv, based on five iterations of twofold cross-validation. Experiments show that this test also has acceptable type I error. The article also measures the power (ability to detect algorithm differences when they do exist) of these tests. The cross-validated t test is the most powerful. The 5×2 cv test is shown to be slightly more powerful than McNemar's test. The choice of the best test is determined by the computational cost of running the learning algorithm. For algorithms that can be executed only once, Mc-Nemar's test is the only test with acceptable type I error. For algorithms that can be executed 10 times, the 5 × 2 cv test is recommended, because it is slightly more powerful and because it directly measures variation due to the choice of training set.},
  author       = {Dietterich, Thomas G},
  url          = {https://direct.mit.edu/neco/article/10/7/1895-1923/6224},
  date         = {1998-10},
  doi          = {10.1162/089976698300017197},
  issn         = {0899-7667},
  journaltitle = {Neural Computation},
  number       = {7},
  pages        = {1895--1923},
  title        = {{Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms}},
  volume       = {10},
}

@article{Hernandez2012,
  abstract     = {Many performance metrics have been introduced in the literature for the evaluation of classification performance, each of them with different origins and areas of application. These metrics include accuracy, unweighted accuracy, the area under the ROC curve or the ROC convex hull, the mean absolute error and the Brier score or mean squared error (with its decomposition into refinement and calibration). One way of understanding the relations among these metrics is by means of variable operating conditions (in the form of misclassification costs and/or class distributions). Thus, a metric may correspond to some expected loss over different operating conditions. One dimension for the analysis has been the distribution for this range of operating conditions, leading to some important connections in the area of proper scoring rules. We demonstrate in this paper that there is an equally important dimension which has so far received much less attention in the analysis of performance metrics. This dimension is given by the decision rule, which is typically implemented as a threshold choice method when using scoring models. In this paper, we explore many old and new threshold choice methods: fixed, score-uniform, score-driven, rate-driven and optimal, among others. By calculating the expected loss obtained with these threshold choice methods for a uniform range of operating conditions we give clear interpretations of the 0-1 loss, the absolute error, the Brier score, the AUC and the refinement loss respectively. Our analysis provides a comprehensive view of performance metrics as well as a systematic approach to loss minimisation which can be summarised as follows: given a model, apply the threshold choice methods that correspond with the available information about the operating condition, and compare their expected losses. In order to assist in this procedure we also derive several connections between the aforementioned performance metrics, and we highlight the role of calibration in choosing the threshold choice method. {©} 2012 Jose Hernandez-Orallo, Peter Flach and Cesar Ferri.},
  author       = {Hernández-Orallo, José and Flach, Peter and Ferri, Cèsar},
  url          = {http://www.jmlr.org/papers/volume13/hernandez-orallo12a/hernandez-orallo12a.pdf},
  date         = {2012},
  doi          = {10.1016/j.ijid.2017.12.001},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Hern{\'{a}}ndez-Orallo, Flach, Ferri - 2012 - A Unified View of Performance Metrics Translating Threshold Choice into Expected Classification.pdf:pdf},
  isbn         = {1532-4435},
  issn         = {15324435},
  journaltitle = {Journal of Machine Learning Research},
  pages        = {2813--2869},
  title        = {{A Unified View of Performance Metrics: Translating Threshold Choice into Expected Classification Loss}},
  volume       = {13},
}

@article{Kiureghian2009,
  abstract     = {The sources and characters of uncertainties in engineering modeling for risk and reliability analyses are discussed. While many sources of uncertainty may exist, they are generally categorized as either aleatory or epistemic. Uncertainties are characterized as epistemic, if the modeler sees a possibility to reduce them by gathering more data or by refining models. Uncertainties are categorized as aleatory if the modeler does not foresee the possibility of reducing them. From a pragmatic standpoint, it is useful to thus categorize the uncertainties within a model, since it then becomes clear as to which uncertainties have the potential of being reduced. More importantly, epistemic uncertainties may introduce dependence among random events, which may not be properly noted if the character of uncertainties is not correctly modeled. Influences of the two types of uncertainties in reliability assessment, codified design, performance-based engineering and risk-based decision-making are discussed. Two simple examples demonstrate the influence of statistical dependence arising from epistemic uncertainties on systems and time-variant reliability problems.},
  author       = {Kiureghian, Armen Der and Ditlevsen, Ove},
  url          = {https://www.sciencedirect.com/science/article/pii/S0167473008000556},
  date         = {2009},
  doi          = {https://doi.org/10.1016/j.strusafe.2008.06.020},
  issn         = {0167-4730},
  journaltitle = {Structural Safety},
  keywords     = {Aleatory,Epistemic,Ergodicity,Parameter uncertainty,Predictive models,Probability distribution choice,Statistical dependence,Systems,Time-variant reliability,Uncertainty},
  number       = {2},
  pages        = {105--112},
  title        = {{Aleatory or epistemic? Does it matter?}},
  volume       = {31},
}

@article{Buhlmann2003,
  author       = {Bühlmann, Peter and Yu, Bin},
  publisher    = {Taylor & Francis},
  url          = {https://doi.org/10.1198/016214503000125},
  annotation   = {doi: 10.1198/016214503000125},
  date         = {2003-06},
  doi          = {10.1198/016214503000125},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/B{\"{u}}hlmann, Yu - 2003 - Boosting With the L2 Loss.pdf:pdf},
  issn         = {0162-1459},
  journaltitle = {Journal of the American Statistical Association},
  number       = {462},
  pages        = {324--339},
  title        = {{Boosting With the L2 Loss}},
  volume       = {98},
}

@article{Woolson1981,
  abstract     = {[For Cox's proportional hazards model, test procedures are described for the comparison of randomly right-censored observed and expected (standard population) survival data. It is observed that the class of linear rank tests proposed by Prentice (1978, Biometrika 65, 167-179) may be employed to test regression hypotheses in this one-sample setting. In order to compare globally the observed and expected survival data the one-sample limit of Mantel's logrank test is formally derived. The statistic derived is identical to one proposed from a different viewpoint by Breslow (1975, International Statistical Review 43, 45-58). Throughout it is assumed that the standard population survival function is a continuous function of time and that the survival or censoring time for each study-sample individual is exact and is not grouped into categories.]},
  author       = {Woolson, Robert F},
  publisher    = {[Wiley, International Biometric Society]},
  url          = {http://www.jstor.org/stable/2530150},
  date         = {1981},
  doi          = {10.2307/2530150},
  issn         = {0006341X, 15410420},
  journaltitle = {Biometrics},
  number       = {4},
  pages        = {687--696},
  title        = {{Rank Tests and a One-Sample Logrank Test for Comparing Observed Survival Data to a Standard Population}},
  volume       = {37},
}

@article{Zupan2000,
  abstract     = {Machine learning techniques have recently received considerable attention, especially when used for the construction of prediction models from data. Despite their potential advantages over standard statistical methods, like their ability to model non-linear relationships and construct symbolic and interpretable models, their applications to survival analysis are at best rare, primarily because of the difficulty to appropriately handle censored data. In this paper we propose a schema that enables the use of classification methods--including machine learning classifiers--for survival analysis. To appropriately consider the follow-up time and censoring, we propose a technique that, for the patients for which the event did not occur and have short follow-up times, estimates their probability of event and assigns them a distribution of outcome accordingly. Since most machine learning techniques do not deal with outcome distributions, the schema is implemented using weighted examples. To show the utility of the proposed technique, we investigate a particular problem of building prognostic models for prostate cancer recurrence, where the sole prediction of the probability of event (and not its probability dependency on time) is of interest. A case study on preoperative and postoperative prostate cancer recurrence prediction shows that by incorporating this weighting technique the machine learning tools stand beside modern statistical methods and may, by inducing symbolic recurrence models, provide further insight to relationships within the modeled data.},
  author       = {Zupan, Blaž and Demšar, Janez and Kattan, Michael W and Beck, J Robert and Bratko, I},
  annotation   = {Uncensoring via grouping and KM estimates},
  date         = {2000},
  doi          = {10.1016/S0933-3657(00)00053-1},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Zupan et al. - 2000 - Machine learning for survival analysis a case study on recurrence of prostate cancer.pdf:pdf},
  isbn         = {0933-3657},
  issn         = {0933-3657},
  journaltitle = {Artifical Intelligence in Medicine},
  keywords     = {reduction,survival to classification},
  pages        = {59--75},
  title        = {{Machine learning for survival analysis: a case study on recurrence of prostate cancer}},
  volume       = {20},
}

@article{Clevert2015,
  author       = {Clevert, Djork-Arné and Unterthiner, Thomas and Hochreiter, Sepp},
  date         = {2015},
  journaltitle = {arXiv preprint arXiv:1511.07289},
  title        = {{Fast and accurate deep network learning by exponential linear units (elus)}},
}

@misc{pkgreticulate,
  author     = {Ushey, Kevin and Allaire, J J and Tang, Yuan},
  publisher  = {CRAN},
  url        = {https://cran.r-project.org/package=reticulate},
  annotation = {R package version 1.15},
  date       = {2020},
  title      = {{reticulate: Interface to 'Python'}},
}

@article{VanBelle2011a,
  abstract     = {This paper studies the task of learning transformation models for ranking problems, ordinal regression and survival analysis. The present contribution describes a machine learning approach termed MINLIP. The key insight is to relate ranking criteria as the Area Under the Curve to monotone transformation functions. Consequently, the notion of a Lipschitz smoothness constant is found to be useful for complexity control for learning transformation models, much in a similar vein as the 'margin' is for Support Vector Machines for classification. The use of this model structure in the context of high dimensional data, as well as for estimating non-linear, and additive models based on primal-dual kernel machines, and for sparse models is indicated. Given n observations, the present method solves a quadratic program existing of O(n) constraints and O(n) unknowns, where most existing risk minimization approaches to ranking problems typically result in algorithms with O(n2) constraints or unknowns. We specify the MINLIP method for three different cases: the first one concerns the preference learning problem. Secondly it is specified how to adapt the method to ordinal regression with a finite set of ordered outcomes. Finally, it is shown how the method can be used in the context of survival analysis where one models failure times, typically subject to censoring. The current approach is found to be particularly useful in this context as it can handle, in contrast with the standard statistical model for analyzing survival data, all types of censoring in a straightforward way, and because of the explicit relation with the Proportional Hazard and Accelerated Failure Time models. The advantage of the current method is illustrated on different benchmark data sets, as well as for estimating a model for cancer survival based on different micro-array and clinical data sets.},
  author       = {{Van Belle}, Vanya and Pelckmans, K and Suykens, Johan A.K. and {Van Huffel}, Sabine},
  date         = {2011},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Van Belle et al. - 2011 - Learning Transformation Models for Ranking and Survival Analysis.pdf:pdf},
  isbn         = {1532-4435},
  issn         = {15324435},
  journaltitle = {Journal of Machine Learning Research},
  keywords     = {ordinal regression,preference learning,ranking,ranking models,support vector machines,survival analysis,svm},
  pages        = {819--862},
  title        = {{Learning Transformation Models for Ranking and Survival Analysis}},
  volume       = {12},
}

@misc{pkgasaur,
  author     = {{F. Moore}, Dirk},
  publisher  = {CRAN},
  url        = {https://cran.r-project.org/package=asaur},
  annotation = {R package version 0.50},
  date       = {2016},
  title      = {{asaur: Data Sets for "Applied Survival Analysis Using R"}},
}

@article{Perez-Guzman2023,
  abstract     = {<p>As the SARS-CoV-2 pandemic progressed, distinct variants emerged and dominated in England. These variants, Wildtype, Alpha, Delta, and Omicron were characterized by variations in transmissibility and severity. We used a robust mathematical model and Bayesian inference framework to analyse epidemiological surveillance data from England. We quantified the impact of non-pharmaceutical interventions (NPIs), therapeutics, and vaccination on virus transmission and severity. Each successive variant had a higher intrinsic transmissibility. Omicron (BA.1) had the highest basic reproduction number at 8.3 (95% credible interval (CrI) 7.7-8.8). Varying levels of NPIs were crucial in controlling virus transmission until population immunity accumulated. Immune escape properties of Omicron decreased effective levels of immunity in the population by a third. Furthermore, in contrast to previous studies, we found Alpha had the highest basic infection fatality ratio (2.9%, 95% CrI 2.7-3.2), followed by Delta (2.2%, 95% CrI 2.0–2.4), Wildtype (1.2%, 95% CrI 1.1–1.2), and Omicron (0.7%, 95% CrI 0.6-0.8). Our findings highlight the importance of continued surveillance. Long-term strategies for monitoring and maintaining effective immunity against SARS-CoV-2 are critical to inform the role of NPIs to effectively manage future variants with potentially higher intrinsic transmissibility and severe outcomes.</p>},
  author       = {Perez-Guzman, Pablo N. and Knock, Edward and Imai, Natsuko and Rawson, Thomas and Elmaci, Yasin and Alcada, Joana and Whittles, Lilith K. and {Thekke Kanapram}, Divya and Sonabend, Raphael and Gaythorpe, Katy A. M. and Hinsley, Wes and FitzJohn, Richard G. and Volz, Erik and Verity, Robert and Ferguson, Neil M. and Cori, Anne and Baguelin, Marc},
  date         = {2023-07},
  doi          = {10.1038/s41467-023-39661-5},
  issn         = {2041-1723},
  journaltitle = {Nature Communications},
  number       = {1},
  pages        = {4279},
  title        = {{Epidemiological drivers of transmissibility and severity of SARS-CoV-2 in England}},
  volume       = {14},
}

@book{Youngner2016,
  abstract  = {This handbook explores the topic of death and dying from the late twentieth to the early twenty-first centuries, with particular emphasis on the United States. In this period, technology has radically changed medical practices and the way we die as structures of power have been reshaped by the rights claims of African Americans, women, gays, students, and, most relevant here, patients. Respecting patients' values has been recognized as the essential moral component of clinical decision making. Technology's promise has been seen to have a dark side: it prolongs the dying process. For the first time in history, human beings have the ability to control the timing of death. With this ability comes a responsibility that is awesome and inescapable. How we understand and manage this responsibility is the theme of this volume. The book has six sections. Section I examines how the law has helped shape clinical practice, emphasizing the roles of rights and patient autonomy. Section II focuses on specific clinical issues, including death and dying in children, continuous sedation as a way to relieve suffering at the end of life, and the problem of prognostication in patients who are thought to be dying. Section III considers psychosocial and cultural issues. Section IV discusses death and dying among various vulnerable populations, such as the elderly and persons with disabilities. Section V deals with physician-assisted suicide and active euthanasia (lethal injection). Finally, Section VI looks at hospice and palliative care as ways to address the psychosocial and ethical problems of death and dying.},
  author    = {Youngner, Stuart J and Arnold, Robert M},
  language  = {English},
  publisher = {Oxford University Press},
  url       = {http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199974412.001.0001/oxfordhb-9780199974412},
  date      = {2016},
  doi       = {10.1093/oxfordhb/9780199974412.001.0001},
  isbn      = {9780199974412},
  title     = {{The Oxford Handbook of Ethics at the End of Life}},
}

@article{Yue2018,
  abstract     = {Motivated by high-throughput profiling studies in biomedical research, variable selection methods have been a focus for biostatisticians. In this paper, we consider semiparametric varying-coefficient accelerated failure time models for right censored survival data with high-dimensional covariates. Instead of adopting the traditional regularization approaches, we offer a novel sparse boosting (SparseL2Boosting) algorithm to conduct model-based prediction and variable selection. One main advantage of this new method is that we do not need to perform the time-consuming selection of tuning parameters. Extensive simulations are conducted to examine the performance of our sparse boosting feature selection techniques. We further illustrate our methods using a lung cancer data analysis.},
  author       = {Yue, Mu and Li, Jialiang and Ma, Shuangge},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/sim.7544},
  annotation   = {doi: 10.1002/sim.7544},
  date         = {2018-02},
  doi          = {10.1002/sim.7544},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Yue, Li, Ma - 2018 - Sparse boosting for high-dimensional survival data with varying coefficients.pdf:pdf},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  keywords     = {accelerated failure time model,boosting,high-dimensional data,minimum description length,varying-coefficient model},
  number       = {5},
  pages        = {789--800},
  title        = {{Sparse boosting for high-dimensional survival data with varying coefficients}},
  volume       = {37},
}

@article{datamtcars,
  author       = {Henderson and Velleman},
  date         = {1981},
  journaltitle = {Biometrics},
  pages        = {391--411},
  title        = {{Building multiple regression models interactively}},
  volume       = {37},
}

@article{datarats,
  author       = {Mantel, N. and Bohidar, N. R. and Ciminera, J. L.},
  date         = {1977},
  journaltitle = {Cancer Research},
  pages        = {3863--3868},
  title        = {{Mantel-Haenszel analyses of litter-matched time to response data, with modifications for recovery of interlitter information.}},
  volume       = {37},
}

@misc{pkgR6,
  author = {Chang, Winston},
  url    = {https://cran.r-project.org/package=R6},
  date   = {2018},
  title  = {{R6: Classes with Reference Semantics}},
}

@article{datacolon,
  abstract     = {A total of 401 eligible patients with resected stages B and C colorectal carcinoma were randomly assigned to no-further therapy or to adjuvant treatment with either levamisole alone, 150 mg/d for 3 days every 2 weeks for 1 year, or levamisole plus fluorouracil (5-FU), 450 mg/m2/d intravenously (IV) for 5 days and beginning at 28 days, 450 mg/m2 weekly for 1 year. Levamisole plus 5-FU, and to a lesser extent levamisole alone, reduced cancer recurrence in comparison with no adjuvant therapy. These differences, after correction for imbalances in prognostic variables, were only suggestive for levamisole alone (P = .05) but quite significant for levamisole plus 5-FU (P = .003). Whereas both treatment regimens were associated with overall improvements in survival, these improvements reached borderline significance only for stage C patients treated with levamisole plus 5-FU (P = .03). Therapy was clinically tolerable with either regimen and severe toxicity was uncommon. These promising results have led to a large national intergroup confirmatory trial currently in progress.},
  author       = {Laurie, J A and Moertel, C G and Fleming, T R and Wieand, H S and Leigh, J E and Rubin, J and McCormack, G W and Gerstner, J B and Krook, J E and Malliard, J},
  language     = {eng},
  date         = {1989-10},
  doi          = {10.1200/JCO.1989.7.10.1447},
  issn         = {0732-183X (Print)},
  journaltitle = {Journal of clinical oncology : official journal of the American Society of Clinical Oncology},
  keywords     = {80 and over,Adult,Aged,Colorectal Neoplasms,Female,Fluorouracil,Humans,Levamisole,Local,Lymphatic Metastasis,Male,Middle Aged,Multiple Primary,Neoplasm Recurrence,Neoplasm Staging,Neoplasms,Patient Compliance,Random Allocation,administration & dosage,drug therapy,mortality,pathology,therapeutic use},
  number       = {10},
  pages        = {1447--1456},
  title        = {{Surgical adjuvant therapy of large-bowel carcinoma: an evaluation of levamisole and the combination of levamisole and fluorouracil. The North Central Cancer Treatment Group and the Mayo Clinic.}},
  volume       = {7},
}

@article{Hochreiter1997,
  author       = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date         = {1997-12},
  doi          = {10.1162/neco.1997.9.8.1735},
  journaltitle = {Neural computation},
  pages        = {1735--1780},
  title        = {{Long Short-term Memory}},
  volume       = {9},
}

@article{Kvamme2021,
  abstract     = {Due to rapid developments in machine learning, and in particular neural networks, a number of new methods for time-to-event predictions have been developed in the last few years. As neural networks are parametric models, it is more straightforward to integrate parametric survival models in the neural network framework than the popular semi-parametric Cox model. In particular, discrete-time survival models, which are fully parametric, are interesting candidates to extend with neural networks. The likelihood for discrete-time survival data may be parameterized by the probability mass function (PMF) or by the discrete hazard rate, and both of these formulations have been used to develop neural network-based methods for time-to-event predictions. In this paper, we review and compare these approaches. More importantly, we show how the discrete-time methods may be adopted as approximations for continuous-time data. To this end, we introduce two discretization schemes, corresponding to equidistant times or equidistant marginal survival probabilities, and two ways of interpolating the discrete-time predictions, corresponding to piecewise constant density functions or piecewise constant hazard rates. Through simulations and study of real-world data, the methods based on the hazard rate parametrization are found to perform slightly better than the methods that use the PMF parametrization. Inspired by these investigations, we also propose a continuous-time method by assuming that the continuous-time hazard rate is piecewise constant. The method, named PC-Hazard, is found to be highly competitive with the aforementioned methods in addition to other methods for survival prediction found in the literature.},
  author       = {Kvamme, H{å}vard and Borgan, {Ø}rnulf},
  url          = {https://doi.org/10.1007/s10985-021-09532-6},
  date         = {2021},
  doi          = {10.1007/s10985-021-09532-6},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Kvamme, Borgan - 2019 - Continuous and discrete-time survival prediction with neural networks.pdf:pdf},
  issn         = {1572-9249},
  journaltitle = {Lifetime Data Analysis},
  number       = {4},
  pages        = {710--736},
  title        = {{Continuous and discrete-time survival prediction with neural networks}},
  volume       = {27},
}

@inproceedings{Baum1988,
  author    = {Baum, Eric B and Wilczek, Frank},
  booktitle = {Neural information processing systems},
  date      = {1988},
  pages     = {52--61},
  title     = {{Supervised learning of probability distributions by neural networks}},
}

@article{Huang2005,
  author       = {Huang, Wenzheng and Fitzmaurice, Garrett M.},
  date         = {2005},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Huang, Fitzmaurice - 2005 - Analysis of Longitudinal Data Unbalanced over Time.pdf:pdf},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  number       = {1},
  pages        = {135--155},
  title        = {{Analysis of Longitudinal Data Unbalanced over Time}},
  volume       = {67},
}

@article{Kent1988,
  abstract     = {In the linear regression model with normal errors the squared product-moment correlation provides the standard measure of dependence between the explanatory variable and the response variable. Using the concept of information gain, a measure of dependence can also be defined for more general regression models used in survival analysis, such as the Weibull regression model or Cox's proportional hazards model. Further, this measure of dependence can be conveniently estimated even when the response variable is subject to censoring, and a simple approximation is available.},
  author       = {Kent, John T. and O'Quigley, John},
  date         = {1988},
  doi          = {10.1093/biomet/75.3.525},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Kent, O'Quigley - 1988 - Measures of dependence for censored survival data.pdf:pdf},
  issn         = {00063444},
  journaltitle = {Biometrika},
  keywords     = {Censoring,Correlation,Dependence,Information gain,Proportional hazards model,Weibull regression model},
  number       = {3},
  pages        = {525--534},
  title        = {{Measures of dependence for censored survival data}},
  volume       = {75},
}

@misc{pkgparam6,
  author     = {Sonabend, Raphael},
  publisher  = {CRAN},
  url        = {https://cran.r-project.org/package=param6},
  annotation = {https://xoopR.github.io/param6/, https://github.com/xoopR/param6/},
  date       = {2021},
  title      = {{param6: A Fast and Lightweight R6 Parameter Interface}},
}

@article{Kullback1951,
  author       = {Kullback, Solomon and Leibler, Richard A},
  publisher    = {JSTOR},
  date         = {1951},
  issn         = {0003-4851},
  journaltitle = {The annals of mathematical statistics},
  number       = {1},
  pages        = {79--86},
  title        = {{On information and sufficiency}},
  volume       = {22},
}

@book{Tutz2016,
  author    = {Tutz, Gerhard and Schmid, Matthias},
  location  = {Cham},
  publisher = {Springer International Publishing},
  url       = {http://link.springer.com/10.1007/978-3-319-28158-2},
  date      = {2016},
  doi       = {10.1007/978-3-319-28158-2},
  isbn      = {978-3-319-28156-8},
  series    = {Springer Series in Statistics},
  title     = {{Modeling Discrete Time-to-Event Data}},
}

@article{Blanche2012,
  author = {Blanche, Paul and Latouche, Aurélien and Viallon, Vivian},
  date   = {2012-10},
  doi    = {10.1007/978-1-4614-8981-8_11},
  title  = {{Time-dependent AUC with right-censored data: a survey study}},
}

@article{Braddock1997,
  author       = {Braddock, Clarence H and Fihn, Stephan D and Levinson, Wendy and Jonsen, Albert R and Pearlman, Robert A},
  publisher    = {Wiley Online Library},
  date         = {1997},
  issn         = {0884-8734},
  journaltitle = {Journal of general internal medicine},
  number       = {6},
  pages        = {339--345},
  title        = {{How doctors and patients discuss routine clinical decisions: informed decision making in the outpatient setting}},
  volume       = {12},
}

@inproceedings{Shivaswamy2007,
  abstract  = {Censored targets, such as the time to events in survival analysis, can generally be represented by intervals on the real line. In this paper, we propose a novel support vector technique (named SVCR) for regression on censored targets. SVCR inherits the strengths of support vector methods, such as a globally optimal solution by convex programming, fast training speed and strong generalization capacity. In contrast to ranking approaches to survival analysis, our approach is able not only to achieve superior ordering performance, but also to predict the survival time very well. Experiments show a significant performance improvement when the majority of the training data is censored. Experimental results on several survival analysis datasets demonstrate that SVCR is very competitive against classical survival analysis models.},
  author    = {Shivaswamy, Pannagadatta K. and Chu, Wei and Jansche, Martin},
  booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
  date      = {2007},
  doi       = {10.1109/ICDM.2007.93},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Shivaswamy, Chu, Jansche - 2007 - A support vector approach to censored targets.pdf:pdf},
  isbn      = {0769530184},
  issn      = {15504786},
  keywords  = {machine learning,ssvm,survival,svcr,svm},
  pages     = {655--660},
  title     = {{A support vector approach to censored targets}},
}

@article{Blanche2013,
  abstract     = {To quantify the ability of a marker to predict the onset of a clinical outcome in the future, time-dependent estimators of sensitivity, specificity, and ROC curve have been proposed accounting for censoring of the outcome. In this paper, we review these estimators, recall their assumptions about the censoring mechanism and highlight their relationships and properties. A simulation study shows that marker-dependent censoring can lead to important biases for the ROC estimators not adapted to this case. A slight modification of the inverse probability of censoring weighting estimators proposed by Uno et al. (2007) and Hung and Chiang (2010a) performs as well as the nearest neighbor estimator of Heagerty et al. (2000) in the simulation study and has interesting practical properties. Finally, the estimators were used to evaluate abilities of a marker combining age and a cognitive test to predict dementia in the elderly. Data were obtained from the French PAQUID cohort. The censoring appears clearly marker-dependent leading to appreciable differences between ROC curves estimated with the different methods.},
  author       = {Blanche, Paul and Dartigues, Jean-François and Jacqmin-Gadda, Hélène},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/bimj.201200045},
  annotation   = {doi: 10.1002/bimj.201200045},
  date         = {2013-09},
  doi          = {10.1002/bimj.201200045},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Blanche, Dartigues, Jacqmin-Gadda - 2013 - Review and comparison of ROC curve estimators for a time-dependent outcome with marker-depend.pdf:pdf},
  issn         = {0323-3847},
  journaltitle = {Biometrical Journal},
  keywords     = {AUC,IPCW,Prediction,ROC curve,Survival analysis},
  number       = {5},
  pages        = {687--704},
  title        = {{Review and comparison of ROC curve estimators for a time-dependent outcome with marker-dependent censoring}},
  volume       = {55},
}

@article{Wiegrebe2024,
  author       = {Wiegrebe, Simon and Kopper, Philipp and Sonabend, Raphael and Bischl, Bernd and Bender, Andreas},
  url          = {https://doi.org/10.1007/s10462-023-10681-3},
  date         = {2024},
  doi          = {10.1007/s10462-023-10681-3},
  issn         = {1573-7462},
  journaltitle = {Artificial Intelligence Review},
  number       = {3},
  pages        = {65},
  title        = {{Deep learning for survival analysis: a review}},
  volume       = {57},
}

@article{Longadge2013,
  author       = {Longadge, Rushi and Dongre, Snehlata S and Malik, Latesh},
  date         = {2013},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Longadge, Dongre, Malik - 2013 - Class Imbalance Problem in Data Mining Review.pdf:pdf},
  journaltitle = {International Journal of Computer Science and Network},
  keywords     = {class imbalance problem,data,detecting network intrusions,fraud in banking operations,imbalance,other areas,rare class mining,situations are observed in,skewed data,such as detecting},
  number       = {1},
  title        = {{Class Imbalance Problem in Data Mining : Review}},
  volume       = {2},
}

@book{Molnar2019,
  author = {Molnar, Christoph},
  url    = {https://christophm.github.io/interpretable-ml-book/},
  date   = {2019},
  title  = {{Interpretable Machine Learning}},
}

@article{MosqueraOrgueira2020,
  abstract     = {Thirty to forty percent of patients with Diffuse Large B-cell Lymphoma (DLBCL) have an adverse clinical evolution. The increased understanding of DLBCL biology has shed light on the clinical evolution of this pathology, leading to the discovery of prognostic factors based on gene expression data, genomic rearrangements and mutational subgroups. Nevertheless, additional efforts are needed in order to enable survival predictions at the patient level. In this study we investigated new machine learning-based models of survival using transcriptomic and clinical data.},
  author       = {{Mosquera Orgueira}, Adrián and {Díaz Arias}, José Ángel and {Cid López}, Miguel and {Peleteiro Raíndo}, Andrés and {Antelo Rodríguez}, Beatriz and {Aliste Santos}, Carlos and {Alonso Vence}, Natalia and {Bendaña López}, Ángeles and {Abuín Blanco}, Aitor and {Bao Pérez}, Laura and {González Pérez}, Marta Sonia and {Pérez Encinas}, Manuel Mateo and {Fraga Rodríguez}, Máximo Francisco and {Bello López}, José Luis},
  url          = {https://doi.org/10.1186/s12885-020-07492-y},
  date         = {2020},
  doi          = {10.1186/s12885-020-07492-y},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Mosquera Orgueira et al. - 2020 - Improved personalized survival prediction of patients with diffuse large B-cell Lymphoma using gene ex.pdf:pdf},
  issn         = {1471-2407},
  journaltitle = {BMC Cancer},
  number       = {1},
  pages        = {1017},
  title        = {{Improved personalized survival prediction of patients with diffuse large B-cell Lymphoma using gene expression profiling}},
  volume       = {20},
}

@article{dataova,
  author       = {{Van Houwelingen}, J C and {ten Bokkel Huinink}, W W and {Van der Burg}, M E and {Van Oosterom}, A T and Neijt, J P},
  date         = {1989},
  issn         = {0732-183X},
  journaltitle = {Journal of Clinical Oncology},
  number       = {6},
  pages        = {769--773},
  title        = {{Predictability of the survival of patients with advanced ovarian cancer.}},
  volume       = {7},
}

@article{Wang2010,
  author       = {Wang, Zhu and Wang, C Y},
  language     = {English},
  location     = {Berlin, Boston},
  publisher    = {De Gruyter},
  url          = {https://www.degruyter.com/view/journals/sagmb/9/1/article-sagmb.2010.9.1.1550.xml.xml},
  date         = {2010},
  doi          = {https://doi.org/10.2202/1544-6115.1550},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Wang - 2010 - Buckley-James Boosting for Survival Analysis with High-Dimensional Biomarker Data.pdf:pdf},
  journaltitle = {Statistical Applications in Genetics and Molecular Biology},
  number       = {1},
  title        = {{Buckley-James Boosting for Survival Analysis with High-Dimensional Biomarker Data}},
  volume       = {9},
}

@article{Smith2014,
  author   = {Smith, Alexander K. and Glare, Paul},
  url      = {http://oxfordhandbooks.com/view/10.1093/oxfordhb/9780199974412.001.0001/oxfordhb-9780199974412-e-9},
  date     = {2014},
  doi      = {10.1093/oxfordhb/9780199974412.013.9},
  file     = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Smith, Glare - 2014 - Ethical Issues in Prognosis and Prognostication.pdf:pdf},
  isbn     = {9780199974412},
  keywords = {disease,end-of-life care,ethics,palliative care,patients,prognosis,prognostication,treatment},
  number   = {January 2019},
  pages    = {1--25},
  title    = {{Ethical Issues in Prognosis and Prognostication}},
  volume   = {1},
}

@article{Schoenfeld1982,
  abstract     = {Residuals are defined for the proportional hazards regression model introduced by Cox (1972). These residuals can be plotted against time to test the proportional hazards assumption. Histograms of these residuals can be used to examine fit and detect outlying covariate values},
  author       = {Schoenfeld, David},
  date         = {1982},
  doi          = {10.1093/biomet/69.1.239},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Schoenfeld - 1982 - Partial residuals for the proportional hazards regression model.pdf:pdf},
  isbn         = {0006-3444},
  issn         = {00063444},
  journaltitle = {Biometrika},
  keywords     = {Censoring,Failure time data,Proportional hazard,Residual},
  number       = {1},
  pages        = {239--241},
  title        = {{Partial residuals for the proportional hazards regression model}},
  volume       = {69},
}

@article{VanBelle2011b,
  abstract     = {Objective: To compare and evaluate ranking, regression and combined machine learning approaches for the analysis of survival data. Methods: The literature describes two approaches based on support vector machines to deal with censored observations. In the first approach the key idea is to rephrase the task as a ranking problem via the concordance index, a problem which can be solved efficiently in a context of structural risk minimization and convex optimization techniques. In a second approach, one uses a regression approach, dealing with censoring by means of inequality constraints. The goal of this paper is then twofold: (i) introducing a new model combining the ranking and regression strategy, which retains the link with existing survival models such as the proportional hazards model via transformation models; and (ii) comparison of the three techniques on 6 clinical and 3 high-dimensional datasets and discussing the relevance of these techniques over classical approaches fur survival data. Results: We compare svm-based survival models based on ranking constraints, based on regression constraints and models based on both ranking and regression constraints. The performance of the models is compared by means of three different measures: (i) the concordance index, measuring the model's discriminating ability; (ii) the logrank test statistic, indicating whether patients with a prognostic index lower than the median prognostic index have a significant different survival than patients with a prognostic index higher than the median; and (iii) the hazard ratio after normalization to restrict the prognostic index between 0 and 1. Our results indicate a significantly better performance for models including regression constraints above models only based on ranking constraints. Conclusions: This work gives empirical evidence that svm-based models using regression constraints perform significantly better than svm-based models based on ranking constraints. Our experiments show a comparable performance for methods including only regression or both regression and ranking constraints on clinical data. On high dimensional data, the former model performs better. However, this approach does not have a theoretical link with standard statistical models for survival data. This link can be made by means of transformation models when ranking constraints are included. {©} 2011 Elsevier B.V.},
  author       = {{Van Belle}, Vanya and Pelckmans, Kristiaan and {Van Huffel}, Sabine and Suykens, Johan A.K.},
  publisher    = {Elsevier B.V.},
  url          = {http://dx.doi.org/10.1016/j.artmed.2011.06.006},
  date         = {2011},
  doi          = {10.1016/j.artmed.2011.06.006},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Van Belle et al. - 2011 - Support vector methods for survival analysis A comparison between ranking and regression approaches.pdf:pdf},
  isbn         = {9789460180545},
  issn         = {09333657},
  journaltitle = {Artificial Intelligence in Medicine},
  keywords     = {Cancer prognosis,Concordance,Support vector machines,Survival analysis,svm},
  number       = {2},
  pages        = {107--118},
  title        = {{Support vector methods for survival analysis: A comparison between ranking and regression approaches}},
  volume       = {53},
}

@article{Huang2020a,
  abstract     = {Cancer is an aggressive disease with a low median survival rate. Ironically, the treatment process is long and very costly due to its high recurrence and mortality rates. Accurate early diagnosis and prognosis prediction of cancer are essential to enhance the patient's survival rate. Developments in statistics and computer engineering over the years have encouraged many scientists to apply computational methods such as multivariate statistical analysis to analyze the prognosis of the disease, and the accuracy of such analyses is significantly higher than that of empirical predictions. Furthermore, as artificial intelligence (AI), especially machine learning and deep learning, has found popular applications in clinical cancer research in recent years, cancer prediction performance has reached new heights. This article reviews the literature on the application of AI to cancer diagnosis and prognosis, and summarizes its advantages. We explore how AI assists cancer diagnosis and prognosis, specifically with regard to its unprecedented accuracy, which is even higher than that of general statistical applications in oncology. We also demonstrate ways in which these methods are advancing the field. Finally, opportunities and challenges in the clinical implementation of AI are discussed. Hence, this article provides a new perspective on how AI technology can help improve cancer diagnosis and prognosis, and continue improving human health in the future.},
  author       = {Huang, Shigao and Yang, Jie and Fong, Simon and Zhao, Qi},
  url          = {http://www.sciencedirect.com/science/article/pii/S0304383519306135},
  date         = {2020},
  doi          = {https://doi.org/10.1016/j.canlet.2019.12.007},
  issn         = {0304-3835},
  journaltitle = {Cancer Letters},
  keywords     = {Cancer diagnosis,Deep learning,Deep neural network,Machine learning,Prognosis prediction},
  pages        = {61--71},
  title        = {{Artificial intelligence in cancer diagnosis and prognosis: Opportunities and challenges}},
  volume       = {471},
}

@book{Hollander1999,
  author     = {Hollander, Myles and Wolfe, Douglas A.},
  publisher  = {John Wiley & Sons, Ltd},
  url        = {https://doi.org/10.1002/(SICI)1097-0258(20000530)19:10%3C1386::AID-SIM463%3E3.0.CO;2-X},
  annotation = {https://doi.org/10.1002/(SICI)1097-0258(20000530)19:103.0.CO;2-X},
  booktitle  = {Statistics in Medicine},
  date       = {1999-05},
  doi        = {https://doi.org/10.1002/(SICI)1097-0258(20000530)19:10<1386::AID-SIM463>3.0.CO;2-X},
  isbn       = {0‐471‐19045‐4},
  pages      = {1386--1388},
  title      = {{Nonparametric Statistical Methods.}},
  volume     = {19},
}

@misc{Spiegelhalter2009,
  author     = {Spiegelhalter, David},
  url        = {https://www.theguardian.com/weather/2009/aug/11/weather-forecasting-probability-science-summer},
  annotation = {11/08/09 response in The Guardian by Spiegelhalter. “If I have a seriously ill relative, I want a reliable assessment of their chances of survival - not a spuriously precise prediction, nor some vague, reassuring platitude” And yet this is precisely what you get when you have a seriously ill relative. Whilst it may seem that a prognostication of “5-year survival has a survival rate of X%”, what does this percentage actually mean? Is it some proportion? If so for which people over which time-frame, how often is it updated? Machine learning? Personal predictions? etc.},
  booktitle  = {The Guardian},
  date       = {2009},
  file       = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Spiegelhalter - 2009 - Weather forecasts are not pseudo-science.pdf:pdf},
  title      = {{Weather forecasts are not pseudo-science}},
}

@article{Antolini2005,
  author       = {Antolini, Laura and Boracchi, Patrizia and Biganzoli, Elia},
  url          = {https://onlinelibrary.wiley.com/doi/10.1002/sim.2427},
  date         = {2005-12},
  doi          = {10.1002/sim.2427},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Antolini, Boracchi, Biganzoli - 2005 - A time-dependent discrimination index for survival data.pdf:pdf},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  number       = {24},
  pages        = {3927--3944},
  title        = {{A time-dependent discrimination index for survival data}},
  volume       = {24},
}

@article{Cox1975,
  author       = {Cox, D. R.},
  date         = {1975},
  doi          = {10.1080/03610910701884021},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Cox - 1975 - Partial Likelihood.pdf:pdf},
  isbn         = {00063444},
  issn         = {03610918},
  journaltitle = {Biometrika},
  keywords     = {classical,cox ph,ph,survival},
  number       = {2},
  pages        = {269--276},
  title        = {{Partial Likelihood}},
  volume       = {62},
}

@book{Klein2003,
  author    = {Klein, John P and Moeschberger, Melvin L},
  publisher = {Springer Science & Business Media},
  date      = {2003},
  edition   = {2},
  isbn      = {0387216456},
  title     = {{Survival analysis: techniques for censored and truncated data}},
}

@article{pkgjm,
  author       = {Rizopoulos, Dimitris},
  url          = {http://www.jstatsoft.org/v35/i09/},
  date         = {2010},
  journaltitle = {Journal of Statistical Software},
  number       = {9},
  pages        = {1--33},
  title        = {{JM: An R Package for the Joint Modelling of Longitudinal and Time-to-Event Data}},
  volume       = {35},
}

@article{Hosmer1980,
  author       = {Hosmer, David W and Lemeshow, Stanley},
  publisher    = {Taylor & Francis},
  date         = {1980},
  issn         = {0361-0926},
  journaltitle = {Communications in statistics-Theory and Methods},
  number       = {10},
  pages        = {1043--1069},
  title        = {{Goodness of fit tests for the multiple logistic regression model}},
  volume       = {9},
}

@report{Wardekker2013,
  author      = {Wardekker, J. Arjan and Kloprogge, Penny and Petersen, Arthur C. and Janssen, Peter H.M. and van der Sluijs, Jeroen P.},
  institution = {PBL Netherlands Environmental Assessment Agency},
  url         = {https://www.pbl.nl/en/publications/guide-for-uncertainty-communication},
  date        = {2013},
  file        = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Wardekker et al. - 2013 - Guide for Uncertainty Communication(2).pdf:pdf;:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Wardekker et al. - 2013 - Guide for Uncertainty Communication.pdf:pdf},
  title       = {{Guide for Uncertainty Communication}},
  type        = {techreport},
}

@article{Ruckdeschel2009,
  author = {Ruckdeschel, Peter and Kohl, Matthias and Stabla, Thomas and Camphausen, Florian},
  url    = {https://cran.r-project.org/web/packages/distrDoc/vignettes/distr.pdf},
  date   = {2009},
  file   = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Ruckdeschel et al. - 2009 - S4 Classes for Distributions — a manual for packages distr etc.pdf:pdf},
  pages  = {1--109},
  title  = {{S4 Classes for Distributions — a manual for packages "distr" etc.}},
}

@article{pkgrcpp,
  author       = {Eddelbuettel, Dirk and Francois, Romain},
  publisher    = {CRAN},
  url          = {http://www.jstatsoft.org/v40/i08/},
  date         = {2011},
  doi          = {10.18637/jss.v040.i08},
  journaltitle = {Journal of Statistical Software},
  number       = {8},
  pages        = {1--18},
  title        = {{Rcpp: Seamless R and C++ Integration}},
  volume       = {40},
}

@article{Mercer1909,
  abstract     = {"... any continuous, symmetric, positive semi-definite kernel function K(x,y) can be expressed as a dot product in a high-dimensional space. ..." (Jstor)'09, http://rsta.royalsocietypublishing.org/content/209/441-458/415.full.pdf+html (PTRS)'10, and (wikip)'09.},
  author       = {Mercer, J.},
  publisher    = {The Royal Society},
  date         = {1909-01},
  doi          = {10.1098/rsta.1909.0016},
  issn         = {1364-503X},
  journaltitle = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  number       = {441-458},
  pages        = {415--446},
  title        = {{Functions of Positive and Negative Type, and their Connection with the Theory of Integral Equations}},
  volume       = {209},
}

@article{Aivaliotis2021,
  abstract     = {Survival analysis with cohort study data has been traditionally performed using Cox proportional hazards models. Random survival forests (RSFs), a machine learning method, now present an alternative method. Using the UK Women's Cohort Study (n = 34,493) we evaluate two methods: a Cox model and an RSF, to investigate the association between Body Mass Index and time to breast cancer incidence. Robustness of the models were assessed by cross validation and bootstraping. Histograms of bootstrap coefficients are reported. C-Indices and Integrated Brier Scores are reported for all models. In post-menopausal women, the Cox model Hazard Ratios (HR) for Overweight (OW) and Obese (O) were 1.25 (1.04, 1.51) and 1.28 (0.98, 1.68) respectively and the RSF Odds Ratios (OR) with partial dependence on menopause for OW and O were 1.34 (1.31, 1.70) and 1.45 (1.42, 1.48). HR are non-significant results. Only the RSF appears confident about the effect of weight status on time to event. Bootstrapping demonstrated Cox model coefficients can vary significantly, weakening interpretation potential. An RSF was used to produce partial dependence plots (PDPs) showing OW and O weight status increase the probability of breast cancer incidence in post-menopausal women. All models have relatively low C-Index and high Integrated Brier Score. The RSF overfits the data. In our study, RSF can identify complex non-proportional hazard type patterns in the data, and allow more complicated relationships to be investigated using PDPs, but it overfits limiting extrapolation of results to new instances. Moreover, it is less easily interpreted than Cox models. The value of survival analysis remains paramount and therefore machine learning techniques like RSF should be considered as another method for analysis.},
  author       = {Aivaliotis, Georgios and Palczewski, Jan and Atkinson, Rebecca and Cade, Janet E and Morris, Michelle A},
  url          = {https://doi.org/10.1038/s41598-021-92944-z},
  date         = {2021},
  doi          = {10.1038/s41598-021-92944-z},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Aivaliotis et al. - 2021 - A comparison of time to event analysis methods, using weight status and breast cancer as a case study.pdf:pdf},
  issn         = {2045-2322},
  journaltitle = {Scientific Reports},
  number       = {1},
  pages        = {14058},
  title        = {{A comparison of time to event analysis methods, using weight status and breast cancer as a case study}},
  volume       = {11},
}

@article{Schwarz1978,
  abstract     = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
  author       = {Schwarz, Gideon},
  url          = {http://projecteuclid.org/euclid.aos/1176344136},
  annotation   = {BIC aka Schwarz criterion},
  date         = {1978},
  doi          = {10.1214/aos/1176344136},
  eprint       = {arXiv:1011.1669v3},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Schwarz - 1978 - Estimating the Dimension of a Model.pdf:pdf},
  isbn         = {0780394224},
  issn         = {0090-5364},
  journaltitle = {The Annals of Statistics},
  number       = {2},
  pages        = {461--464},
  title        = {{Estimating the Dimension of a Model}},
  volume       = {6},
}

@article{MayHosmer1998,
  abstract     = {Gr{ø}nnesby and Borgan (1996) propose an overall goodness-of-fit test for the Cox proportional hazards model. The basis of their test is a grouping of subjects by their estimated risk score. We show that the Gr{ø}nnesby and Borgan test is algebraically identical to one obtained from adding group indicator variables to the model and testing the hypothesis the coefficients of the group indicator variables are zero via the score test. Thus showing that the test can be calculated using existing software. We demonstrate that the table of observed and estimated expected number of events within each group of the risk score is a useful adjunct to the test to help identify potential problems in fit.},
  author       = {May, Susanne and Hosmer, David W.},
  date         = {1998},
  doi          = {10.1023/A:1009612305785},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/May, Hosmer - 1998 - A Simplified Method of Calculating an Overall Goodness-of-Fit Test for the Cox Proportional Hazards Model.pdf:pdf},
  isbn         = {1380-7870 (Print)1̊380-7870 (Linking)},
  issn         = {13807870},
  journaltitle = {Lifetime Data Analysis},
  keywords     = {Goodness-of-fit,Martingale residuals,Risk score,Score test},
  number       = {2},
  pages        = {109--120},
  title        = {{A Simplified Method of Calculating an Overall Goodness-of-Fit Test for the Cox Proportional Hazards Model}},
  volume       = {4},
}

@article{Kalke2021,
  abstract     = {Objective To conduct a scoping review of existing studies that examine communication strategies that address uncertainty in health and categorize them using the taxonomy of uncertainty. Methods Relevant articles retrieved from ten databases were categorized according to the dimensions of the taxonomy of uncertainty, and study characteristics were extracted from each article. Results All articles (n = 63) explored uncertainty in the context of probabilistic risk and related to scientific issues (n = 63; 100%). The majority focused on complexity (n = 24; 38.1%) and uncertainty experienced by patients (n = 52; 82.5%). Most utilized quantitative methods (n = 46; 73.0%), hypothetical scenarios (n = 49; 77.8%), and focused on cancer (n = 20; 31.7%). Theory guided messages and study design in fewer than half (n = 27; 42.9%). Conclusions Heterogeneity in terminology used to refer to different types of uncertainties preclude a unified research agenda on uncertainty communication. Research predominately focuses on probability as the source of uncertainty, uncertainties related to scientific issues, and uncertainty experienced by patients. Practice implications Additional efforts are needed to understand providers' experience of uncertainty, and to identify strategies to address ambiguity. Future studies should use consistent terminology to allow for coherence and advancement of uncertainty communication scholarship. Continued efforts to refine the existing taxonomy should be undertaken.},
  author       = {Kalke, Kerstin and Studd, Hannah and Scherr, Courtney L},
  url          = {https://www.sciencedirect.com/science/article/pii/S0738399121000616},
  date         = {2021},
  doi          = {https://doi.org/10.1016/j.pec.2021.01.034},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Kalke, Studd, Scherr - 2021 - The communication of uncertainty in health A scoping review.pdf:pdf},
  issn         = {0738-3991},
  journaltitle = {Patient Education and Counseling},
  keywords     = {Decision-making,Health communication,Patient-provider communication,Risk communication,Taxonomy,Uncertainty},
  number       = {8},
  pages        = {1945--1961},
  title        = {{The communication of uncertainty in health: A scoping review}},
  volume       = {104},
}

@article{Jiang2017,
  author       = {Jiang, Gaoxia and Wang, Wenjian},
  publisher    = {Elsevier Inc.},
  date         = {2017},
  doi          = {10.1016/j.ins.2016.09.061},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Jiang, Wang - 2017 - Markov cross-validation for time series model evaluations.pdf:pdf},
  journaltitle = {Information Sciences},
  keywords     = {Markov cross-validation,Model evaluation,Time series},
  pages        = {219--233},
  title        = {{Markov cross-validation for time series model evaluations}},
  volume       = {375},
}

@book{dataapplied,
  author    = {{Hosmer Jr}, David W and Lemeshow, Stanley and May, Susanne},
  publisher = {John Wiley & Sons},
  date      = {2011},
  isbn      = {1118211588},
  title     = {{Applied survival analysis: regression modeling of time-to-event data}},
  volume    = {618},
}

@article{Haider2020,
  author       = {Haider, Humza and Hoehn, Bret and Davis, Sarah and Greiner, Russell},
  annotation   = {D-calibration},
  date         = {2020},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Haider et al. - 2020 - Effective ways to build and evaluate individual survival distributions.pdf:pdf},
  issn         = {1533-7928},
  journaltitle = {Journal of Machine Learning Research},
  number       = {85},
  pages        = {1--63},
  title        = {{Effective ways to build and evaluate individual survival distributions}},
  volume       = {21},
}

@thesis{Sonabend2021b,
  abstract    = {Survival analysis is an important field of Statistics concerned with mak- ing time-to-event predictions with ‘censored' data. Machine learning, specifically supervised learning, is the field of Statistics concerned with using state-of-the-art algorithms in order to make predictions on unseen data. This thesis looks at unifying these two fields as current research into the two is still disjoint, with ‘classical survival' on one side and su- pervised learning (primarily classification and regression) on the other. This PhD aims to improve the quality of machine learning research in survival analysis by focusing on transparency, accessibility, and predic- tive performance in model building and evaluation. This is achieved by examining historic and current proposals and implementations for models and measures (both classical and machine learning) in survival analysis and making novel contributions. In particular this includes: i) a survey of survival models including a crit- ical and technical survey of almost all supervised learning model classes currently utilised in survival, as well as novel adaptations; ii) a survey of evaluation measures for survival models, including key definitions, proofs and theorems for survival scoring rules that had previously been missing from the literature; iii) introduction and formalisation of composition and reduction in survival analysis, with a view on increasing transparency of modelling strategies and improving predictive performance; iv) imple- mentation of several R software packages, in particular mlr3proba for machine learning in survival analysis; and v) the first large-scale bench- mark experiment on right-censored time-to-event data with 24 survival models and 66 datasets. Survival analysis has many important applications in medical statistics, engineering and finance, and as such requires the same level of rigour as other machine learning fields such as regression and classification; this thesis aims to make this clear by describing a framework from prediction and evaluation to implementation.},
  author      = {Sonabend, Raphael Edward Benjamin},
  institution = {University College London (UCL)},
  url         = {https://discovery.ucl.ac.uk/id/eprint/10129352/},
  date        = {2021},
  file        = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Sonabend - 2021 - A Theoretical and Methodological Framework for Machine Learning in Survival Analysis Enabling Transparent and Accessib.pdf:pdf},
  pages       = {345},
  title       = {{A Theoretical and Methodological Framework for Machine Learning in Survival Analysis: Enabling Transparent and Accessible Predictive Modelling on Right-Censored Time-to-Event Data}},
  type        = {PhD},
}

@article{Faria2021,
  abstract     = {Cases of SARS-CoV-2 infection in Manaus, Brazil, resurged in late 2020, despite previously high levels of infection. Genome sequencing of viruses sampled in Manaus between November 2020 and January 2021 revealed the emergence and circulation of a novel SARS-CoV-2 variant of concern. Lineage P.1, acquired 17 mutations, including a trio in the spike protein (K417T, E484K and N501Y) associated with increased binding to the human ACE2 receptor. Molecular clock analysis shows that P.1 emergence occurred around mid-November 2020 and was preceded by a period of faster molecular evolution. Using a two-category dynamical model that integrates genomic and mortality data, we estimate that P.1 may be 1.7–2.4-fold more transmissible, and that previous (non-P.1) infection provides 54–79% of the protection against infection with P.1 that it provides against non-P.1 lineages. Enhanced global genomic surveillance of variants of concern, which may exhibit increased transmissibility and/or immune evasion, is critical to accelerate pandemic responsiveness.},
  author       = {Faria, Nuno R and Mellan, Thomas A and Whittaker, Charles and Claro, Ingra M and Candido, Darlan da S and Mishra, Swapnil and Crispim, Myuki A E and Sales, Flavia C. S. and Hawryluk, Iwona and McCrone, John T and Hulswit, Ruben J G and Franco, Lucas A M and Ramundo, Mariana S and de Jesus, Jaqueline G and Andrade, Pamela S and Coletti, Thais M and Ferreira, Giulia M. and Silva, Camila A. M. and Manuli, Erika R and Pereira, Rafael H M and Peixoto, Pedro S and Kraemer, Moritz U. G. and Gaburo, Nelson and Camilo, Cecilia da C and Hoeltgebaum, Henrique and Souza, William M and Rocha, Esmenia C and de Souza, Leandro M and de Pinho, Mariana C and Araujo, Leonardo J T and Malta, Frederico S V and de Lima, Aline B and Silva, Joice do P and Zauli, Danielle A G and Ferreira, Alessandro C de S and Schnekenberg, Ricardo P and Laydon, Daniel J and Walker, Patrick G T and Schlüter, Hannah M and dos Santos, Ana L. P. and Vidal, Maria S and {Del Caro}, Valentina S and Filho, Rosinaldo M F and dos Santos, Helem M and Aguiar, Renato S and Proença-Modena, José L. and Nelson, Bruce and Hay, James A and Monod, Mélodie and Miscouridou, Xenia and Coupland, Helen and Sonabend, Raphael and Vollmer, Michaela and Gandy, Axel and Prete, Carlos A. and Nascimento, Vitor H. and Suchard, Marc A and Bowden, Thomas A and Pond, Sergei L K and Wu, Chieh-Hsi and Ratmann, Oliver and Ferguson, Neil M and Dye, Christopher and Loman, Nick J and Lemey, Philippe and Rambaut, Andrew and Fraiji, Nelson A and Carvalho, Maria do P S S and Pybus, Oliver G and Flaxman, Seth and Bhatt, Samir and Sabino, Ester C},
  url          = {https://science.sciencemag.org/content/early/2021/04/13/science.abh2644.full.pdf https://www.sciencemag.org/lookup/doi/10.1126/science.abh2644},
  date         = {2021-04},
  doi          = {10.1126/science.abh2644},
  issn         = {0036-8075},
  journaltitle = {Science},
  pages        = {eabh2644},
  title        = {{Genomics and epidemiology of the P.1 SARS-CoV-2 lineage in Manaus, Brazil}},
}

@article{Pearl2009,
  abstract     = {This reviewpresents empirical researcherswith recent advances in causal inference, and stresses the paradigmatic shifts that must be un- dertaken in moving fromtraditional statistical analysis to causal analysis of multivariate data. Special emphasis is placed on the assumptions that un- derly all causal inferences, the languages used in formulating those assump- tions, the conditional nature of all causal and counterfactual claims, and the methods that have been developed for the assessment of such claims. These advances are illustrated using a general theory of causation based on the Structural Causal Model (SCM) described in Pearl (2000a), which subsumes and unifies other approaches to causation, and provides a coher- entmathematical foundation for the analysis of causes and counterfactuals. In particular, the paper surveys the development of mathematical tools for inferring (from a combination of data and assumptions) answers to three types of causal queries: (1) queries about the effects of potential interven- tions, (also called “causal effects” or “policy evaluation”) (2) queries about probabilities of counterfactuals, (including assessment of “regret,” “attri- bution” or “causes of effects”) and (3) queries about direct and indirect effects (also known as “mediation”). Finally, the paper defines the formal and conceptual relationships between the structural and potential-outcome frameworks and presents tools for a symbiotic analysis that uses the strong features of both.},
  author       = {Pearl, Judea},
  date         = {2009},
  doi          = {10.1214/09-ss057},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Pearl - 2009 - Causal inference in statistics An overview.pdf:pdf},
  issn         = {1935-7516},
  journaltitle = {Statistics Surveys},
  keywords     = {Structural equation models,confounding,graphical,and phrases,causal effects,causes of effects,confounding,counterfactuals,graph-,ical methods,mediation,policy evaluation,potential-outcome,received september 2009,structural equation models},
  pages        = {96--146},
  title        = {{Causal inference in statistics: An overview}},
  volume       = {3},
}

@article{Martinez2018,
  author       = {Martinez-Martin, Nicole and Dunn, Laura B. and Roberts, Laura Weiss},
  url          = {https://journalofethics.ama-assn.org/article/it-ethical-use-prognostic-estimates-machine-learning-treat-psychosis/2018-09},
  date         = {2018},
  doi          = {10.1001/amajethics.2018.804},
  journaltitle = {AMA Journal of Ethics},
  number       = {9},
  title        = {{Is It Ethical to Use Prognostic Estimates from Machine Learning to Treat Psychosis?}},
  volume       = {20},
}

@article{Hennig2009,
  abstract     = {Problem: Evidence is quantified by statistical methods such as p-values and Bayesian posterior probabilities in a routine way despite the fact that there is no consensus about the meanings and implications of these approaches. A high level of confusion about these methods can be observed among students, researchers and even professional statisticians. How can a constructivist view of mathematical models and reality help to resolve the confusion? Method: Considerations about the foundations of statistics and probability are revisited with a constructivist attitude that explores which ways of thinking about the modelled phenomena are implied by different approaches to probability modelling. Results: The understanding of the implications of probability modelling for the quantification of evidence can be strongly improved by accepting that whether models are “true” or not cannot be checked from the data, and the use of the models should rather be justified and critically discussed in terms of their implications for the thinking and communication of researchers. Implications: Some useful questions that researchers can use as guidelines when deciding which approach and which model to choose are listed in the paper, along with some implications of using frequentist p-values or Bayesian posterior probability, which can help to address the questions. It is the – far too often ignored – responsibility of the researchers to decide which model is chosen and what the evidence suggests rather than letting the results decide themselves in an “objective way.”},
  author       = {Hennig, Christian},
  url          = {http://www.univie.ac.at/constructivism/journal/5/1/039.hennig},
  date         = {2009},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Hennig - 2009 - A Constructivist View of the Statistical Quantification of Evidence.pdf:pdf},
  issn         = {1782348X},
  journaltitle = {Constructivist Foundations},
  keywords     = {Bayesian subjectivism,foundations of probability,frequentism,mathematical modelling,objective Bayes,p-values,reality},
  number       = {1},
  pages        = {39--54},
  title        = {{A Constructivist View of the Statistical Quantification of Evidence}},
  volume       = {5},
}

@article{Rosthoj2004,
  abstract     = {When studying a regression model measures of explained variation are used to assess the degree to which the covariates determine the outcome of interest. Measures of predictive accuracy are used to assess the accuracy of the predictions based on the covariates and the regression model. We give a detailed and general introduction to the two measures and the estimation procedures. The framework we set up allows for a study of the effect of misspecification on the quantities estimated. We also introduce a generalization to survival analysis.},
  author       = {Rosth{ø}j, Susanne and Keiding, Niels},
  url          = {https://doi.org/10.1007/s10985-004-4778-6},
  date         = {2004},
  doi          = {10.1007/s10985-004-4778-6},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Rosth{\o}j, Keiding - 2004 - Explained Variation and Predictive Accuracy in General Parametric Statistical Models The Role of Model Misspe.pdf:pdf},
  issn         = {1572-9249},
  journaltitle = {Lifetime Data Analysis},
  number       = {4},
  pages        = {461--472},
  title        = {{Explained Variation and Predictive Accuracy in General Parametric Statistical Models: The Role of Model Misspecification}},
  volume       = {10},
}

@article{Royston2001,
  abstract     = {Despite their long history, parametric survival-time models have largely been neglected in the modern biostatistical and medical literature in favour of the Cox proportional hazards model. Here, I present a case for the use of the lognormal distribution in the analysis of survival times of breast and ovarian cancer patients, specifically in modelling the effects of prognostic factors. The lognormal provides a completely specified probability distribution for the observations and a sensible estimate of the variation explained by the model, a quantity that is controversial for the Cox model. I show how imputation of censored observations under the model may be used to inspect the data using familiar graphical and other technques. Results from the Cox and lognormal models are compared and shown apparently to differ to some extent. However, it is hard to judge which model gives the more accurate estimates. It is concluded that provided the lognormal model fits the data adequately, it may be a useful approach to the analysis of censored survival data.},
  author       = {Royston, P},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1111/1467-9574.00158},
  annotation   = {doi: 10.1111/1467-9574.00158},
  date         = {2001-03},
  doi          = {10.1111/1467-9574.00158},
  issn         = {0039-0402},
  journaltitle = {Statistica Neerlandica},
  keywords     = {explained variation,imputation,non-proportional hazards,parametric models},
  number       = {1},
  pages        = {89--104},
  title        = {{The Lognormal Distribution as a Model for Survival Time in Cancer, With an Emphasis on Prognostic Factors}},
  volume       = {55},
}

@article{Khan2008,
  abstract     = {A crucial challenge in predictive modeling for survival analysis is managing censored observations in the data. The Cox proportional hazards model is the standard tool for the analysis of continuous censored survival data. We propose a novel machine learning algorithm, Support Vector Regression for Censored Data (SVRc) for improved analysis of medical survival data. SVRc leverages the high-dimensional capabilities of traditional SVR while adapting it for use with censored data through a modified asymmetric loss/penalty function which allows censored (left and right censored) data to be processed. We applied the new algorithm to predict the recurrence and disease progression of prostate cancer, breast cancer and lung cancer. Compared with the traditional Cox model, SVRc achieves significant improvement in overall accuracy as well as in the ability to identify high-risk and low-risk patient populations},
  author       = {Khan, Faisal M. and {Bayer Zubek}, Valentina},
  publisher    = {IEEE},
  date         = {2008},
  doi          = {10.1109/ICDM.2008.50},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Khan, Bayer-Zubek - 2008 - Support vector regression for censored data (SVRc) A novel tool for survival analysis.pdf:pdf},
  isbn         = {9780769535029},
  issn         = {15504786},
  journaltitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
  keywords     = {SVR,SVRc,censoring,machine learning,survival,svm},
  pages        = {863--868},
  title        = {{Support vector regression for censored data (SVRc): A novel tool for survival analysis}},
}

@article{Li2005,
  abstract     = {Motivation: An important area of research in the postgenomics era is to relate high-dimensional genetic or genomic data to various clinical phenotypes of patients. Due to large variability in time to certain clinical events among patients, studying possibly censored survival phenotypes can be more informative than treating the phenotypes as categorical variables. Due to high dimensionality and censoring, building a predictive model for time to event is more difficult than the classification/linear regression problem. We propose to develop a boosting procedure using smoothing splines for estimating the general proportional hazards models. Such a procedure can potentially be used for identifying non-linear effects of genes on the risk of developing an event.Results: Our empirical simulation studies showed that the procedure can indeed recover the true functional forms of the covariates and can identify important variables that are related to the risk of an event. Results from predicting survival after chemotherapy for patients with diffuse large B-cell lymphoma demonstrate that the proposed method can be used for identifying important genes that are related to time to death due to cancer and for building a parsimonious model for predicting the survival of future patients. In addition, there is clear evidence of non-linear effects of some genes on survival time.Contact:hli@ucdavis.edu},
  author       = {Li, Hongzhe and Luan, Yihui},
  url          = {https://doi.org/10.1093/bioinformatics/bti324},
  date         = {2005-02},
  doi          = {10.1093/bioinformatics/bti324},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Li, Luan - 2005 - Boosting proportional hazards models using smoothing splines, with applications to high-dimensional microarray data.pdf:pdf},
  issn         = {1367-4803},
  journaltitle = {Bioinformatics},
  number       = {10},
  pages        = {2403--2409},
  title        = {{Boosting proportional hazards models using smoothing splines, with applications to high-dimensional microarray data}},
  volume       = {21},
}

@article{Hothorn2003,
  abstract     = {The construction of simple classification rules is a frequent problem in medical research. Maximally selected rank statistics allow the evaluation of cutpoints, which provide the classification of observations into two groups by a continuous or ordinal predictor variable. The computation of the exact distribution of a maximally selected rank statistic is discussed and a new lower bound of the distribution is derived based on an extension of an algorithm for the exact distribution of a linear rank statistic. Therefore, the test based on the upper bound of the P-value is of level $\alpha$. For small to moderate sample sizes the lower bound of the exact distribution is a substantial improvement compared to approximations based on an improved Bonferroni inequality or based on the asymptotic Gaussian process. The lower bound of the distribution is compared to the exact distribution by means of a simulation study and the proposal is illustrated by three clinical studies.},
  author       = {Hothorn, Torsten and Lausen, Berthold},
  publisher    = {North-Holland},
  url          = {https://www.sciencedirect.com/science/article/pii/S0167947302002256},
  date         = {2003-06},
  doi          = {10.1016/S0167-9473(02)00225-6},
  issn         = {0167-9473},
  journaltitle = {Computational Statistics & Data Analysis},
  number       = {2},
  pages        = {121--137},
  title        = {{On the exact distribution of maximally selected rank statistics}},
  volume       = {43},
}

@book{Bischl2024,
  editor    = {Bischl, Bernd and Sonabend, Raphael and Kotthoff, Lars and Lang, Michel},
  publisher = {CRC Press},
  url       = {https://mlr3book.mlr-org.com},
  date      = {2024},
  isbn      = {9781032507545},
  title     = {{Applied Machine Learning Using mlr3 in R}},
}

@article{Floridi2016,
  author       = {Floridi, Luciano and Taddeo, Mariarosaria},
  date         = {2016},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Floridi, Taddeo - 2016 - What is data ethics.pdf:pdf},
  journaltitle = {Philosophical Transactions of the Royal Society of London},
  title        = {{What is data ethics?}},
}

@article{Horton2005,
  author       = {Horton, Nicholas J and Switzer, Suzanne S},
  publisher    = {Mass Medical Soc},
  date         = {2005},
  issn         = {0028-4793},
  journaltitle = {New England Journal of Medicine},
  number       = {18},
  pages        = {1977--1979},
  title        = {{Statistical methods in the journal}},
  volume       = {353},
}

@book{Rijsbergen1979,
  author    = {Rijsbergen, C. J. Van},
  publisher = {Butterworth-Heinemann},
  date      = {1979},
  edition   = {2nd},
  isbn      = {0408709294},
  title     = {{Information Retrieval}},
}

@article{Schwarzer2000,
  abstract     = {Abstract The application of artificial neural networks (ANNs) for prognostic and diagnostic classification in clinical medicine has become very popular. In particular, feed-forward neural networks have been used extensively, often accompanied by exaggerated statements of their potential. In this paper, the essentials of feed-forward neural networks and their statistical counterparts (that is, logistic regression models) are reviewed. We point out that the uncritical use of ANNs may lead to serious problems, such as the fitting of implausible functions to describe the probability of class membership and the underestimation of misclassification probabilities. In applications of ANNs to survival data, further difficulties arise. Finally, the results of a search in the medical literature from 1991 to 1995 on applications of ANNs in oncology and some important common mistakes are reported. It is concluded that there is no evidence so far that application of ANNs represents real progress in the field of diagnosis and prognosis in oncology. Copyright ? 2000 John Wiley & Sons, Ltd.},
  author       = {Schwarzer, Guido and Vach, Werner and Schumacher, Martin},
  publisher    = {John Wiley & Sons, Ltd},
  url          = {https://doi.org/10.1002/sim.3758},
  annotation   = {doi: 10.1002/(SICI)1097-0258(20000229)19:43.0.CO;2-V},
  date         = {2010-01},
  doi          = {10.1002/(SICI)1097-0258(20000229)19:4<541::AID-SIM355>3.0.CO;2-V},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Schwarzer, Vach, Schumacher - 2000 - On the misuses of artificial neural networks for prognostic and diagnostic classification in oncolo.pdf:pdf},
  issn         = {0277-6715},
  journaltitle = {Statistics in Medicine},
  keywords     = {absolute error,confidence intervals,expected loss,inverse probability of censoring weights,misspecified models,survival time predictors},
  number       = {2},
  pages        = {262--274},
  title        = {{Estimation of prediction error for survival models}},
  volume       = {29},
}

@misc{pkgsurvivalmodels,
  author    = {Sonabend, Raphael},
  publisher = {CRAN},
  url       = {https://raphaels1.r-universe.dev/ui#package:survivalmodels},
  date      = {2020},
  title     = {{survivalmodels: Models for Survival Analysis}},
}

@article{Good1952,
  abstract     = {[This paper deals first with the relationship between the theory of probability and the theory of rational behaviour. A method is then suggested for encouraging people to make accurate probability estimates, a connection with the theory of information being mentioned. Finally Wald's theory of statistical decision functions is summarised and generalised and its relation to the theory of rational behaviour is discussed.]},
  author       = {Good, I J},
  publisher    = {[Royal Statistical Society, Wiley]},
  url          = {http://www.jstor.org/stable/2984087},
  date         = {1952-06},
  issn         = {00359246},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
  number       = {1},
  pages        = {107--114},
  title        = {{Rational Decisions}},
  volume       = {14},
}

@misc{pkgrfsrc,
  author = {Ishwaran, Hemant and Kogalur, Udaya B},
  url    = {https://cran.r-project.org/package=randomForestSRC},
  date   = {2018},
  title  = {{randomForestSRC}},
}

@article{Walker2003,
  author       = {Walker, W E and Harremoës, P and Rotmans, J and van der Sluijs, J P and van Asselt, M B A and Janssen, P and {Krayer von Krauss}, M P},
  publisher    = {Taylor & Francis},
  url          = {https://doi.org/10.1076/iaij.4.1.5.16466},
  annotation   = {doi: 10.1076/iaij.4.1.5.16466},
  date         = {2003-03},
  doi          = {10.1076/iaij.4.1.5.16466},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Walker et al. - 2003 - Defining Uncertainty A Conceptual Basis for Uncertainty Management in Model-Based Decision Support.pdf:pdf},
  issn         = {1389-5176},
  journaltitle = {Integrated Assessment},
  number       = {1},
  pages        = {5--17},
  title        = {{Defining Uncertainty: A Conceptual Basis for Uncertainty Management in Model-Based Decision Support}},
  volume       = {4},
}

@article{Steyerberg2010,
  abstract     = {The performance of prediction models can be assessed using a variety of different methods and metrics. Traditional measures for binary and survival outcomes include the Brier score to indicate overall model performance, the concordance (or c ) statistic for discriminative ability (or area under the receiver operating characteristic (ROC) curve), and goodness-of-fit statistics for calibration. Several new measures have recently been proposed that can be seen as refinements of discrimination measures, including variants of the c statistic for survival, reclassification tables, net reclassification improvement (NRI), and integrated discrimination improvement (IDI). Moreover, decision–analytic measures have been proposed, including decision curves to plot the net benefit achieved by making decisions based on model predictions. We aimed to define the role of these relatively novel approaches in the evaluation of the performance of prediction models. For illustration we present a case study of predicting the presence of residual tumor versus benign tissue in patients with testicular cancer (n=544 for model development, n=273 for external validation). We suggest that reporting discrimination and calibration will always be important for a prediction model. Decision-analytic measures should be reported if the predictive model is to be used for making clinical decisions. Other measures of performance may be warranted in specific applications, such as reclassification metrics to gain insight into the value of adding a novel predictor},
  author       = {Steyerberg, Ewout W. and Vickers, Andrew J. and Cook, Nancy R. and Gerds, Thomas and Gonen, Mithat and Obuchowski, Nancy and Pencina, Michael J. and Kattan, Michael W.},
  date         = {2010},
  doi          = {10.1097/EDE.0b013e3181c30fb2},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Steyerberg et al. - 2010 - Assessing the performance of prediction models A framework for traditional and novel measures.pdf:pdf},
  isbn         = {1531-5487 (Electronic) 1044-3983 (Linking)},
  issn         = {10443983},
  journaltitle = {Epidemiology},
  number       = {1},
  pages        = {128--138},
  title        = {{Assessing the performance of prediction models: A framework for traditional and novel measures}},
  volume       = {21},
}

@article{Lu2008,
  abstract     = {We propose a general class of nonlinear transformation models for analyzing censored survival data, of which the nonlinear proportional hazards and proportional odds models are special cases. A cubic smoothing spline–based component-wise boosting algorithm is derived to estimate covariate effects nonparametrically using the gradient of the marginal likelihood, that is computed using importance sampling. The proposed method can be applied to survival data with high-dimensional covariates, including the case when the sample size is smaller than the number of predictors. Empirical performance of the proposed method is evaluated via simulations and analysis of a microarray survival data.},
  author       = {Lu, Wenbin and Li, Lexin},
  url          = {https://doi.org/10.1093/biostatistics/kxn005},
  date         = {2008-03},
  doi          = {10.1093/biostatistics/kxn005},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Lu, Li - 2008 - Boosting method for nonlinear transformation models with censored survival data.pdf:pdf},
  issn         = {1465-4644},
  journaltitle = {Biostatistics},
  number       = {4},
  pages        = {658--667},
  title        = {{Boosting method for nonlinear transformation models with censored survival data}},
  volume       = {9},
}

@article{Harris1950,
  author       = {Harris, T. E. and Meier, Paul and Tukey, J. W.},
  date         = {1950},
  journaltitle = {Human Biology},
  pages        = {249--70},
  title        = {{Timing of the distribution of events between observations; a contribution to the theory of follow-up studies.}},
  volume       = {22},
}

@incollection{Salado2012,
  author     = {Salado, Alejandro and Nilchiani, Roshanak and Efatmaneshnik, Mahmoud},
  publisher  = {American Institute of Aeronautics and Astronautics},
  url        = {https://doi.org/10.2514/6.2012-5195},
  annotation = {doi:10.2514/6.2012-5195},
  booktitle  = {AIAA SPACE 2012 Conference & Exposition},
  date       = {2012-09},
  doi        = {doi:10.2514/6.2012-5195},
  file       = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Salado, Nilchiani, Efatmaneshnik - 2012 - Taxonomy and Categorization of Uncertainties in Space Systems with an Application to the Measu.pdf:pdf},
  series     = {AIAA SPACE Forum},
  title      = {{Taxonomy and Categorization of Uncertainties in Space Systems with an Application to the Measurement of the Value of Adaptability}},
}

@inproceedings{Freund1996,
  author    = {Freund, Yoav and Schapire, Robert E},
  publisher = {Citeseer},
  date      = {1996},
  title     = {{Experiments with a new boosting algorithm}},
}

@article{Andres2018,
  abstract     = {Deciding who should receive a liver transplant (LT) depends on both urgency and utility. Most survival scores are validated through discriminative tests, which compare predicted outcomes between patients. Assessing post-transplant survival utility is not discriminate, but should be “calibrated” to be effective. There are currently no such calibrated models. We developed and validated a novel calibrated model to predict individual survival after LT for Primary Sclerosing Cholangitis (PSC). We applied a software tool, PSSP, to adult patients in the Scientific Registry of Transplant Recipients (n = 2769) who received a LT for PSC between 2002 and 2013; this produced a model for predicting individual survival distributions for novel patients. We also developed an appropriate evaluation measure, D-calibration, to validate this model. The learned PSSP model showed an excellent D-calibration (p = 1.0), and passed the single-time calibration test (Hosmer-Lemeshow p-value of over 0.05) at 0.25, 1, 5 and 10 years. In contrast, the model based on traditional Cox regression showed worse calibration on long-term survival and failed at 10 years (Hosmer-Lemeshow p value = 0.027). The calculator and visualizer are available at: http://pssp.srv.ualberta.ca/calculator/liver_transplant_2002. In conclusion we present a new tool that accurately estimates individual post liver transplantation survival.},
  author       = {Andres, Axel and Montano-Loza, Aldo and Greiner, Russell and Uhlich, Max and Jin, Ping and Hoehn, Bret and Bigam, David and Shapiro, James Andrew Mark and Kneteman, Norman Mark},
  publisher    = {Public Library of Science},
  url          = {https://doi.org/10.1371/journal.pone.0193523},
  date         = {2018-03},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Andres et al. - 2018 - A novel learning algorithm to predict individual survival after liver transplantation for primary sclerosing chol.pdf:pdf},
  journaltitle = {PLOS ONE},
  number       = {3},
  pages        = {e0193523},
  title        = {{A novel learning algorithm to predict individual survival after liver transplantation for primary sclerosing cholangitis}},
  volume       = {13},
}

@article{pkgdistrmod,
  author       = {Kohl, Matthias and Ruckdeschel, Peter},
  url          = {http://books.google.com/books?hl=en&lr=&id=OyD4toi-XDIC&oi=fnd&pg=PA2&dq=Game+theory+:+Decisions,+Interactions+and+Evolution&ots=TfgJJReigk&sig=AnVOJ8VTGk60lUCldvLrPPnrJD4},
  date         = {2010},
  doi          = {10.1086/423055},
  eprint       = {arXiv:1011.1669v3},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Kohl, Ruckdeschel - 2010 - R package distrMod Object-Oriented Implementation of Probability Models.pdf:pdf},
  isbn         = {9781846280405},
  issn         = {03040208},
  journaltitle = {Journal of Statistical Software},
  title        = {{R package distrMod: Object-Oriented Implementation of Probability Models.}},
}

@misc{pkgsurvival,
  author = {Therneau, Terry M.},
  url    = {https://cran.r-project.org/package=survival},
  date   = {2015},
  title  = {{A Package for Survival Analysis in S}},
}

@article{Katzman2018,
  abstract     = {Medical practitioners use survival models to explore and understand the relationships between patients' covariates (e.g. clinical and genetic features) and the effectiveness of various treatment options. Standard survival models like the linear Cox proportional hazards model require extensive feature engineering or prior medical knowledge to model treatment interaction at an individual level. While nonlinear survival methods, such as neural networks and survival forests, can inherently model these high-level interaction terms, they have yet to be shown as effective treatment recommender systems.},
  author       = {Katzman, Jared L and Shaham, Uri and Cloninger, Alexander and Bates, Jonathan and Jiang, Tingting and Kluger, Yuval},
  url          = {https://doi.org/10.1186/s12874-018-0482-1},
  date         = {2018},
  doi          = {10.1186/s12874-018-0482-1},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Katzman et al. - 2018 - DeepSurv personalized treatment recommender system using a Cox proportional hazards deep neural network.pdf:pdf},
  issn         = {1471-2288},
  journaltitle = {BMC Medical Research Methodology},
  number       = {1},
  pages        = {24},
  title        = {{DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network}},
  volume       = {18},
}

@unpublished{Gressmann2018,
  abstract   = {Predictive modelling and supervised learning are central to modern data science. With predictions from an ever-expanding number of supervised black-box strategies - e.g., kernel methods, random forests, deep learning aka neural networks - being employed as a basis for decision making processes, it is crucial to understand the statistical uncertainty associated with these predictions. As a general means to approach the issue, we present an overarching framework for black-box prediction strategies that not only predict the target but also their own predictions' uncertainty. Moreover, the framework allows for fair assessment and comparison of disparate prediction strategies. For this, we formally consider strategies capable of predicting full distributions from feature variables, so-called probabilistic supervised learning strategies. Our work draws from prior work including Bayesian statistics, information theory, and modern supervised machine learning, and in a novel synthesis leads to (a) new theoretical insights such as a probabilistic bias-variance decomposition and an entropic formulation of prediction, as well as to (b) new algorithms and meta-algorithms, such as composite prediction strategies, probabilistic boosting and bagging, and a probabilistic predictive independence test. Our black-box formulation also leads (c) to a new modular interface view on probabilistic supervised learning and a modelling workflow API design, which we have implemented in the newly released skpro machine learning toolbox, extending the familiar modelling interface and meta-modelling functionality of sklearn. The skpro package provides interfaces for construction, composition, and tuning of probabilistic supervised learning strategies, together with orchestration features for validation and comparison of any such strategy - be it frequentist, Bayesian, or other.},
  author     = {Gressmann, Frithjof and Király, Franz J. and Mateen, Bilal and Oberhauser, Harald},
  url        = {http://arxiv.org/abs/1801.00753},
  date       = {2018},
  doi        = {10.1002/iub.552},
  eprint     = {1801.00753},
  eprinttype = {arXiv},
  file       = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Gressmann et al. - 2018 - Probabilistic supervised learning.pdf:pdf},
  isbn       = {0171-5216 (Print)},
  issn       = {1521-6551},
  title      = {{Probabilistic supervised learning}},
}

@inproceedings{Birattari2002,
  author    = {Birattari, Mauro and Stützle, Thomas and Paquete, Luis and Varrentrapp, Klaus},
  booktitle = {Gecco},
  date      = {2002},
  file      = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Birattari et al. - 2002 - A Racing Algorithm for Configuring Metaheuristics.pdf:pdf},
  keywords  = {coding,f-race,hyper-parameter tuning,irace,mlr3,optimisation,racing,software,tuning},
  number    = {2002},
  title     = {{A Racing Algorithm for Configuring Metaheuristics.}},
  volume    = {2},
}

@misc{pkgintervalsets,
  author = {Navelkar, Anchit},
  url    = {https://github.com/JuliaMath/IntervalSets.jl},
  date   = {2016},
  title  = {{IntervalSets.jl}},
}

@article{databladder1,
  author       = {Wei, L J and Lin, D Y and Weissfeld, L},
  publisher    = {Taylor & Francis},
  url          = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478873},
  annotation   = {doi: 10.1080/01621459.1989.10478873},
  date         = {1989-12},
  doi          = {10.1080/01621459.1989.10478873},
  issn         = {0162-1459},
  journaltitle = {Journal of the American Statistical Association},
  number       = {408},
  pages        = {1065--1073},
  title        = {{Regression Analysis of Multivariate Incomplete Failure Time Data by Modeling Marginal Distributions}},
  volume       = {84},
}

@book{dataaml,
  author    = {Miller, Rupert G.},
  publisher = {John Wiley & Sons},
  date      = {1997},
  isbn      = {0-471-25218-2},
  title     = {{Survival Analysis}},
}

@article{NationalNumeracy2015,
  author = {{National Numeracy}},
  url    = {https://www.nationalnumeracy.org.uk/sites/default/files/numeracy_for_health_full.pdf},
  date   = {2015},
  file   = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/National Numeracy - 2015 - Numeracy for Health.pdf:pdf},
  pages  = {1--15},
  title  = {{Numeracy for Health}},
}

@misc{pkgmlr3learners,
  author     = {Lang, Michel and Au, Quay and Coors, Stefan and Schratz, Patrick},
  publisher  = {CRAN},
  url        = {https://cran.r-project.org/package=mlr3learners},
  annotation = {https://mlr3learners.mlr-org.com, https://github.com/mlr-org/mlr3learners},
  date       = {2020},
  title      = {{mlr3learners: Recommended Learners for 'mlr3'}},
}

@article{Zhang2021,
  abstract     = {Survival analysis is a branch of statistics that deals with both, the tracking of time and of the survival status simultaneously as the dependent response. Current comparisons of survival model performance mostly center on clinical data with classic statistical survival models, with prediction accuracy often serving as the sole metric of model performance. Moreover, survival analysis approaches for censored omics data have not been thoroughly investigated. The common approach is to binarise the survival time and perform a classification analysis.Here, we develop a benchmarking framework, SurvBenchmark, that evaluates a diverse collection of survival models for both clinical and omics datasets. SurvBenchmark not only focuses on classical approaches such as the Cox model, but it also evaluates state-of-art machine learning survival models. All approaches were assessed using multiple performance metrics, these include model predictability, stability, flexibility and computational issues. Our systematic comparison framework with over 320 comparisons (20 methods over 16 datasets) shows that the performances of survival models vary in practice over real-world datasets and over the choice of the evaluation metric. In particular, we highlight that using multiple performance metrics is critical in providing a balanced assessment of various models. The results in our study will provide practical guidelines for translational scientists and clinicians, as well as define possible areas of investigation in both survival technique and benchmarking strategies.Contact jean.yang{at}sydney.edu.auCompeting Interest StatementThe authors have declared no competing interest.},
  author       = {Zhang, Yunwei and Wong, Germaine and Mann, Graham and Muller, Samuel and Yang, Jean Y H},
  url          = {http://biorxiv.org/content/early/2021/09/29/2021.07.11.451967.abstract},
  date         = {2021-01},
  doi          = {10.1101/2021.07.11.451967},
  journaltitle = {bioRxiv},
  pages        = {2021.07.11.451967},
  title        = {{SurvBenchmark: comprehensive benchmarking study of survival analysis methods using both omics data and clinical data}},
}

@article{Abner2013,
  abstract     = {A variety of statistical methods are available to investigators for analysis of time-to-event data, often referred to as survival analysis. Kaplan-Meier estimation and Cox proportional hazards regression are commonly employed tools but are not appropriate for all studies, particularly in the presence of competing risks and when multiple or recurrent outcomes are of interest. Markov chain models can accommodate censored data, competing risks (informative censoring), multiple outcomes, recurrent outcomes, frailty, and non-constant survival probabilities. Markov chain models, though often overlooked by investigators in time-to-event analysis, have long been used in clinical studies and have widespread application in other fields.},
  author       = {Abner, Erin L and Charnigo, Richard J and Kryscio, Richard J},
  language     = {eng},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/24818062 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4013002/},
  date         = {2013-10},
  doi          = {10.4172/2155-6180.S1-e001},
  issn         = {2155-6180},
  journaltitle = {Journal of biometrics & biostatistics},
  number       = {e001},
  pages        = {19522},
  title        = {{Markov chains and semi-Markov models in time-to-event analysis}},
  volume       = {Suppl 1},
}

@article{Kwakkel2010,
  author       = {Kwakkel, Jan and Walker, Warren and Marchau, Vincent},
  date         = {2010-11},
  doi          = {10.1504/IJTPM.2010.036918},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Kwakkel, Walker, Marchau - 2010 - Classifying and communicating uncertainties in model-based policy analysis.pdf:pdf},
  journaltitle = {International Journal of Technology Policy and Management},
  pages        = {299--315},
  title        = {{Classifying and communicating uncertainties in model-based policy analysis}},
  volume       = {10},
}

@misc{pkgcoxboost,
  author    = {Binder, Harald},
  publisher = {CRAN},
  date      = {2013},
  title     = {{CoxBoost: Cox models by likelihood based boosting for a single survival endpoint or competing risks}},
}

@article{Ishwaran2011,
  author       = {Ishwaran, Hemant and Kogalur, Udaya B and Chen, Xi and Minn, Andy J},
  url          = {http://arxiv.org/abs/1010.4784},
  date         = {2011},
  doi          = {10.1002/sam},
  eprint       = {1010.4784},
  eprinttype   = {arXiv},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Ishwaran et al. - 2011 - Random Survival Forests for High-Dimensional Data.pdf:pdf},
  isbn         = {9781634393973},
  issn         = {09574174},
  journaltitle = {Statistical Analysis and Data Mining},
  keywords     = {forests,maximal subtree,minimal depth,random forests,random survival forests,survival forests,trees,variable selection,vimp},
  number       = {1},
  pages        = {115--132},
  title        = {{Random Survival Forests for High-Dimensional Data}},
  volume       = {4},
}

@article{Imai2021,
  author       = {Imai, N and Hogan, A B and Williams, L and Cori, A and Mangal, T D and Winskill, P and Whittles, L K and Watson, O J and Knock, E S and Baguelin, M and Perez-Guzman, P N and Gaythorpe, K A M and Sonabend, R and Ghani, A C and Ferguson, N M},
  url          = {https://wellcomeopenresearch.org/articles/6-185/v1},
  date         = {2021},
  doi          = {10.12688/wellcomeopenres.16992.1},
  journaltitle = {Wellcome Open Research},
  number       = {185},
  title        = {{Interpreting estimates of coronavirus disease 2019 (COVID-19) vaccine efficacy and effectiveness to inform simulation studies of vaccine impact: a systematic review [version 1; peer review: awaiting peer review]}},
  volume       = {6},
}

@article{Vock2016,
  abstract     = {Models for predicting the probability of experiencing various health outcomes or adverse events over a certain time frame (e.g., having a heart attack in the next 5years) based on individual patient characteristics are important tools for managing patient care. Electronic health data (EHD) are appealing sources of training data because they provide access to large amounts of rich individual-level data from present-day patient populations. However, because EHD are derived by extracting information from administrative and clinical databases, some fraction of subjects will not be under observation for the entire time frame over which one wants to make predictions; this loss to follow-up is often due to disenrollment from the health system. For subjects without complete follow-up, whether or not they experienced the adverse event is unknown, and in statistical terms the event time is said to be right-censored. Most machine learning approaches to the problem have been relatively ad hoc; for example, common approaches for handling observations in which the event status is unknown include (1) discarding those observations, (2) treating them as non-events, (3) splitting those observations into two observations: one where the event occurs and one where the event does not. In this paper, we present a general-purpose approach to account for right-censored outcomes using inverse probability of censoring weighting (IPCW). We illustrate how IPCW can easily be incorporated into a number of existing machine learning algorithms used to mine big health care data including Bayesian networks, k-nearest neighbors, decision trees, and generalized additive models. We then show that our approach leads to better calibrated predictions than the three ad hoc approaches when applied to predicting the 5-year risk of experiencing a cardiovascular adverse event, using EHD from a large U.S. Midwestern healthcare system.},
  author       = {Vock, David M and Wolfson, Julian and Bandyopadhyay, Sunayan and Adomavicius, Gediminas and Johnson, Paul E and Vazquez-Benitez, Gabriela and O'Connor, Patrick J},
  url          = {http://www.sciencedirect.com/science/article/pii/S1532046416000496},
  date         = {2016},
  doi          = {https://doi.org/10.1016/j.jbi.2016.03.009},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Vock et al. - 2016 - Adapting machine learning techniques to censored time-to-event health record data A general-purpose approach using.pdf:pdf},
  issn         = {1532-0464},
  journaltitle = {Journal of Biomedical Informatics},
  keywords     = {Censored data,Electronic health data,Inverse probability weighting,Machine learning,Risk prediction,Survival analysis,censoring,composition,machine learning,reduction,survival},
  pages        = {119--131},
  title        = {{Adapting machine learning techniques to censored time-to-event health record data: A general-purpose approach using inverse probability of censoring weighting}},
  volume       = {61},
}

@article{Mayr2016,
  abstract     = {When constructing new biomarker or gene signature scores for time-to-event outcomes, the underlying aims are to develop a discrimination model that helps to predict whether patients have a poor or good prognosis and to identify the most influential variables for this task. In practice, this is often done fitting Cox models. Those are, however, not necessarily optimal with respect to the resulting discriminatory power and are based on restrictive assumptions. We present a combined approach to automatically select and fit sparse discrimination models for potentially high-dimensional survival data based on boosting a smooth version of the concordance index (C-index). Due to this objective function, the resulting prediction models are optimal with respect to their ability to discriminate between patients with longer and shorter survival times. The gradient boosting algorithm is combined with the stability selection approach to enhance and control its variable selection properties.},
  author       = {Mayr, Andreas and Hofner, Benjamin and Schmid, Matthias},
  url          = {https://doi.org/10.1186/s12859-016-1149-8},
  date         = {2016},
  doi          = {10.1186/s12859-016-1149-8},
  file         = {:Users/raphaelsonabend/Library/Application Support/Mendeley Desktop/Downloaded/Mayr, Hofner, Schmid - 2016 - Boosting the discriminatory power of sparse survival models via optimization of the concordance index and.pdf:pdf},
  issn         = {1471-2105},
  journaltitle = {BMC Bioinformatics},
  number       = {1},
  pages        = {288},
  title        = {{Boosting the discriminatory power of sparse survival models via optimization of the concordance index and stability selection}},
  volume       = {17},
}

@incollection{Foss2024,
  author    = {Foss, Natalie and Kotthoff, Lars},
  editor    = {Bischl, Bernd and Sonabend, Raphael and Kotthoff, Lars and Lang, Michel},
  publisher = {CRC Press},
  url       = {https://mlr3book.mlr-org.com/data_and_basic_modeling.html},
  booktitle = {Applied Machine Learning Using {m}lr3 in {R}},
  date      = {2024},
  title     = {Data and Basic Modeling},
}

@incollection{Casalicchio2024,
  author    = {Casalicchio, Giuseppe and Burk, Lukas},
  editor    = {Bischl, Bernd and Sonabend, Raphael and Kotthoff, Lars and Lang, Michel},
  publisher = {CRC Press},
  url       = {https://mlr3book.mlr-org.com/chapters/chapter3/evaluation_and_benchmarking.html},
  booktitle = {Applied Machine Learning Using {m}lr3 in {R}},
  date      = {2024},
  title     = {Evaluation and Benchmarking},
}

@incollection{Becker2024,
  author    = {Becker, Marc and Schneider, Lennart and Fischer, Sebastian},
  editor    = {Bischl, Bernd and Sonabend, Raphael and Kotthoff, Lars and Lang, Michel},
  publisher = {CRC Press},
  url       = {https://mlr3book.mlr-org.com/hyperparameter_optimization.html},
  booktitle = {Applied Machine Learning Using {m}lr3 in {R}},
  date      = {2024},
  title     = {Hyperparameter Optimization},
}

@book{Kuhn2023,
  author = {Kuhn, Max and Silge, Julia},
  url    = {https://www.tmwr.org/},
  date   = {2023},
  title  = {Tidy Modeling with {R}},
}

@book{Geron2019,
  author    = {Géron, Aurélien},
  publisher = {O'Reilly},
  url       = {https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/},
  date      = {2019},
  isbn      = {9781492032649},
  title     = {Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition},
}

@article{Bischl2012,
  author       = {Bischl, Bernd and Mersmann, O. and Trautmann, H. and Weihs, C.},
  url          = {https://doi.org/10.1162/EVCO\_a\_00069},
  date         = {2012-06},
  doi          = {10.1162/EVCO_a_00069},
  eprint       = {https://direct.mit.edu/evco/article-pdf/20/2/249/1494436/evco\_a\_00069.pdf},
  issn         = {1063-6560},
  journaltitle = {Evolutionary Computation},
  number       = {2},
  pages        = {249--275},
  title        = {{Resampling Methods for Meta-Model Validation with Recommendations for Evolutionary Computation}},
  volume       = {20},
}

@article{Probst2019,
  author       = {Probst, Philipp and Boulesteix, Anne-Laure and Bischl, Bernd},
  url          = {http://jmlr.org/papers/v20/18-444.html},
  date         = {2019},
  journaltitle = {Journal of Machine Learning Research},
  number       = {53},
  pages        = {1--32},
  title        = {Tunability: Importance of Hyperparameters of Machine Learning Algorithms},
  volume       = {20},
}

@article{Efron1997,
  author       = {Efron, Bradley and Tibshirani, Robert},
  publisher    = {[American Statistical Association, Taylor & Francis, Ltd.]},
  url          = {http://www.jstor.org/stable/2965703},
  date         = {1997},
  issn         = {01621459},
  journaltitle = {Journal of the American Statistical Association},
  number       = {438},
  pages        = {548--560},
  title        = {Improvements on Cross-Validation: The .632+ Bootstrap Method},
  urldate      = {2024-08-04},
  volume       = {92},
}

@article{Gray1988,
  author       = {Gray, Robert J.},
  publisher    = {Institute of Mathematical Statistics},
  url          = {https://doi.org/10.1214/aos/1176350951},
  date         = {1988},
  doi          = {10.1214/aos/1176350951},
  journaltitle = {The Annals of Statistics},
  keywords     = {$G^\rho$ tests,Censored data,counting processes,Martingales},
  number       = {3},
  pages        = {1141--1154},
  title        = {{A Class of $K$-Sample Tests for Comparing the Cumulative Incidence of a Competing Risk}},
  volume       = {16},
}

@article{Schmid2016,
  author       = {Schmid, Matthias and Wright, Marvin and Ziegler, Andreas},
  date         = {2016-07},
  doi          = {10.1016/j.eswa.2016.07.018},
  journaltitle = {Expert Systems with Applications},
  title        = {On the use of Harrell's C for clinical risk prediction via random survival forests},
  volume       = {63},
}

@article{Burk2024,
  author     = {Burk, Lukas and Zobolas, John and Bischl, Bernd and Bender, Andreas and Wright, Marvin N. and Sonabend, Raphael},
  url        = {http://arxiv.org/abs/2406.04098},
  date       = {2024-06},
  eprint     = {2406.04098},
  eprinttype = {arXiv},
  title      = {{A Large-Scale Neutral Comparison Study of Survival Models on Low-Dimensional Data}},
}

@misc{Hornung2023,
  author      = {Hornung, Roman and Nalenz, Malte and Schneider, Lennart and Bender, Andreas and Bothmann, Ludwig and Bischl, Bernd and Augustin, Thomas and Boulesteix, Anne-Laure},
  url         = {https://arxiv.org/abs/2310.15108},
  date        = {2023},
  eprint      = {2310.15108},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  title       = {Evaluating machine learning models in non-standard settings: An overview and new findings},
}

@article{Erdem2022,
  author       = {Erdem, Sabri and Ipek, Fulya and Bars, Aybars and Genç, Volkan and Erpek, Esra and Mohammadi, Shabnam and Alt{ı}nata, An{ı}l and Akar, Servet},
  publisher    = {British Medical Journal Publishing Group},
  url          = {https://bmjopen.bmj.com/content/12/2/e055562},
  date         = {2022},
  doi          = {10.1136/bmjopen-2021-055562},
  eprint       = {https://bmjopen.bmj.com/content/12/2/e055562.full.pdf},
  issn         = {2044-6055},
  journaltitle = {BMJ Open},
  number       = {2},
  title        = {Investigating the effect of macro-scale estimators on worldwide COVID-19 occurrence and mortality through regression analysis using online country-based data sources},
  volume       = {12},
}

@article{Benavoli2017,
  author       = {Benavoli, Alessio and Corani, Giorgio and Demšar, Janez and Zaffalon, Marco},
  url          = {http://jmlr.org/papers/v18/16-305.html},
  date         = {2017},
  journaltitle = {Journal of Machine Learning Research},
  number       = {77},
  pages        = {1--36},
  title        = {Time for a Change: a Tutorial for Comparing Multiple Classifiers Through Bayesian Analysis},
  volume       = {18},
}

@inbook{Simon2007,
  author    = {Simon, Richard},
  editor    = {Dubitzky, Werner and Granzow, Martin and Berrar, Daniel},
  location  = {Boston, MA},
  publisher = {Springer US},
  url       = {https://doi.org/10.1007/978-0-387-47509-7_8},
  booktitle = {Fundamentals of Data Mining in Genomics and Proteomics},
  date      = {2007},
  doi       = {10.1007/978-0-387-47509-7_8},
  isbn      = {978-0-387-47509-7},
  pages     = {173--186},
  title     = {Resampling Strategies for Model Assessment and Selection},
}

@article{vanGeloven2022,
  author       = {van Geloven, Nan and Giardiello, Daniele and Bonneville, Edouard F and Teece, Lucy and Ramspek, Chava L and van Smeden, Maarten and Snell, Kym I E and van Calster, Ben and Pohar-Perme, Maja and Riley, Richard D and Putter, Hein and Steyerberg, Ewout},
  publisher    = {BMJ Publishing Group Ltd},
  url          = {https://www.bmj.com/content/377/bmj-2021-069249},
  date         = {2022},
  doi          = {10.1136/bmj-2021-069249},
  eprint       = {https://www.bmj.com/content/377/bmj-2021-069249.full.pdf},
  journaltitle = {BMJ},
  title        = {Validation of prediction models in the presence of competing risks: a guide through modern methods},
  volume       = {377},
}

@article{Morales-Hernandez2023,
  author       = {Morales-Hernández, Alejandro and {Van Nieuwenhuyse}, Inneke and {Rojas Gonzalez}, Sebastian},
  url          = {https://doi.org/10.1007/s10462-022-10359-2},
  date         = {2023},
  doi          = {10.1007/s10462-022-10359-2},
  issn         = {1573-7462},
  journaltitle = {Artificial Intelligence Review},
  number       = {8},
  pages        = {8043--8093},
  title        = {{A survey on multi-objective hyperparameter optimization algorithms for machine learning}},
  volume       = {56},
}

@article{Monterrubio2024,
author = {Monterrubio-Gómez, Karla and Constantine-Cooke, Nathan and Vallejos, Catalina A.},
title = {A review on statistical and machine learning competing risks methods},
journal = {Biometrical Journal},
volume = {66},
number = {2},
pages = {2300060},
keywords = {competing risks, risk prediction, survival analysis, time-to-event data},
doi = {https://doi.org/10.1002/bimj.202300060},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.202300060},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.202300060},
year = {2024}
}

@article{Austin2022,
author = {Austin, Peter C and Putter, Hein and Giardiello, Daniele and van Klaveren, David},
doi = {10.1186/s41512-021-00114-6},
issn = {2397-7523},
journal = {Diagnostic and Prognostic Research},
number = {1},
pages = {2},
title = {{Graphical calibration curves and the integrated calibration index (ICI) for competing risk models}},
url = {https://doi.org/10.1186/s41512-021-00114-6},
volume = {6},
year = {2022}
}

@article{Wolbers2009,
author = {Wolbers, Marcel and Koller, Michael T and Witteman, Jacqueline C M and Steyerberg, Ewout W},
issn = {1044-3983},
journal = {Epidemiology},
number = {4},
title = {{Prognostic Models With Competing Risks: Methods and Application to Coronary Risk Prediction}},
url = {https://journals.lww.com/epidem/fulltext/2009/07000/prognostic_models_with_competing_risks__methods.14.aspx},
volume = {20},
year = {2009}
}
