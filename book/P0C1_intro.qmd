---
abstract: TODO (150-200 WORDS)
---

::: {.content-visible when-format="html"}
{{< include _macros.tex >}}
:::

# Introduction {#sec-intro}

{{< include _wip_minor.qmd >}}

In the broadest sense, survival analysis is concerned with predicting the time until an event occurs.
This definition will be refined in this chapter and Part I.
Survival analysis has important applications to fields that directly impact day-to-day life, including healthcare, finance, and engineering.
Machine learning techniques can identify patterns and make predictions on unseen data, even in very large datasets with many covariates and/or large sample sizes.
Despite this, the canonical machine learning texts focus almost entirely on regression and classification [@Bishop2006; @Hastie2001; @Hastie2013].
There are also excellent books dedicated to survival analysis, such as @Collett2014 and @KalbfleischPrentice1973, but they do not include modern machine learning models.
Using regression or classification models to solve problems based on survival datasets can lead to biased results as they poorly capture 'censored' observations - those that do not experience the event of interest within an observation window.
This book is intended to bridge the gap between survival analysis and machine learning to formalize and demystify the combined field of 'machine learning survival analysis'.

## Defining Machine Learning Survival Analysis

This chapter highlights the importance of survival analysis and why predictions in a survival setting differ from regression and classification.
Some motivating examples are first provided and then important concepts for survival analysis and machine learning are introduced.

### Survival analysis

The term survival analysis highlights the field's close relation to medical statistics and in particular predicting survival times (the time until death).
However, this is certainly not the only application of the field.
In engineering, the field is often referred to as _reliability analysis_, as a common task is predicting the reliability of a component in a machine, for example predicting when an engine in a plane needs to be replaced.
In economics, the term _duration analysis_ is often found, and in other areas _failure-time analysis_ may also be used.

In @sec-surv, 'survival analysis' is introduced to specifically refer to the case when the event of interest can occur exactly once (e.g., death).
In @sec-eha, 'event history analysis' is defined, which is a generalization of survival analysis to the case when one or more events can occur one or more times.
To align with common practice, the term 'survival analysis' is used throughout the book and context will make clear when the more general event history methods apply.

One of the key aims in this book is to highlight the ubiquitous nature of survival analysis and to encourage more machine learning practitioners to use survival analysis when appropriate.
Machine learning practitioners are likely familiar with regression and classification.
However, there are many cases when survival analysis should be used instead, for example at your local bus stop...

#### Not quite regression: Waiting for a bus {.unnumbered .unlisted}

Every day you are making survival predictions.
For example, by guessing when a bus will arrive at your bus stop.
At first glance, many would call this a regression problem, as regression is tasked with making continuous predictions.
However, transport systems are not perfect and there is a non-zero probability that your bus will never arrive.
Maybe it will break down, be so delayed that you stop waiting, or be taken on a diversion that misses your stop.
Regression analysis assumes the outcome is always observed, so in this example, a regression model would either dismiss these broken down buses, or act like they arrived at a time they did not.
In survival analysis, these possibilities are referred to as censoring events.
An observation (the bus) is *censored* when the event of interest (arriving at your bus stop) does not occur (due to break down, diversion, etc.) or its time of occurrence is unknown (for example, because you stopped waiting).
A defining characteristic of survival analysis is that censored observations are used for model training, not dismissed as missing data.
In other words, whilst your bus did not arrive, you still know that it 'survived' for at least some amount of time (its 'censoring time') before the censoring event.
@fig-bus illustrates these concepts, where the bottom, red bus arrives two minutes after leaving the previous stop.
Whereas it took the top, blue bus four minutes between leaving the previous bus stop and breaking down.

![Demonstrating censoring. The top, blue bus is censored for some reason after 4 minutes, the bottom, red bus arrives at its intended destination after 2 minutes.](Figures/introduction/bus.png){#fig-bus width=400 fig-alt="Two cartoon buses are pictured over a number line from 0 to 5.The top, blue bus has a yellow line from '0' to '4' with a warning sign at the end indicated successful arrival. The bottom, red bus has a green line from '0' to '2' with a check mark at the end indicated successful arrival."}

#### Still not regression: Relative risks {.unnumbered .unlisted}

As well as survival distribution predictions, another common task in survival analysis is to rank observations or separate them into risk groups, for example to allocate resources.
Take the example of triaging patients in an emergency department.
A patient presents with a set of symptoms and is compared to all other patients arriving at the emergency department.
Their symptoms are assessed to understand how urgently they need to receive help and how their level of risk compares to others.
This is again a survival prediction where the event of interest is escalation of care, and reasons for censoring might include the patient leaving before being seen.
As will be seen in @sec-survtsk, risk rankings are mathematically tied to survival distributions.

#### Not quite classification: Survival probabilities {.unnumbered .unlisted}

The bus example highlighted the relationship between survival analysis and regression.
This example examines the close connection to classification.
Support an elderly man is diagnosed with prostate cancer, the oncologist might tell him (in some softer words) "the ten-year survival rate of cancer in 75-year old males is 67.5\%".
This initially appears as a probabilistic classification problem as the clinician is implicitly saying "if we look at who did or did not die from prostate cancer (the outcome), given a sample of similar people (the covariates), then approximately 67.5\% survived".
But in the real-world, data is rarely this clean and there will be many patients in a dataset for whom it is unknown if they did or not die from their cancer 10 years from diagnosis.
Perhaps because they moved abroad, died of another cause, or simply stopped turning up for appointments -- each of these is a censoring event.
Once again, using techniques from survival analysis would allow more precise modelling of these censored observations.
In fact, one can actually use a combination of classification and survival analysis methods through reductions and this will be returned to in @sec-discrete-time-reduction.

### Machine learning survival analysis {#sec-intro-mlsa}

Machine learning is the field of Statistics primarily concerned with building models to either predict outputs from inputs or to learn relationships from data [@Hastie2001].
This book focuses on the supervised learning setting, where the goal is to predict outcomes from labelled training data.

Defining if a model is 'machine learning' is surprisingly difficult, with terms such as 'iterative' and 'loss optimization' being frequently used in often clunky definitions.
This book defines machine learning models as those that:

1. Require intensive model fitting procedures such as iteration;
2. Aim to minimize the difference between predictions and ground truth;
3. Place few assumptions on the underlying data;
4. Trained using some form of loss optimization;
5. Are flexible with tunable hyper-parameters (@sec-ml);
6. Often function as 'black boxes' without simple closed-form expressions that can be easily interpreted.

There are some models that one might think of as 'traditional statistical' models (@sec-classical), that can also be used for machine learning.
These 'traditional' models are low-complexity and are either non-parametric or have a simple closed-form expression with parameters that can be fit with maximum likelihood estimation (or an analogous method).
While these traditional models are usually fast to train and highly interpretable, they can be inflexible and may impose rigid and/or unrealistic assumptions about the underlying distribution of teh data.
Augmenting these models with certain methods (@sec-classical-improving) leads to powerful models that can also be considered 'machine learning' tools; hence their inclusion in this book.

Relative to other areas of supervised learning, development in survival analysis has been slow, as will be seen when discussing models in Part III.
Despite this, development of models has converged on three primary tasks of interest, or _survival problems_, which are defined by the type of prediction the model makes.
The mathematical definition of a machine learning survival analysis task is provided in @sec-survtsk.
Generally, one is encountering a survival problem if training a model on data where censoring is present in order to predict one of (@sec-survtsk):

i. A _relative risk_: The risk of an event taking place (the triage example above).
i. A _survival time_: The time at which the event will take place.
i. A _survival distribution_: The probability distribution over event times (the bus example above).

Each prediction type serves a different purpose and you may require training multiple models to optimally predict each one.
However, as will be seen in @sec-survtsk, there are mathematical methods to transform these tasks between one another.
As an example, an engineer is unlikely to care about the exact time at which a plane engine fails, but they might greatly value knowing when the probability of failure increases above 50\% -- a survival distribution prediction.
Returning to the triage example, a physician cannot process a survival distribution prediction to make urgent decisions, but they could assign limited resources if it is clear that one patient is at substantially greater risk than another -- a relative risk prediction.

When it comes to making distribution predictions, survival analysis stands out again.
Common _distribution defining functions_, functions that uniquely define a probability distribution, are the probability density function (PDF) and cumulative distribution function (CDF).
In survival settings, interpreting the PDF can be counter-intuitive as it is an unconditional quantity that does not account for whether the event has or has not already occurred.
The CDF is the probability that the event has 'already' taken place at $t$, which is opposite to the usual survival prediction: whether the event 'will' take place.
Therefore, survival analysis focuses instead on predicting the *survival function*, which is simply one minus the CDF, and the *hazard function*, which is the conditional rate of the event occurring at $t$ given that the observation has survived up to $t$.

These functions are formally defined in @sec-surv and are visualized in @fig-distrfunctions with a Gompertz distribution, which is often chosen to model adult lifespans.
The figure demonstrates the utility of the survival and hazard functions for survival analysis.
The survival function (bottom right) is a decreasing function from one to zero.
This is the probability of surviving until a given time, $t$, or equivalently the probability that the event of interest has not yet occurred.
The hazard function (top right), starts at zero and is not upper-bounded at one as it is a conditional probability.
Even though the PDF peaks just before 0.5, the hazard function continues to increase as it is conditioned on not having experienced the event, hence the risk of event continues to increase.

![Probability density (top left), hazard (top right), cumulative density (bottom left), and survival (bottom right) functions of a Gompertz(1, 2) distribution.](Figures/introduction/gompertz.png){#fig-distrfunctions fig-alt="Four line graphs. The density plot increases from (0,1) to (0.4, 1.25) and then decreases to (1.5, 0). The hazard smoothly increases from (0, 0) to (1.5, 20). The cumulative density smoothly increases from (0,1) to (1.5, 1) and the survival function is the reverse shape to the cumulative density from (0, 1) to (1.5, 0)."}

<!-- reviewed to here -->
## Censoring and Truncation

As already discussed, censoring is the defining feature of survival analysis.
As well censoring, some survival problems may also deal with 'truncation', which can be thought of as a stricter form of censoring.
In general terms, truncation is the process of *fully* excluding data whereas censoring is the process of *partially* excluding data.
In survival analysis, data is measured over time and censoring or truncation apply to the survival time outcome.
The precise definitions of different types of censoring and truncation are provided in @sec-surv, for now we just provide a non-technical summary of the most common form of both to highlight how prominent they are.

The most common form of censoring is right-censoring (@fig-intro-censoring, middle), this occurs when the true survival time is 'to the right of', i.e., larger than (if you imagine a number line), the observed censoring time.
The examples above are all forms of right-censoring.
A canonical example of right-censoring is as follows: if a medical study lasts five years and patients are censored if they are alive at the end of the study, then all patients alive at five years are right-censored as their true survival time must be greater than five years.

In contrast, the most common form of truncation is left-truncation, which can occur in a few different situations and results in data before the truncation time either being partially or fully removed (@fig-intro-censoring, bottom).
The examples below look at partial left-truncation, complete truncation, and using partial truncation to fix biases.

**Example 1: Partial left-truncation**
Say a study begins in 2024 to predict the time until death for a patient diagnosed with tuberculosis (TB) treated with a novel treatment.
The study consists of many individuals, however some of these already had TB before 2024.
Left-truncation would occur if the study excludes data about the individuals who had TB before 2024, i.e., treated them as if they were diagnosed with TB on entry to the study - this is partial left-truncation.
Excluding this data would bias the results, especially if the novel treatment is most effective for people in the early-stages of the disease.
One common way to account for this bias would be to record the *initial* date of diagnosis and the date the treatment began.

**Example 2: Complete left-truncation**
Consider researchers exploring if survival models can be used to more accurately predict the exact day a pregnant person will give birth, with abortion and miscarriage treated as competing risks.
Unfortunately, there will be cases of miscarriage occurring before a person finds out they are pregnant, which means data about their pregnancy will never be collected.
The first, more obvious, consequence of this omission is that the true number of miscarriages will be underestimated.
The second consequence is that the time until miscarriage outcome will be overestimated, as earlier miscarriages are not recorded so the outcome time is skewed towards later ones.
To demonstrate why this is problematic, if someone discovers they are pregnant early and this new model is used to predict the outcome of their pregnancy, then even if it correctly predicts they have a miscarriage, it is likely to predict this happens later than it actually does, which could have an effect on any potential treatment (physical and/or mental) that may need to be allocated.
The bias in this example is particularly hard to address as the data is completely truncated - the subjects are omitted without the researchers being aware of them.

![Censoring and truncation. Top row: no censoring or truncation, a patient is diagnosed with a disease, enrolled in a study, and dies during the observed period, their survival time is known. Middle: right censoring, a patient is diagnosed and enrolled in a study, they leave the study either due it ending or for some other reason, this is recorded as their censoring time and they die at some unknown later time to the right of the censoring time. Right: left truncation, a patient is diagnosed with a disease but not enrolled to a study until some time afterwards, their data before this time has not been collected and is completely unknown, they then die during the study period.](Figures/introduction/censoring.png){#fig-intro-censoring fig-alt="TODO"}

**Example 3: Immortal time bias**
Immortal time bias occurs when an observation in the data is guaranteed to survive a period of time (hence being 'immortal' in that time) by virtue of the study design.
For example, say a randomised controlled trial is conducted to test if a novel chemotherapy treatment improves survival rates from a given cancer.
The trial is split into two arms, one for existing treatments and one for the novel treatment, and a patient is eligible for the novel treatment only if they have been living with cancer for two years and no other treatment has been shown to work.
Finally, patients are enrolled into the study from the date of diagnosis.
This dataset now includes immortal time bias as patients can only be in the novel treatment arm if they have already survived two years, whereas patients in the other arm have no such restriction.
Hence, even if the novel treatment has no benefit, patients in the novel arm are guaranteed to have a better survival outcome (from diagnosis) as they would not have been included if they died before two years, whereas patients may die (even if by chance) in the control arm before this time (@fig-intro-immortal).
To control for this bias, one could improve the study design by considering the time to death given two years of surviving and therefore partially left-truncating data before this time.
Or, one could determine eligibility at the time of diagnosis by randomly assigning patients to each group but only starting the novel treatment at two years.
Detecting immortal time bias is hugely important, especially in randomised controlled trials to ensure novel treatments are not oversold.

Censoring is a mechanism applied at the data collection stage in order to yield more useful results by recording as much data as possible.
Truncation is due to the study design and may either create or remove biases in data collection/curation.
The first Part of this book will continue to demystify the concepts of censoring, truncation, and prediction types.

![Immortal time bias. Top row: a patient is in the control group and ineligible for treatment, they are observed from the date of enrolment and die during the study period. Bottom row: a patient is in the treatment group and is observed once the treatment is administered, therefore they are guaranteed to survive until the treatment is provided, skewing results towards longer survival times.](Figures/introduction/immortal.png){#fig-intro-immortal fig-alt="TODO"}

## Start to finish {#sec-mlsa}

## Reproducibility

This book includes simulations and figures generated in $\Rstats$, the code for any figures or experiments in this book are freely available at [https://github.com/mlsa-book/MLSA](https://github.com/mlsa-book/MLSA) under an MIT license and all content on this website is available under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/).

:::: {.callout-tip icon=false}

## Further reading

* [@Wang2017] provides a light-touch but comprehensive survey of machine learning models for survival analysis.

::::
