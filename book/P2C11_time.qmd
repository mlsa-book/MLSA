---
abstract: TODO (150-200 WORDS)
---

::: {.content-visible when-format="html"}
{{< include _macros.tex >}}
:::

# Evaluating Survival Time {#sec-eval-det}

{{< include _wip_minor.qmd >}}

When it comes to evaluating survival time predictions, there are few measures available at our disposal.
As a result of survival time predictions being uncommon compared to other prediction types (@sec-survtsk), there are limited survival time evaluation measures in the literature.
To our knowledge, there are no specialized 'survival time measures', instead regression measures are used by ignoring censored observations with some possible IPC weighting.

Before presenting these measures, consider what happens when censored observations are discarded.
If censoring is truly independent, occurs randomly, and is *very* limited in the data, then there is little harm in discarding observations and effectively the predictive problem reduces to the regression setting.
However, if censoring is not independent and there are competing risks then there is no meaningful interpretation for cause-specific or all-cause survival times, this is discussed further in next chapter.

## Distance measures

Survival time measures are often referred to as 'distance' measures as they measure the distance between the true, $(t, \delta=1)$, and predicted, $\hatt$, values.
These are presented in turn with brief descriptions of their interpretation.
The measures below can be inflated using an IPC weighting by dividing by $\KMG(t_i)$.
However, evidence suggests that adding this weighting does not improve the measure with respect to ranking if one model is better than another [@Qi2023].

For all measures define $d := \sum_i \delta_i$, which is the number of uncensored observations in the dataset.

**Censoring-ignored mean absolute error, $MAE_C$**

In regression, the mean absolute error (MAE) is a popular measure because it is intuitive to understand as it measures the absolute difference between true and predicted outcomes; hence intuitively one can understand that a model predicting a height of 175cm is clearly better than one predicting a height of 180cm, for a person with true height of 174cm.

$$
MAE_C(\hattt, \tt, \bsdelta) = \frac{1}{d} \sum^d_{i=1} \delta_i|t_i - \hatt_i|
$$

**Censoring-ignored mean squared error**

In comparison to MAE, the mean squared error (MSE), computes the squared differences between true and predicted values.
While the MAE provides a smooth, linear, 'penalty' for increasingly poor predictions (i.e., the difference between an error of predicting 2 vs. 5 is still 3), but the square in the MSE means that larger errors are quickly magnified (so the difference in the above example is 9).
By taking the mean over all predictions, the effect of this inflation is to increase the MSE value as larger mistakes are made.

$$
MSE_C(\hattt, \tt, \bsdelta) = \frac{1}{d}\sum^d_{i=1}\delta_i(t_i - \hatt_i)^2
$$

**Censoring-ignored root mean squared error**

Finally, the root mean squared error (RMSE), is simply the square root of the MSE.
This allows interpretation on the original scale (as opposed to the squared scale produced by the MSE).
Given the inflation effect for the MSE, the RMSE will be larger than the MAE as increasingly poor predictions are made; it is common practice for the MAE and RMSE to be reported together.

$$
RMSE_C(\hattt, \tt, \bsdelta) = \sqrt{MSE_C(\hattt, \tt, \bsdelta)}
$$

## Over- and under-predictions

All of these distance measures assume that the error for an over-prediction ($\hatt > t$) should be equal to an under-prediction ($\hatt < t$), i.e., that it is 'as bad' if a model predicts an outcome time being 10 years longer than the truth compared to being 10 years shorter.
In the survival setting, this assumption is often invalid as it is generally preferred for models to be overly cautious, hence to predict negative events to happen sooner (e.g., predict a life-support machine fails after three years not five if the truth is actually four) and to predict positive events to happen later (e.g., predict a patient recovers after four years not two if the truth is actually three).
A simple method to incorporate this imbalance between over- and under-predictions is to add a weighting factor to any of the above measures, for example the $MAE_C$ might become

$$
MAE_C(\hattt, \tt, \bsdelta, \lambda, \mu, \phi) = \frac{1}{d} \sum^m_{i=1} \delta_i|(t_i - \hatt_i) [\lambda\II(t_i>\hatt_i) + \mu\II(t_i<\hatt_i) + \phi\II(t_i=\hatt_i)]|
$$

where $\lambda, \mu, \phi$ are any Real number to be used to weight over-, under-, and exact-predictions, and $d$ is as above.
The choice of these are highly context dependent and could even be tuned (@sec-ml-opt).

## Beyond right-censoring

Given the above measures simply remove censored observations, the same measures can be easily applied to datasets with left-censoring and interval-censoring -- with the same caveats applied.
To handle truncation, the formulae above can be extended to remove truncated events, which will introduce the same biases as removing censored events.

## Conclusion

:::: {.callout-warning icon=false}

## Key takeaways

* There are few measures for evaluating survival time predictions, likely due this being a less popular survival task;
* Simple analogues to regression measures can be created by removing censored observations and optionally adding an IPC weighting;
* Weighting measures to account for over- and under-predictions may be useful in real-world settings.

::::

:::: {.callout-tip icon=false}

## Further reading

* @Qi2023 examine the mean absolute error in more detail and look at alternate weighting schemes.
@Haider2020 discuss hinge losses to incorporate censored observations in calculations.

::::
