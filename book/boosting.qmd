---
abstract: TODO (150-200 WORDS)
---

{{< include _setup.qmd >}}

# Boosting Methods

{{< include _wip.qmd >}}

Boosting is a machine learning strategy that can be applied to any model class.
Similarly to random forests, boosting is an 'ensemble' method that creates a model from a 'committee' of learners.
The committee is formed of 'weak' learners that make poor predictions individually, which creates a 'slow learning' approach (as opposed to 'greedy') that requires many iterations for a model to be a good fit to the data.
Boosting models are similar to random forests in that both make predictions from a large committee of learners.
However the two differ in how this committee is correlated and in how it is combined to a prediction.
In random forests, each decision tree is grown independently and their predictions are combined by a simple mean calculation.
In contrast, weak learners in a boosting model are fit sequentially with errors from one learner used to train the next, predictions are then made by a linear combination of predictions from each learner (@fig-boosting).

## GBMs for Regression {#sec-surv-ml-models-boost-regr}

The best known boosting algorithm is likely AdaBoost  [@Freund1996], which is more generally a Forward Stagewise Additive Model (FSAM) with an exponential loss  [@Hastie2001].
Today, the most widely used boosting model is the Gradient Boosting Machine (GBM)  [@Friedman2001].

@fig-boosting illustrates the process of training a GBM in a least-squares regression setting:

1. A weak learner, $f_1$, often a decision tree of shallow depth is fit on the training data $(\XX, \yy)$.
2. Predictions from the learner, $f_1(\XX)$, are compared to the ground truth, $\yy$, and the residuals are calculated as $\rr_1 = f_1(\XX) - \yy$.
3. The next weak learner, $f_2$, uses the previous residuals for the target prediction, $(\XX, \rr_1)$
4. This is repeated to train $M$ learners, $f_1,...,f_M$

Predictions are then made as $\hat{\yy} = f_1(\XX) + f_2(\XX) + ... + f_M(\XX)$.

![Least squares regression Boosting algorithm where the gradient is calculated as the difference between ground truth and predictions.](Figures/boosting/boosting.png){#fig-boosting fig-alt="TODO"}

This is a simplification of the general gradient boosting algorithm, where the residuals are used to train the next model.
More generally, a suitable, differentiable loss function relating to the problem of interest is chosen and the negative gradient is computed by comparing the predictions in each iteration with the ground truth.
Residuals can be used in the regression case as these are proportional to the negative gradient of the mean squared error.

The algorithm above is also a simplification as no hyper-parameters other than $M$ were included for controlling the algorithm.
In order to reduce overfitting, three common hyper-parameters are utilised:

1. Number of iterations, $M$: This is the basic hyper-parameter for controlling overfitting. The more iterations the better the performance however if $M$ is too large the model will be overfit to the training data. It is common to monitor the model's performance in each iteration using a validation set and stopping training when a set performance is reached or when performance stagnates (i.e., no improvement over a set number of rounds).
2. Shrinkage parameter, $\nu$: The shrinkage parameter, also known as the learning rate, slow down the learning rate of the model by scaling down the contribution from each weak learner. This parameter trades-off against $M$ with a larger value of $M$ requiring a smaller $\nu$ and vice versa. Commonly $\nu$ is set around $0.1$ or below.
3. Subsampling proportion, $\phi$: Sampling a fraction, $\phi$, of the training data at each iteration can improve performance and reduce runtime [@Hastie2001], with $\phi = 0.5$ often used. Motivated by the success of bagging in random forests, stochastic gradient boosting  [@Friedman1999] randomly samples the data in each iteration. It appears that subsampling performs best when also combined with shrinkage [@Hastie2001] and as with the other hyper-parameters, selection of $\phi$ is usually performed by nested cross-validation.

As well as these parameters, the underlying weak learner hyper-parameters are also commonly tuned.
If using a decision tree, then it is usual to restrict the number of terminal nodes in the tree to be between $4$ and $8$, which corresponds to two or three splits in the tree.
Including these hyper-parameters, the general gradient boosting machine algorithm is as follows:

1. $g_0 \gets \text{ Initial guess}$
2. **For** $m = 1,...,M$:
3. \ \ \ \ \ $\dtrain^* \gets \text{ Randomly sample } \dtrain \text{ with probability } \phi$
4. \ \ \ \ \ $r_{im} \gets -[\frac{\partial L(y_i, g_{m-1}(X_i))}{\partial g_{m-1}(X_i)}], \forall i \in \{i: X_i \in \dtrain^*\}$
5. \ \ \ \ \ Fit a weak learner, $h_m$, to $(\XX, \rr_m)$
6. \ \ \ \ \ $g_m \gets g_{m-1} + \nu h_m$
7. **end For**
8.  **return** $\hatg = g_M$

Note:

1. The initial guess, $g_0$, is often the mean of $y$ for regression problems but can also simply be $0$.
2. Line 4 is the calculation of the negative gradient, which is equivalent to calculating the residuals in a regression problem with the mean squared error loss.
3. Lines 5-6 differ between implementations, with some fitting multiple weak learners and selecting the one that minimizes a simple optimization problem. The version above is simplest to implement and quickest to run, whilst still providing good model performance.

### Training a GBM {.unnumbered .unlisted}

Pseudo-code for training a componentwise GBM is presented in (@alg-surv-gbm). The term 'componentwise' is explained fully below, only this variation of GBM is presented as it is the most common in implementation  [@pkggbm; @pkgmboost]. Line 1: the initial function is initialized as $g_0 = 0$;\footnote{Some algorithms may instead initialize $g_0$ by finding the value that minimises the given loss function, however setting $g_0 = 0$ appears to be the most common practice for componentwise GBMs.} Line 2: iterate over boosting steps $m = 1,...,M$ and; Line 3: randomly sample the training data, $\dtrain$, to a smaller sample, $\dtrain^*$, this may be ignored if $\phi = 1$; Line 4: for all training observations in the reduced dataset, $i \in \{i:X_i \in \dtrain^*\}$, compute the negative gradient, $r_{im}$, of the differentiable loss function, $L$, with respect to predictions from the previous iteration, $g_{m-1}(X_i)$; Line 5: fit one weak learner for each feature, $j = 1,...,p$, in the training data, where the feature, $X_{;j}$, is the single covariate and $r_{im}$ are the labels; Line 6: select the optimal weak learner as the one that minimises the squared error between the prediction and the true gradient; Line 7: update the fitted model by adding the optimal weak learner with a shrinkage penalty, $\nu$; Line 9: return the model updated in the final iteration as the fitted GBM.



### Predicting with a GBM {.unnumbered .unlisted}

In general, predictions from a trained GBM are simple to compute as the fitted model (and all individual weak learners) take the same inputs, which are passed sequentially to each of the weak learners. In (@alg-surv-gbm), the fitted GBM is a single model, which is a linear combination of weak learners. Instead one could think of the returned model as a collection of the optimal weak learners, i.e. let $w_{m;j^*}$ be the optimal weak learner from iteration $m$ and let the fitted GBM (Line 9 (@alg-surv-gbm)) be $\hatg := \{w_{m;j^*}\}^M_{m=1}$.\footnote{This formulation is computationally and mathematically identical to the formulation in (@alg-surv-gbm) and is practically more convenient for implementation, indeed this is the implementation in $\pkg{mboost}$  [@pkgmboost]. Despite this, the formulation in (@alg-surv-gbm) is common in the literature, which often conflates model training and predicting.} With this formulation, making predictions from the GBM can be demonstrated simply in (@alg-surv-gbm-pred).

<!-- \begin{algorithm}[H]
\caption{Predicting from a Gradient Boosting Machine. \\
**Input** Fitted GBM, $\hatg := \{w_{m;j^*}\}^M_{m=1}$, trained with step-size $\nu$. Testing data $X^* \sim \calX$. \\
**Output** Prediction, $\hatY \sim \calY$.}
\begin{algorithmic}[1]
\State Initialize $\hatY = 0$
\For{$m = 1,...,M$}
\State $\hatY \gets \hatY + \nu w_{m;j^*}(X^*)$
\EndFor
\Return $\hatY$
\end{algorithmic}
\end{algorithm} -->
<!-- {#alg-surv-gbm-pred} -->

The biggest advantages of boosting are firstly relatively few hyper-parameters, which all have a meaningful and intuitive interpretation, and secondly its modular nature means that, like random forests, relatively few parts need to be updated to derive a novel model. First the model components will be discussed and then the hyper-parameters. Once this has been established, deriving survival variants can be simply presented.

### Losses and Learners

### Losses {.unnumbered .unlisted}

Building a GBM requires selection of the loss to minimise, $L$, selection of weak learners, $w_j$, and a method to compare the weak learners to the loss gradient. The only constraint in selecting a loss, $L$, is that it must be differentiable with respect to $g(X)$  [@Hastie2001]. Of course a sensible loss should be chosen (a classification loss should not be used for regression) and different choices of losses will optimise different tasks. $L_2$-losses have been demonstrated to be effective for regression boosting, especially with high-dimensional data  [@Buhlmann2003]; this is referred to as $L_2$-boosting.

### Weak Learners {.unnumbered .unlisted}

(@alg-surv-gbm) is specifically a *componentwise* GBM  [@Buhlmann2003], which means that each of the $p$ weak learners is fit on a single covariate from the data. This method simplifies selecting the possible choices for the weak learners to selecting the class of weak learner (below). Additionally, componentwise GBMs provide a natural and interpretable feature selection method as selecting the optimal learner ((@alg-surv-gbm), line 6) corresponds to selecting the feature that minimises the chosen loss in iteration $m$.

Only three weak, or 'base', learner classes are commonly used in componentwise GBMs  [@pkgmboost; @Wang2010]. These are linear least squares  [@Friedman2001], smoothing splines  [@Buhlmann2003], and decision stumps  [@Buhlmann2003; @Friedman2001]. Let $L$ be a loss with negative gradient for observation $i$ in the $m$th iteration, $r_{im}$, and let $\dtrain$ be the usual training data. For linear least squares, an individual weak learner is fit by  [@Friedman2001; @Wang2010],
$$
w_j(\dtrain) = X_{;j}\frac{\sum^n_{i=1} X_{ij}r_{im}}{\sum^n_{i=1} (X_{ij})^2}
$$
For smoothing splines, usually cubic splines are implemented, these fit weak learners as the minimisers of the equation  [@Buhlmann2003],
$$
w_j := \argmin_{g \in \calG} \mean{(r_{im} - g(X_{ij}))^2} + \lambda \int (g''(u))^2 du
$$
where $g''$ is the second derivative of $g$, $\calG$ is the set of functions, \\ $\calG := \{g: g \text{ is twice continuously differentiable and } \int (g''(x))^2 dx < \infty\}$, and $\lambda$ is a hyper-parameter usually chosen so that the number of degrees of freedom, df, is small, with df $\approx 4$ suggested  [@Buhlmann2003; @Schmid2008a; @Wang2010].

Finally for decision stumps ((@fig-surv-stump)), a decision tree, $w_j$, is grown (@alg-dt-fit) on $(X_{;j}, r_m)$ to depth one (equivalently to two terminal nodes) for each of the $j = 1,...,p$ covariates  [@Friedman2001].

<!-- \begin{figure}[H]
\centering
\begin{tikzpicture}[state/.style={circle, draw, minimum size=15mm}]
\node (t0) [state]  {Root};

\node (t4) [state, draw = none, below=of t0, yshift = 5mm]{};

\node (t1) [state,left=of t4] {Node 1};
\node (t2) [state,right=of t4, label={[label distance=1.0cm]0: - Depth 1}] {Node 2};

\node (t3) [state, draw = none, above=of t2, yshift = -5mm, label={[label distance=1.0cm]0: - Depth 0}]{};

\path[-]
   (t0)  edge (t1)
   (t0)  edge (t2);

\draw [dashed] (-5,1) -- (5,1);
\draw [dashed] (-5,-1) -- (5, -1);
\draw [dashed] (-5,-3) -- (5,-3);
\end{tikzpicture}
\caption[A decision stump]{A decision tree of depth one, known as a decision stump. The root layer is separated at depth 0 from the terminal nodes at depth 1. A decision stump is defined by a decision tree with a single split at the root node.}
 {#fig-surv-stump}
\end{figure} -->

### Hyper-Parameters

The hyper-parameters in (@alg-surv-gbm) are the 'step-size', $\nu$, the sampling fraction, $\phi$, and the number of iterations, $M$.

### Number of iterations, $M$ {.unnumbered .unlisted}

The number of iterations is often claimed to be the most important hyper-parameter in GBMs and it has been demonstrated that as the number of iterations increases, so too does the model performance (with respect to a given loss on test data) up to a certain point of overfitting  [@Buhlmann2006; @Hastie2001; @Schmid2008a]. This is an intuitive result as the foundation of boosting rests on the idea that weak learners can slowly be combined to form a single powerful model. This is especially true in componentwise GBMs as time is required to learn which features are important. Finding the optimal value of $M$ is critical as a value too small will result in poor predictions, whilst a value too large will result in model overfitting. Two primary methods have been suggested for finding the optimal value of $M$. The first is to find the $M \in \PNaturals$ that minimises a given measure based on the AIC  [@Akaike1974], the second is the 'usual' empirical selection by nested cross-validation. In practice the latter method is usually employed.

### Step-size, $\nu$ {.unnumbered .unlisted}

The step-size parameter ((@alg-surv-gbm), line 7), $\nu$, is a shrinkage parameter that controls the contribution of each weak learner at each iteration. Several studies have demonstrated that GBMs perform better when shrinkage is applied and a value of $\nu = 0.1$ is often suggested  [@Buhlmann2007; @Hastie2001; @Friedman2001; @Lee2018; @Schmid2008a]. The optimal values of $\nu$ and $M$ depend on each other, such that smaller values of $\nu$ require larger values of $M$, and vice versa. This is intuitive as smaller $\nu$ results in a slower learning algorithm and therefore more iterations are required to fit the model. Accurately selecting the $M$ parameter is generally considered to be of more importance, and therefore a value of $\nu$ is often chosen heuristically (e.g. the common value of $0.1$) and then $M$ is tuned by cross-validation and/or early-stopping.




## GBMs for Survival Analysis {#sec-surv-ml-models-boost-surv}

In a componentwise GBM framework, adapting boosting to survival analysis requires only selecting a sensible choice of loss function $L$. Therefore fitting and predicting algorithms for componentwise survival GBMs are not discussed as these are fully described in algorithms (@alg-surv-gbm) and (@alg-surv-gbm-pred) respectively. However, some GBMs in this section are not componentwise and therefore require some more detailed consideration. Interestingly, unlike other machine learning algorithms that historically ignored survival analysis, early GBM papers considered boosting in a survival context  [@Ridgeway1999]; though there appears to be a decade gap before further considerations were made in the survival setting. After that period, several developments by Binder, Schmid, and Hothorn, adapted componentwise GBMs to a framework suitable for survival analysis. Their developments are covered exhaustively in the R packages $\pkg{gbm}$  [@pkggbm] and $\pkg{mboost}$  [@pkgmboost]. This survey continues with the predict type taxonomy.

### Cox Survival Models

All survival GBMs make ranking predictions and none are able to directly predict survival distributions. However, the GBMs discussed in this section all have natural compositions to distributions as they are modelled in the semi-parametric proportional hazards framework (@sec-car). The models discussed in the next section can also be composed to distributions though the choice of composition is less clear and therefore they are listed as pure 'ranking' models.

**GBM-COX** {#mod-gdcox} {#mod-gbmcox}\\
The 'GBM-COX' aims to predict the distribution of data following the PH assumption by estimating the coefficients of a Cox model in a boosting framework  [@Ridgeway1999]. The model attempts to predict $\hatg(X^*) = \hat{\eta} := X^*\hat{\beta}$, by minimising a suitable loss function. As the model assumes a PH specification, the natural loss to optimise is the Cox partial likelihood  [@Cox1972; @Cox1975], more specifically to minimise the negative partial log-likelihood, $-l$, where

$$
l(\beta) = \sum^n_{i=1} \Delta_i \Big[\eta_i \ - \ \log\Big(\sum^n_{j \in \calR_{t_i}} \exp(\eta_i)\Big)\Big]
$$ {#eq-surv-logpartial}
where $\calR_{t_i}$ is the set of patients at risk at time $t_i$ and $\eta_i = X_i\beta$. The gradient of $-l(\beta)$ at iteration $m$ is
$$
r_{im} := \Delta_i - \sum^n_{j=1} \Delta_j \frac{\II(T_i \geq T_j) \exp(g_{m-1}(X_i))}{\sum_{k \in \calR_{t_j}} \exp(g_{m-1}(X_k))}
$$ {#eq-surv-partialgrad}
where $g_{m-1}(X_i) = X_i\beta_{m-1}$.

(@alg-surv-gbm) now follows with the loss $L := -l(\beta)$.\footnote{Early implementations and publications of the GBM algorithm  [@Friedman1999; @Friedman2001] included an additional step to the algorithm in which a step size is estimated by line search. More recent research has determined that this additional step is unneccesary  [@Buhlmann2007] and the line search method does not appear to be used in practice.}

The GBM-COX is implemented in $\pkg{mboost}$  [@pkgmboost] and has been demonstrated to perform well even when the data violates the PH assumption  [@Johnson2011]. Despite being a black-box, GBMs are well-understood and individual weak learners are highly interpretable, thus making GBMs highly transparent. Several well-established software packages implement GBM-COX and those that do not tend to be very flexible with respect to custom implementations.

\noindent **CoxBoost** {#mod-coxboost}\\
The CoxBoost algorithm boosts the Cox PH by optimising the penalized partial-log likelihood; additionally the algorithm allows for mandatory (or 'forced') covariates  [@Binder2008]. In medical domains the inclusion of mandatory covariates may be essential, either for model interpretability, or due to prior expert knowledge. This is not a feature usually supported by boosting. CoxBoost deviates from (@alg-surv-gbm) by instead using an offset-based approach for generalized linear models  [@Tutz2007]. This model has a non-componentwise and componentwise framework but only the latter is implemented by the authors  [@pkgcoxboost] and discussed here. Let $\calI_{mand}$ be the indices of the mandatory covariates to be included in all iterations, $m = 1,...,M$, then for an iteration $m$ the indices to consider for fitting are the set
$$
 I_m = \{\{1\} \cup \calI_{mand},...,\{p\} \cup \calI_{mand}\} / \{\{j\} \cup \calI_{mand} : j \in \calI_{mand}\}
$$
i.e. in each iteration the algorithm fits a weak learner on the mandatory covariates and one additional (non-mandatory) covariate (hence still being componentwise).

In addition, a penalty matrix $\mathbf{P} \in \Reals^{p \times p}$ is considered such that $P_{ii} > 0$ implies that covariate $i$ is penalized and $P_{ii} = 0$ means no penalization. In practice this is usually a diagonal matrix  [@Binder2008] and by setting $P_{ii} = 0, i \in I_{mand}$ and $P_{ii} > 0, i \not\in I_{mand}$, only optional (non-mandatory) covariates are penalized. The penalty matrix can be allowed to vary with each iteration, which allows for a highly flexible approach, however in implementation a simpler approach is to either select a single penalty to be applied in each iteration step or to have a single penalty matrix  [@pkgcoxboost].

At the $m$th iteration and the $k$th set of indices to consider ($k = 1,...,p$), the loss to optimize is the penalized partial-log likelihood given by
$$
\begin{split}
&l_{pen}(\gamma_{mk}) = \sum^n_{i=1} \Delta_i \Big[\eta_{i,m-1} + X_{i,\calI_{mk}}\gamma^T_{mk}\Big] - \\
&\quad\Delta_i\log\Big(\sum^n_{j = 1} \II(T_j \leq T_i) \exp(\eta_{i,{m-1}} + X_{i, \calI_{mk}}\gamma^T_{mk}\Big) - \lambda\gamma_{mk}\mathbf{P}_{mk}\gamma^T_{mk}
\end{split}
$$

where $\eta_{i,m} = X_i\beta_m$, $\gamma_{mk}$ are the coefficients corresponding to the covariates in $\calI_{mk}$ which is the possible set of candidates for a subset of total candidates $k = 1,...,p$, $\mathbf{P}_{mk}$ is the penalty matrix, and $\lambda$ is a penalty hyper-parameter to be tuned or selected.\footnote{On notation, note that $\mathbf{P}_{ij}$ refers to the penalty matrix in the $i$th iteration for the $j$th set of indices, whereas $P_{ij}$ is the $(i,j)$th element in the matrix $\mathbf{P}$.}

In each iteration, all potential candidate sets (the union of mandatory covariates and one other covariate) are updated by
$$
\hat{\gamma}_{mk} = \mathbf{I}^{-1}_{pen}(0)U(0)
$$
where $U(\gamma) = \partial l / \partial \gamma (\gamma)$ and $\mathbf{I}^{-1}_{pen} = \partial^2 l/\partial\gamma\partial\gamma^T (\gamma + \lambda\mathbf{P}_{mk})$ are the first and second derivatives of the unpenalized partial-log-likelihood. The optimal set is then found as
$$
k^* := \argmax_k l_{pen}(\gamma_{mk})
$$
and the estimated coefficients are updated with
$$
\hat{\beta}_m = \hat{\beta}_{m-1} + \gamma_{mk^*}, \quad k^* \in \calI_{mk}
$$
The step size, $\nu$, is then one, but this could potentially be altered.

The algorithm deviates from (@alg-surv-gbm) as $l_{pen}$ is directly optimised and not its gradient, additionally model coefficients are iteratively updated instead of a more general model form. The algorithm is implemented in $\pkg{CoxBoost}$  [@pkgcoxboost]. Experiments suggest that including the 'correct' mandatory covariates may increase predictive performance  [@Binder2008]. CoxBoost is less accessible than other boosting methods as it requires a unique boosting algorithm, as such only one off-shelf implementation appears to exist and even this implementation has been removed from CRAN as of 2020-11-11. CoxBoost is also less transparent as the underlying algorithm is more complex, though is well-explained by the authors  [@Binder2008]. There is good indication that CoxBoost is performant [@Sonabend2021b]. In a non-medical domain, where performance may be the most important metric, then perhaps CoxBoost can be recommended as a powerful model. However, when sensitive predictions are required, CoxBoost may not be recommended. Further papers studying the model and more off-shelf implementations could change this in the future.

### Ranking Survival Models

The ranking survival models in this section are all unified as they make predictions of the linear predictor, $\hat{g}(X^*) = X^*\hat{\beta}$.\footnote{This is commonly referred to as a 'linear predictor' as it directly relates to the boosted linear model (e.g. Cox PH), however it is more accurately a 'prognostic index' as the final prediction is not the true linear predictor.}

**GBM-AFT** {#mod-gbmaft}\\
Schmid and Hothorn (2008)  [@Schmid2008b] published a GBM for accelerated failure time models in response to PH-boosted models that may not be suitable for non-PH data. Their model fits into the GBM framework by assuming a fully-parametric AFT and simultaneously estimating the linear predictor, $\hatg(X_i) =\hat{\eta}$, and the scale parameter, $\hat{\sigma}$, controlling the amount of noise in the distribution. The (fully-parametric) AFT is defined by
$$
\log Y = \eta + \sigma W
$$
where $W$ is a random variable independent of the covariates that follows a given distribution and controls the noise in the model. By assuming a distribution on $W$, a distribution is assumed for the full parametric model. The full likelihood, $\calL$, is given by
$$
\calL(\dtrain|\mu, \sigma, W) = \prod^n_{i=1} \Big[\frac{1}{\sigma} f_W\Big(\frac{\log(T_i) - \mu}{\sigma}\Big)\Big]^{\Delta_i}\Big[S_W\Big(\frac{\log(T_i) - \mu}{\sigma}\Big)\Big]^{(1-\Delta_i)}
$$ {#eq-surv-aft-like}
where $f_W, S_W$ is the pdf and survival function of $W$ for a given distribution. By setting $\mu := g(X_i)$, $\sigma$ is then rescaled according to known results depending on the distribution  [@Klein2003]. The gradient of the negative log-likelihood, $-l$, is minimised in the $m$th iteration where
$$
\begin{split}
l(\dtrain|\hat{g}, \hat{\sigma},W) = \sum^n_{i=1} \Delta_i\Big[- \log\sigma + \log f_W\Big(\frac{\log(T_i) - \hat{g}_{m-1}(X_i)}{\hat{\sigma}_{m-1}}\Big)\Big] + \\
(1-\Delta_i)\Big[\log S_W\Big(\frac{\log(T_i) - \hat{g}_{m-1}(X_i)}{\hat{\sigma}_{m-1}}\Big)\Big]
\end{split}
$$
where $\hatg_{m-1}, \hat{\sigma}_{m-1}$ are the location-scale parameters estimated in the previous iteration. Note this key difference to other GBM methods in which two estimates are made in each iteration step. In order to allow for this, (@alg-surv-gbm) is run as normal but in addition, after updating $\hatg_m$, one then updates $\hat{\sigma}_m$ as
$$
\hat{\sigma}_m := \argmin_\sigma -l(\dtrain|g_m,\sigma, W)
$$
$\sigma_0$ is initialized at the start of the algorithm with $\sigma_0 = 1$ suggested  [@Schmid2008b].

This algorithm provides a ranking prediction without enforcing an often-unrealistic PH assumption on the data. This model is implemented in $\pkg{mboost}$ and $\pkg{xgboost}$. Experiments indicate that this may outperform the Cox PH  [@Schmid2008b]. Moreover the model has the same transparency and accessibility as the GBM-COX.

**GBM-GEH** {#mod-gbmgeh}\\
The concordance index is likely the most popular measure of discrimination, this in part due to the fact that it makes little-to-no assumptions about the data (@sec-eval-crank). A less common measure is the Gehan loss, motivated by the semi-parametric AFT. Johnson and Long proposed the GBM with Gehan loss, here termed GBM-GEH, to optimise separation within an AFT framework  [@Johnson2011].

The semi-parametric AFT is defined by the linear model,
$$
\log Y = \eta + \epsilon
$$
for some error term, $\epsilon$.

The D-dimensional Gehan loss to minimise is given by,
$$
G_D(\dtrain, \hatg) = -\frac{1}{n^2} \sum^n_{i=1}\sum^n_{j=1} \Delta_i (\hat{e}_i - \hat{e}_j)\II(\hat{e}_i \leq \hat{e}_j)
$$
where $\hat{e}_i = \log T_i - \hat{g}(X_i)$. The negative gradient of the loss is,
$$
r_{im} := \frac{\sum^n_{j=1} \Delta_j \II(\hat{e}_{m-1,i} \geq \hat{e}_{m-1,j}) -\Delta_i\II(\hat{e}_{m-1,i} \leq \hat{e}_{m-1,j})}{n}
$$
where $\hat{e}_{m-1,i} = \log T_i - \hatg_{m-1}(X_i)$.

(@alg-surv-gbm) then follows naturally substituting the loss and gradient above. The algorithm is implemented in $\pkg{mboost}$. Simulation studies on the performance of the model are inconclusive  [@Johnson2011] however the results in [@Sonabend2021b] indicate strong predictive performance.

**GBM-BUJAR** {#mod-gbmbujar}\\
GBM-BUJAR is another boosted semi-parametric AFT. However the algorithm introduced by Wang and Wang (2010)  [@Wang2010] uses Buckley-James imputation and minimisation. This algorithm is almost identical to a regression GBM (i.e. using squared loss or similar for $L$), except with one additional step to iteratively impute censored survival times. Assuming a semi-parametric AFT model, the GBM-BUJAR algorithm iteratively updates imputed outcomes with the Buckley-James estimator  [@Buckley1979],
$$
T^*_{m, i} := \hatg_{m-1}(X_i) + e_{m-1, i}\Delta_i + (1-\Delta_i)\Big[\KMS(e_{m-1, i})^{-1}\sum_{e_{m-1, j} > e_{m-1, i}} e_{m-1, j}\Delta_j \hatp_{KM}(e_{m-1, j})\Big]
$$
where $\hatg_{m-1}(X_i) = \hat{\eta}_{m-1}$, and $\KMS, \hatp_{KM}$ are Kaplan-Meier estimates of the survival and probability mass functions respectively fit on some training data, and $e_{m-1,i} := \log(T_i) - g_{m-1}(X_i)$. Once $T^*_{m, i}$ has been updated, (@alg-surv-gbm) continues from with least squares as with any regression model.

GBM-BUJAR is implemented in $\pkg{bujar}$  [@pkgbujar] though without a separated fit/predict interface, its accessibility is therefore limited. There is no evidence of wide usage of this algorithm nor simulation studies demonstrating its predictive ability. Finally, there are many known problems with semi-parametric AFT models and the Buckey-James procedure  [@Wei1992], hence GBM-BUJAR is also not transparent.

**GBM-UNO** {#mod-gbmuno}\\
Instead of optimising models based on a given model form, Chen $\etal$ [@Chen2013] studied direct optimisation of discrimination by Harrell's C whereas Mayr and Schmid  [@Mayr2014] focused instead on Uno's C. Only an implementation of the Uno's C method could be found, this is therefore discussed here and termed 'GBM-UNO'.

The GBM-UNO attempts to predict $\hatg(X^*) := \hat{\eta}$ by optimising Uno's C (@sec-eval-crank-disc-conc),
$$
C_U(\hat{g}, \dtrain) = \frac{\sum_{i \neq j}\Delta_i\{\KMG(T_i)\}^{-2}\II(T_i < T_j)\II(\hatg(X_i) >\hatg(X_j))}{\sum_{i \neq j}\Delta_i\{\KMG(T_i)\}^{-2}\II(T_i < T_j)}
$$

The GBM algorithm requires that the chosen loss, here $C_U$, be differentiable with respect to $\hatg(X)$, which is not the case here due to the indicator term, $\II(\hatg(X_i) > \hatg(X_j))$. Therefore a smoothed version is instead considered where the indicator is approximated by the sigmoid function  [@Ma2006],

$$
K(u|\sigma) = (1 + \exp(-u/\sigma))^{-1}
$$

where $\sigma$ is a hyper-parameter controlling the smoothness of the approximation. The measure to optimise is then,

$$
C_{USmooth}(\dtrain|\sigma) = \sum_{i \neq j} \frac{k_{ij}}{1 + \exp\big[(\hatg(X_j) - \hatg(X_i))/\sigma)\big]}
$$ {#eq-surv-gbm-cus}

with

$$
k_{ij} = \frac{\Delta_i (\KMG(T_i))^{-2}\II(T_i < T_j)}{\sum^n_{i \neq j} \Delta_i(\KMG(T_i))^{-2}\II(T_i < T_j)}
$$

The negative gradient at iteration $m$ for observation $i$ can then be found,

$$
r_{im} := - \sum^n_{j = 1} k_{ij} \frac{-\exp(\frac{\hatg_{m-1}(X_j) - \hatg_{m-1}(X_i)}{\sigma})}{\sigma(1 + \exp(\frac{\hatg_{m-1}(X_j) - \hatg_{m-1}(X_i)}{\sigma}))}
$$ {#eq-surv-gbm-cus-grad}

(@alg-surv-gbm) can then be followed exactly by substituting this loss and gradient; this is implemented in $\pkg{mboost}$. One disadvantage of GBM-UNO is that C-index boosting is more insensitive to overfitting than other methods  [@Mayr2016], therefore stability selection  [@Meinshausen2010] can be considered for variable selection; this is possible with $\pkg{mboost}$. Despite directly optimising discrimination, simulation studies do not indicate that this model has better separation than other boosted or lasso models  [@Mayr2014]. GBM-UNO has the same accessibility, transparency, and performance [@Sonabend2021b] as previous boosting models.

## Conclusion

:::: {.callout-warning icon=false}

## Key takeaways

* Componentwise gradient boosting machines are a highly flexible and powerful machine learning tool. They have proven particularly useful in survival analysis as minimal adjustments are required to make use of off-shelf software.
* The flexibility of the algorithm allows all the models above to be implemented in relatively few open-source packages.
* There is evidence that boosting models can outperform the Cox PH even in low-dimensional settings [@Schmid2008b], which is not not something all ML models can claim.

::::

:::: {.callout-important icon=false}

## Limitations

* Boosting, especially with tree learners, is viewed as a black-box model that is increasingly difficult to interpret as the number of iterations increase. However, there are several methods for increasing interpretability, such as variable importance and SHAPs  [@Lundberg2017].
* Boosting often relies on intensive computing power, however, dedicated packages such as $\pkg{xgboost}$  [@pkgxgboost], exist to push CPU/GPUs to their limits in order to optimise predictive performance.

::::

:::: {.callout-tip icon=false}

## Further reading

*

::::
