---
abstract: TODO (150-200 WORDS)
---

{{< include _setup.qmd >}}

# What are Survival Measures? {#sec-eval}

{{< include _wip.qmd >}}

This chapter studies how to evaluate the predictions arising from the surveyed models in the previous chapter. 'Model evaluation' is as vague a phrase as 'human evaluation'. A human could be evaluated by a series of exams, physical or neurological tests, aesthetics, etc. Likewise a model could be evaluated according to how well it fits to training data, the quality of predictions on new data, the average prediction, and many more methods. This chapter aims to provide a nuanced approach to defining, understanding, and examining model evaluation. Evaluation is defined in further detail in @sec-eval-why and throughout this chapter the definition will continue to be refined and specialised to specific sub-types of evaluation, including discrimination (@sec-eval-crank), calibration (@sec-eval-distr-calib), and overall predictive performance (@sec-eval-distr).

Evaluation is a surprising source of disagreement in the literature with some arguing that the process can often be ignored completely [@vanderLaan2007; @Wolpert1992]. There is a larger divide in survival analysis as many believe that the primary (possibly only) goal is risk prediction [@Chen2012; @Newson1983; @Pencina2012] and thus other forms of evaluation are not required. These strict views can undermine an integral part of the model building and deployment process, and create more division than necessary. This book advocates for strict implementation of model evaluation as a critical part of the model building process as well as in continuous monitoring of deployed models. Without rigorous evaluation, a model cannot be 'trusted' to perform well and could be as useless as making random guesses for all predictions. This is critical in survival analysis, which has important applications in healthcare and finance, in these sectors models that have not been evaluated are potentially dangerous.

An infamous example of evaluation going wrong is the Google Flu Trends (GFT) model, which claimed to accurately predict future flu trends but was in fact deemed by many a complete failure as it significantly overestimated all predictions, in some cases doubling the true figures [@Lazer2014].  The GFT model was never utilised (at least openly) in policy and as such no lasting harm was created. However it is not hard to imagine the problems that would be caused by such a model if it was utilised and trusted during the time of COVID-19. On a more individual level, as machine learning is increasingly deployed in public sectors, major decisions for patients could become increasingly automated (or at least machine-assisted). Patients should expect their models to be as trained and tested as their doctors.

This chapter attempts to highlight the purpose and need of evaluation in survival analysis by first giving a high-level overview to evaluation as a concept, then providing a brief review of commonly-used survival measures and finally extensive treatment to scoring rules for evaluation of probabilistic predictions, including novel definitions and proofs for properness of scoring rules. The term *measure* will be used throughout this chapter to refer to functions or 'metrics' that quantify some aspect of model evaluation, this should not be confused with a mathematical measure.


#### Notation and Terminology {.unnumbered .unlisted}

The notation introduced in @sec-surv is recapped for use in this chapter. The generative template is given by $(X,T,\Delta,Y,C) \ t.v.i. \ \calX \times \calT \times \bset \times \calT \times \calT$ where $\calX \subseteq \Reals^p$ and $\calT \subseteq \NNReals$, where $C,Y$ are unobservable, $T := \min\{Y,C\}$, and $\Delta = \II(Y = T)$. Specific survival data is given by training data, $\dtrain = \{(X_1,T_1,\Delta_1),...,(X_n,T_n,\Delta_n)\}$ where $(X_i,T_i,\Delta_i) \iid (X,T,\Delta)$, and test data, $\dtest = \{(X^*_1,T^*_1,\Delta^*_1),...,(X^*_m,T^*_m,\Delta^*_m)\}$ where $(X^*_i,T^*_i,\Delta^*_i) \iid (X,T,\Delta)$.

## Evaluation Overview {#sec-eval-why}

### What is Evaluation? {#sec-eval-why-what}

Evaluation is the process of examining a model's relationship to data, which may refer to the model's relationship to training data, i.e. how well the model is 'fit' to this data, or the relationship to testing data, i.e. how 'good' are the predictions from the model. In this book, only three types of evaluation measure are considered and qualitative definitions of these are given here; more precise definitions appear later in the chapter.

* Discrimination -- A model's discriminatory power refers to how well it separates observations that are at a higher or lower risk of event. Therefore discrimination is also sometimes referred to as *separation*. For example, a model with good discrimination will predict that (at a given time) a dead patient has a higher probability of being dead than an alive patient. These measures are the most common in survival and assess relative risk or rank predictions.
* Calibration -- There is no single agreed upon definition of model calibration, with definitions varying from paper to paper [@Collins2014; @Harrell1996; @Rahman2017; @VanHouwelingen2000]. Generally, a model is said to be well-calibrated if the average predicted values from the model are in some 'agreement' (which is specified by the chosen measure) with the average true observed values.
* Predictive Performance -- A model is said to have good predictive performance (or sometimes 'predictive accuracy') if its predictions for new data are 'close to' the truth.

These are referred to as measures of predictive ability as they draw conclusions about the ability of the model to make predictions.^[Measures of predictive ability measure a model's *ability* to make any form of prediction. Measures of predictive performance measure the *performance* of the predictions. In this section a model's predictive ability refers to all three of discrimination, calibration, and predictive performance.]

Using these definitions as a primary taxonomy for survival measures is problematic as without clear definitions there can be significant overlap between model 'classes'. Instead this book advocates for the same taxonomy as in the previous chapter and categorises measures by the return type that they evaluate: survival time, ranking, or survival distribution.

Goodness-of-fit measures are very briefly discussed in @sec-eval-insample for completeness, however these are generally out of scope in this book as the vast majority (if any) cannot evaluate machine learning models.

## Why are Models Evaluated? {#sec-eval-why-why}

A key element of the scientific method is experiments and validation. In the usual workflow of the scientific method:

* a hypothesis is proposed;
* predictions are made; and
* experiments are performed to test the hypothesis based on these predictions.

For statistical models the same principles are upheld:

i. a model is proposed (by manual or automated selection with possible tuning);
i. predictions are made either internally (cross-validation) or externally (held-out data); and
i. validation is performed on these predictions in order to infer something about the model's performance.

The model can then be considered 'good' or 'bad' and either deployed, adjusted, or discarded. As these are models that are run on a computer (as opposed to experiments in the real-world), the process from fitting to validating is relatively quick and as such multiple proposed models can be evaluated and compared at the same time. This provides two key use-cases for evaluation:

i. demonstrating model performance; and
i. model comparison/selection.

Resistance to model evaluation can be found in the machine learning community. One such example are proponents of inhomogeneous ensemble methods, which combine predictions from multiple different models into a single prediction. The arguments for these models are that:

i. model evaluation can never be precise enough, or strong enough guarantees cannot be given [@Jiao2016]; and
i. ensemble methods can guarantee a better performance than the individual component models and therefore evaluation of the components is not required.

For example, 'super learners' [@vanderLaan2007] are a class of such model and claim^[Testing this claim is tangential so for now will be assumed true.] to guarantee that a super learner will always perform as well as, if not better, than its component models: ''...the super learner framework allows a researcher to try many prediction algorithms...knowing that the final combined super learner fit will either be the best fit or near the best fit" [@Polley2010]. This has three problems, it:

i. assumes that researchers will only fit sensible prediction algorithms;
i. advocates for complex ensemble models instead of transparent and parsimonious ones; and
i. assumes that a super learner is guaranteed to be the (near) 'best fit', which actively discourages simpler models being tested separately.

Each of these problems can be resolved by researchers only fitting sensible models and opting for an Occam's Razor approach where inhomogeneous ensemble methods are used only if they outperform simpler models, thus requiring validation to test this.

By the parsimony principle, if two models have the same predictive performance (within some degree of confidence), then the simpler and more transparent model is preferred. Even a very slight gain in predictive performance could be outweighed by a large increase to complexity. All models, whether simple or complex, should be critically compared to many alternatives. At the very least a model should be compared to a baseline (@sec-eval-distr-score-base) as many performance measures are uninterpretable without a point of comparison [@Gressmann2018].

### How are Models Evaluated?

The process of evaluation in machine learning is briefly given as a key method in @sec-surv-setml and relevant parts are repeated here. The evaluation process itself is a simple application of a suitable mathematical function to predictions and true data. Let $L$ be some evaluation measure and for now assume $L$ is a measure evaluating deterministic predictions (the following generalises to other types trivially). A model will either be evaluated on each prediction separately, in which case $L: \Reals \times \Reals \rightarrow \ExtReals$ or the measure is calculated for all predictions simultaneously, in which case $L: \Reals^m \times \Reals^m \rightarrow \ExtReals$. Specifically the loss parameters are observed (true) outcomes, $Y$, and predictions of this outcome, $\hatY$. $L$ is usually referred to as a *loss* when $L$ should be minimised for optimal prediction, whereas a *score* is the term given when $L$ should be maximised.

All evaluation measures discussed in this book are out-of-sample measures and therefore evaluation takes place after the model makes predictions on held-out test data.

Specific choices for $L$ are now reviewed.

## In-Sample Measures {#sec-eval-insample}

In-sample measures are not examined in this book as no in-sample measures could be found that are applicable to all machine learning methods and therefore are out of scope for this book. Instead, the interested reader is referred to the papers and references listed below:

#### Residuals {.unnumbered .unlisted}

For discussion about model residuals, refer to texts on survival modelling fitting and goodness-of-fit such as:

* @Collett2014
* @dataapplied


Both provide a comprehensive overview to model residuals for semi- and fully-parametric low-complexity survival models.

#### $R^2$ measures {.unnumbered .unlisted}

$R^2$ type measures have been the focus of several reviews and surveys, in particular the following are recommended:

* @Choodari2012a --- For a comprehensive review and simulation study of $R^2$ type measures
* @Kent1988 --- Defines the commonly utilised Kent and O'Quigley $R^2$ measure
* @Royston2004 --- Defines the commonly utilised Royston and Sauerbrei $R^2$ measure

#### Likelihood and Information Criteria {.unnumbered .unlisted}

Measures of likelihood and information criteria (e.g. AIC, BIC) are commonly utilised in in-sample model comparison of low-complexity survival models though in general are harder (if not impossible) to compute on ML alternatives.

These criterion are originally defined in:

* @Akaike1974 --- For the introduction of the AIC
* @Schwarz1978 --- For the introduction of the BIC

These are discussed for survival analysis in:

* @VolinskyRaftery2000 --- For discussion on the BIC for survival models.
* @HURVICH1989 --- Definition of corrected $AIC$ for survival models, $AIC_C$
* @Liang2008 --- 'Improved' AIC for survival models.
