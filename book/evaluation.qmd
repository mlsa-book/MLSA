{{< include _setup.qmd >}}

# Evaluation {sec-eval}

This chapter studies how to evaluate the predictions arising from the surveyed models in the previous chapter. 'Model evaluation' is as vague a phrase as 'human evaluation'. A human could be evaluated by a series of exams, physical or neurological tests, aesthetics, etc. Likewise a model could be evaluated according to how well it fits to training data, the quality of predictions on new data, the average prediction, and many more methods. This chapter aims to provide a nuanced approach to defining, understanding, and examining model evaluation. Evaluation is defined in further detail in @sec-eval-why and throughout this chapter the definition will continue to be refined and specialised to specific sub-types of evaluation, including discrimination (@sec-eval-crank), calibration (@sec-eval-distr-calib), and overall predictive performance (@sec-eval-distr).

Evaluation is a surprising source of disagreement in the literature with some arguing that the process can often be ignored completely [@vanderLaan2007; @Wolpert1992]. There is a larger divide in survival analysis as many believe that the primary (possibly only) goal is risk prediction [@Chen2012; @Newson1983; @Pencina2012] and thus other forms of evaluation are not required. These strict views can undermine an integral part of the model building and deployment process, and create more division than necessary. This thesis advocates for strict implementation of model evaluation as a critical part of the model building process as well as in continuous monitoring of deployed models. Without rigorous evaluation, a model cannot be 'trusted' to perform well and could be as useless as making random guesses for all predictions. This is critical in survival analysis, which has important applications in healthcare and finance, in these sectors models that have not been evaluated are potentially dangerous.

An infamous example of evaluation going wrong is the `r link("https://www.google.org/flutrends/about/", "Google Flu Trends (GFT) model")`, which claimed to accurately predict future flu trends but was in fact deemed by many a complete failure as it significantly overestimated all predictions, in some cases doubling the true figures [@Lazer2014].  The GFT model was never utilised (at least openly) in policy and as such no lasting harm was created. However it is not hard to imagine the problems that would be caused by such a model if it was utilised and trusted during the time of COVID-19. On a more individual level, as machine learning is increasingly deployed in public sectors, major decisions for patients could become increasingly automated (or at least machine-assisted). Patients should expect their models to be as trained and tested as their doctors.