---
abstract: TODO (150-200 WORDS)
---

{{< include _setup.qmd >}}

# Choosing Measures {#sec-eval-choose}

{{< include _wip.qmd >}}

Calibration measures assess the avaerage performance across all observatins on a population level.
This is different to scoring rules which evaluate the sample mean of individual predictions across all observations in a test set.
Hence conclusions about calibration measures can only be drawn at an average *population* level, with no individual-level measurements.

This chapter briefly reviewed different classes of survival measures before focusing on the application of scoring rules to survival analysis.

One finding of note from the review of survival measures is the possibility that research and debate has become too focused on measures of discrimination. For example, many papers state the flaws of Harrell's C index [@Gonen2005; @Rahman2017; @Schmid2012; @Uno2007] however few acknowledge that simulation experiments have demonstrated that common alternatives yield very similar results to Harrell's C [@Rahman2017; @Therneau2020] and moreover some promimnent alternatives, such as Uno's C [@Uno2007], are actually harder to interpret due to very high variance [@Rahman2017; @Schmid2012]. Whilst all concordance indices may be considered accessible and transparent, there is considerable doubt over their performance due to influence from censoring.

Focus on discrimination could be the reason for less development in survival time and calibration measures. There is evidence [@Wang2017] of the censoring-adjusted RMSE, MAE, and MSE (@sec-eval-det) being used in evaluation but without any theoretical justification, which may lead to questionable results. Less development in calibration measures is likely due to these measures being more widely utilised for re-calibration of models and not in model comparison. The new D-Calibration measure [@Andres2018; @Haider2020] could prove useful for model comparison however independent simulation experiments and theoretical studies of the measure's properties would first be required. No calibration measures can be considered performant due to a lack of clear definition of a calibration measure for survival, moreover the reviewed measures may not even be transparent and accessible due to requiring expert interpretation.

The most problematic findings in this chapter lie in the survival scoring rules. @sec-eval-distr-score-proper proved that no commonly used scoring rule is proper, which means that any results regarding model comparison based on these measures are thrown into question. It is also conjectured that no approximate survival loss can be strictly proper (in general), which is due to the joint distribution of the censoring and survival distribution always being unknown and impossible to estimate (though the marginal censoring distribution can be estimated). As demonstrated in @sec-eval-distr-score-reg, a proper scoring rule is not necessarily a useful one and therefore is not enough for robust model validation.

As an important caveat to the findings in this chapter, this book presents one particular definition of properness for survival scoring rules. This definition is partially subjective and other definitions could instead be considered. Therefore these losses should not be immediately dismissed outright. As well as deriving new losses that are (strictly) proper with respect to the definitions provided here, research may also be directed towards finding other sensible definitions of properness, or in confirming that the definition here is the only sensible option.<!-- As these are open research questions, the scoring rules discussed in this chapter are still utilised in evaluation for the benchmark experiment in -->

This chapter demonstrates that no survival measure on its own can capture enough information to fully evaluate a survival prediction.
This is a serious problem that will either lead (or already is leading) to less interest and uptake in survival modelling, or misunderstanding and deployment of sub-optimal models. Evaluation of survival models is still possible but currently requires expert interpretation to prevent misleading results. If the aim of a study is solely in assessing a model's discriminatory power, then measures of discrimination alone are sufficient, otherwise a range of classes should be included to capture all aspects of model performance. This book advocates reporting *all* of the below to evaluate model performance:


* **Calibration**: Houwelingen's $\alpha$ and [@VanHouwelingen2007] *and* D-calibration [@Haider2020].
* **Discrimination**: Harrell's [@Harrell1984] *and* Uno's [@Uno2011] C. By including two (or even more) measures of concordance, one can determine a feasible range for the 'true' discriminatory ability of the model instead of basing results on a single measure. Time-dependent AUCs can also be considered but these may require expert-interpretation and may only be advisable for discrimination-specific studies.
* **Scoring Rules**: When censoring is outcome-independent and a large enough training dataset is available, then the re-weighted integrated Graf score and re-weighted integrated survival log loss (@sec-eval-distr-commonsurv). Otherwise the IGS *and* ISLL [@Graf1999] which should be interpreted together to ensure consistency in results.

If survival time prediction is the primary goal then RMSE$_C$ and MAE$_C$ can be included in the analysis however these should not form the primary conclusions due to a lack of theoretical justification. Instead, scoring rules should be utilised as a distributional prediction can always be composed into a survival time prediction (@sec-car).

All measures discussed in this chapter, with the exception of the Blanche AUC, have been implemented in  $\proba$ [@pkgmlr3proba]. <!-- The listed measures above are utilised in the benchmark experiment in  -->
