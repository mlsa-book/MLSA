---
abstract: TODO (150-200 WORDS)
---

{{< include _setup.qmd >}}

# Choosing Measures {#sec-eval-conc}

{{< include _wip.qmd >}}

After reading this part of the book, evaluating survival analysis models may appear more daunting than regression and classification settings, which, in contrast, have fewer (common) measures to choose from.
In regression problems, the RMSE and MAE are common choices for evaluating how far predictions are from the truth.
In classification, the Brier score or logloss may be used to evaluate probabilistic predictions and the accuracy score or TPR/TNR/FPR/FNR are common for deterministic predictions.
In contrast, there are many more measures in survival analysis which are necessarily more complex, due to the need to handle censoring with many possible methods for doing so.
Therefore, this final chapter aims to provide some simple to follow guidelines for selecting measures for different types of experiments.

## Defining the experiment

Experiments may be performed to make predictions for new data, compare the performance of multiple models ('benchmark experiments'), investigate patterns in observed data, or some combination of these.
Each experiment requires different choices of measures, with different levels of strictness applied to measure assumptions.

### Predictive experiments

In the real world, predictive experiments are most common.
These are now daily occurrences as machine learning models are routinely deployed on servers to make ongoing predictions.
In these cases, the exact task must be precisely stated before any model is deployed and evaluated.
Common survival problems to solve include: identifying low and high risk groups in new data (for resource allocation); predicting the survival distribution for an individual over time; and predicting the survival probability for an individual at a specific time.

The first of these is a discrimination problem and it is therefore most important that the model optimises corresponding measures and that measure assumptions are justified.
However, even this task may be more complex than it initially seems.
For example, while some papers have shown flaws in Harrell's C [@Gonen2005; @Rahman2017; @Schmid2012; @Uno2007], others have demonstrated that common alternatives yield very similar results [@Rahman2017; @Therneau2020] and moreover some prominent alternatives may be harder to interpret due to high variance [@Rahman2017; @Schmid2012].
In predictive experiment that may require more level of automation, it is important to be careful of C-hacking (@sec-eval-crank-choose) and to avoid overoptimistic results.
Hence one should not compute a range of concordance indices and report the maximum but instead calculate a single discrimination measure and then establish a pre-defined threshold to determine if the deployed model is optimal, a natural threshold would be 0.5 as anything above this is better than a baseline model.
Given Harrell's C to be increasingly over-optimistic with additional censoring [@Rahman2017], it is advisable to use Uno's C instead.

If the task of interest is to predict survival distributions *over time*, then the choice of measure is more limited and only the RCLL and the proper Graf score are recommended.
Both these measures can only be interpreted with respect to a baseline so use of the ERV representation is strongly recommended.
As with the previous task, establishing a threshold for performance is essential prior to deployment and for ongoing evaluation.
It is less clear in these cases what this threshold might be, but the simplest starting point would be to ensure that the model continues to outperform the baseline or a simpler gold-standard model (e.g., the Cox PH).

The final task of interest differs from the previous by only making predictions at a specific time.
In this case, prediction error curves, and single-time point calibration measures can be used, as well as scoring rules with shorter cut-offs (i.e., the upper limit of the integral).
It is imperative that model performance is never extrapolated outside of the pre-specified time.

### Benchmark experiments

When conducting benchmark experiments, it is advisable to use a spread of measures so that results can be compared across various properties.
In this case, models should be tested against discrimination, calibration, and overall predictive ability (i.e., with scoring rules).
As models make different types of predictions, results from these experiments should be limited to metrics that are directly comparable, in other words, two models should only be compared based on the *same* metric.
In benchmark experiments, models are compared across the same data and same resampling strategy, hence measure assumptions become less important as they are equally valid or flawed for all models.
For example, if one dataset has particularly high amounts of censoring leading to an artificially higher concordance index, then this bias would affect all models equally and the overall experiment would not be affected.
Hence, in these experiments it suffices to pick one or two measures for concordance, discrimination, and predictive ability, without having to be overly concerned with the individual metric.

This book recommends using Harrell's C and Uno's C for concordance as these are simplest to compute and including both enables more confidence in model comparison, i.e., if a model outperforms another with respect to both these measures then there can be higher confidence in drawing statements about the model's discriminatory power.
For calibration, D-calibration is recommended as it can be meaningfully compared between models, and the RCLL is recommended for a scoring rule (which is proper for outcome-independent censoring).
No distance measure is recommended as these do not apply to the vast majority of models.
All these measures can be used for automated tuning, in the case of discrimination tuning to Harrell's C alone should suffice (without also tuning to Uno's C).

### Investigation

Investigating patterns in observed data is increasingly common as model interpretability methods have become more accessible [@Molnar2019].
Before data can be investigated, any model that is trained on the data must first be demonstrated to be a good fit to the data.
A model's fit to data can also be evaluated by resampling the data (@sec-ml) and evaluating the predictions.
In this case, it is important to choose measures that are interpretable and have justified assumptions.
Calibration measures are particularly useful for evaluating if a model is well fit to data, and any of the methods described in @sec-eval-distr-calib are recommended for this purpose.
Discrimination measures *may* be useful, however, given how susceptible they are to censoring, they can be difficult to interpret on their own, and the same is true for scoring rules.
One method to resolve ambiguity is to perform a benchmark experiment of multiple models on the same data (ideally with some automated tuning) and then select the best model from this experiment and refit it on the full data [@Becker2024] -- this is a robust, empirical method that demonstrates a clear trail to selecting a model that outperforms other potential candidates.
When investigating a dataset, one may also consider using different measures to assess algorithmic fairness [@Sonabend2022a], any measure that can be optimised (i.e., where the lowest or highest value is the best) may be used in this case.
Finally, there are survival adaptations to the well-known AIC [@Liang2008] and BIC [@VolinskyRaftery2000] however as these are generally only applicable to 'classical ' models (@sec-models-classical), they are out of scope for this book and hence have not been discussed.
<!-- FIXME: UPDATE 'classical' to whatever term we use for that chapter -->

## Conclusions

This part of the book focused on survival measures.
Measures may be used to evaluate model predictions, to tune a model, or to train a model (e.g., in boosting or neural networks).
Unlike other settings, there are many different choices of survival measures and it can be hard to determine which to use and when.
In practice, like many areas of Statistics, the most important factor is to clearly define any experiment upfront and to be clear about which measures will be used and why.
As a rule of thumb, good choices for measures are Harrell's C for evaluating discrimination, with Uno's C supporting findings, D-calibration for calibration, and the RCLL for evaluating overall predictive ability from distribution predictions.
Finally, if you are restricted to a single measure choice (e.g., for automated tuning or continuous evaluation of deployed models), then we recommended selecting a scoring rule such as RCLL which captures information about calibration and discrimination simultaneously.
<!-- FIXME: DO WE KNOW THIS IS TRUE FOR RCLL? -->