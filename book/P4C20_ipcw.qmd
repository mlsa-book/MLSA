---
abstract: TODO (150-200 WORDS)
---

::: {.content-visible when-format="html"}
{{< include _macros.tex >}}
:::

# IPC weighted classification {#sec-icpw-reduction}

The inverse probability of censoring weights (IPCW; @sec-surv-ipcw) based reduction transforms a survival task to a weighted classification task (@Vock2016).
Conceptually, it is one of the simplest reductions, but currently also the least general. 

Consider a right-censored data set $\mathcal{D} = \{(\mathbf{x}_i, y_i, \delta_i)\}_{i=1}^n$ with $\mathbf{x}_i \in \mathbb{R}^p$ as introduced in @sec-data-rc. 
Sometimes practitioners are not interested in estimation of the entire event time distribution. 
Instead, one just wants to know the probability that an event occurs until a prespecified time point $\tau$ (sometimes referred to as $\tau$-year prediction in survival analysis):
<!--  -->
$$
\pi(\mathbf{x}_i;\tau) := P(T \leq \tau|\mathbf{x}_i) = 1-S(\tau|\mathbf{x}_i).
$$
<!--  -->
It might be tempting to estimate this probability by defining a binary target variable
<!--  -->
$$
e_i(\tau):= \mathbb{I}(y_i \leq \tau \wedge \delta_i = 1)
$$ {#eq-binary-target}
<!--  -->
where all observations with event before time $\tau$ are considered ones (events) and all other observations zeros (non-events).
Then one could estimate $\pi(\mathbf{x}_i;\tau)$ using any binary classification method that outputs (calibrated) probabilities as 
$$
\pi(\mathbf{x}_i;\tau) = P(e_i(\tau) = 1|\mathbf{x}_i)
$$ {#eq-binary-prob}

This approach could work if there was no censoring before $\tau$ in the data.
However, in the presence of censoring, (@eq-binary-target) does not define a valid target variable, as observations censored before time $\tau$ ($y_i < \tau \wedge \delta_i = 0$) are neither events nor non-events at time $\tau$ (as the event possibly occurred between $y_i$ and $\tau$). 
Treating those observations as non-events or removing them from the data without further modification would introduce bias.

@Vock2016 suggest to adapt the estimation procedure to obtain unbiased estimates of (@eq-binary-prob) by first calculating weights for each observation as 
<!--  -->
$$
\tilde{w}_i(\tau) = \begin{cases}
0 & \text{if } y_i < \tau \wedge \delta_i = 0,\\
\hat{w}_i(\min(y_i, \tau)) & \text{else}
\end{cases}
$$ {#eq-adapted-ipc-weights}
<!--  -->
where $\hat{w}_i(\min(y_i, \tau))$ are the IPC weights as defined in @eq-ipcw-km.
These weights then need to be integrated into the estimation procedure, particularly by optimizing a weighted objective function. 
In words, censored observations are removed (the weight is zero) and uncensored observations are upweighted in order to compensate for the information loss.
The higher the probability of an observation to be censored at $\tau$, the higher its weight will be.
These weights need to be integrated into the estimation procedure by optimizing a weighted objective function. 

Incorporating this weighting scheme allows the binary target variable in (@eq-binary-target) to be used as a valid object of prediction.
Let $\ell_i(e_i(\tau), \pi(\mathbf{x}_i;\tau))$ be the point wise loss, then the learner needs to optimize the weighted objective function
<!--  -->
$$
\mathcal{l}(\pi, \tau) = \sum_{i=1}^n \tilde{w}_i(\tau) \ell_i(e_i(\tau), \pi(\mathbf{x}_i;\tau))
$$ {#eq-ipcw-loss-weighted}
<!--  -->

Thus, (@eq-ipcw-loss-weighted) can be optimized by any classification learner that can handle weights (which is the case for most popular machine learning methods).
The exact form of (@eq-ipcw-loss-weighted) will depend on the choice of learner and objective function. 
For example, using the log loss (binary cross-entropy) $\ell_i(e_i, \pi(\mathbf{x}_i)) = e_i\log(\pi(\mathbf{x}_i)) + (1-e_i)\log(1 - \pi(\mathbf{x}_i))$ as loss function, 
@eq-ipcw-loss-weighted becomes 
$$
\mathcal{l}(\pi, \tau) = \sum_{i=1}^n \tilde{w}_i(\tau) (e_i(\tau)\log(\pi(\mathbf{x}_i;\tau)) + (1-e_i(\tau))\log(1 - \pi(\mathbf{x}_i;\tau))),
$$ {#eq-ipcw-loss-weighted-logistic}

which can be optimized by many implemented learners out-of-the-box without further modifications, for example logistic regression, variants of boosting algorithms (such as XGBoost [@pkgxgboost]), Deep Learning algorithms and others. 


The IPCW reduction can simplify the estimation of $\hat{S}(\tau|\mathbf{x}_i) = 1 - \hat{\pi}(\mathbf{x}_i;\tau)$, as it doesn't require survival specific learners. 
For practical applications, it is nevertheless important to take into account the following considerations:

- While the weighting in (@eq-ipcw-loss-weighted) takes into account censoring, the success of the prediction still depends on the choice of model and loss function.
For example, the estimates might not be well calibrated if the chosen loss function does not yield well calibrated probabilities (for example, the hinge loss used by support vector machines).
- When evaluating predictions $\hat{\pi}(\mathbf{x};\tau)$ on unseen test data, one cannot use standard evaluation metrics for binary classification, because the test data contains zeros (non-events), ones (events), but also censored observations. 
However, as the procedure provides an estimate of the linear predictor, risk score as well as the event probability for each observation, we can use evaluation metrics for survival data discussed in the "Evaluation" part of the book. Use of standard classfication metrics based on only uncensored data during evaluation would require IPC weighting similar to the training phase.
- (@eq-adapted-ipc-weights) implies that contributions of observations censored before time $\tau$ are multiplied by zero in (@eq-ipcw-loss-weighted) while uncensored observations are upweighted. Algorithmically, it would be more efficient to remove censored observations from the data set before training to avoid unnecessary computations of $\ell_i$. However, this might interfere with the usual training and evaluation procedures, for example when used in combination with cross-validation.

## Conclusion

:::: {.callout-warning icon=false}

## Key takeaways

* The IPCW reduction transforms a survival task to a weighted classification task.
* It can greatly simplify the estimation of survival probabilities at a specific time point of interest.
* Many learners for binary classification can be used out-of-the-box without further modifications.
* Learners that support gradient based optimization such as gradient boosting and deep learning are particularly well suited for this task, as they support specification of (custom) loss functions and support integration of weights.
::::

:::: {.callout-important icon=false}

## Limitations
* Currently, the IPCW approach has only been described for right-censored data. Extensions to other settings might be possible, but have not been explored yet.
* Extensions to event-history analysis is also not well explored at the moment of writting, although an extension to competing risks has been proposed recently (see further reading).

::::

:::: {.callout-tip icon=false}

## Further reading

* Vock2016 provide the main reference where they explicitly show how different learners (logistic regression, bayesian networks, decision trees and k-nearest neighbors) can be adapted to obtain unbiased estimates of the event probability in the presence of censoring based on adapted IPC weights. They also discuss suitable evaluation metrics.
* @gonzalezginestet2021stackedinverse extend the approach to the competing risks setting
::::








