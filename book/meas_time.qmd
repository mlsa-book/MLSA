---
abstract: TODO (150-200 WORDS)
---

{{< include _setup.qmd >}}

# Evaluating Survival Time {#sec-eval-det}

{{< include _wip.qmd >}}

When it comes to evaluating survival time predictions, there are few measures available at our disposal.
It makes sense this is an under-researched area, as survival time predictions are very uncommon when compared to other prediction types (@sec-surv-set-types).
In practice, if you want to make predictions for survival times and evaluate these, you will likely have more success by using a reduction to regression.
To our knowledge, there are no specialised 'survival time measures', instead regression measures are used once censored observations have been removed.

```{r echo = FALSE}
learn("sec-redux-regr", "reductions to regression")
```

Before presenting these measures, we will consider what happens by discarding censored observations.
If censoring is truly independent and occurs randomly, then there is no harm in discarding observations and treating this as a regression problem.
The advantage of using survival models in this case (and not also removing censored observations from the training data) is that we are still maximising all available training data and helping models learn as much as possible before censoring.
However, if censoring is not independent, then the removal of censored observations results in increased bias as the proportion of censoring increases (@sec-eval-crank-disc-conc).
Moreover, by discarding censored observations, we could miss very valuable insights about the model.
In the real-world, we do not know the ground truth for observations in our test data, we simply make predictions and wait for the truth to be observed (which could be years depending on our time scale).
Therefore, we need to be sure that predictions for people who end up being censored, are as good as those who are not, simply because we do not know which group people will fall into!
In this context, it is worth considering if discarding censoring times, and providing no weighting to compensate (for example, IPCW), is truly the best strategy?
There has been limited (if any) research into IPCW adjusted survival time measures, so below we just present the simplest case of regression measures for independent censoring that at least has some grounding in literature [@Wang2017].

## Distance measures

These measures are often referred to as 'distance' measures as they measure the distance between the true, $(t, \delta=1)$, and predicted, $\hatt$, values.
We will present each in turn and briefly describe their interpretation in the regression setting.

**Censoring-adjusted mean absolute error, $MAE_C$**

In regression, the mean absolute error (MAE) is a popular measure because it is intuitive to understand as it measures the absolute difference between true and predicted outcomes; hence intuitively one can understand that a model predicting a height of 175cm is clearly better than one predicting a height of 180cm, for a person with true height of 174cm.

$$
MAE_C(\hatt, t, \delta) = \frac{1}{d} \sum^m_{i=1} \delta_i|t_i - \hat{t_i}|
$$

**Censoring-adjusted mean squared error**

In comparison to MAE, the mean squared error (MSE), computes the squared differences between true and predicted values.
While the MAE provides a smooth, linear, 'penalty' for increasingly poor predictions (i.e., the difference between an error of predicting 2 vs. 5 is still 3), but the square in the MSE means that larger errors are quickly magnified (so the difference in the above example is 9).
By taking the mean over all predictions, the effect of this inflation is to increase the MSE value as larger mistakes are made.

$$
MSE_C(\hatt, t, \delta) = \frac{1}{d}\sum^m_{i=1}\delta_i(t_i - \hatt_i)^2
$$

**Censoring-adjusted root mean squared error**

Finally, the root mean squared error (RMSE), is simply the square root of the MSE.
This allows interpretation on the original scale (as opposed to the squared scale produced by the MSE).
Given the inflation effect for the MSE, the RMSE will be larger than the MAE as increasingly poor predictions are made; it is common practice for the MAE and RMSE to be reported together.

$$
RMSE_C(\hatt, t, \delta) = \sqrt{MSE_C(t, \hat{t}, \delta)}
$$

## Over- and under-predictions

All of these distance measures assume that the error for an over-prediction ($\hatt > t$) should be equal to an under-prediction ($\hatt < t$), i.e., that it is 'as bad' if a model predicts an outcome time being 10 years longer than the truth compared to being 10 years shorter.
In the survival setting, this assumption is unlikely to be valid as we would generally prefer models to be overly cautious and therefore predict higher risk and lower survival times, as the converse could lead to observations not being responded to quick enough.
For example, if a model were to predict a life-support machine will fail after 5 years but in fact it fails after 3, this could have disastrous consequences.
