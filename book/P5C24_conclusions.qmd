::: {.content-visible when-format="html"}
{{< include _macros.tex >}}
:::

# Conclusions {#sec-conclusions}

{{< include _soon.qmd >}}

## Common problems in survival analysis {#sec-conclusions-faq}

### Data cleaning

#### Events at t=0 {.unnumbered .unlisted}

Throughout this book we have defined survival times taking values in the non-negative Reals (zero inclusive) $\NNReals$.
In practice, model implementations assume time is over the positive Reals (zero exclusive).
One must therefore consider how to deal with subjects that experience the outcome at $0$. 
There is no established best practice for dealing with this case as the answer may be data-dependent.
Possible choices include:

1. Deleting all data where the outcome occurs at $t=0$, this may be appropriate if it only happens in a small number of observations and therefore deletion is unlikely to bias predictions;
2. Update the survival time to the next smallest observed survival time. For example, if the first observation to experience the event after $t=0$ happens at $t=0.1$, then set $0.1$ as the survival time for any observation experiencing the event at $t=0$. Note this method will not be appropriate when data is over a long period, for example if measuring time over years, then there could be a substantial difference between $t=0$ and $t=1$;
3. Update the survival time to a very small value $\epsilon$ that makes sense given the context of the data, e.g., $\epsilon = 0.0001$.

#### Continuous v Discrete Time {.unnumbered .unlisted}

We defined survival tasks throughout this book assuming continuous time predictions in $\NNReals$.
In practice, many outcomes in survival analysis are recorded on a discrete scale, such as in medical statistics where outcomes are observed on a yearly, daily, monthly, hourly, etc. basis.
Whilst discrete-time survival analysis exists for this purpose (@sec-discrete), software implementations overwhelming use theory from the 'continuous-time setting.
There has not been a lot of research into whether discrete-time methods outperform continuous-time methods when correctly applied to discrete data, however available experiments do not indicate that discrete methods outperform their continuous counterparts [@Suresh2022].
Therefore it is recommended to use available software implementations, even when data is recorded on a discrete scale.

### Evaluation and prediction {#sec-conclusions-faq-eval}

* Which time points to make predictions for?
*

#### Choosing survival measures {.unnumbered .unlisted}

There are many survival measures to choose from and selecting the right one for the task might seem daunting.
We have put together a few heuristics to support decision making.

When evaluating a model's discriminatory ability we recommend Harrell's and Uno's C.
Whilst other discriminations measures can assess time-dependent trends, these are also captured in scoring rules.
If you are only looking to use a single measure (for example for reporting an experiment) then we recommend Uno's C, whilst it does have higher variance compared to others [@@Rahman2017; @Schmid2012], it performs better than Harrel's C when censoring increases [@Rahman2017].
In practice the choice of measure matters less than ensuring your reporting is transparent and honest [ @Therneau2020, @Sonabend2021].

When evaluating if a model is well-calibrated, graphical comparisons to the Kaplan-Meier provide a useful and interpretable method to quickly see if a model is a good fit to the data.
However, if you are looking to compare the calibration of models to one another, then we recommend D-calibration, which can be meaningful optimized and used for comparison.

If your goal is to compare overall predictive ability, then we recommend using both the ISBS and RCLL, if a model outperforms another with respect to both measures then that can be a strong indicator of performance.
We do not recommend any survival time measures, ultimately a probabilistic model that performs well with respect to the measures above, can be reduced to a single deterministic prediction but research for metrics in this area is nascent.

If you are looking to use a measure for model optimization then we recommend tuning with a scoring rule, which should capture discrimination and calibration simultaneously.

#### Interpreting survival models {.unnumbered .unlisted}

Interpreting models is increasingly important as we rely on more complex 'black-box' models [@Molnar2019].
Classic methods that test if a model is fit well to data, such as the AIC and BIC, have been extended to survival models however are limited in application to 'classical' models (@sec-models-classical) only [@Liang2008, @VolinskyRaftery2000].
As a more flexible alternative, any of the calibration measures in @sec-eval-distr-calib can be used to evaluate a model's fit to data.
To assess algorithmic fairness, the majority of measures discussed in @sec-eval can be used to detect bias in a survival context [@Sonabend2022a].
Gold-standard interpretability methods such as SHAP and LIME [@Molnar2019] can be extended to survival analysis off-shelf, and time-dependent extensions also exist to observe the impact of variables on the survival probability over time [@Krzyzi≈Ñski2023].
<!-- FIXME: UPDATE 'classical' above to whatever term we use for that chapter -->

## What's next for MLSA?
