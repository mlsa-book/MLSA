::: {.content-visible when-format="html"}
{{< include _macros.tex >}}
:::

# Conclusions {#sec-conclusions}

{{< include _soon.qmd >}}

## Common problems in survival analysis {#sec-conclusions-faq}

### Data cleaning

#### Events at t=0 {.unnumbered .unlisted}

Throughout this book we have defined survival times taking values in the non-negative Reals (zero inclusive) $\NNReals$.
In practice, model implementations assume time is over the positive Reals (zero exclusive).
One must therefore consider how to deal with subjects that experience the outcome at $0$. 
There is no established best practice for dealing with this case as the answer may be data-dependent.
Possible choices include:

1. Deleting all data where the outcome occurs at $t=0$, this may be appropriate if it only happens in a small number of observations and therefore deletion is unlikely to bias predictions;
2. Update the survival time to the next smallest observed survival time. For example, if the first observation to experience the event after $t=0$ happens at $t=0.1$, then set $0.1$ as the survival time for any observation experiencing the event at $t=0$. Note this method will not be appropriate when data is over a long period, for example if measuring time over years, then there could be a substantial difference between $t=0$ and $t=1$;
3. Update the survival time to a very small value $\epsilon$ that makes sense given the context of the data, e.g., $\epsilon = 0.0001$.

#### Continuous v Discrete Time {.unnumbered .unlisted}

We defined survival tasks throughout this book assuming continuous time predictions in $\NNReals$.
In practice, many outcomes in survival analysis are recorded on a discrete scale, such as in medical statistics where outcomes are observed on a yearly, daily, monthly, hourly, etc. basis.
Whilst discrete-time survival analysis exists for this purpose (@sec-discrete), software implementations overwhelming use theory from the 'continuous-time setting.
There has not been a lot of research into whether discrete-time methods outperform continuous-time methods when correctly applied to discrete data, however available experiments do not indicate that discrete methods outperform their continuous counterparts [@Suresh2022].
Therefore it is recommended to use available software implementations, even when data is recorded on a discrete scale.

### Evaluation and prediction {#sec-conclusions-faq-eval}

* Which time points to make predictions for?
*

#### Choosing survival measures {.unnumbered .unlisted}

There are many survival measures to choose from and selecting the right one for the task might seem daunting.
We have put together a few heuristics to support decision making.
Evaluation should always be according to the goals of analysis, which means using discrimination measures to evaluate rankings, calibration measures to evaluate average performance, and scoring rules to evaluate overall performance and distribution predictions.

For discrimination measures, we recommend Harrell's and Uno's C.
Whilst others can assess time-dependent trends, these are also captured in scoring rules.
In practice the choice of measure matters less than ensuring your reporting is transparent and honest [@Therneau2020; @Sonabend2021].

To assess a single model's calibration, graphical comparisons to the Kaplan-Meier provide a useful and interpretable method to quickly see if a model is a good fit to the data (@sec-calib-km).
When choosing between models, we recommend D-calibration, which can be meaningful optimized and thus used for comparison.

When picking scoring rules, we recommend using both the ISBS and RCLL.
If a model outperforms another with respect to both measures then that can be a strong indicator of performance.

Given the lack of research, if you are interested in survival time predictions then treat evaluation with caution and check for new developments in the literature.

For automated model optimization, we recommend tuning with a scoring rule, which should capture discrimination and calibration simultaneously [@Rindt2022; @Yanagisawa2023; FIXME ECML].
Though if you are only ever using a model for ranking, then we recommend tuning with Uno's C.
Whilst it does have higher variance compared to other concordance measures [@Rahman2017; @Schmid2012], it performs better than Harrell's C as censoring increases [@Rahman2017].

#### Interpreting survival models {.unnumbered .unlisted}

Interpreting models is increasingly important as we rely on more complex 'black-box' models [@Molnar2019].
Classic methods that test if a model is fit well to data, such as the AIC and BIC, have been extended to survival models however are limited in application to 'classical' models (@sec-models-classical) only [@Liang2008; @VolinskyRaftery2000].
As a more flexible alternative, any of the calibration measures in @sec-eval-distr-calib can be used to evaluate a model's fit to data.
To assess algorithmic fairness, the majority of measures discussed in @sec-eval can be used to detect bias in a survival context [@Sonabend2022a].
Gold-standard interpretability methods such as SHAP and LIME [@Molnar2019] can be extended to survival analysis off-shelf [@Langbein2024], and time-dependent extensions also exist to observe the impact of variables on the survival probability over time [@Krzyzi≈Ñski2023; @Langbein2024].
<!-- FIXME: UPDATE 'classical' above to whatever term we use for that chapter -->

## What's next for MLSA?
