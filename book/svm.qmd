---
abstract: This chapter introduces support vector machines (SVMs) for regression and then describes the extensions to survival analysis. Regression SVMs extend simple linear methods by estimating flexible, non-linear hyperplanes that minimise the difference between predictions and the truth for individual observations. In survival analysis, SVMs may make survival time or ranking predictions, however there is no current formulation for survival distribution predictions. The chapter begins by discussing survival time SVMs and then ranking models before concluding with a hybrid formulation that combines both model forms. This primarily covers the work of Shivaswamy and Van Belle. SVMs are a powerful method for estimating non-linear relationships in data and have proven to be well-performing models in regression and classification. However, SVMs are less developed in survival analysis and have been shown to perform worse than other models in experiments.
---

{{< include _setup.qmd >}}

# Support Vector Machines {#sec-surv-ml-models-svm}

{{< include _wip_minor.qmd >}}

Support vector machines are a popular class of model in regression and classification settings due to their ability to make accurate predictions for complex high-dimensional, non-linear data.
In survival analysis, survival support vector machines (SSVMs), are one of the few model classes that can directly estimate survival times.
However, experiments with SSVMs have demonstrated that they are best used for ranking predictions as their survival time predictions can be wildly out of range.
As a consequence, SSVMs cannot easily be used to make distribution predictions.
As usual this chapter starts with SVMs in the regression setting before moving to adaptions for survival analysis.

## SVMs for Regression

In simple linear regression, the aim is to estimate the line $y = b + x\beta_1$ by estimating the $b,\beta_1$ coefficients.
As the number of coefficients increases, the goal is to instead estimate the *hyperplane*, which divides the higher-dimensional space into two separate parts.
To visualize a hyperplane, imagine looking at a room from a birds eye view that has a dividing wall cutting the room into two halves (@fig-svm-hyper).
In this view, the room appears to have two dimensions (x=left-right, y=top-bottom) and the divider is a simple line of the form $y = b + x\beta_1$.
In reality, this room is actually three dimensional and has a third dimension (z=up-down) and the divider is therefore a hyperplane of the form $y = b + x\beta_1 + z\beta_2$.

![Visualising a hyperplane by viewing a 3D room in two-dimensions with a wall that is now seen as a simple line. When standing in this room, the wall will clearly exist in three dimensional space.](Figures/svm/hyperplane.png){#fig-svm-hyper fig-alt="AI generated image of a birds-eye view of an office cut in half by a dividing wall." width=400}

Continuing the linear regression example, consider a simple model where the objective is to find the $\bb$ coefficients that minimize $\sum^n_{i=1} (g(\xx_i) - y_i)^2$ where $g(\xx_i) = b + \xx_i\bb$ and $(\XX, \yy)$ is training data such that $\XX \in \Reals^{n \times p}$ and $\yy \in \Reals^n$.
In a higher-dimensional space, a penalty term can be added for variable selection to reduce model complexity, commonly of the form

$$
\frac{1}{2} \sum^n_{i=1} (g(\xx_i) - y_i)^2 + \frac{\lambda}{2} \|\bb\|^2
$$

for some penalty term $\lambda \in \Reals$ and $\dvec{\beta}{n}$.
Minimizing this error function effectively minimizes the *average* difference between all predictions and true outcomes, resulting in a hyperplane that represents the best *linear* relationship between coefficients and outcomes.

Similarly to linear regression, support vector machines (SVMs) [@CortesVapnik1995] also fit a hyperplane, $g$, on given training data, $\XX$.
However, in SVMs, the goal is to fit a *flexible* (non-linear) hyperplane that minimizes the difference between predictions and the truth for *individual* observations.
A core feature of SVMs is that one does not try to fit a hyperplane that makes perfect predictions as this would overfit the training data and is unlikely to perform well on unseen data.
Instead, SVMs use a regularized error function, which allows incorrect predictions (errors) for some observations, with the magnitude of error controlled by an $\epsilon>0$ parameter as well as slack parameters, $\xi, \xi^*$:

$$
\begin{aligned}
& \min_{\bb,b, \xi, \xi^*} \frac{1}{2} \|\bb\|^2 + \gamma \sum^n_{i=1}(\xi_i + \xi_i^*) \\
& \textrm{subject to}
\begin{dcases}
g(\xx_i) & \geq y_i -\epsilon - \xi_i \\
g(\xx_i) & \leq y_i + \epsilon + \xi_i^* \\
\xi_i, \xi_i^* & \geq 0 \\
\end{dcases}
\end{aligned}
$$ {#eq-svm-opt}

$\forall i\in 1,...,n$ where $g(\xx_i) = \bb^T\xx_i + b$ for model weights $\bb,b \in \Reals$ and the same training data $(\XX, \yy)$ as above.

@fig-svm-regr visualizes a support vector regression model in two dimensions.
The red circles are values within the $\epsilon$-tube and are thus considered to have a negligible error.
In fact, the red circles do not affect the fitting of the optimal line $g$ and even if they moved around, as long as they remain within the tube, the shape of $g$ would not change.
In contrast the blue diamonds have an unacceptable margin of error -- as an example the top blue diamond will have $\xi_i = 0$ but $\xi_i^* > 0$, thus influencing the estimation of $g$.
Points on or outside the epsilon tube are referred to as *support vectors* as they affect the construction of the hyperplane.
The $\gamma \in \PReals$ hyperparameter controls the slack parameters and thus as $\gamma$ increases, the number of errors (and subsequently support vectors) is allowed to increase resulting in low variance but higher bias, in contrast a lower $\gamma$ is more likely to introduce overfitting with low bias but high variance [@Hastie2001].
$\gamma$ should be tuned to control this trade-off.

![Visualising a support vector machine with an $\epsilon$-tube and slack parameters $\xi$ and $\xi^*$. Red circles are values within the $\epsilon$-tube and blue diamonds are support vectors on and outside the tube. x-axis is single covariate, $x$, and y-axis is $g(x) = x\beta_1 + b$.](Figures/svm/regression.png){#fig-svm-regr fig-alt="Line graph with g(x) on the y-axis and 'x' on the x-axis. A solid black line labelled 'y' runs along g(x)=x, i.e., from the bottom-left to the top-right of the graph. Parallel to this line, above and below, are two dotted lines labelled 'y+epsilon' and 'y-epsilon' respectively. These dotted lines form the boundaries of the epsilon-tube. Red dots lie between the dotted lines and blue diamonds are outside the lines. One blue dot above the top dotted line is labelled to reflect that it represents the distance xi* and one blue dot below the bottom line is the distance xi." width=500}

The other core feature of SVMs is exploiting the *kernel trick*, which uses functions known as *kernels* to allow the model to learn a non-linear hyperplane whilst keeping the computations limited to lower-dimensional settings.
Once the model coefficients have been estimated using the optimization above, predictions for a new observation $\xx^*$ can be made using a function of the form

$$
\hatg(\xx^*) = \sum^n_{i=1} \alpha_iK(\xx^*,\xx_i) + b
$$ {#eq-svm-pred}

Details (including estimation) of the $\alpha_i$ Lagrange multipliers are beyond the scope of this book, references are given at the end of this chapter for the interested reader.
$K$ is a kernel function, with common functions including the linear kernel, $K(x^*,x_i) = \sum^p_{j=1} x_{ij}x^*_j$, radial kernel, $K(x^*,x_i) = \exp(-\omega\sum^p_{j=1} (x_{ij} - x^*_j)^2)$ for some $\omega \in \PReals$, and polynomial kernel, $K(x^*,x_i) = (1 + \sum^p_{j=1} x_{ij}x^*_j)^d$ for some $d \in \PNaturals$.

The choice of kernel and its parameters, the regularization parameter $\gamma$, and the acceptable error $\epsilon$, are all tunable hyper-parameters, which makes the support vector machine a highly adaptable and often well-performing machine learning method.
The parameters $\gamma$ and $\epsilon$ often have no clear apriori meaning (especially true in the survival setting predicting abstract rankings) and thus require tuning over a great range of values; no tuning usually results in a poor model fit [@Probst2019].

## SVMs for Survival Analysis {#sec-surv-ml-models-svm-surv}

Extending SVMs to the survival domain (SSVMs) is a case of: i) identifying the quantity to predict; and ii) updating the optimization problem (@eq-svm-opt) and prediction function (@eq-svm-pred) to accommodate for censoring.
In the first case, SSVMs can be used to either make survival time or ranking predictions, which are discussed in turn.
The notation above is reused below for SSVMs, with additional notation introduced when required and now using the survival training data $(\XX, \tt, \dd)$.

### Survival time SSVMs

To begin, consider the objective for support vector regression with the $y$ variable replaced with the usual survival time outcome $t$, for all $i\in 1,...,n$:

$$
\begin{aligned}
& \min_{\bb, b, \xi, \xi^*} \frac{1}{2} \|\bb\|^2 + \gamma \sum^n_{i=1}(\xi_i + \xi_i^*) \\
& \textrm{subject to}
\begin{dcases}
g(\xx_i) & \geq t_i -\epsilon - \xi_i \\
g(\xx_i) & \leq t_i + \epsilon + \xi_i^* \\
\xi_i, \xi_i^* & \geq 0 \\
\end{dcases}
\end{aligned}
$$ {#eq-svm-survnaive}

In survival analysis, this translates to fitting a hyperplane in order to predict the true survival time.
However, as with all adaptations from regression to survival analysis, there needs to be a method for incorporating censoring.

Recall the $(t_l, t_u)$ notation to describe censoring as introduced in @sec-surv such that the outcome occurs within the range $(t_l, t_u)$.
<!-- FIXME: MAKE CROSS-REFERENCE MORE SPECIFIC -->
Let $t \in \PReals$ be some known time-point, then an observation is:

* left-censored if the survival time is less than $t$: $(t_l, t_u) = (-\infty, t)$;
* right-censored if the true survival time is greater than $t$: $(t_l, t_u) = (t, \infty)$; or
* uncensored if the true survival time is known to be $t$: $(t_l, t_u) = (t, t)$.

Define $L$ as the set of observations with a finite lower-bounded time, which can be seen above to be those that are right-censored or uncensored, and define $U$ as the analogous set of observations with a finite upper-bounded time, which are those that are left-censored or uncensored.

Consider these definitions in the context of the constraints in @eq-svm-survnaive.
The first constraint ensures the hyperplane is greater than some lower-bound created by subtracting the slack parameter from the true outcome -- given the set definitions above this constraint only has meaning for observations with a finite lower-bound, $i \in L$, otherwise the constraint would include $g(\xx_i) \geq -\infty$, which is clearly not useful.
Similarly the second constraint ensures the hyperplane is less than some upper-bound, which again can only be meaningful for observations $i \in U$.
Restricting the constraints in this way leads to the optimization problem [@Shivaswamy2007] below and visualised in @fig-svm-surv:

$$
\begin{aligned}
& \min_{\bb, b, \xi, \xi^*} \frac{1}{2}\|\bb\|^2 + \gamma\Big(\sum_{i \in U} \xi_i + \sum_{i \in L} \xi_i^*\Big) \\
& \textrm{subject to}
\begin{dcases}
g(\xx_i) & \geq t_i -\xi_i, i \in L \\
g(\xx_i) & \leq t_i + \xi^*_i, i \in U \\
\xi_i \geq 0, i\in L; \xi^*_i \geq 0, i \in U
\end{dcases}
\end{aligned}
$$

If no observations are censored then the optimization becomes the regression optimization in (@eq-svm-opt).
Note that in SSVMs, the $\epsilon$ parameters are typically removed to better accommodate censoring and to help prevent the same penalization of over- and under-predictions.
In contrast to this formulation, one *could* introduce more $\epsilon$ and $\gamma$ parameters to separate between under- and over-predictions and to separate right- and left-censoring, however this leads to eight tunable hyperparameters, which is inefficient and may increase overfitting [@pkgsurvivalsvm; @Land2011].
The algorithm can be simplified to right-censoring only by removing the second constraint completely for anyone censored:

$$
\begin{aligned}
& \min_{\bb, b, \xi, \xi^*} \frac{1}{2}\|\bb\|^2 + \gamma \sum_{i = 1}^n (\xi_i + \xi_i^*) \\
& \textrm{subject to}
\begin{dcases}
g(\xx_i) & \geq t_i - \xi^*_i \\
g(\xx_i) & \leq t_i + \xi_i, i:\delta_i=1 \\
\xi_i, \xi_i^* & \geq 0 \\
\end{dcases}
\end{aligned}
$$

$\forall i\in 1,...,n$. With the prediction for a new observation $\xx^*$ calculated as,

$$
\hatg(\xx^*) = \sum^n_{i=1} \alpha^*_i K(\xx_i, \xx^*) - \delta_i\alpha_i'K(\xx_i, \xx^*) + b
$$

Where again $K$ is a kernel function and the calculation of the Lagrange multipliers is beyond the scope of this book.

![Visualising a survival time SVM. Blue diamonds are influential support vectors, which are uncensored or left-censored when $g(\xx)<t$ or uncensored or right-censored when $g(\xx)>t$. Red circles are non-influential observations.](Figures/svm/survival.png){#fig-svm-surv fig-alt="Line graph with g(x) on the y-axis and 'x' on the x-axis. A solid black line labelled 'y' runs along g(x)=x, i.e., from the bottom-left to the top-right of the graph. Red circles and blue dots lie on both sides of the line. Blue dots above the line represent observations with finite upper bounds and blue dots below the line represent observations with finite lower bounds. The blue dots are labelled to show they represent the distance xi* and xi." width=500}

### Ranking SSVMs

Support vector machines can be used to estimate rankings by penalizing predictions that result in disconcordant predictions.
Recall the definition of concordance from @sec-eval-crank: ranking predictions for a pair of comparable observations $(i, j)$ where $t_i < t_j \cap \delta_i = 1$, are called concordant if $r_i > r_j$ where $r_i, r_j$ are the predicted ranks for observations $i$ and $j$ respectively where a higher value implies greater risk.
Using the prognostic index as a ranking prediction (@sec-surv-setmltask), a pair of observations is concordant if $g(\xx_i) > g(\xx_j)$ when $t_i < t_j$, leading to:

$$
\begin{aligned}
& \min_{\bb, b, \xi} \frac{1}{2}\|\bb\|^2 + \gamma\sum_{i =1}^n \xi_i \\
& \textrm{subject to}
\begin{dcases}
g(\xx_i) - g(\xx_j) & \geq \xi_i, \forall i,j \in CP \\
\xi_i & \geq 0, i = 1,...,n \\
\end{dcases}
\end{aligned}
$$

where $CP$ is the set of comparable pairs defined by $CP = \{(i, j) : t_i < t_j \wedge \delta_i = 1\}$.
Given the number of pairs, the optimization problem quickly becomes difficult to solve with a very long runtime.
To solve this problem @VanBelle2011b found an efficient reduction that sorts observations in order of outcome time and then compares each data point $i$ with the observation with the next smallest *survival* time, skipping over censored observations.
For example, consider the observed outcomes, unsorted on the left and sorted on the right:

| Index | Unsorted | Sorted |
| ---   |   ---    |   ---  |
| 1 | $(t_1 = 2, \delta_1 = 1)$ | $(t_1 = 2, \delta_1 = 1)$
| 2 | $(t_2 = 4, \delta_2 = 1)$ | $(t_2 = 3, \delta_2 = 0)$
| 3 | $(t_3 = 3, \delta_3 = 0)$ | $(t_3 = 4, \delta_3 = 1)$
| 4| $(t_4 = 8, \delta_4 = 0)$ | $(t_4 = 8, \delta_4 = 0)$

Once sorted, the reduced set of observations and survival times to compare are

| Comparable observations | Survival times |
| - | - |
| $(i=4, \ j(i)=3)$ | $(t_i=8, \ t_{j(i)}=4)$ |
| $(i=3, \ j(i)=1)$ | $(t_i=4, \ t_{j(i)}=2)$ |
| $(i=2, \ j(i)=1)$ | $(t_i=3, \ t_{j(i)}=2)$ |

where $j(i)$ is the observation with the next smallest survival time compared to $i$.
Note how the censored second (ordered) observation is skipped over when comparing to the third (ordered) observation.

In this algorithm, the first observation is always assumed to be uncensored even if censored in reality.
Using this reduction, the algorithm becomes

$$
\begin{aligned}
& \min_{\bb, b, \xi} \frac{1}{2}\|\bb\|^2 + \gamma\sum_{i =1}^n \xi_i \\
& \textrm{subject to}
\begin{dcases}
g(\xx_i) - g(\xx_{j(i)}) & \geq t_i - t_{j(i)} - \xi_i \\
\xi_i & \geq 0
\end{dcases}
\end{aligned}
$$

$\forall i = 1,...,n$.
Note the updated right hand side of the constraint, which plays a similar role to the $\epsilon$ parameter by allowing 'mistakes' in predictions without penalty.

Predictions for a new observation $\xx^*$ are calculated as,

$$
\hatg(\xx^*) = \sum^n_{i=1} \alpha_i(K(\xx_i, \xx^*) - K(\xx_{j(i)}, \xx^*)) + b
$$

Where $\alpha_i$ are again Lagrange multipliers.


### Hybrid SSVMs

Finally, @VanBelle2011b noted that the ranking algorithm could be updated to add the constraints of the regression model, thus providing a model that simultaneously optimizes for ranking whilst providing continuous values that can be interpreted as survival time predictions.
This results in the hybrid SSVM with constraints $\forall i = 1,...,n$:

$$
\begin{aligned}
& \min_{\bb, b, \xi, \xi', \xi^*} \frac{1}{2}\|\bb\|^2 + \textcolor{CornflowerBlue}{\gamma\sum_{i =1}^n \xi_i} + \textcolor{Rhodamine}{\mu \sum^n_{i=1}(\xi_i' + \xi_i^*)} \\
& \textrm{subject to}
\begin{dcases}
\textcolor{CornflowerBlue}{g(\xx_i) - g(\xx_{j(i)})} & \textcolor{CornflowerBlue}{\geq t_i - t_{j(i)} - \xi_i} \\
\textcolor{Rhodamine}{g(\xx_i)} & \textcolor{Rhodamine}{\leq t_i + \xi^*_i, i:\delta_i=1} \\
\textcolor{Rhodamine}{g(\xx_i)} & \textcolor{Rhodamine}{\geq t_i - \xi'_i} \\
\textcolor{CornflowerBlue}{\xi_i}, \textcolor{Rhodamine}{\xi_i', \xi_i^*} & \geq 0 \\
\end{dcases}
\end{aligned}
\label{eq:surv_ssvmvb2}
$$

The blue parts of the equation make up the ranking model and the red parts are the regression model.
$\gamma$ is the penalty associated with the regression method and $\mu$ is the penalty associated with the ranking method setting $\gamma = 0$ results in the regression SVM and $\mu = 0$ results in the ranking SSVM.
Hence, fitting the hybrid model and tuning these parameters is an efficient way to automatically detect which SSVM is best suited to a given task.

Once the model is fit, a prediction from given features $\xx^* \in \Reals^p$, can be made using the equation below, again with the ranking and regression contributions highlighted in blue and red respectively.

$$
\hatg(\xx^*) = \sum^n_{i=1} \textcolor{CornflowerBlue}{\alpha_i(K(\xx_i, \xx^*) - K(\xx_{j(i)}, \xx^*))} + \textcolor{Rhodamine}{\alpha^*_i K(\xx_i, \xx^*) - \delta_i\alpha_i'K(\xx_i, \xx^*)} + b
$$

where $\alpha_i, \alpha_i^*, \alpha_i'$ are Lagrange multipliers and $K$ is a chosen kernel function, which may have further hyper-parameters to select or tune.

## Conclusion

:::: {.callout-warning icon=false}

## Key takeaways

* Support vector machines (SVMs) are a highly flexible machine learning method that can use the 'kernel trick' to represent infinite dimensional spaces in finite domains;
* Survival SVMs (SSVMs) extend regression SVMs by either making survival time predictions, ranking predictions, or a combination of the two;
* The hybrid SSVM provides an efficient method that encapsulates all the elements of regression and ranking SSVMs and is therefore a good model to include in benchmark experiments to test the potential of SSVMs.

::::

:::: {.callout-important icon=false}

## Limitations

* SSVMs can only perform well with extensive tuning of hyper-parameters over a wide domain. To-date, no papers have experimented with the tuning range for the $\gamma$ and $\mu$ parameters, we note [@pkgsurvivalsvm] tune over $(2^{-5}, 2^5)$.
* Even using the regression or hybrid model, the authors' experiments with the SSVM have consistently shown 'survival time' estimates tend to be unrealistically large.
* Due to the above limitation, regression estimates cannot be meaningful interpreted and as a consequence there is no sensible composition to create a distribution prediction from an SSVM. Hence, we are hesitant to suggest usage of SSVMs outside of ranking-based problems.

::::

:::: {.callout-tip icon=false}

## Further reading

* @Shivaswamy2007, @Khan2008, @Land2011, and @VanBelle2011b to learn more about regression SSVMs.
* @Evers2008, @VanBelle2007, @VanBelle2008, and @VanBelle2011b for more information about ranking SSVMs.
* @Goli2016a and @Goli2016b introduce mean residual lifetime optimization SSVMs.
* @pkgsurvivalsvm surveys and benchmarks SSVMs.

::::
