---
abstract: TODO (150-200 WORDS)
---

{{< include _setup.qmd >}}

# Support Vector Machines {#sec-surv-ml-models-svm}

{{< include _wip.qmd >}}

## SVMs for Regression

In the simplest explanation, support vector machines (SVMs)  [@CortesVapnik1995] fit a hyperplane, $g$, on given training data and make predictions for new values as $\hatg(X^*)$ for some testing covariate $X^*$. One may expect the hyperplane to be fit so that all training covariates would map perfectly to the observed labels (a 'hard-boundary') however this would result in overfitting and instead an acceptable ('soft'-)boundary of error, the `$\epsilon$-tube', dictates how 'incorrect' predictions may be, i.e. how large an underestimate or overestimate. (@fig-surv-svm) visualises support vector machines for regression with a linear hyperplane $g$, and an acceptable boundary of error within the dashed lines (the $\epsilon$-tube). SVMs are not limited to linear boundaries and *kernel* functions are utilised to specify more complex hyperplanes. Exact details of the optimization/separating procedure are not discussed here but many off-shelf 'solvers' exist in different programming languages for fitting SVMs.

In the regression setting, the goal of SVMs is to estimate the function
$$
g: \Reals^p \rightarrow \Reals; \quad (x) \mapsto x\beta + \beta_0
$$ {#eq-svm}
by estimation of the weights $\beta \in \Reals^p, \beta_0 \in \Reals$ via the optimisation problem
$$
\begin{aligned}
& \min_{\beta,\beta_0, \xi, \xi^*} \frac{1}{2} \|\beta\|^2 + \gamma \sum^n_{i=1}(\xi_i + \xi_i^*) \\
& \textrm{subject to}
\begin{dcases}
y_i - g(x_i) & \leq \epsilon + \xi_i \\
g(x_i) - y_i & \leq \epsilon + \xi_i^* \\
\xi_i, \xi_i^* & \geq 0 \\
\forall i\in 1,...,n
\end{dcases}
\end{aligned}
$$ {#eq-svm-opt}
where $\gamma \in \Reals$ is the regularization/cost parameter, $\xi_i,\xi_i^*$ are slack parameters and $\epsilon$ is a margin of error for observations on the wrong side of the hyperplane, and $g$ is defined in (@eq-svm). The effect of the slack parameters is seen in (@fig-surv-svm) in which a maximal distance from the $\epsilon$-tube is dictated by the slack variables.

In fitting, the dual of the optimisation is instead solved and substituting the optimised parameters into (@eq-svm) gives the prediction function,
$$
\hatg(x^*) = \sum^n_{i=1} (\alpha_i - \alpha_i^*)K(x^*,x_i) + \beta_0
$$
where $\alpha_i, \alpha_i^*$ are Lagrangrian multipliers and $K$ is some kernel function.\footnote{Discussion about the purpose of kernels and sensible choices can be found in  [@pkgsurvivalsvm; @Hastie2013; @Vapnik1998].} The Karush-Kuhn-Tucker conditions required to solve the optimisation for $\alpha$ result in the key property of SVMs, which is that values $\alpha_i = \alpha_i^* = 0$ indicate that observation $i$ is 'inside' the $\epsilon$-tube and if $\alpha_i \neq 0$ or $\alpha^*_i \neq 0$ then $i$ is outside the tube and termed a *support vector*. It is these 'support vectors' that influence the shape of the separating boundary.

The choice of kernel and its parameters, the regularization parameter $\gamma$, and the acceptable error $\epsilon$, are all tunable hyper-parameters, which makes the support vector machine a highly adaptable and often well-performing machine learning method. However the parameters $\gamma$ and $\epsilon$ often have no clear apriori meaning (especially true when predicting abstract rankings) and thus require extensive tuning over a great range of values; no tuning will result in a very poor model fit.

![Visualising a support vector machine with an $\epsilon$-tube and slack parameters $\xi$ and $\xi^*$. Red circles are values within the $\epsilon$-tube and blue diamonds are values outside the tube. x-axis is single covariate, $x$, and y-axis is $g(x) = x\beta + \beta_0$.](Figures/svm/svm.png){#fig-surv-svm fig-alt="TODO"}

## SVMs for Survival Analysis {#sec-surv-ml-models-svm-surv}

From predicting continuous values in regression, SVMs can be extended to survival analysis by either making survival time or survival ranking predictions.
To distinguish between prediction types, this chapter separates between survival time support vector machines (SSVMs) and ranking SSVMs, which will be discussed in turn.

For both types of SSVM let: $\xi_i,\xi_i^*,\xi_i'$ be slack variables; $\bb,\beta_0$ be model weights in $\Reals$; $\gamma, \mu$ be regularisation hyper-parameters in $\Reals$; $(\XX, \tt, \dd)$ be the usual training data; and $g(x_i) = x_i\beta_i + \beta_0$.
<!-- FIXME: CHECK BETA_0 CORRECT ABOVE -->

### Survival time SSVMs

To begin, consider the objective for support vector regression with the $y$ variable replaced with the usual survival time outcome $t$

$$
\begin{aligned}
& \min_{\beta,\beta_0, \xi, \xi^*} \frac{1}{2} \|\beta\|^2 + \gamma \sum^n_{i=1}(\xi_i + \xi_i^*) \\
& \textrm{subject to}
\begin{dcases}
t_i - g(x_i) & \leq \epsilon + \xi_i \\
g(x_i) - t_i & \leq \epsilon + \xi_i^* \\
\xi_i, \xi_i^* & \geq 0 \\
\forall i\in 1,...,n
\end{dcases}
\end{aligned}
$$

In survival analysis, this translates to fitting a hyperplane in order to predict the true survival time.
However, as with all adaptations from regression to survival analysis, there needs to be a method for incorporating censoring.
Beginning with the simple right-censoring setting, consider how censoring effects the constraints above.

When $g(x_i) < t_i$ it is clear the model has made an under-prediction, regardless of whether an observation is censored at $t_i$ or experiences the event.
In contrast, when $g(x_i) > t_i$ it is *known* that this is an over-prediction if $i$ experiences the event but it is unknown if it's an over-prediction otherwise.
As a concrete example, say a patient is discharged from hospital after three weeks and Model A predicts they died after two weeks whereas Model B predicts they died after four.
It is clear Model A made an under-prediction as the patient was at least alive long enough to be discharged, on the other hand it is unknown if they died immediately after, making Model B's prediction correct, if if they died ten years after, making its prediction very wrong.
With no way to know the truth, over-predictions must be omitted when an observation is censored, leading to the optimization problem [@Shivaswamy2007]:

$$
\begin{aligned}
& \min_{\beta, \beta_0, \xi, \xi^*} \frac{1}{2}\|\beta\|^2 + \gamma\Big(\sum_{i = 1}^n \xi_i + \xi_i^*\Big) \\
& \textrm{subject to}
\begin{dcases}
t_i - g(x_i) & \leq \xi^*_i \\
\Delta_i(g(x_i) - t_i) & \leq \xi_i \\
\xi_i, \xi_i^* & \geq 0 \\
\forall i\in 1,...,n
\end{dcases}
\end{aligned}
$$

To clarify how this relates to the discussion above note that in the first constraint $\xi^*_i$ must be $\geq 0$ and hence $t_i - g(x_i) \geq 0$ and $t_i \geq g(x_i)$, and analogously in the second equation.
Note that in SSVMs, the $\epsilon$ parameters are typically removed to better accommodate censoring and to help prevent the same penalization of over- and under-predictions.
In contrast, one could introduce more $\epsilon$ and $\gamma$ parameters to separate between under- and over-predictions and to separate right- and left-censoring, however this leads to eight tunable hyperparameters, which is inefficient and may actually increase overfitting [@pkgsurvivalsvm; @Land2011].

The above algorithm can also be extended to incorporate left-censoring relatively trivially by using a similar argument to the right-censoring case and noting that an under-prediction can only be known if there is no left-censoring, hence [@Shivaswamy2007]:

$$
\begin{aligned}
& \min_{\beta, \beta_0, \xi, \xi^*} \frac{1}{2}\|\beta\|^2 + \gamma\Big(\sum_{i \in R} \xi_i + \sum_{i \in L} \xi_i^*\Big) \\
& \textrm{subject to}
\begin{dcases}
t_i - g(x_i) & \leq \xi_i, i \in L \\
g(x_i) - t_i & \leq \xi^*_i, i \in U \\
\xi_i \geq 0, i\in U; \xi^*_i \geq 0, i \in L
\end{dcases}
\end{aligned}
$$

Recall the $(t_l, t_u)$ notation to describe censoring as introduced in @sec-surv where an observation is left-censored if the survival time is known to be less than some $t \in \PReals$, hence $(t_l, t_u) = (\infty, t)$, right-censored if $(t_l, t_u) = (t, \infty)$, or uncensored if $(t_l, t_u) = (t, t)$.
<!-- FIXME: MAKE CROSS-REFERENCE MORE SPECIFIC -->
In the constraints above, $L$ is the set of observations with a finite lower-bound time such as those who are right-censored or uncensored, and $U$ is the analogous set of observations with a finite upper-bounded time.
Once again to aid in interpretation, the first constraint penalizes over-predictions which occur when $t_i \geq g(x_i)$ and thus can only be identified if the survival time outcome has a finite lower-bound, in contrast the second constraint penalizes under-predictions when $t_i \leq g(x_i)$ which are only knowable if the outcome has a finite upper-bound.
If no one is censored then the optimisation is identical to the regression optimisation in (@eq-svm-opt).

### Ranking SSVMs

As discussed in @sec-eval-det, survival time predictions are actually quite rare in survival analysis and more often than not survival times are used to infer rankings between observations.
However, when ranking is the primary goal, SSVMs can be more effectively trained by updating the optimization function to penalize incorrect rankings as opposed to incorrect survival times.

A natural method to optimize with respect to ranking is to penalize predictions that result in disconcordant predictions.
Recall the definition of concordance from @sec-eval-crank: ranking predictions for a pair of comparable observations $(i, j)$ where $t_i < t_j \cap \delta_i = 1$, is called concordant if $r_i > r_j$.
Using the prognostic index as a ranking prediction (@sec-surv-setmltask), a pair of observations is concordant if $g(x_i) < g(x_j)$ when $t_i < t_j$, leading to:

$$
\begin{aligned}
& \min_{\beta, \beta_0, \xi} \frac{1}{2}\|\beta\|^2 + \gamma\sum_{i =1}^n \xi_i \\
& \textrm{subject to}
\begin{dcases}
g(x_i) - g(x_j) & \leq \xi_i, \forall i,j \in CP \\
\xi_i & \geq 0, i = 1,...,n \\
\end{dcases}
\end{aligned}
$$

<!-- FIXME: CONFIRM INEQUALITY IN FIRST CONSTRAINT CORRECT -->

where $CP$ is the set of comparable pairs defined by $CP = \{(i, j) : t_i < t_j \wedge \delta_i = 1\}$.
Given the number of pairs, the optimization problem quickly becomes difficult to solve with a very long runtime.
To solve this problem @VanBelle2011b found an efficient reduction that sorts observations in order of outcome time and then compares each data point $i$ with the observation with the next smallest *survival* time, skipping over censored observations.
For example, consider the observed outcomes $\{(1, 1), (4, 0), (2, 1), (8, 0)\}$, sorting by outcome time gives  $\{(1, 1), (2, 0), (4, 1), (8, 0)\}$.
The comparable pairs are then $(i=2, j=1)$, $(i=3,j=1)$, $(i=4,j=3)$ where the censored second outcome is skipped over when finding a pair for observation $3$.
In this algorithm, the first observation is always assumed to be uncensored even if it was censored in reality.
Using this reduction, the algorithm becomes

$$
\begin{aligned}
& \min_{\beta, \beta_0, \xi} \frac{1}{2}\|\beta\|^2 + \gamma\sum_{i =1}^n \xi_i \\
& \textrm{subject to}
\begin{dcases}
g(x_i) - g(x_{j(i)}) & \leq t_i - t_{j(i)} + \xi_i \\
\xi_i & \geq 0 \\
\forall i = 1,...,n
\end{dcases}
\end{aligned}
$$

where $j(i)$ is the observation with the next smallest survival time compared to $i$.
Note the updated right hand side of the constraint, which, plays a similar role to the $\epsilon$ parameter, by allowing prediction mistakes without penalty but where the $\epsilon$-tube is defined by the true difference between the observed outcomes.

### Hybrid SSVMs

Finally, @VanBelle2011b noted that the ranking algorithm could be updated to add the constraints of the regression model, thus providing a model that simultaneously optimizes for ranking whilst providing continuous values that can be interpreted as survival time predictions.
This results in the hybrid SSVM with constraints:

$$
\begin{aligned}
& \min_{\beta, \beta_0, \xi, \xi', \xi^*} \frac{1}{2}\|\beta\|^2 + \textcolor{CornflowerBlue}{\gamma\sum_{i =1}^n \xi_i} + \textcolor{Rhodamine}{\mu \sum^n_{i=1}(\xi_i' + \xi_i^*)} \\
& \textrm{subject to}
\begin{dcases}
\textcolor{CornflowerBlue}{g(x_i) - g(x_{j(i)})} & \textcolor{CornflowerBlue}{\leq t_i - t_{j(i)} + \xi_i} \\
\textcolor{Rhodamine}{\Delta_i(g(x_i) - t_i)} & \textcolor{Rhodamine}{\leq \xi^*_i} \\
\textcolor{Rhodamine}{t_i - g(x_i)} & \textcolor{Rhodamine}{\leq \xi'_i} \\
\textcolor{CornflowerBlue}{\xi_i}, \textcolor{Rhodamine}{\xi_i', \xi_i^*} & \geq 0 \\
\forall i = 1,...,n \\
\end{dcases}
\end{aligned}
\label{eq:surv_ssvmvb2}
$$

The blue parts of the equation make up the ranking model and the red parts are the regression model.
$\gamma$ is the penalty associated with the regression method and $\mu$ is the penalty associated with the ranking method setting $\gamma = 0$ results in the regression SVM and $\mu = 0$ results in the ranking SSVM.
Hence, fitting the hybrid model and tuning these parameters is an efficient way to automatically detect which SSVM is best suited to a given task.

Once the model is fit, a prediction from given features $\xx^* \in \Reals^p$, can be made using the equation below, again with the ranking and regression contributions highlighted in blue and red respectively.

$$
\hatg(\xx^*) = \sum^n_{i=1} \textcolor{CornflowerBlue}{\alpha_i(K(\xx_i, \xx^*) - K(\XX_{j(i)}, \xx^*))} + \textcolor{Rhodamine}{\alpha^*_i K(\xx_i, \xx^*) - \Delta_i\alpha_i'K(\xx_i, \xx^*)} + \beta_0
$$

where $\alpha_i, \alpha_i^*, \alpha_i'$ are Lagrange multipliers and $K$ is a chosen kernel function, which may have further hyper-parameters to select or tune.


### Conclusions

#### Further reading

* To learn more about regression SSVMs see [@Shivaswamy2007, @Khan2008, @Land2011, @VanBelle2011b]
* For more information about ranking SSVMs, the following may be useful resources [@Evers2008; @VanBelle2007; @VanBelle2008; @VanBelle2011b].
* Whilst most research and implementation of SSVMs has focused on the classes above, also of note is the mean residual lifetime optimization introduced in [@Goli2016a; @Goli2016b].
* A survey of SSVMs can be found in @pkgsurvivalsvm