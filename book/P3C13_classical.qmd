---
abstract: TODO (150-200 WORDS)
---

::: {.content-visible when-format="html"}
{{< include _macros.tex >}}
:::

# Classical Statistical Models {#sec-models-classical}

{{< include _wip.qmd >}}

Defining if a model is 'machine learning' is surprisingly difficult, with terms such as 'iterative' and 'loss optimization' being frequently used in often clunky definitions.
This book defines machine learning models as those that require intensive model fitting procedures such as recursion or iteration, that are flexible with hyper-parameters that can be tuned, that require some form of loss optimization, but that are often 'black boxes' that are difficult to interpret.
In contrast, 'classical statistical' (or 'classical' for short) models, refer here to low-complexity models that are either non-parametric or have parameters that can be fit with maximum likelihood estimation (or an analogous method).
Further, classical models are assumed to be fast to fit and highly interpretable, though can be inflexible and may make unreasonable assumptions.

Whilst this book is focused on machine learning, there are three primary reasons to include a chapter on 'classical statistical' models ('classical' for short).

Firstly, the boundary between machine learning and classical models is fuzzy as the latter can be augmented with modern machine learning methods such as feature selection and regularization, to create a model that could be termed as 'machine learning'.
In addition, classical models can be used within machine learning algorithms, such as boosting and stacking.

Secondly, the vast majority of machine learning survival algorithms make use of classical models.
Often by using the Kaplan-Meier estimator, or assuming a proportional hazards form, as a central component to construct an algorithm around.

Finally, several papers have demonstrated that augmenting classical survival models with regularization methods, can lead to models that outperform machine learning alternatives even on high-dimensional data [@Zhang2021, @Spooner2020].
On lower dimensional data, classical algorithms often outperform machine learning models even without further adjustment [@Beaulac2020, @Burk2024].

Therefore, a robust understanding of classical models is imperative to fairly construct and evaluate machine learning survival models.
This chapter begins with a recap of estimators seen in the first part of this book and demonstrating how they can be used as predictive models, with further estimators then introduced.
Semi-parametric estimators are then introduced, most notably the Cox proportional hazards models.
Finally, accelerated failure time, proportional odds, and flexible spline models are discussed.

## Non-Parametric Estimators {#sec-surv-models-uncond}

The Kaplan-Meier estimator [@Kaplan1958] was introduced in @sec-surv-km as the step-function

$$
\KMS(\tau) = \prod_{k:t_{(k)} \leq \tau}\left(1-\frac{d_{t_{(k)}}}{n_{t_{(k)}}}\right)
$$ {#eq-km-ml}

where $t(k)$ are ordered event times, $d_{t_{(k)}}$ and $n_{t_{(k)}}$ are the number of events and observations at risk at $t_{(k)}$ respectively.
In @sec-surv-km, this was discussed as an estimator for visualizing survival distributions and for descriptive analysis.
However, now consider how this estimator could be used as a simple predictive method.
As an unconditional non-parametric method, the estimator assumes no distribution for survival times and ignores any covariate data.
Say we are given the following dataset of survival outcomes:

|litter |rx |sex |time |status |
|:------|:--|:---|:----|:------|
|6      |0  |m   |62   |0      |
|57     |0  |f   |55   |1      |
|60     |1  |m   |104  |0      |
|85     |1  |f   |92   |1      |
|97     |0  |f   |91   |0      |

: Sample of the $\code{rats}$ dataset from $\Rstats$ package $\pkg{survival}$. {#tbl-rats}

@tbl-rats-km below demonstrates construction of the Kaplan-Meier estimator at each possible time-point, where the second column is the contribution to the estimator at the given time:

| $\tau$ | $1 - d/n$ | $\KMS(\tau)$ |
|------| --- | -- |
| 55 | 0.8 | 0.8 |
| 62 | 1 | 0.8 |
| 91 | 1 | 0.8 |
| 92 | 0.5 | 0.4 |
| 104 | 1 | 0.4 |

: Kaplan-Meier contributions (second column) and estimation (third column) at each time $\tau$ based on the data from @tbl-rats. {#tbl-rats-km}

Now if @tbl-rats is taken to be training data, then the values in @tbl-rats-km can be used to predict the probability of survival at a given time.
For example, without needing to know any other information about a new rat that enters the data (for these estimators ignore covariates), there is an $80\%$ chance of survival until $\tau = 60$ is 80\%, which drops to $40\%$ after 92 days.

Estimating the Kaplan-Meier on training data and using those values for predictions of new data, yields a 'baseline' model that can be compared to more sophisticated alternatives to improve interpretability in model evaluation (@sec-eval-distr-score-base).
In this book, $\KMS$ specifically refers to the Kaplan-Meier estimator fit on some training data.
Similarly, $\KMG$ specifically refers to the Kaplan-Meier estimator fit on the observed censoring times $(t_i, 1-\delta_i)$ from some training data.

The Nelson-Aalen estimator [@Aalen1978; @Nelson1972], introduced in @sec-nonparametric, can also be used as a baseline predictive model.

$$
\hatH_{NA}(\tau) = \sum_{t_{(k)}\leq \tau} \frac{d_{t_{(k)}}}{n_{t_{(k)}}}.
$$

The Kaplan-Meier and Nelson-Aalen estimators are consistent estimators for the survival and cumulative hazard functions respectively; the former is more widely utilized as a baseline estimator [@Herrmann2020; @Huang2020; @Wang2017].
As well as supporting in interpretability, both methods can be used for graphical calibration of models (@sec-eval-distr-calib-prob), components of complex models (@sec-car), and other diagnostic graphical tools [@Habibi2018; @Jager2008; @Moghimi-dehkordi2008].

By ignoring covariates, neither estimator is expected to perform well in practice as predictive models.
Moreover, the assumption of uninformative censoring rarely holds true.
One alternative is to consider a conditional non-parametric estimator that can take into account covariates.
The Akritas estimator [@Akritas1994] is usually defined by [@Blanche2013]:

$$
\hatS(\tau|\xx^*, \lambda) = \prod_{k:t(k)\leq \tau} \Big(1 - \frac{\sum^n_{i=1} K(\xx^*,\xx_i|\lambda)\II(t_i = t(k), \delta_i = 1)}{\sum^n_{i=1} K(\xx^*,\xx_i|\lambda)\II(t_i \geq t(k))}\Big)
$$ {#eq-akritas}

where $K$ is a kernel function, usually $K(x,y|\lambda) = \II(\lvert \hat{F}_X(x) - \hat{F}_X(y)\rvert < \lambda), \lambda \in (0, 1]$, $\hat{F}_X$ is the empirical distribution function of the data, and $\lambda$ is a hyper-parameter.
The estimator can be interpreted as a conditional Kaplan-Meier estimator which is computed on a neighbourhood of subjects closest to $\xx^*$.
In fact, if $\lambda = 1$ then $K(\cdot|\lambda) = 1$ and @eq-akritas is identical to @eq-km-ml.

The formulation in (@eq-akritas) includes fitting and predicting in one step as the usual application of the model is as a non-parametric estimator.
By first estimating $\hatF_X$ on separate training data, the estimator can be used as a baseline predictive model.

## Semi-Parametric Models: Cox PH {#sec-surv-models-crank}

The Cox Proportional Hazards (CPH)  [@Cox1972], or Cox model, is likely the most widely known semi-parametric model and the most studied survival model [@Habibi2018; @Moghimi-dehkordi2008; @Reid1994; @Wang2017]. The Cox model assumes that the hazard for a subject is proportionally related to their explanatory variables, $X_1,...,X_n$, via some baseline hazard that all subjects in a given dataset share ('the PH assumption'). The hazard function in the Cox PH model is defined by
$$
h(\tau|X_i)= h_0(\tau)\exp(X_i\beta)
$$
where $h_0$ is the non-negative *baseline hazard function* and $\beta = \beta_1,...,\beta_p$ where $\beta_i \in \Reals$ are coefficients to be fit. Note the proportional hazards (PH) assumption can be seen as the estimated hazard, $h(\tau|X_i)$, is directly proportional to the model covariates $\exp(X_i\beta)$. Whilst a form is assumed for the 'risk' component of the model, $\exp(X_i\beta)$, no assumptions are made about the distribution of $h_0$, hence the model is semi-parametric.

The coefficients, $\beta$, are estimated by maximum likelihood estimation of the 'partial likelihood'  [@Cox1975], which only makes use of ordered event times and does not utilise all data available (hence being 'partial'). The partial likelihood allows study of the informative $\beta$-parameters whilst ignoring the nuisance $h_0$. The predicted linear predictor, $\hat{\eta} := X^*\hat{\beta}$, can be computed from the estimated $\hat{\beta}$ to provide a ranking prediction.

Inspection of the model is also useful without specifying the full hazard by interpreting the coefficients as 'hazard ratios'. Let $p = 1$ and $\hat{\beta} \in \Reals$ and let $X_i,X_j \in \Reals$ be the covariates of two training observations, then the *hazard ratio* for these observations is the ratio of their hazard functions,
$$
\frac{h(\tau|X_i)}{h(\tau|X_j)} = \frac{h_0(\tau)\exp(X_i\hat{\beta})}{h_0(\tau)\exp(X_j\hat{\beta})} =  \exp(\hat{\beta})^{X_i - X_j}
$$

If $\exp(\hat{\beta}) = 1$ then $h(\tau|X_i) = h(\tau|X_j)$ and thus the covariate has no effect on the hazard. If $\exp(\hat{\beta}) > 1$ then $X_i > X_j \rightarrow h(\tau|X_i) > h(\tau|X_i)$ and therefore the covariate is positively correlated with the hazard (increases risk of event). Finally if $\exp(\hat{\beta}) < 1$ then $X_i > X_j \rightarrow h(\tau|X_i) < h(\tau|X_i)$ and the covariate is negatively correlated with the hazard (decreases risk of event).

Interpreting hazard ratios is known to be a challenge, especially by clinicians who require simple statistics to communicate to patients  [@Sashegyi2017; @Spruance2004]. For example the full interpretation of a hazard ratio of '2' for binary covariate $X$ would be: 'assuming that the risk of death is constant at all time-points then the instantaneous risk of death is twice as high in a patient with $X$ than without'. Simple conclusions are limited to stating if patients are at more or less risk than others in their cohort. Further disadvantages of the model also lie in its lack of real-world interpretabilitity, these include  [@Reid1994]:

* the PH assumption may not be realistic and the risk of event may not be constant over time;
* the estimated baseline hazard from a non-parametric estimator is a discrete step-function resulting in a discrete survival distribution prediction despite time being continuous; and
* the estimated baseline hazard will be constant after the last observed time-point in the training set  [@Gelfand2000].


Despite these disadvantages, the model has been demonstrated to have excellent predictive performance and routinely outperforms (or at least does not underperform) sophisticated ML models  [@Gensheimer2018; @Luxhoj1997; @VanBelle2011b] (and [@Sonabend2021b]). It's simple form and wide popularity mean that it is also highly transparent and accessible.

The next class of models address some of the Cox model disadvantages by making assumptions about the baseline hazard.

## Parametric Models{#sec-surv-models-param}

### Parametric Proportional Hazards

The CPH model can be extended to a fully parametric PH model by substituting the unknown baseline hazard, $h_0$, for a particular parameterisation. Common choices for distributions are Exponential, Weibull and Gompertz  [@Kalbfleisch2011; @Wang2017]; their hazard functions are summarised in (@tab-survivaldists) along with the respective parametric PH model. Whilst an Exponential assumption leads to the simplest hazard function, which is constant over time, this is often not realistic in real-world applications. As such the Weibull or Gompertz distributions are often preferred. Moreover, when the shape parameter, $\gamma$, is $1$ in the Weibull distribution or $0$ in the Gompertz distribution, their hazards reduce to a constant risk ((@fig-survhazards)). As this model is fully parametric, the model parameters can be fit with maximum likelihood estimation, with the likelihood dependent on the chosen distribution.

| Distribution$^1$ | $h_0(\tau)^2$ | $h(\tau|X_i)^3$ |
| -- | -- | --- |
| $\Exp(\lambda)$ | $\lambda$ | $\lambda\exp(X_i\beta)$ |
| $\Weib(\gamma, \lambda)$ | $\lambda\gamma \tau^{\gamma-1}$ | $\lambda\gamma \tau^{\gamma-1}\exp(X_i\beta)$ |
| $\Gomp(\gamma, \lambda)$ |  $\lambda \exp(\gamma \tau)$ | $\lambda \exp(\gamma \tau)\exp(X_i\beta)$ |

: Exponential, Weibull, and Gompertz hazard functions and PH specification. {#tbl-survivaldists}

<sup>
* 1. Distribution choices for baseline hazard. $\gamma,\lambda$ are shape and scale parameters respectively.
* 2. Baseline hazard function, which is the (unconditional) hazard of the distribution.
* 3. PH hazard function, $h(\tau|X_i) = h_0(\tau)\exp(X_i\beta)$.
</sup>

![Comparing the hazard curves under Weibull and Gompertz distributions for varying values of the shape parameter; scale parameters are set so that each parametrisation has a median of 20. x-axes are time and y-axes are Weibull (top) and Gompertz (bottom) hazards as a function of time.](Figures/classical/hazards.png){#fig-survhazards fig-alt="TODO"}

In the literature, the Weibull distribution tends to be favoured as the initial assumption for the survival distribution  [@Gensheimer2018; @Habibi2018; @Hielscher2010; @CoxSnell1968; @Rahman2017], though Gompertz is often tested in death-outcome models for its foundations in modelling human mortality  [@Gompertz1825].  There exist many tests for checking the goodness-of-model-fit (@sec-eval-insample) and the distribution choice can even be treated as a model hyper-parameter. Moreover it transpires that model inference and predictions are largely insensitive to the choice of distribution  [@Collett2014; @Reid1994]. In contrast to the Cox model, fully parametric PH models can predict absolutely continuous survival distributions, they do not treat the baseline hazard as a nuisance, and in general will result in more precise and interpretable predictions if the distribution is correctly specified  [@Reid1994; @RoystonParmar2002].

Whilst misspecification of the distribution tends not to affect predictions too greatly, PH models will generally perform worse when the PH assumption is not valid. PH models can be extended to include time-varying coefficients or model stratification  [@Cox1972] but even with these adaptations the model may not reflect reality. For example, the predicted hazard in a PH model will be either monotonically increasing or decreasing but there are many scenarios where this is not realistic, such as when recovering from a major operation where risks tends to increase in the short-term before decreasing. Accelerated failure time models overcome this disadvantage and allow more flexible modelling, discussed next.

### Accelerated Failure Time
In contrast to the PH assumption, where a unit increase in a covariate is a multiplicative increase in the hazard rate, the Accelerated Failure Time (AFT) assumption means that a unit increase in a covariate results in an acceleration or deceleration towards death (expanded on below). The hazard representation of an AFT model demonstrates how the interpretation of covariates differs from PH models,
$$
h(\tau|X_i)= h_0(\exp(-X_i\beta)\tau)\exp(-X_i\beta)
$$
where $\beta = (\beta_1,...,\beta_p)$ are model coefficients. In contrast to PH models, the 'risk' component, $\exp(-X_i\beta)$, is the exponential of the *negative* linear predictor and therefore an increase in a covariate value results in a decrease of the predicted hazard. This representation also highlights how AFT models are more flexible than PH as the predicted hazard can be non-monotonic. For example the hazard of the Log-logistic distribution ((@fig-litreview-logloghaz)) is highly flexible depending on chosen parameters. Not only can the AFT model offer a wider range of shapes for the hazard function but it is more interpretable. Whereas covariates in a PH model act on the hazard, in an AFT they act on time, which is most clearly seen in the log-linear representation,
$$
\log Y_i = \mu + \alpha_1X_{i1} + \alpha_2X_{i2} + ... + \alpha_pX_{ip} + \sigma\epsilon_i
$$
where $\mu$ and $\sigma$ are location and scale parameters respectively, $\alpha_1,...,\alpha_p$ are model coefficients, and $\epsilon_i$ is a random error term. In this case a one unit increase in covariate $X_{ij}$ means a $\alpha_j$ increase in the logarithmic survival time. For example if $\exp(X_i\alpha) = 0.5$ then $i$ 'ages' at double the baseline 'speed'. Or less abstractly if studying the time until death from cancer then  $\exp(X_i\alpha) = 0.5$ can be interpreted as 'the entire process from developing tumours to metastasis and eventual death in subject $i$ is twice as fast than the normal', where 'normal' refers to the baseline when all covariates are $0$.

Specifying a particular distribution for $\epsilon_i$ yields a fully-parametric AFT model. Common distribution choices include Weibull, Exponential, Log-logistic, and Log-Normal  [@Kalbfleisch2011; @Wang2017]. The Buckley-James estimator  [@Buckley1979] is a semi-parametric AFT model that non-parametrically estimates the distribution of the errors however this model has no theoretical justification and is rarely fit in practice  [@Wei1992]. The fully-parametric model has theoretical justifications, natural interpretability, and can often provide a better fit than a PH model, especially when the PH assumption is violated  [@Patel2006; @Qi2009; @Zare2015].

![Log-logistic hazard curves with a fixed scale parameter of 1 and a changing shape parameter. x-axis is time and y-axis is the log-logistic hazard as a function of time.](Figures/classical/llog_hazard.png){#fig-litreview-logloghaz fig-alt="TODO"}

### Proportional Odds

Proportional odds (PO) models  [@Bennett1983] fit a proportional relationship between covariates and the odds of survival beyond a time $\tau$,
$$
O_i(\tau) = \frac{S_i(\tau)}{F_i(\tau)} = O_0(\tau)\exp(X_i\beta)
$$
where $O_0$ is the baseline odds.

In this model, a unit increase in a covariate is a multiplicative increase in the odds of survival after a given time and the model can be interpreted as estimating the log-odds ratio. There is no simple closed form expression for the partial likelihood of the PO model and hence in practice a Log-logistic distribution is usually assumed for the baseline odds and the model is fit by maximum likelihood estimation on the full likelihood  [@Bennett1983].

Perhaps the most useful feature of the model is convergence of hazard functions  [@Kirmani2001], which states $h_i(\tau)/h_0(\tau) \rightarrow 1$ as $\tau \rightarrow \infty$. This property accurately reflects real-world scenarios, for example if comparing chemotherapy treatment on advanced cancer survival rates, then it is expected that after a long period (say 10 years) the difference in risk between groups is likely to be negligible. This is in contrast to the PH model that assumes the hazard ratios are constant over time, which is rarely a reflection of reality.

In practice, the PO model is harder to fit and is less flexible than PH and AFT models, both of which can also produce odds ratios. This may be a reason for the lack of popularity of the PO model, in addition there is limited off-shelf implementations  [@Collett2014]. Despite PO models not being commonly utilised, they have formed useful components of neural networks (@sec-surv-ml-models-nn) and flexible parametric models (below).

### Flexible Parametric Models

Royston-Parmar flexible parametric models  [@RoystonParmar2002] extend PH and PO models by estimating the baseline hazard with natural cubic splines. The model was designed to keep the form of the PH or PO methods but without the semi-parametric problem of estimating a baseline hazard that does not reflect reality (see above), or the parametric problem of misspecifying the survival distribution.

To provide an interpretable, informative and smooth hazard, natural cubic splines are fit in place of the baseline hazard. The crux of the method is to use splines to model time on a log-scale and to either estimate the log cumulative Hazard for PH models, $\log H(\tau|X_i) = \log H_0(\tau) + X_i\beta$, or the log Odds for PO models, $\log O(\tau|X_i) = \log O_0(\tau) + X_i\beta$, where $\beta$ are model coefficients to fit, $H_0$ is the baseline cumulative hazard function and $O_0$ is the baseline odds function. For the flexible PH model, a Weibull distribution is the basis for the baseline distribution and a Log-logistic distribution for the baseline odds in the flexible PO model. $\log H_0(\tau)$ and $\log O_0(\tau)$ are estimated by natural cubic splines with coefficients fit by maximum likelihood estimation. The standard full likelihood is optimised, full details are not provided here. Between one and three internal knots are recommended for the splines and the placement of knots does not greatly impact upon the fitted model  [@RoystonParmar2002].

Advantages of the model include being: interpretable, flexible, can be fit with time-dependent covariates, and it returns a continuous function. Moreover many of the parameters, including the number and position of knots, are tunable, although Royston and Parmar advised against tuning and suggest often only one internal knot is required  [@RoystonParmar2002]. A recent simulation study demonstrated that even with an increased number of knots (up to seven degrees of freedom), there was little bias in estimation of the survival and hazard functions  [@Bower2019]. Despite its advantages, a 2018 review  [@Ng2018] found only twelve instances of published flexible parametric models since Royston and Parmar's 2002 paper, perhaps because it is more complex to train, has a less intuitive fitting procedure than alternatives, and has limited off-shelf implementations; i.e. is less transparent and accessible than parametric alternatives.

The PH and AFT models are both very transparent and accessible, though require slightly more expert knowledge than the CPH in order to specify the 'correct' underlying probability distribution. Interestingly whilst there are many papers comparing PH and AFT models to one another using in-sample metrics (@sec-eval-insample) such as AIC [@Georgousopoulou2015; @Habibi2018; @Moghimi-dehkordi2008; @Zare2015], no benchmark experiments could be found for out-of-sample performance. PO and spline models are less transparent than PH and AFT models and are even less accessible, with very few implementations of either. No conclusions can be drawn about the predictive performance of PO or spline models due to a lack of suitable benchmark experiments.
