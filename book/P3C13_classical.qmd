---
abstract: TODO (150-200 WORDS)
---

::: {.content-visible when-format="html"}
{{< include _macros.tex >}}
:::

# Classical Statistical Models {#sec-models-classical}

---

TODO

* Objective functions
* Move PO into flexible

---

{{< include _wip.qmd >}}

Defining if a model is 'machine learning' is surprisingly difficult, with terms such as 'iterative' and 'loss optimization' being frequently used in often clunky definitions.
This book defines machine learning models as those that:

1. Require intensive model fitting procedures such as recursion or iteration;
2. Are flexible with hyper-parameters that can be tuned;
3. Trained using some form of loss optimization;
4. Are 'black boxes' without simple closed-form expressions that can be easily interpreted;
5. Aim to minimize the difference between predictions and truth whilst placing minimal assumptions on the underlying data form.

In contrast, 'classical statistical' (or 'classical') models, refer here to low-complexity models that are either non-parametric or have a simple closed-form expression with parameters that can be fit with maximum likelihood estimation (or an analogous method).
Further, classical models are assumed to be fast to fit and highly interpretable, though can be inflexible and may make unreasonable assumptions as they assume underlying probability distributions to model data.

Whilst this book is focused on machine learning, there are a few reasons to include a chapter on classical models.

Firstly, the boundary between machine learning and classical models is fuzzy as the latter can be augmented with modern machine learning methods to create a model that could be termed as 'machine learning' (@sec-classical-improving).
In fact, several papers have demonstrated that augmenting classical models in this way yields models that outperform machine learning alternatives even on high-dimensional data (when the number of variables far exceeds the number of rows) [@Zhang2021; @Spooner2020].
Secondly, the majority of machine learning survival algorithms make use of classical models, often using the Kaplan-Meier estimator (@sec-classical-nonpar) and/or assuming a proportional hazards form (@sec-classical-cox), as a central component to construct an algorithm around.
Finally, on lower dimensional data, classical algorithms often outperform machine learning models even without further adjustment [@Beaulac2020; @Burk2024].

Therefore, a robust understanding of classical models is imperative to fairly construct and evaluate machine learning survival models.
This chapter begins with a recap of estimators seen in the first part of this book and demonstrating how they can be used as predictive models, with further estimators then introduced.
Semi- and fully-parametric models are then introduced, most notably the Cox proportional hazards model and the accelerated failure time models.
Finally, methods to improve classical models through machine learning methodology is presented.

## Non-Parametric Estimators {#sec-classical-nonpar}

The Kaplan-Meier estimator [@Kaplan1958] was introduced in @sec-surv-km as the step-function

$$
\KMS(\tau) = \prod_{k:t_{(k)} \leq \tau}\left(1-\frac{d_{t_{(k)}}}{n_{t_{(k)}}}\right)
$$ {#eq-km-ml}

where $t(k)$ are ordered event times, $d_{t_{(k)}}$ and $n_{t_{(k)}}$ are the number of events and observations at risk at $t_{(k)}$ respectively.
In @sec-surv-km, this was discussed as an estimator for visualizing survival distributions and for descriptive analysis.
However, now consider how this estimator could be used as a simple predictive method.
As an unconditional non-parametric method, the estimator assumes no distribution for survival times and ignores any covariate data.
Say we are given the following dataset of survival outcomes:

|litter |rx |sex |time |status |
|:------|:--|:---|:----|:------|
|6      |0  |m   |62   |0      |
|57     |0  |f   |55   |1      |
|60     |1  |m   |104  |0      |
|85     |1  |f   |92   |1      |
|97     |0  |f   |91   |0      |

: Sample of the $\code{rats}$ dataset from $\Rstats$ package $\pkg{survival}$. {#tbl-rats}

@tbl-rats-km below demonstrates construction of the Kaplan-Meier estimator at each possible time-point, where the second column is the contribution to the estimator at the given time:

| $\tau$ | $1 - d/n$ | $\KMS(\tau)$ |
|------| --- | -- |
| 55 | 0.8 | 0.8 |
| 62 | 1 | 0.8 |
| 91 | 1 | 0.8 |
| 92 | 0.5 | 0.4 |
| 104 | 1 | 0.4 |

: Kaplan-Meier contributions (second column) and estimation (third column) at each time $\tau$ based on the data from @tbl-rats. {#tbl-rats-km}

Now if @tbl-rats is taken to be training data, then the values in @tbl-rats-km can be used to predict the probability of survival at a given time.
For example, without needing to know any other information about a new rat that enters the data (for these estimators ignore covariates), there is an $80\%$ chance of survival until $\tau = 60$ is 80\%, which drops to $40\%$ after 92 days.

Estimating the Kaplan-Meier on training data and using those values for predictions of new data, yields a 'baseline' model that can be compared to more sophisticated alternatives to improve interpretability in model evaluation (@sec-eval-distr-score-base).
In this book, $\KMS$ specifically refers to the Kaplan-Meier estimator fit on some training data.
Similarly, $\KMG$ specifically refers to the Kaplan-Meier estimator fit on the observed censoring times $(t_i, 1-\delta_i)$ from some training data.

The Nelson-Aalen estimator [@Aalen1978; @Nelson1972], introduced in @sec-nonparametric, can also be used as a baseline predictive model.

$$
\hatH_{NA}(\tau) = \sum_{t_{(k)}\leq \tau} \frac{d_{t_{(k)}}}{n_{t_{(k)}}}.
$$

The Kaplan-Meier and Nelson-Aalen estimators are consistent estimators for the survival and cumulative hazard functions respectively; the former is more widely utilized as a baseline estimator [@Herrmann2020; @Huang2020; @Wang2017].
As well as supporting in interpretability, both methods can be used for graphical calibration of models (@sec-eval-distr-calib-prob), components of complex models (@sec-car), and other diagnostic graphical tools [@Habibi2018; @Jager2008; @Moghimi-dehkordi2008].

By ignoring covariates, neither estimator is expected to perform well in practice as predictive models.
Moreover, the assumption of uninformative censoring rarely holds true.
One alternative is to consider a conditional non-parametric estimator that can take into account covariates.
The Akritas estimator [@Akritas1994] is usually defined by [@Blanche2013]:

$$
\hatS(\tau|\xx^*, \lambda) = \prod_{k:t(k)\leq \tau} \Big(1 - \frac{\sum^n_{i=1} K(\xx^*,\xx_i|\lambda)\II(t_i = t(k), \delta_i = 1)}{\sum^n_{i=1} K(\xx^*,\xx_i|\lambda)\II(t_i \geq t(k))}\Big)
$$ {#eq-akritas}

where $K$ is a kernel function, usually $K(x,y|\lambda) = \II(\lvert \hat{F}_X(x) - \hat{F}_X(y)\rvert < \lambda), \lambda \in (0, 1]$, $\hat{F}_X$ is the empirical distribution function of the data, and $\lambda$ is a hyper-parameter.
The estimator can be interpreted as a conditional Kaplan-Meier estimator which is computed on a neighbourhood of subjects closest to $\xx^*$.
In fact, if $\lambda = 1$ then $K(\cdot|\lambda) = 1$ and @eq-akritas is identical to @eq-km-ml.

The formulation in @eq-akritas includes fitting and predicting in one step as the usual application of the model is as a non-parametric estimator.
By first estimating $\hatF_X$ on separate training data, the estimator can be used as a baseline predictive model.

## Proportional Hazards {#sec-classical-cox}

This section begins with an introduction to the proportional hazards concept using the Cox PH model and then moves to fully parametric proportional hazards, with WeibullPH as a motivating example.

### Semi-Parametric PH

The Cox Proportional Hazards (Cox PH)  [@Cox1972], or Cox model, is likely the most widely known semi-parametric model and the most studied survival model [@Reid1994; @Wang2017].
Let $\eta_i = \xx_i^\trans\bsbeta$ be the linear predictor for some observation $i$ with covariates $\xx_i$ and model coefficients $\bsbeta \in \Reals^p$, then the Cox model assumes the hazard function for $i$ follows the form:

$$
h_{Cox}(\tau|\xx_i)= h_0(\tau)\exp(\eta_i)
$$ {#eq-cox}

or in equivalent forms:

$$
S_{Cox}(\tau|\xx_i)= S_0(\tau)^{\exp(\eta_i)}
$$ {#eq-cox-surv}

$$
H_{Cox}(\tau|\xx_i)= H_0(\tau)\exp(\eta_i)
$$ {#eq-cox-cum}

Ignoring the $h_0$ term for now, this form encodes the eponymous 'proportional hazards' assumption as the individual's hazard at time $\tau$ is directly proportional to their own covariates: $h(\tau|\xx_i) \propto \exp(\eta_i)$.
In other words, a unit change in a covariate acts multiplicatively on the estimated hazard.
Further, the hazard ratio, which is a measure of the difference in risk, between two different subjects, depends solely on the value of their linear predictors and not on time.
Take the special case of a model with one covariate, $x$, and coefficient, $\beta$, see what happens when $x$ is increased by one unit:

$$
\frac{h_{Cox}(\tau|x+1)}{h_{Cox}(\tau|x)} = \frac{h_0(\tau)\exp((x+1)\beta)}{h_0(\tau)\exp(x\beta)} = \exp(\beta)
$$ {#eq-hazratio}

The difference in risk depends only on $\beta$ and is independent of time, specifically

$$
h_{Cox}(\tau|x+1) = \exp(\beta)h_{Cox}(\tau|x)
$$ {#eq-hazratio-diff}

Estimation of the $\beta$ terms is possible by ignoring $h_0$ as a 'nuissance parameter' and using the 'partial likelihood' function [@Cox1975]:

$$
\calL(\beta) = \prod_{k:\tbk} \Bigg(\frac{\exp(\eta_k)}{\sum_{j \in \calR_{\tbk}} \exp(\eta_j)}\Bigg)
$$ {#eq-partial}

with log-likelihood

$$
\call(\beta) = \sum_{k:\tbk} \Bigg(\eta_k - \log \Big(\sum_{j \in \calR_{\tbi}} \exp(\eta_j)\Big)\Bigg)
$$

Given these formulas, one can make relative risk predictions (@sec-survtsk) as well as analysing patterns in the data by computing hazard ratios, which provide a method for identifying the covariates that have a significant effect on an individual's hazard.
However, to make a full survival distribution prediction, the baseline hazard must be considered.

The baseline hazard, $h_0$, captures the average (baseline) risk for all observations at a given time-point.
The Kaplan-Meier estimator is commonly used to estimate the baseline survival, $S_0$, in complex machine learning algorithms, the hazard then follows from $h_0(\tau) = -S'(\tau)/S(\tau)$.
However, in the case of the Cox model, the conditional non-parametric Breslow estimator [@Cox1975] is more suitable as it accounts for covariates and is directly related to the partial likelihood function (@eq-partial):

$$
\hat{H}_{Bres}(\tau) = \sum_{\tbk \leq \tau} \frac{d_{\tbk}}{\sum_{j \in \calR_{\tbk}} \exp(\hat{\eta}_j)}
$$ {#eq-breslow}

where $\hat{\eta}_j$ is the estimated linear predictor for observation $j$.
Note that if the value for all covariates was zero, which one could think of equivalent as there being no covariates, then the Breslow estimator is identical to the Nelson-Aalen estimator:

$$
\hat{H}_{Bres}(\tau) = \sum_{\tbk \leq \tau} \frac{d_{\tbk}}{\sum_{j \in \calR_{\tbk}} 1} = \sum_{\tbk \leq \tau} \frac{d_{\tbk}}{n_{\tbk}} = \hat{H}_{NA}(\tau)
$$

With these formulae, the Cox PH can be used as a predictive model by using training data to estimate $\hat{\bsbeta}$ with (@eq-partial), using these fitted coefficients to predict $\hat{\bseta}$ for new observations, estimating the cumulative baseline hazard with (@eq-breslow), then calculating the derivative and using the original form in (@eq-cox):

$$
\hat{h}(\tau|\xx_i)= \hatH'_{Bres}(\tau)\exp(\hat{\eta}_i)
$$

where $\hatH'$ is the first derivative of the Breslow estimator, which is the (non-cumulative) hazard function.

The Cox model is a powerful tool that is constantly being found to outperform machine learning models [@Gensheimer2018; @Luxhoj1997; @VanBelle2011b].
Moreover, it is highly interpretable and as such has a long history of usage in clinical prediction modelling and analysis.
However, the proportional hazards assumption is often violated in real life, leaving the model to be a questionable choice when used for data analysis.
It is possible to extend the model to handle time-varying coefficients and model stratification [@Cox1972], however it is surprisingly difficult to make meaningful and interpretable predictions from a time-varying or stratified model [@Reid1994].

In a predictive setting, it also may not matter if the PH assumption is violated, as long as the model is clearly demonstrated to outperform others (with less stringent assumptions) in an empirical benchmark experiment.

### Parametric PH

The baseline hazard is advantageous as it removes any burden to assume a particular underlying probability distribution.
However, there are some cases where modelling a particular distribution may make sense.
On these occasions, the baseline hazard can be replaced by the hazard function corresponding to a particular probability distribution, with three common choices [@Kalbfleisch2011; @Wang2017] being the Exponential, Gompertz, and Weibull distributions.
The Weibull distribution is particularly important as it reduces to the Exponential distribution when the shape parameter equals $1$ and moreover it is the only distribution for which both the proportional hazards and accelerated failure time assumptions can hold true.

Assuming a PH model one can plug in the hazard and survival functions from the Weibull distribution into @eq-cox and @eq-cox-surv respectively.
First recall for a $\Weib(\gamma, \lambda)$ distribution with shape parameter $\gamma$ and scale parameter $\lambda$, the relevant functions can be given by [@KalbfleischPrentice1973]:

$$
h(x) = \lambda\gamma x^{\gamma-1}
$$

and

$$
S(x) = \exp(-\lambda x^\gamma)
$$

Taking these to be the baseline hazard and survival functions respectively, they can be substituted into the Cox model as follows:

$$
h_{WeibullPH}(\tau|\xx_i)= (\lambda\gamma \tau^{\gamma-1}) \exp(\eta_i)
$$

or equivalently

$$
S_{WeibullPH}(\tau|\xx_i)= (\exp(-\lambda \tau^\gamma))^{\exp(\eta_i)}
$$

Finally, these formulae can now be used to define the full likelihood (@sec-surv-estimation-param) for the WeibullPH model:

$$
\begin{aligned}
\calL(\bstheta) &= \prod_{i=1}^n h_Y(t_i|\bstheta)^{\delta_i}S_Y(t_i|\bstheta) \\
&= \prod_{i=1}^n \Big((\lambda\gamma t_i^{\gamma-1} \exp(\eta_i))^{\delta_i}\Big)\Big(\exp(-\lambda t_i^\gamma)^{\exp(\eta_i)}\Big) \\
\end{aligned}
$$

with log-likelihood

$$
\begin{aligned}
\call(\bstheta) &= \sum_{i=1}^n \delta_i[\log(\lambda\gamma) + (\gamma-1)\log(t_i) + \eta_i] - \lambda\exp(\eta_i)t_i^\gamma \\
&\propto \sum_{i=1}^n \delta_i[\log(\lambda\gamma) + \gamma\log(t_i) + \eta_i] - \lambda\exp(\eta_i)t_i^\gamma \\
\end{aligned}
$$

These likelihoods can be passed into machine learning models that are constructed around closed-form likelihood expressions to be optimized, for example in boosting (@sec-boost) and neural networks (@sec-nnet).
Parameters can then be fit using maximum likelihood estimation (MLE) with respect to all unknown parameters $\bstheta = \{\bsbeta, \gamma, \lambda\}$.
Expansion to other censoring types and truncation follows by using other likelihood forms presented in @sec-surv-estimation.

When considering which probability distributions to model in predictive experiments, Weibull is a common starting choice [@Hielscher2010; @CoxSnell1968; @Rahman2017], its two parameters make it a flexible fit to data but on the other hand it can be easily reduced to Exponential when $\gamma=1$.
Gompertz [@Gompertz1825] is commonly used in medical domains, especially when describing adult lifespans.
In a machine learning context, one can select the optimal distribution for future predictive performance by running a benchmark experiment.
Moreover, it transpires that model inference and predictions are largely insensitive to the choice of distribution  [@Collett2014; @Reid1994].
In contrast to the semi-parametric Cox model, fully parametric PH models can predict absolutely continuous survival distributions, they do not treat the baseline hazard as a nuisance, and in general will result in more precise and interpretable predictions if the distribution is correctly specified  [@Reid1994; @RoystonParmar2002].

### Competing risks

Fine and Gray [@Fine1999] proposed a semi-parametric PH model in the competing risks setting based on modelling the subdistribution hazard function.
Cause-specific hazards (@eq-cause-specific-hazard) represent the instantaneous risk of an individual experiencing the cause of interest, given that they have not yet experienced *any* event.
In contrast, subdistribution hazards model the risk of an individual experiencing the cause of interest, given they have not yet experienced the event of interest, but may have experienced a competing event.
As will be shown below, the benefit of the subdistribution model is the ability to directly predict the cumulative incidence function under a PH model.
In this framework, the subdistribution hazard provides a relationship between covariates and the CIF for an event of interest [@Austin2016].
Mathematically, the difference between cause-specific and subdistribution hazards comes from the definition of the risk set.
The cause-specific risk set for cause $e$ is defined in the usual way as:

$$
\calR_{e;\tau} := \{i: t_i \geq \tau\}
$$ {#eq-riskset-standard}

Which is independent of $e$ and therefore identical to the usual risk set definition in (@eq-riskset).
Whereas the subdistribution risk set is defined as:

$$
\calR_{e;\tau} := \{i: t_i \geq \tau \cup e_i \neq e\}
$$ {#eq-riskset-sd}

Using this latter definition of the risk set gives rise to the subdistribution hazard function for cause $e$:

$$
h^{SD}_{e}(\tau) = \lim_{\Delta \tau \to 0} \frac{P(\tau \leq Y \leq \tau + \Delta \tau, E = e\ |\ Y \geq \tau \cup E \neq e)}{\Delta \tau}
$$ {#eq-hazard-sd}

Subsituting (@eq-hazard-sd) into the baseline hazard of the Cox model (@eq-cox) yields the subhazard model for the $e$th cause:

$$
h_{FG,e}(\tau|\xx_i)= h^{SD}_{e_0}(\tau)\exp(\eta_i) 
$$ {#eq-cox-sd}

This model form assumes that the subdistribution hazards are all proportional.
Estimating the $\beta$ coefficients follows from a weighted, subdistribution adaptation of the partial likelihood (@eq-partial) which depends on the subdistribution risk set definition given in (@eq-riskset-sd).
For the $e$th cause, the partial likelihood is

$$
\calL_e(\beta) = \prod_{k:t_{e;(k)}} \Bigg(\frac{\exp(\eta_k)}{\sum_{j \in \calR_{e;t_{e;(k)}}} w_{kj}\exp(\eta_j)}\Bigg)
$$ {#eq-fg-likelihood}

Where $t_{e;(k)}$ is the ordered event times for observations that experience event $e$.
To account for the updated risk set definition in (@eq-riskset-sd), Fine and Gray defined the weights

$$
w_{kj} := \frac{\KMG(t_{e;(k)})}{\KMG(\min\{t_{e;(k)}, t_j\})}
$$

Where $\KMG$ is the Kaplan-Meier estimator fit on the censoring distribution (@sec-surv-km).
The Fine-Gray model therefore uses inverse probability of censoring weighting (as in many measures @sec-eval-crank-conc) in order to compensate for the effect of competing events.
Because of the way the subdistribution risk set is defined in (@eq-riskset-sd), the denominator of (@eq-fg-likelihood) is a sum over individuals, $j \in \calR_{e;t_{e;(k)}}$,  at time $t_{e;(k)}$ who have either yet to experience the event ($t_j \geq t_{e;(k)}$) or experienced a different event (implying $t_j < t_{e;(k)}$ as they would not be in the dataset if $t_j < t_{e;(k)} \cap e_j = e$).
The weighting function handles these cases as follows:

1. If $t_j \geq t_{e;(k)}$ then $w_{kj} = 1$ and thus observations who have not experienced any event contribute the most to the likelihood.
2. If $t_j < t_{e;(k)}$ then $w_{kj} < 1$ as $\KMG$ is a monotonically decreasing function with $w_{kj}$ continuing to decrease as the distance between $t_j$ and $t_{e;(k)}$ increases. Thus the contribution from events that have experienced competing events reduces over time.

Whilst modelling the subdistribution can seem unintuitive note that if there is only one event of interest then $w_{kj} = 1$ for all $k$ and $j$ and further $e_i \neq e$ must always be false, meaning (@eq-riskset-sd) reduces to (@eq-riskset-standard) as only the left condition can ever be true and by the same logic the subdistribution hazard reduces to the usual hazard definition.
Therefore the standard Cox PH for single events is perfectly recovered.

Instead of interpreting the subdistribution hazards directly, @Austin2017b recommend interpreting the fitted coefficients via the cause-specific CIF and cumulative hazard forms of (@eq-cox-sd), which can be obtained in the 'usual' way by first integrating to obtain the cumulative hazard form:

$$
H_{FG,e}(\tau|\xx_i)= H^{SD}_{e_0}(\tau)\exp(\eta_i) 
$$ {#eq-fg-cum}

where $H^{SD}_{e_0}$ is the cause-specific baseline cumulative hazard for cause $e$.
Then using (@eq-surv-haz) to relate the survival and hazard functions and representing this in terms of the CIF:

$$
F_{FG,e}(\tau|\xx_i) = 1 - \exp(-H^{SD}_{e_0}(\tau))^{\exp(\eta_i)}
$$ {#eq-fg-cif-long}

Or more simply:

$$
F_{FG,e}(\tau|\xx_i) = 1 - (1 - F^{SD}_{e_0}(\tau))^{\exp(\eta_i)}
$$ {#eq-fg-cif}

where $F^{SD}_{e_0}$ is the cause-specific baseline cumulative incidence function for cause $e$.
The model in (@eq-fg-cif) is fit by estimating the baseline cumulative hazard function and substituting into (@eq-fg-cif-long).
Similarly to how the subdistribution hazard was created, estimation of $\hat{H}^{SD}$ follows by updating (@eq-breslow) to use the subdistribution risk set definition and applying the same weighting to compensate for multiple events: 

$$
\hat{H}^{SD}_{Bres}(\tau) = \sum_{\tbk \leq \tau} \frac{d_{\tbk}}{\sum_{j \in \calR_{e;\tbk}} w_{kj}\exp(\hat{\eta}_j)}
$$ {#eq-fg-breslow}

Use of the Fine-Gray model has to be carefully considered before model fitting.
The subdistribution risk set definition, which includes competing events, implicitly implies that competing events are 'curable', meaning an observation can move from one event to another (as in the multi-state setting in @sec-multi-state).
In practice this is often not the case, especially in medical models where death is often a competing risk.
Moreover, combining subdistribution CIF estimates across causes can result in probabilities that exceed $1$, which should be impossible as events are mutually exclusive and exhaustive [@Austin2022].
In these above cases, estimating and summing cause-specific hazard functions is preferred [@Austin2022].
<!-- TODO: REFERENCE SECTION ON SUMMING CAUSE-SPECIFIC HAZARDS -->

The Fine-Gray model is most appropriate when only concerned with a single event of interest and all-cause estimates are not of interest.
Interpreting coefficient changes via (@eq-fg-cif) is slightly more intuitive as making inferences about the magnitude of effects of covariates on the incidence is more standard in clinical settings [@Austin2017b].

## Accelerated Failure Time {#sec-surv-models-param}

Whilst the proportional hazards models is a powerful model, it often does not represent real-world phenomena well.
The accelerated failure time (AFT) model is a popular alternative which models the effect of covariates as 'acceleration factors' that act multiplicatively on time.
In contrast to the PH model, AFT models are all fully-parametric.
A semi-parametric model has been suggested [@Buckley1979] however this is not used in practice as it lacks 'theoretical justification' and is 'not reliable' [@Wei1992].

### Understanding acceleration

Moving from a PH to AFT framework can be confusing, so to elucidate this further, take the following example adapted from @Kleinbaum1996.
Consider a model that compares the lifespans of humans and small dogs, which on average are said to age five times faster than humans.
If we take this heuristic to be true, then one would expect the probability of a dog surviving to age $\tau$ to be approximately the same probability of a human surviving to age $5\tau$:

$$
S_{dog}(\tau) = S_{human}(5\tau)
$$ {#eq-aft-dogs}

This is visualised in @fig-dogs, alongside the AFT curve (red) which 'correctly' models the relationship between humans and dogs, the analogous PH curve (blue) is also plotted.
The PH curve assumes the difference in risk between humans and dogs remains the same over time, specifically that at a given time, $S_{dog}(\tau) = S_{human}(\tau)^5$ (from (@eq-cox-surv)).

![Comparing human (black) and dog lifespans where the latter is modelled using an AFT model (red) versus a PH model (blue). Clearly the AFT model is (sadly) a better reflection of reality. Human lifespan modelled with Gompertz(0.09, 0.00005).](Figures/classical/dogs.png){#fig-dogs fig-alt="TODO"}

More generally, the accelerated failure time model estimates survival functions as

$$
S_{AFT}(\tau|\xx_i) = S_0(\tau e^{-\eta_i})
$$ {#eq-aft-surv}

with respective hazard function

$$
h_{AFT}(\tau|\xx_i) = e^{-\eta_i} h_0(\tau e^{-\eta_i})
$$ {#eq-aft-haz}

Note three key differences compared to the PH model.
Firstly, $\exp(-\eta_i)$ is modelled instead of $\exp(\eta_i)$, hence in a PH model $h_{PH}(\eta_i+1) > h_{PH}(\eta_i)$ whereas in an AFT model $h_{AFT}(\eta_i+1) < h_{AFT}(\eta_i)$.
Secondly, the baseline risk now clearly depends on both time and the linear predictor.
Thirdly, an increase in a covariate results in a multiplicative increase over *time* compared to the PH model in which the hazard is increased -- this is often seen as more intuitive to understand than hazard ratio, especially to clinicians who may be more interested in survival times and not abstract relative risks.

This third point is visualised in @fig-phaft in which a covariate is increase by $\log(2)$.
The left panel shows that the estimated hazard function from a PH model is double the baseline at all time points -- the multiplicative effect is seen on the y-axis (risk).
In contrast, the right panel shows how the survival function from an AFT model decreases at double the speed to the baseline -- the multiplicative effects is now on the x-axis (time).
Another way to demonstrate this effect is through the log-linear form of the accelerated failure time model:

$$
\log(t_i) = \mu + \eta_i + \sigma\epsilon_i
$$ {#eq-aft-log}

where $\sigma$ is a scale parameter, $\epsilon_i$ is an error term, and $\mu$ is an intercept.
Now consider the difference in $t_i$ when $\eta_i$ is increased by one (assuming just one covariate):

$$
\log(t_i|x_i + 1) - \log(t_i|x_i) = (\mu + \beta (x_i + 1) + \sigma\epsilon_i) - (\mu + \beta x_i + \sigma\epsilon_i) = \beta
$$

Taking exponentials

$$
\frac{t_i|x_i + 1}{t_i|x_i} = \exp(\beta)
$$

Hence increasing a covariate effectively multiplies the survival time by $\exp(\beta)$:

$$
t_i|x_i + 1 = e^{\beta}t_i | x_i
$$

![Comparing increasing a covariate $x_i$ between PH (left) and AFT (right) models. An increase of $x_i + \log(2)$ multiplies $h(t)$ by $\exp(\log(2)) = 2$ in a PH model. Whereas the result in the AFT is to multiply time $t$ by $2$, hence for any $t$, the AFT model reaches $S(t)$ in half the time as the baseline.](Figures/classical/compare.png){#fig-phaft fig-alt="TODO"}

### Parametric AFTs

As stated, in practice AFTs are always fully parametric, which means $S_0$ and $h_0$ are chosen according to some specific distribution.
Common distribution choices include Weibull, Exponential, Log-logistic, and Log-Normal  [@Kalbfleisch2011; @Wang2017].
The Weibull distribution (and Exponential as a special case) is unique in that it has both the PH and AFT property (technically a less known representation of Gompertz also has this property).
Therefore, selecting an alterative distribution, for example log-logistic or log-normal, can be particular useful if the PH assumption does not hold up.
The hazard function of the log-logistic distribution is plotted in @fig-logloghaz, note the hazard is non-monotonic, allowing non-PH representations to be modelled where the risk of an event may increase before decreasing, or vice versa.
When distributions are well-specified and the PH assumption is violated, AFTs can outperform PH alternatives [@Patel2006; @Qi2009; @Zare2015].

![Log-logistic hazard curves with a fixed scale parameter of 1 and a changing shape parameter. x-axis is time and y-axis is the log-logistic hazard as a function of time.](Figures/classical/llog_hazard.png){#fig-logloghaz fig-alt="TODO"}

As with the PH model, AFT models can be fit using maximum likelihood estimation of the full-likelihood by plugging in distribution defining functions into (@eq-aft-surv) and (@eq-aft-surv) and likelihoods defined in (@sec-surv-estimation-param).
Using Exponential this time as an example (the maths is a bit more friendly), first recall that if $X \sim \Exp(\lambda)$ then $h_X(\tau) = \lambda$ and $S_X(\tau) = \exp(-\lambda\tau)$.
Then:

$$
h_{ExpAFT}(\tau|\xx_i)= \lambda e^{\eta_i}
$$

and

$$
S_{ExpAFT}(\tau|\xx_i)= \exp(-\lambda\tau e^{-\eta_i})
$$

Giving the ExpAFT likelihood (@sec-surv-estimation-param):

$$
\begin{aligned}
\calL(\bstheta) &= \prod_{i=1}^n h_Y(t_i|\bstheta)^{\delta_i}S_Y(t_i|\bstheta) \\
&= \prod_{i=1}^n \Big(\lambda e^{\eta_i})^{\delta_i}\Big(\exp(-\lambda\tau e^{-\eta_i})\Big) \\
&= \prod_{i=1}^n \lambda^{\delta_i} \exp(-\lambda\tau e^{-\eta_i} + \delta_i\eta_i) \\
\end{aligned}
$$

with log-likelihood

$$
\begin{aligned}
\call(\bstheta) &= \sum_{i=1}^n \log(\lambda^{\delta_i} \exp(-\lambda\tau e^{-\eta_i} + \delta_i\eta_i)) \\
&= \sum_{i=1}^n \delta_i\log(\lambda) - \lambda\tau e^{-\eta_i} + \delta_i\eta_i \\
\end{aligned}
$$

Likelihoods can also be derived using the log-linear form in (@eq-aft-log) however these are beyond the scope of this book.
As before, extensions to other censoring types and truncation follows by specifying the correct likelihood form from @sec-surv-estimation-param.

### Proportional Odds

Proportional odds (PO) models  [@Bennett1983] fit a proportional relationship between covariates and the odds of survival beyond a time $\tau$,
$$
O_i(\tau) = \frac{S_i(\tau)}{F_i(\tau)} = O_0(\tau)\exp(X_i\beta)
$$
where $O_0$ is the baseline odds.

In this model, a unit increase in a covariate is a multiplicative increase in the odds of survival after a given time and the model can be interpreted as estimating the log-odds ratio. There is no simple closed form expression for the partial likelihood of the PO model and hence in practice a Log-logistic distribution is usually assumed for the baseline odds and the model is fit by maximum likelihood estimation on the full likelihood  [@Bennett1983].

Perhaps the most useful feature of the model is convergence of hazard functions  [@Kirmani2001], which states $h_i(\tau)/h_0(\tau) \rightarrow 1$ as $\tau \rightarrow \infty$. This property accurately reflects real-world scenarios, for example if comparing chemotherapy treatment on advanced cancer survival rates, then it is expected that after a long period (say 10 years) the difference in risk between groups is likely to be negligible. This is in contrast to the PH model that assumes the hazard ratios are constant over time, which is rarely a reflection of reality.

In practice, the PO model is harder to fit and is less flexible than PH and AFT models, both of which can also produce odds ratios. This may be a reason for the lack of popularity of the PO model, in addition there is limited off-shelf implementations  [@Collett2014]. Despite PO models not being commonly utilised, they have formed useful components of neural networks (@sec-nnet) and flexible parametric models (below).

### Flexible Parametric Models

Royston-Parmar flexible parametric models  [@RoystonParmar2002] extend PH and PO models by estimating the baseline hazard with natural cubic splines. The model was designed to keep the form of the PH or PO methods but without the semi-parametric problem of estimating a baseline hazard that does not reflect reality (see above), or the parametric problem of misspecifying the survival distribution.

To provide an interpretable, informative and smooth hazard, natural cubic splines are fit in place of the baseline hazard. The crux of the method is to use splines to model time on a log-scale and to either estimate the log cumulative Hazard for PH models, $\log H(\tau|X_i) = \log H_0(\tau) + X_i\beta$, or the log Odds for PO models, $\log O(\tau|X_i) = \log O_0(\tau) + X_i\beta$, where $\beta$ are model coefficients to fit, $H_0$ is the baseline cumulative hazard function and $O_0$ is the baseline odds function. For the flexible PH model, a Weibull distribution is the basis for the baseline distribution and a Log-logistic distribution for the baseline odds in the flexible PO model. $\log H_0(\tau)$ and $\log O_0(\tau)$ are estimated by natural cubic splines with coefficients fit by maximum likelihood estimation. The standard full likelihood is optimised, full details are not provided here. Between one and three internal knots are recommended for the splines and the placement of knots does not greatly impact upon the fitted model  [@RoystonParmar2002].

Advantages of the model include being: interpretable, flexible, can be fit with time-dependent covariates, and it returns a continuous function. Moreover many of the parameters, including the number and position of knots, are tunable, although Royston and Parmar advised against tuning and suggest often only one internal knot is required  [@RoystonParmar2002]. A recent simulation study demonstrated that even with an increased number of knots (up to seven degrees of freedom), there was little bias in estimation of the survival and hazard functions  [@Bower2019]. Despite its advantages, a 2018 review  [@Ng2018] found only twelve instances of published flexible parametric models since Royston and Parmar's 2002 paper, perhaps because it is more complex to train, has a less intuitive fitting procedure than alternatives, and has limited off-shelf implementations; i.e. is less transparent and accessible than parametric alternatives.

The PH and AFT models are both very transparent and accessible, though require slightly more expert knowledge than the CPH in order to specify the 'correct' underlying probability distribution. Interestingly whilst there are many papers comparing PH and AFT models to one another using in-sample metrics (@sec-eval-insample) such as AIC [@Georgousopoulou2015; @Habibi2018; @Moghimi-dehkordi2008; @Zare2015], no benchmark experiments could be found for out-of-sample performance. PO and spline models are less transparent than PH and AFT models and are even less accessible, with very few implementations of either. No conclusions can be drawn about the predictive performance of PO or spline models due to a lack of suitable benchmark experiments.

## Improving classical models {#sec-classical-improving}

* Non-linear effects: splines/additive models
* Dimension reduction: Regularization, feature selection and filters
* Ensembles: Bagging, boosting (mboost) and stacking

A number of model-agnostic algorithms have been created to improve a model's predictive ability.
When applied to classical algorithms, these methods can be used to create powerful models that outperform other machine learning.
As each could be the subject of a whole book, this section remains brief and just covers the general overview.
These are split into methods for:

1. modelling non-linear data effects;
2. reducing the number of variables in a dataset; and
3. combining predictions from multiple models.

### Non-linear effects

### Dimension reduction and feature selection

Many classical models work well for a small number of features (there is no heuristic for what 'small' means but often less than 10) but perform poorly with more variables.
To handle this, dimension reduction or filter selection methods can be used to reduce the number of variables in the dataset or selectively pick variables based on some rules.
In the context of classical models, regularization is a common dimension reduction method, split into L1 regularization, known as 'lasso', and L2 regularization, 'ridge regression'.
Given a generic learning algorithm, L1 and L2 regularization respectively constrain the model coefficients subject to the $\cal{l}^1$-norm and $\cal{l}^2$-norm respectively.
Concretely, the lasso-Cox model is defined by

<!-- CITE TIBSHIRANI - THE LASSO METHOD FOR VARIABLE SELECTION IN THE COX MODEL -->


### Ensemble methods


## Conclusion

:::: {.callout-warning icon=false}

## Key takeaways

* The boundary between machine learning and classical statistical models is fuzzy, and classical survival models often outperform machine learning alternatives.

::::

:::: {.callout-tip icon=false}

## Further reading

* TODO: Proportional odds
* To learn more about hazard ratios from Cox models and complexities in interpretation, we recommend @Sashegyi2017 and @Spruance2004.
* @Collett2014 and @KalbfleischPrentice1973 both provide comprehensive reading for classical statistical models. The former is slightly less technical and covers extensions to multiple settings.

::::
