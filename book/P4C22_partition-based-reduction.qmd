---
abstract: TODO (150-200 WORDS)
---

::: {.content-visible when-format="html"}
{{< include _macros.tex >}}
:::


# Partition based reductions {#sec-partition-based-reductions}

{{< include _wip.qmd >}}

In contrast to the previously introduced reductions that focus on the estimation of a specific quantity of interest at one or few time points, partition-based reduction aim to estimate the entire distribution of the event times (similar to Cox models and other survival learners).
The general idea is to partition the time axis into $J$ intervals $(a_0,a_1],\ldots,(a_{J-1},a_J]$ and estimate a discrete or continuous hazard rate for each interval.

For illustration, consider the partitioning of the follow up in Figure @fig-fu-partitioning.
Note that the intervals do not necessarily have to be equidistant.
The $j$th interval is given by $I_j:=(a_{j-1},a_j]$.
Let further $J_i$ the index of the interval in which observed time $t_i$ of subject $i$ falls, that is $t_i \in I_{J_i} = (a_{J_i-1},a_{J_i}]$.

![Partitioning of the follow-up time into $J$ discrete intervals $(a_0,a_1], \ldots, (a_{J-1},a_J]$. This partitioning forms the basis for both reduction techniques discussed in this chapter.](Figures/reductions/fu-partitioning.png){#fig-fu-partitioning fig-alt="Partitioning the follow-up time into discrete intervals." width=80%}

Formally, define the partitioning of the follow-up via the set of interval boundary points $a_j$ for $j=0,\ldots,J$ as

$$\mathcal{A} = \{a_0, \ldots, a_J\}, \quad a_0 < a_1 < \ldots < a_J$$ {#eq-cut-points}

This partitioning is used to create a transformed data set. 
Using this transformed data, standard regression or classification methods can be used in order to estimate (discrete) hazards in each interval. 
Section @sec-data-transformation introduces the required data transformation formally. 
Sections @sec-discrete-time-reduction and @sec-piecewise-constant-hazards then provide theoretical justification why models applied to such transformed data are valid approaches for estimation in time-to-event analysis and provide illustrative examples for such models.


## Data Transformation {#sec-data-transformation}

In order to apply the reduction techniques introduced in the upcoming sections, standard time-to-event data needs to be transformed into a specific format. 
The transformation is based on the partitioning of the follow-up as illustrated in Figure @fig-fu-partitioning and boundaries as defined in @eq-cut-points.
The new data set contains $J_i$ rows for each subject $i$, that is one row for each interval in which subject $i$ was at risk of experiencing an event. 
<!-- Thus the overall number of rows in the transoformed data is given by  -->
 <!-- -->
<!-- $$\tilde{n} = \sum_{i=1}^n J_i = \sum_{i=1}^n \sum_{j=1}^J \II(t_i > a_{j-1}).$$ -->
<!--  -->

For each subject $i$, we record the the subject's interval-specific event indicator
<!--  -->
$$\delta_{ij} = \begin{cases}
1, & t_i \in (a_{j-1}, a_j] \text{ and } \delta_i = 1 \\
0, & \text{ else }
\end{cases},\quad j=1,\ldots,J_i,$$ {#eq-interval-indicator}
<!--  -->

the time at risk in interval $j$
<!--  -->
$$t_{ij} = \begin{cases}
a_j - a_{j-1}, & \text{ if } t_i > a_j \\
t_i - a_{j-1}, & \text{ if } t_i \in (a_{J_i-1}, a_{J_i}]\\
\end{cases},\quad j=1,\ldots,J_i,$$ {#eq-time-at-risk}
<!--  -->

as well as some representation of the time in interval $j$, for example the interval midpoint or endpoint
<!--  -->
$$t_j = a_j$$ {#eq-interval-endpoint}
<!--  -->

and the subject's (potentially time-dependent)features 
<!--  -->
$$\xx_{ij}$$ {#eq-interval-features}
<!--  -->
with $\xx_{ij} = \xx_{i}$ for all $j$, if the features are constant over time.


Thus, given the partition $\mathcal{A}$, the standard time-to-event data $\calD = \{(t_i, \delta_i, \xx_i)\}_{i=1,\ldots,n}$ is transformed to data 
$$\calD_{\mathcal{A}} = \{(i, j, \delta_{ij}, t_{ij}, t_j, \xx_{ij})\}_{i=1,\ldots,n; j=1,\ldots,J_i},$$  {#eq-transformed-data}
where $i$ and $j$ indices are usually only used for "book-keeping" purposes but are not used in modeling later on.

The data transformation procedure described above is illustrated in Figure @fig-data-trafo for hypothetical data. 
The left hand side of Figure @fig-data-trafo shows data in standard time-to-event format with one row per subject $i=1,\ldots,3$ and $\mathcal{D} = \{(1.3, 0, 31), (0.5, 0, 67), (2.7, 1, 42)\}$. For the transformation, we choose (arbitrary) interval boundaries $\mathcal{A} = \{1, 1.5, 3\}$, which gives intervals $(0,1], (1,1.5], (1.5,3]$. The right-hand side of Figure @fig-data-trafo shows the transformed data $\mathcal{D}_{\mathcal{A}}$, where the first three rows keep track of subject and interval information and the other columns are the interval-specific event indicator, representation of time and (time-dependent) features as defined in Equations @eq-interval-indicator, @eq-time-at-risk, @eq-interval-endpoint and @eq-interval-features, respectively. 
For censored subjects, $\delta_{ij} = 0$ for all $j$ and for subjects who experienced an event, $\delta_{ij} = 1$ for the last interval $j = J_i$, and zero otherwise.

<!-- FIXME: create proper figure/table with better columns -->
![Illustration of the data transfomration needed for partition based reductions. Left: Data in standard time-to-event format. Right: Transformed data using interval boundaries $\mathcal{A} = \{1, 1.5, 3\}$.](Figures/reductions/ped-data-trafo.png){#fig-data-trafo fig-alt="Partitioning the follow-up time into discrete intervals." width=80%}

Note that the transformed data contains the same time-to-event information as the original data since, given the interval boundaries $\mathcal{A}$, the variables $\delta_{ij}$ and $t_{ij}$ are deterministic functions of the original data.






## Discrete Time Survival Analysis {#sec-discrete-time-reduction}

Consider the partitioning of the follow-up @eq-cut-points and assume we are only interested in whether the event occured within an interval rather than the exact event time. 
Using the notation from @sec-distributions-discrete, we can write the likelihood of the $i$th observed data point $P(Y_i \in (a_{J_i-1}, a_{J_i}]) = P(\tilde{Y}_i = J_i)$ for subjects who experienced an event ($\delta_i = 1$) and 
$P(Y_i > a_{J_i}) = P(\tilde{Y}_i > J_i)$ for subjects who were censored ($\delta_i = 0$). 

Thus, using definitions @eq-discrete-surv-prob and @eq-discrete-prob, the likelihood contribution of subject $i$ is given by
<!--  -->
$$\begin{aligned}L_i 
& = P(Y_i \in (a_{J_i-1}, a_{J_i}])^{\delta_i} P(Y_i > a_{J_i})^{1-\delta_i}\\
& = \left[S^d(J_i-1)h^d(J_i)\right]^{\delta_i} \left[S^d(J_i)\right]^{1-\delta_i}\\
& = \left[\left(\prod_{j=1}^{J_i-1} (1-h^d(j))\right)h^d(J_i)\right]^{\delta_i} \left[\prod_{j=1}^{J_i} (1-h^d(j))\right]^{1-\delta_i},
\end{aligned}$${#eq-discrete-likelihood-1}
<!--  -->

Now recall from Equation @eq-interval-indicator the definition of the interval-specific event indicators $\delta_{ij}$, which always take value 0, except for the last interval, where $\delta_{iJ_i} = \delta_i$. 
Thus, the first part of @eq-discrete-likelihood-1 can be written as
$\prod_{j=1}^{J_i}(1-h^d(j))^{1-\delta_{ij}}h^d(j)^{\delta_{ij}}$ and the second part as $\prod_{j=1}^{J_i}(1-h^d(j))^{1-\delta_{ij}}$.
It follows, that the likelihood @eq-discrete-likelihood-1 can be written as
<!--  -->
$$\begin{aligned}
L_i 
& = \left[\prod_{j=1}^{J_i} (1-h^d(j))^{1-\delta_{ij}}h^d(j)^{\delta_{ij}}\right]^{\delta_i} \left[\prod_{j=1}^{J_i} (1-h^d(j))^{1-\delta_{ij}}\right]^{1-\delta_i}\\
& = \prod_{j=1}^{J_i} (1-h^d(j))^{1-\delta_{ij}}h^d(j)^{\delta_{ij}},
\end{aligned}$${#eq-discrete-likelihood-2}
<!--  -->
where the last equality follows from $\delta_{ij}=0 \forall j=1,\ldots,J_i$, if $\delta_i = 0$ and thus 
$\prod{1-h^d(j)}^{1-\delta_{ij}}h^d(j)^{\delta_{ij}} = \prod{1-h^d(j)}^{1-\delta_{ij}}$.

The importance of this result may not be immediately apparent, but recall that if $Z \sim Bernoulli(\pi)$, where $\pi = P(Z = 1)$, then likelihood contribution of $Z$ is given by $P(Z = z) = \pi^z (1-\pi)^{1-z}$. 
We therefore recognize that the likelihood contribution @eq-discrete-likelihood-2 can also be obtained by assuming that the interval-specific event indicators $\delta_{ij}$ are realizations of random variables $\Delta_{ij} \stackrel{iid}{\sim} Bernoulli(\pi_j = h^d(j))$. 
Thus
<!--  -->
$$\begin{aligned}
\pi_j & = P(\Delta_{ij} = 1|t_j)\\ & = P(Y_i \in (a_{j-1}, a_j] | Y_i > a_{j-1}) = h^d(j)
\end{aligned}$$ {#eq-discrete-hazard-probability}
<!--  -->
Note that in @eq-discrete-hazard-probability, $\Delta_{ij} =1$ is equivalent to $Y_i \in (a_{j-1}, a_j]$, while the conditioning on $Y_i > a_{j-1}$ is implicit in the definition of $\delta_{ij}$ in @eq-interval-indicator.

This implies that we can estimate the discrete time hazards $h^d(j)$ for each interval $j$ by fitting any binary classifcation model to the transformed data set
<!--  -->
$$\calD_{\mathcal{A}} = \{(\delta_{ij}, t_{j})\}_{i=1,\ldots,n;\quad j=1,\ldots,J_i},$${#eq-discrete-data}
<!--  -->
where $\delta_{ij}$ are the targets and $t_{j}$, some representation of time in interval $j$ (for example $t_j = j$ or $t_j = a_{j}$), enters the estimation as a feature and is needed in order to estimate different hazards/probabilities in different intervals.

In the presence of features $\xx_{i}$ we assume $\Delta_{ij}| \xx_{i}, t_j \stackrel{iid}{\sim} Ber(\pi_{ij})$, where $\pi_{ij} = P(\Delta_{ij} = 1| \xx_{i}, t_j) = h^d(j|\xx_{i})$ is the discrete hazard rate for interval $j$ given features $\xx_{i}$ and is estimated by applying binary classifiers to 
<!--  -->
$$\calD_{\mathcal{A}} = \{(\delta_{ij}, \xx_{i}, t_{j})\}_{i=1,\ldots,n;\quad j=1,\ldots,J_i}.$$
<!--  -->

### Example: Logistic Regression

To illustrate the discrete time reduction approach, once again consider the tumor data set introduced in Table @tbl-surv-data-tumor. The follow-up time is partitioned into $J = 100$ equidistant intervals and the data is transformed the data according to the procedure described in @sec-data-transformation.
Define $x_{i1} = \mathbb{I}(\text{complications}_i = \text{"yes"})$.
We then fit a logistic regression model 
<!--  -->
$$\text{logit}(\pi_{ij}) = \beta_{0j} + \beta_1x_{i1}
$${#eq-discrete-time-glm-po}
<!--  -->
to the transformed data set, where $j$ denotes the interval index, $\beta_{0j}$ are the interval-specific intercepts (technically we include the interval index $j$ as a reference coded categorical feature in the model) and $\beta_1$ is the common effect of complications on the discrete hazard rate (accross all intervals).

The estimated survival probabilities are obtained by calculating $\hat{S}^d(j|x_{1}) = \prod_{k=1}^j (1-\hat{h}^d(k|x_{1}))$ for each interval $j$ and complication group, where $\hat{h}^d(k|x_{1})$ are the predicted discrete hazards from the logistic regression model.

Figure @fig-discrete-time-glm shows the estimated survival probabilities from the discrete time model (dashed lines) together with the Kaplan-Meier estimates (solid lines) for comparison.
The model specified in Equation @eq-discrete-time-glm-po is a proportional odds model, where the baseline hazard is the same for both complication groups, thus the shape of the hazard and therefore the survival probability curve is the same for both complication groups, shifted by the common effect of complications. This does not describe the data too well as the hazards are different in the two groups (as already established in XXX)

![Comparison of survival probability estimates using the discrete time model (logistic regression) and the Kaplan-Meier estimator, for patients with and without complications.](Figures/reductions/discrete-time-complications-glm.png){#fig-discrete-time-glm fig-alt="Single plot showing survival curves for patients with and without complications. Kaplan-Meier estimates are shown as solid lines (black for no complications, gray for complications) and discrete time model estimates are shown as dashed blue lines (darker blue for no complications, lighter blue for complications)."}

In order to estimate separate discrete baseline hazards for each group, we can introduce an interaction term between the interval and complications variables.
<!--  -->
$$\text{logit}(\pi_{ij}) = \beta_{0j} + \beta_{1j}x_{i1}
$${#eq-discrete-time-glm-interaction}
<!--  -->
where $\beta_{0j}$ are the interval-specific intercepts for the reference group (no complications), $\beta_{1j}$ are the deviations from the reference group for the complications group (technically this is fit as an interaction model using reference coded categorical features for interval and complications).
This specification allows the shape of the hazard and therefore the survival probability curve to be different for the two groups. 

Figure @fig-discrete-time-glm-interaction shows the estimated survival probabilities from the interaction model (dashed lines) together with the Kaplan-Meier estimates (solid lines) for comparison.
The interaction model provides a much better fit to the data compared to the proportional odds model, as it allows for separate baseline hazards for each complications group. The difference to the Kaplan-Meier estimates is barely detectable. 

![Comparison of survival probability estimates using the discrete time model with interaction (logistic regression) and the Kaplan-Meier estimator, for patients with and without complications.](Figures/reductions/discrete-time-complications-glm-interaction.png){#fig-discrete-time-glm-interaction fig-alt="Single plot showing survival curves for patients with and without complications. Kaplan-Meier estimates are shown as solid lines and discrete time model estimates with interaction are shown as dashed lines, both colored by complications status."}

While this is a simple example with only one feature, it shows that the discrete time reduction approach can be used to estimate the distribution of event times well when the number of intervals is large enough, despite the fact that we ignore the information about the exact event time.
Importantly, the estimated survival probabilities are discrete, representing survival probabilities at the interval endpoints. 
However, a simple solution to generate continuous survival function predictions is to linearly interpolate between the interval endpoints.

The example also raises the question about the choice of the number of intervals $J$ and the placement of interval boundaries $\mathcal{A}$. These questions will be discussed in more detail in @sec-choice-of-interval-boundaries, which is relevant for both, the discrete time reduction approach and the piecewise constant hazards approach discussed in the next section.

## Survival Stacking {#sec-survival-stacking}

Survival stacking (@craigreviewsurvival) casts survival tasks to classification tasks similarly to the discrete time method described in Section @sec-discrete-time-reduction.
However, there is a small but important difference in the creation of event indicators $\delta_{ij}$. Rather than constructing intervals based on partitioning of the follow-up along boundary points $\mathcal{A}$, survival stacking uses the unique, observed event times $t_{(1)}, < \ldots < t_{(m)}$ (@eq-unique-ordered-event-times) and defines
<!--  -->
$$\delta_{ij} = \begin{cases}
1, & t_i = t_{(j)} \text{ and } \delta_i = 1 \\
0, & \text{ else }
\end{cases},\quad j \in \{1,\ldots,m: i \in R_{t_{(j)}}\}$$ {#eq-interval-indicator-stacking}
<!--  -->
where $R_{t_{(j)}}$ is the risk set (@eq-risk-set) at time $t_{(j)}$.

Definition @eq-interval-indicator-stacking implies that the event indicators $\delta_{ij}$ are only defined for timepoints $t_{(j)}$ at which subject $i$ is still at risk for the event. Therefore, this method can be easily applied to left-truncated data by using the more general definition of the risk set (@eq-risk-lt).

For illustration, consider once again, the example from Figure @fig-data-trafo.
The adapted data-transformation for survival stacking is shown in Figure @fig-data-trafo-stacking (droping columns that are not meaningful here).
At the first event time $t_{(1)} = 0.5$, all subjects are sitll at risk for the event. 
At time $t_{(2)} = 2.7$, however, only subject $3$ is still at risk for the event.

<!-- https://drive.google.com/file/d/1kOnoJpER1hrM19x5IQ1PqAAAh0G-wvqZ/view?usp=sharing -->
![Illustration of the data transfomration needed for survival stacking. Left: Data in standard time-to-event format. Right: Transformed data using unique, observed event times $t_{(1)}, < \ldots < t_{(m)}$.](Figures/reductions/data-trafo-pooled-log-reg.png){#fig-data-trafo-stacking fig-alt="Partitioning the follow-up time into discrete intervals." width=80%}

Note that in contrast to @fig-data-trafo, here subject $1$ has only one row and subject $3$ has two rather than three rows. This could suggest that the data transformation for survival stacking creates smaller data sets, however, we need to create a data set for each observed event time $t_{(j)}, j=1,\ldots,m$, whereas for the discret time approach in @sec-discrete-time-reduction one can freely choose the number and placement of interval boundaries $\mathcal{A}$, such that $|\mathcal{A}| < m$.

Data transformation @eq-interval-indicator-stacking yields data 
<!--  -->
$$\calD_{\mathcal{A}} = \{(\delta_{ij}, t_{j}, \xx_{ij})\}_{i=1,\ldots,n;\ j=1,\ldots,m: i \in R_{t_{(j)}}},$${#eq-survival-stacking-data}
<!--  -->
where the $\delta_{ij}$ can be used as targets for binary classification and as before, any algorithm that returns class probabilities can be used for estimation. 

### Example: Pooled Logistic Regression

When the learner used to estimate the probabilities for an event at time $t_{(j)}$ is a logistic regression model, the method is known as pooled logistic regression (@dagostinoRelationPooledLogistic1990a). Pooled because rather than fitting one separate logistic regression model for each event time $t_{(j)}$, a single logistic regression model is fitted to the stacked data set, conditional on the interval information $t_j$ and potentially other features $\xx_{i}$.









## Piecewise Constant Hazards {#sec-piecewise-constant-hazards}

The general idea of the piecewise constant hazards approach is conceptually simple: 

1. Partition the follow-up into many intervals
2. Estimate a constant hazard rate for each interval

Intuitively, any underlying continuous hazard function of the data generating process can be approximated arbitrarily well given enough intervals (and events).
This model class is known as the piecewise exponential model (PEM) because assuming event times to be exponentially distributed, implies constant hazards within each interval.

Consider the partitioning of the follow-up as defined in @eq-cut-points and assume that within each interval $I_j = (a_{j-1}, a_j]$, the hazard function is constant, that is 
<!--  -->
$$h(\tau) = h_j,\ \forall\ \tau \in I_j$${#eq-constant-hazard}
<!--  -->

First we derive the likelihood contribution of subject $i$ under assumption @eq-constant-hazard, 
starting with the general likelihood for right-censored data (@eq-right-censoring-likelihood):
<!--  -->
$$\begin{aligned}
\calL_{i} 
& = h(t_i)^{\delta_{i}}S(t_i)\\
& = h(t_i)^{\delta_{i}}\exp\left(-\int_{0}^{t_i} h(u) \ \du\right)\\
& = h_{J_i}^{\delta_{i}}\exp\left(-\left[\sum_{j=1}^{J_i-1} (a_j - a_{j-1})h_j + (t_i - a_{J_i-1})h_{J_i}\right]\right)\\
& = h_{J_i}^{\delta_{i}}\exp\left(-\sum_{j=1}^{J_i} h_j t_{ij}\right) = h_{J_i}^{\delta_{i}}\prod_{i=1}^{J_i}\exp\left(- h_j t_{ij}\right)\\
& = \prod_{j=1}^{J_i} h_j^{\delta_{ij}} \exp(-h_j t_{ij}),
\end{aligned}$${#eq-pem-likelihood-general}
<!--  -->
where the first equality follows from @eq-surv-haz, the third and fourth equalities follow from assumption @eq-constant-hazard and definition @eq-time-at-risk, and the last equality follows from @eq-interval-indicator such that $h_{J_i}^{\delta_i} = \prod_{j=1}^{J_i} h_j^{\delta_{ij}}$ (when $\delta_i = 0$, all $\delta_{ij} = 0$, when $\delta_{i} = 1$, only $\delta_{iJ_i} = 1$).

Now assume that the interval-specific event indicators $\delta_{ij}$ are realizations of random variables $\Delta_{ij} \stackrel{iid}\sim \text{Poisson}(\mu_{ij} : = h_j t_{ij})$ and recall that $Z\sim \text{Poisson}(\mu)$ implies $P(Z = z) = \frac{\mu^z \exp(-\mu)}{z!}$.
Thus, the likelihood contribution of subject $i$ can be written as 
<!--  -->
$$\begin{aligned}
\calL_{\text{Poisson},i} 
& = \prod_{j=1}^{J_i} P(\Delta_{ij} = \delta_{ij}) = \prod_{j=1}^{J_i} \frac{(h_j t_{ij})^{\delta_{ij}} \exp(-h_j t_{ij})}{\delta_{ij}!}\\
& = \prod_{j=1}^{J_i} h_j^{\delta_{ij}} t_{ij}^{\delta_{ij}}\exp(-h_j t_{ij}),
\end{aligned}$$ {#eq-pem-poisson-likelihood}
<!--  -->
where the last equality follows from $\delta_{ij} \in \{0,1\}$ and $0!=1!=1$.

Note that $\calL_{i,\text{Poisson}} \propto \calL_i$ from equation @eq-pem-likelihood-general, since $t_{ij}$ is a constant and does not depend on the parameters of interest (here $h_j$). 
This implies that we can estimate a model with piecewise constant hazards by optimizing the Poisson likelihood @eq-pem-poisson-likelihood with respect to $h_j$. The $t_{ij}$ term enters as an offset term $\log(t_{ij})$ in the Poisson regression model.

More generally, in the presence of features $\xx_i$, assume $\delta_{ij}|\xx_i \stackrel{iid}{\sim} \text{Poisson}(\mu_{ij})$, where $\mu_{ij} = h_{ij} t_{ij}$ is the expected number of events in interval $j$ given features $\xx_i$ and $h_{ij} = g(t_j, \xx_i)$ is the hazard rate in interval $j$ given features $\xx_i$.
This can be estimated by fitting a Poisson regression model with log-link to the transformed data set
<!--  -->
$$\calD_{\mathcal{A}} = \{(\delta_{ij}, t_{ij}, t_j, \xx_{ij})\}_{i=1,\ldots,n; j=1,\ldots,J_i},$${#eq-pem-data}
<!--  -->
where the model specification is
<!--  -->
$$\log(\mathbb{E}[\Delta_{ij}]) = \log(h_{ij}) + \log(t_{ij}),$${#eq-pem-model}
<!--  -->
where $\log(h_{ij}) = g(\xx_i, t_j)$ is the interval and feature specific hazard rate, $g$ is a function learned by the model of choice and $\log(t_{ij})$ is included as an offset term in the Poisson likelihood/Loss function.

Importantly, in contrast to the discrete time reductions in Section @sec-discrete-time-reduction, the piecewise exponential model likelihood has no information loss regarding the exact time-to-event by including the $t_{ij}$ terms (@eq-time-at-risk) and estimates continuous time hazards rather than discrete time hazards.

<!-- TODO: The model predicts $\hat{h}_{ij}$ and we need to transform this to $\hat{S}_{ij}$ if we want S(t) predictions -->

### Example: Poisson Regression

To illustrate the piecewise constant hazards approach, we once again consider the tumor data set.
We partition the follow-up time into $J = 100$ equidistant intervals.
We then fit a Poisson regression model with interaction between interval and complications to allow for separate baseline hazards for each complications group: 

<!-- FIXME: eq not correct -->
$$\log(\mathbb{E}[\Delta_{ij}]) = \beta_{0} + \sum_{k=1}^{J-1} \beta_{0k} \mathbb{I}(j=k) + \log(t_{ij}),$${#eq-pem-model-interaction}

<!-- TODO: show estimated hazards? -->

Figure @fig-pem-interaction shows the estimated survival probabilities from the piecewise exponential model (dashed lines) together with the Kaplan-Meier estimates (solid lines) for comparison.
The piecewise exponential model provides an excellent fit to the data, closely approximating the Kaplan-Meier estimates.
This demonstrates that the piecewise constant hazards approach can effectively estimate the survival distribution when using a sufficient number of intervals.

![Comparison of survival probability estimates using the piecewise exponential model (Poisson regression) and the Kaplan-Meier estimator, for patients with and without complications.](Figures/reductions/pem-complications-interaction.png){#fig-pem-interaction fig-alt="Single plot showing survival curves for patients with and without complications. Kaplan-Meier estimates are shown as solid lines and piecewise exponential model estimates are shown as dashed lines, both colored by complications status."}





## Choice of Interval Boundaries {#sec-choice-of-interval-boundaries}

As mentioned before, the choice of interval boundaries $\mathcal{A}$ is important in context of partition based reductions. 
This choice is also closely related to the choice of model class used for estimation (as will be illustrated below).

A general trade-off is between the flexibility and the robustness of the interval-specific hazard estimates. 
Larger number of intervals $J$ leads to more flexible hazard estimation but also potentially to small intervals with only few observations or events. 
This can lead to large differences in hazard estimates between neighboring intervals (which is implausible in many use cases) and sensitivity to the placement of interval boundaries $\mathcal{A}$.
This is illustrated in Figure @fig-pem-interval-comparison.

![Comparison of piecewise exponential model (PEM) hazard estimation with different numbers of intervals $J$ and different placements of interval boundaries $\mathcal{A}$. The figure shows the true underlying hazard function (solid black line) together with PEM estimates using different configurations: (a) few intervals with equidistant boundaries, (b) many intervals with equidistant boundaries, (c) few intervals with data-driven boundaries, and (d) many intervals with data-driven boundaries. This illustrates the trade-off between flexibility and robustness, as well as the sensitivity to the choice of interval boundaries.](Figures/reductions/pem-interval-comparison.png){#fig-pem-interval-comparison fig-alt="Comparison of PEM hazard estimates with different interval configurations against the true underlying hazard function." width=100%}

Therefore, the number and placement of interval boundaries can be viewed as a hyperparameter in the estimation process. 
However the search space quickly becomes large, expensive to explore and doesn't necessarily solve the problem of large differences in neighboring (discrete) baseline hazards. 

A more prorgrammatic approach is to use a sufficiently large number of intervals in order to allow enough flexibility and then use some form of regularization that smooths the baseline hazard estimates. Examples ... and ... present examples of such approaches. 


### Example: Penalized spline regression {#sec-pam}

... show estimtion of PAM and compare to PEM for different number of intervals. Mention big GAM methods by wood, runtime, ...

mention limitation: baseline hazard is assumed to be smooth over entire follow-up. will oversmooth if there are break points. 





### Example: Gradient boosted trees

Recall from section @sec-surv-ml-models-boost-regr the definition of boosting. Such models can be used to estimate the GAM model from @sec-pam using univariate GAM models as base learners (reference mboost and friends XXX). However, we can also use trees as base learners, which is often preferred, especially in combination with the popular XGBoost implementation (@cheng2016xgboost).
(Cite Bender 2021 XXX)

Once again, the variable representing time $t_j$ is included as numeric variable in the model. A split w.r.t. to $t_j$ implies a change in the hazard rate before and after the split. Interactions between other features and $t_j$ are also learned automatically and allow time-varying effects and non-proportional hazards. 

To apply gradient boosting to the piecewise exponential model, we need to specify the loss function and derive the corresponding gradients (pseudo-residuals) for the Poisson regression model with offset.

Recall from @eq-pem-model that the model specification is
$$\log(\mathbb{E}[\Delta_{ij}]) = \log(h_{ij}) + \log(t_{ij}) = g(\xx_i, t_j) + \log(t_{ij}),$$
where $g(\xx_i, t_j)$ is the function to be learned and $\log(t_{ij})$ is the offset term.

The negative log-likelihood for the Poisson model with offset is given by
$$\begin{aligned}
L(\delta_{ij}, g(\xx_i, t_j)) 
& = -\log P(\Delta_{ij} = \delta_{ij})\\
& = -\left[\delta_{ij}\log(\mu_{ij}) - \mu_{ij} - \log(\delta_{ij}!)\right]\\
& = -\left[\delta_{ij}(\log(h_{ij}) + \log(t_{ij})) - h_{ij}t_{ij} - \log(\delta_{ij}!)\right]\\
& = -\left[\delta_{ij}g(\xx_i, t_j) + \delta_{ij}\log(t_{ij}) - \exp(g(\xx_i, t_j))t_{ij} - \log(\delta_{ij}!)\right],
\end{aligned}$$
where $\mu_{ij} = \exp(g(\xx_i, t_j) + \log(t_{ij})) = \exp(g(\xx_i, t_j))t_{ij}$ is the expected number of events and the term $\log(\delta_{ij}!)$ is constant with respect to the parameters and can be ignored for optimization purposes.

The gradient of the loss with respect to $g(\xx_i, t_j)$ is
$$\frac{\partial L(\delta_{ij}, g(\xx_i, t_j))}{\partial g(\xx_i, t_j)} = -\delta_{ij} + \exp(g(\xx_i, t_j))t_{ij} = -\delta_{ij} + \mu_{ij}.$$

Therefore, the negative gradient (pseudo-residual) at iteration $m$ for observation $(i,j)$ is
$$r_{ijm} = -\frac{\partial L(\delta_{ij}, g_{m-1}(\xx_i, t_j))}{\partial g_{m-1}(\xx_i, t_j)} = \delta_{ij} - \exp(g_{m-1}(\xx_i, t_j))t_{ij} = \delta_{ij} - \hat{\mu}_{ij}^{(m-1)},$${#eq-poisson-pseudo-residual}
where $g_{m-1}(\xx_i, t_j)$ is the current estimate of the log-hazard from the previous $m-1$ iterations and $\hat{\mu}_{ij}^{(m-1)} = \exp(g_{m-1}(\xx_i, t_j))t_{ij}$ is the predicted expected number of events.

The pseudo-residual @eq-poisson-pseudo-residual represents the difference between the observed event indicator $\delta_{ij}$ and the predicted expected number of events $\hat{\mu}_{ij}^{(m-1)}$, which is the natural residual for Poisson regression.

**Initial guess:** The initial guess $g_0(\xx_i, t_j)$ is typically set to a constant value. A common choice is to set $g_0(\xx_i, t_j) = \log(\bar{h})$, where $\bar{h}$ is the overall average hazard rate, estimated as
$$\bar{h} = \frac{\sum_{i=1}^n \sum_{j=1}^{J_i} \delta_{ij}}{\sum_{i=1}^n \sum_{j=1}^{J_i} t_{ij}},$$
which is the total number of events divided by the total time at risk. Alternatively, one can simply initialize $g_0(\xx_i, t_j) = 0$ for all observations, which corresponds to an initial hazard rate of $h_0 = 1$.

**Update step:** At each iteration $m$, a regression tree $h_m$ is fitted to the pseudo-residuals $r_{ijm}$ using the features $(\xx_i, t_j)$ as predictors. The tree partitions the feature space and assigns a constant value to each leaf node. The model is then updated as
$$g_m(\xx_i, t_j) = g_{m-1}(\xx_i, t_j) + \nu h_m(\xx_i, t_j),$${#eq-poisson-boost-update}
where $\nu \in (0,1]$ is the shrinkage parameter (learning rate) that controls the step size of each update.

The complete gradient boosting algorithm for Poisson regression with offset using tree base learners is given in Algorithm @alg-poisson-boost.

**Algorithm:** Gradient Boosting for Poisson Regression with Offset {#alg-poisson-boost}

1. **Initialize:** Set $g_0(\xx_i, t_j) = \log(\bar{h})$ for all $(i,j)$, where $\bar{h} = \frac{\sum_{i,j} \delta_{ij}}{\sum_{i,j} t_{ij}}$, or alternatively $g_0(\xx_i, t_j) = 0$.

2. **For** $m = 1, \ldots, M$:
   1. **Compute pseudo-residuals:** For each observation $(i,j)$ in the training data $\calD_{\mathcal{A}}$:
      $$r_{ijm} = \delta_{ij} - \exp(g_{m-1}(\xx_i, t_j))t_{ij}$$
   2. **Fit base learner:** Fit a regression tree $h_m$ to the pseudo-residuals:
      $$h_m = \arg\min_{h} \sum_{(i,j) \in \calD_{\mathcal{A}}} (r_{ijm} - h(\xx_i, t_j))^2$$
      The tree partitions the feature space based on $(\xx_i, t_j)$ and assigns constant values to each leaf node.
   3. **Update model:** 
      $$g_m(\xx_i, t_j) = g_{m-1}(\xx_i, t_j) + \nu h_m(\xx_i, t_j)$$
      where $\nu$ is the shrinkage parameter.

3. **Return:** Final model $\hat{g}(\xx_i, t_j) = g_M(\xx_i, t_j)$.

**Prediction:** For new observations with features $\xx^*$ and time $t_j^*$, the predicted log-hazard is $\hat{g}(\xx^*, t_j^*) = g_M(\xx^*, t_j^*)$, and the predicted hazard rate is $\hat{h}(\xx^*, t_j^*) = \exp(\hat{g}(\xx^*, t_j^*))$.

The algorithm iteratively improves the model by fitting trees to the residuals from the previous iteration. Each tree captures patterns in the data that were not well explained by the previous model, and the shrinkage parameter $\nu$ ensures that updates are gradual, reducing overfitting. The offset term $\log(t_{ij})$ is implicitly accounted for in the pseudo-residual calculation, as the predicted expected number of events $\hat{\mu}_{ij}^{(m-1)} = \exp(g_{m-1}(\xx_i, t_j))t_{ij}$ includes the time at risk $t_{ij}$. 











