---
abstract: TODO (150-200 WORDS)
---

{{< include _setup.qmd >}}

# Machine Learning {#sec-ml}

{{< include _wip.qmd >}}

This chapter covers core concepts in machine learning.
This is not intended as a comprehensive introduction and does not cover mathematically theory nor how to run machine learning models using software.
Instead, the focus is on introducing important concepts and providing basic intuition into how these can be applied to any machine learning problem, these include machine learning tasks, data splitting, model training and predicting, evaluation, and model comparison.
@Hastie2013 provides a very good introduction to machine learning, with more detail in @Hastie2001.
There are several books covering machine learning implementations in various programming languages, for example @Bischl2024 and @Kuhn2023 for $\Rstats$ and @Geron2019 for Python.

## Basic workflow {#sec-ml-basics}

This book focuses on *supervised learning*, in which predictions are made for outcomes based on data with observed dependent and independent variables.
For example, predicting someone's height is a supervised learning problem as data can be collected for features (independent variables) such as age and sex, and an observable outcome (dependent variable), which is height.
Alternatives to supervised learning include *unsupervised learning*, *semi-supervised learning*, and *reinforcement learning*.
This book is primarily concerned with *predictive* survival analysis, i.e., making future predictions based on (partially) observed survival outcomes, which falls naturally within the supervised learning domain.

The basic machine learning workflow is represented in @fig-ml-basic.
Data is split into training and test datasets.
A learner is selected and is trained on the training data, becoming a fitted model.
The features from the test data is passed to the model which makes predictions for the unseen labels.
The labels from the test data are passed to a chosen measure with the predictions, which evaluates the performance of the model.
The process if repeating this procedure to test different training and test data is called *resampling* and multiple resampling experiments with different models is called *benchmarking*.
All these concepts will be explained in this chapter.

<!-- FIXME - WE'LL WANT TO REDRAW THIS -->
```{mermaid}
%%| label: fig-ml-basic
%%| fig-cap: "Basic machine learning workflow with data splitting, model training, predicting, and evaluating."
flowchart LR
  D(Data) --> Tr(Train)
  D --> Te(Test)
  Tr --> L(Learner)
  L --> Mo(Model)
  Te --Features--> Mo
  Mo --> Pr(Prediction)
  Pr --> Me(Measure)
  Te --Labels--> Me
  Me --> Pe(Performance)
```

## Tasks {#sec-ml-tasks}

A machine learning task is the specification of the mathematical problem that is to be solved by a given algorithm.
For example, "predict the height of a male, 13 year old child", is a machine learning task.
Tasks are derived from datasets and one dataset will give rise to many tasks across any machine learning domain.
The dataset described by columns: 'age', 'weight', 'height', 'sex', 'diagnosis', 'time of death', 'clinician notes', could give rise to any of the following tasks:

* Predict age from all other features - supervised regression
* Predict sex from a subset of features - supervised classification
* Predict time of death from all other features - supervised survival
* Categorise observations into clusters - unsupervised clustering
* Learn to speak like a clinician depending on client diagnosis - natural language processing, likely with reinforcement learning

As this book is focused only on supervised learning, only the first three of these is covered in this chapter and beyond.
The specification of a task is vital for interpreting predictions from a model and its subsequent performance.
Say three models are using the above features to predict 'sex', which of the following are comparable?

* M1: Feature selection followed by training. During feature selection, only 'age', 'weight', and 'height' are kept;
* M2: No feature selection. Trained on all features.
* M3: No feature selection. Trained on 'age', 'weight', 'height'.

Only M1 and M2 can be fairly compared as these are solving the same task: given all features predict 'sex'.
In contrast M3 solves the task: given only 'age', 'weight' and 'height', predict 'sex'.
This subtle difference is important for evaluating model performance and in a medical context can remove potential biases.
For example, for M1 one might conclude "given all available data, XYZ", whereas with M3 data has been deliberately omitted.

Formally, let $\xx \in \calX \subseteq \Reals^{n \times p}$ be a matrix with $p$ features for $n$ observations and let $y \in \calY$ be a vector of labels (or *outcomes* or *targets*) for all observations.
A dataset is then given by $\calD = ((\xx_1, y_1) , . . . , (\xx_n, y_n))$ where it is assumed $\calD \iid (\mathbb{P}_{xy})^n$ for some unknown distribution $\mathbb{P}$.

A machine learning task is the problem of learning the unknown function: $f : \calX \rightarrow \calY$ where $\calY$ specifies the nature of the task, for example classification, regression, or survival.

Regression tasks make continuous predictions, for example someone's height, hence a regression task is specified by $f_R : \calX \rightarrow \calY \subseteq \Reals^n$.

Classification tasks make discrete predictions, for example whether it will rain, snow, or be sunny tomorrow; these are given by $f_C : \calX \rightarrow \calY \subseteq \Naturals$.
If only two categories are possible, for example whether it will rain tomorrow or not, then *binary classification* tasks are represented with $f_B: \calX \rightarrow \{0, 1\}$ or sometimes $f_B: \calX \rightarrow \{-1, 1\}$.

## Training and evaluating models {#sec-ml-models}

The terms *algorithm*, *learner*, and *model* are often conflated in machine learning.
A *learning algorithm*, *algorithm*, or *learner*, is a strategy to estimate the unknown mapping from features to outcome as represented by a task, $f: \calX \rightarrow \calY$.
Given a learner, $LA$, and data, $\calD$, then $\hat{f} := LA(\calD | \theta, \Theta)$.
The terms $\theta$ and $\Theta$ represent model parameters and hyperparameters that are used to fit and control the algorithm respectively.
Model *parameters* (or *weights*) are coefficients to be estimated during model training, these are not directly controlled by a user (i.e., the person training the model) but are instead solely determined by the data and influenced by hyperparameters.
Model *hyperparameters* control *how* the algorithm is run, for example determining if the intercept should be included in a linear regression model (@cnj-lm).
The number of hyperparameters usually increases with learner complexity and performance, and often hyperparameters need to be tuned (@sec-ml-train) instead of manually set.

The process of passing data, $\calD$, setting hyperparameters, $\Theta$, to a learner is known as *training* and the learner is *trained* by estimating the parameters, $\theta$, this trained learner is called a *model*.
Computationally, storing $(\hat{\theta}, \Theta)$ is sufficient to recreate any trained model (assuming the learner is known), and sharing of model weights is common for deep learning models.

Once trained, a model can be used for predictions.
As well as encoding a specific learning strategy, learners also define a prediction strategy.
For traditional statistical models this strategy might be a simple calculation based on the trained coefficients (e.g., predicting a linear predictor @sec-surv-set-types), or for more complex machine learning models this could be an iterative algorithmic procedure with multiple steps.
Given a trained model, $\hat{f}$, and some new features $\xx^* \in \Reals^p$, the model's prediction for the unseen label is $\haty := \hat{f}(\xx^* | \theta, \Theta)$ or more simply $\hatyy := \hatf(\xx^*)$.
Note that there can also be hyperparameters specific to the prediction step.

:::: {.callout-note icon=false}

## Linear regression

::: {#cnj-lm}
Let $f_R : \calX \rightarrow \calY$ be the regression task of interest with $\calX \subseteq \Reals^n$ and $\calY \subseteq \Reals^n$.
Let $(\xx, \yy)$ be data such that $\xx \in \calX$ and $\yy \in \calY$.

Say the learner of interest is a linear regression model with learning algorithm:

$$
(\hat{\beta_0}, \hat{\beta_1}) := \argmin_{\beta_0,\beta_1} \Big\{\sum^n_{i=1} (y_i - \gamma\beta_0 - \beta_1 x_i)^2\Big\}\text{ for }i = 1,...,n
$$

and prediction algorithm:

$$
\hatg(x) = \gamma\hat{\beta_0} + \hat{\beta_1}x
$$

The learner hyperparameters are $\Theta = (\gamma)$ which can take values $0$ or $1$ and the parameters are $\theta = (\beta_0, \beta_1)$.
The learner is fit by passing $(\xx, \yy)$ to the learning algorithm and thus estimating $\hat{\theta}$ and $\hatg$.
A prediction, $\haty$, is made for a new observation by passing $x^* \in \calX$ to the fitted model $\hatg$.

:::

::::

## Evaluating models {#sec-ml-train}

To understand if a model is 'good', its predictions are evaluated with a *loss function*.
Loss functions assign a score to the discrepancy between predictions and true values, $L: \calY \times \calY \rightarrow \ExtReals$.
Given real-world data, $(\XX^*, \yy^*)$, and a trained model $\hatf$, then the loss is given by $L(\hatf{\XX^*}, \yy^*) = L(\hatyy, \yy^*)$.
For a model to be useful, it should perform well in general, and not just for the data used for training and development, which is known as a model's *generalization error*.

A model should not be deployed, i.e., manually or automatically used to make predictions, its generalization error must be estimated to understand if the model is 'good'.
If a model were to be trained and evaluated on the same data, the resulting loss, known as the *training error*, would be an overoptimistic estimate of the true generalization error (@Hastie2013) as the model is making predictions for data it has already 'seen' and the loss is not evaluating its ability to generalize to new, unseen data.
Estimation of the generalization error requires *data splitting*, which is the process of splitting available data, $\calD$, into *training data*, $\dtrain \subset \calD$, and *testing data*, $\dtest = \calD \setminus \dtrain$.
<!-- FIXME: ABOVE COULD BE BETTER WORDED -->

The simplest method to estimate the generalization error is to use *holdout resampling*, which is the process of partitioning the data into one training dataset and one testing dataset, with the model trained on the former and predictions made for the latter.
Using 2/3 of the data for training and 1/3 for testing is a common splitting ratio (@Kohavi1995).
When splitting, it is essential the data is partitioned randomly to ensure any information encoded in data ordering is removed.
Ordering is often important in the real-world, for example in healthcare data when patients are recorded in order of enrolment to a study.
Whilst ordering can provide useful information, it does not generalize to new, unseen data.
For example, the number of days a patient has been in hospital is more useful than the patient's index in the dataset, as the former could be calculated for a new patient whereas the latter is meaningless.
Another example is in the `rats` dataset that was briefly described in @sec-surv.
The `rats` data explores how a novel drug effects tumor incidence.
However, the data is ordered by rat litters with every three rats being in the same litter.
Hence if rats were to be bred or raised differently over time,  even if the `litter` column were removed, this information would still be encoded in the order of the dataset and could impact upon any findings.
Randomly splitting breaks any possible association between order and outcome.
<!-- FIXME: ABOVE IS ALSO BAD! -->

Holdout resampling is a quick method to estimate the generalization error, however it is an imperfect method as, even with random splitting, it can still result in 'unlucky' splits.
Returning to the `rats` data, consider a split that results in only male rats in the training data, the model would then be unable to handle female rats in the testing data, which it had never encountered before.
Alternatively, consider a random split with an even sex balance, as only two out of the 150 rats have a tumor, there is a high probability that the training data will not include any male rat with a tumor.
Hence, in the case of a male rat, the model is likely to ignore the treatment variable and instead just use sex as the most important predictor of tumor incidence.

To deal with this problem, $k$-fold cross-validation (CV) can be used as a more robust method.
$k$-fold CV partitions the data into $k$ subsets, called *folds*.
The training data comprises of $k-1$ of the folds and the remaining one is used for testing and evaluation.
This is repeated $k$ times until each of the folds has been used exactly once as the testing data.
The performance from each fold is averaged into a final performance estimate.
It is common to use $k = 5$ or $k = 10$.
This process can be repeated multiple times (*repeated $k$-fold CV*) and/or $k$ can even be set to $n$, which is known as *leave-one-out cross-validation*.

Cross-validation can also be stratified, which ensures that a variable of interest will have the same distribution in each fold as in the original data.
This is important, and often recommended, in survival analysis to ensure that the proportion of censoring in each fold is representative of the full dataset.
<!-- FIXME: CITE OUR SURVIVAL BENCHMARK PAPER -->

See @Bischl2012 for discussion about more resampling strategies including bootstrapping and subsampling.


## Optimization and benchmarking {#sec-ml-misc}

___

TODO

* Optimisation and tuning
* Benchmarking

___

#### Evaluation {.unnumbered .unlisted}
Models are *evaluated* by evaluation measures called *losses* or *scores*,\footnote{The term 'loss' is usually utilised to refer to evaluation measures to be minimised, whereas 'scores' should be maximised, this is returned to in (@sec-eval).} $L: \calY \times \calY \rightarrow \ExtReals$. Let $(X^*, Y^*) \sim (X,Y)$ be test data (i.e. independent of $\dtrain$) and let $\hatg: \calX \rightarrow \calY$ be a prediction functional fit on $\dtrain$, then these evaluation measures determine how closely predictions, $\hatg(X^*)$, relate to the truth, $Y^*$, thereby providing a method for determining if a model is 'good'.\footnote{Here evaluation refers specifically to predictive ability; other forms of evaluation and further discussion of the area are provided in (@sec-eval).}

#### Resampling {.unnumbered .unlisted}
Models are *tested* on their ability to make predictions. In order to avoid 'optimism of training error'  [@Hastie2013] -- overconfidence caused by testing the model on training data -- models are tested on previously unseen or 'held-out' data. *Resampling* is the procedure of splitting one dataset into two or more for separated training and testing. In this paper only two resampling methods are utilised: *holdout* and *cross-validation*. Holdout is the process of splitting a primary dataset into training data for model fitting and testing data for model predicting. This is an efficient method but may not accurately estimate the expected generalisation error for future model performance, instead this is well-estimated by $K$-fold cross-validation (KCV)  [@Hastie2001]. In KCV, data is split into $K \in \PNaturals$ 'folds' such that $K-1$ of the folds are used for model training and the final $K$th fold for testing. The testing fold is iterated over all $K$ folds, so that each at some point is used for testing and then training (though never at the same time). In each iteration the model is fit on the training folds, and predictions are made and evaluated on the testing fold, giving a loss $L_k := L(\hatg(X^k), Y^k)$, where $(X^k, Y^k)$ are data from the $k$th fold. A final loss is defined by, $L^* := \frac{1}{K} \sum^K_{k = 1} L_k$. Commonly $K = 5$ or $K = 10$  [@Breiman1992; @Kohavi1995].

#### Model Performance Benchmarking {.unnumbered .unlisted}
Whilst *benchmarking* often refers to speed tests, i.e. the time taken to complete an operation, it can also refer to any experiment in which objects (mathematical or computational) are compared. In this report, a benchmark experiment will either refer to the comparison of multiple models' predictive abilities, or comparison of computational speeds and object sizes for model fitting; which of these will be clear from context.

#### Model Comparison {.unnumbered .unlisted}
Models can be analytically compared on how well they make predictions for new data. Model comparison is a complex topic with many open questions  [@Demsar2006; @Dietterich1998; @Nadeau2003] and as such discussion is limited here. When models are compared on multiple datasets, there is more of a consensus in how to evaluate models  [@Demsar2006] and this is expanded on further in [@Sonabend2021b]. Throughout this book there are small simulation experiments for model comparison on single datasets however as these are primarily intended to aid exposition and not to generalise results, it suffices to compare models with the conservative method of constructing confidence intervals around the sample mean and standard error of the loss when available  [@Nadeau2003].

#### Hyper-Parameters and Tuning {.unnumbered .unlisted}
*Tuning* is the process of choosing the optimal hyper-parameter value via automation. In the simplest setting, tuning is performed by selecting a range of values for the hyper-parameter(s) and treating each choice (combination) as a different model. For example if tuning the number of trees in a random forest (@sec-surv-ml-models-ranfor), $m_r$, then a range of values, say $100, 200, 500$ are chosen, and three models $m_{r100}, m_{r200}, m_{r500}$ are benchmarked. The optimal hyper-parameter is given by whichever model is the best performing. *Nested resampling* is a common method to prevent overfitting that could occur from using overlapping data for tuning, training, or testing. Nested resampling is the process of resampling the training set again for tuning.

## Classification and regression {#sec-surv-ml-car}

Before introducing machine learning for survival analysis, which is considered 'non-classical', the more standard classification and regression set-ups are provided; these are referenced throughout this book.

#### Classification {#sec-surv-ml-car-class}

Classification problems make predictions about categorical (or discrete) events, these may be *deterministic* or *probabilistic*.  Deterministic classification predicts which category an observation falls into, whereas probabilistic classification predicts the probability of an observation falling into each category. In this brief introduction only binary single-label classification is discussed, though the multi-label case is considered in \ref{sec:car_reduxes_r7_mlc}. In binary classification, there are two possible categories an observation can fall into, usually referred to as the 'positive' and 'negative' class. For example predicting the probability of death due to a virus is a probabilistic classification task where the 'positive' event is death.

A probabilistic prediction is more informative than a deterministic one as it encodes uncertainty about the prediction. For example it is clearly more informative to predict a $70%$ chance of rain tomorrow instead of simply 'rain'. Moreover the latter prediction implicitly contains an erroneous assumption of certainty, e.g. 'it will rain tomorrow'.

:::: {.callout-note icon=false}

## Classification Task

::: {#cnj-task-classif}
Let $(X,Y)$ be random variables t.v.i. $\calX \times \calY$ where $\calX \subseteq \Reals^p$ and $\calY = \{0, 1\}$. Then,


* The *probabilistic classification task* is the problem of predicting the probability of a single event taking place and is specified by $g: \calX \rightarrow [0, 1]$.
* The *deterministic classification task* is the problem of predicting if a single event takes place and is specified by $g: \calX \rightarrow \calY$.


The estimated prediction functional $\hatg$ is fit on training data \\$(X_1,Y_1),...,(X_n,Y_n) \iid (X,Y)$ and is considered 'good' if $\EE[L(Y^*, \hatg(X^*))]$ is low, where $(X^*, Y^*) \sim (X, Y)$ is independent of $(X_1,Y_1),...,(X_n,Y_n)$ and $\hatg$.

In the probabilistic case, the prediction $\hat{g}$ maps to the estimated probability mass function $\hat{p}_Y$ such that $\hat{p}_Y(1) = 1 - \hat{p}_Y(0)$.
:::

::::

#### Regression {#sec-surv-ml-regr}

A regression prediction is one in which the goal is to predict a continuous outcome from a set of features. For example predicting the time until an event (without censoring) occurs, is a regression problem.

:::: {.callout-note icon=false}

## Regression Task

::: {#cnj-task-regr}
Let $(X,Y)$ be random variables t.v.i. $\calX \times \calY$ where $\calX \subseteq \Reals^p$ and $\calY \subseteq \Reals$. Let $\calS \subset \Distr(\calY)$ be a convex set of distributions on $\calY$. Then,

* The *probabilistic regression task* is the problem of predicting a conditional distribution over the Reals and is specified by $g : \calX \rightarrow \calS$.
* The *deterministic regression task* is the problem of predicting a single continuous value in the Reals and is specified by $g: \calX \rightarrow \calY$.


The estimated prediction functional $\hatg$ is fit on training data \\$(X_1,Y_1),...,(X_n,Y_n) \iid (X,Y)$ and is considered 'good' if $\EE[L(Y^*, \hatg(X^*))]$ is low, where $(X^*, Y^*) \sim (X, Y)$ is independent of $(X_1,Y_1),...,(X_n,Y_n)$ and $\hatg$.
:::
::::

Whilst regression can be either probabilistic or deterministic, the latter is much more common and therefore in this book 'regression' refers to the deterministic case unless otherwise stated.
