---
abstract: TODO (150-200 WORDS)
---

{{< include _setup.qmd >}}

# Calibration Measures {#sec-eval-distr-calib}

{{< include _wip.qmd >}}

Calibration measures evaluate the 'average' quality of survival distribution predictions.
This chapter is kept relatively short as the literature in this area is scarce [@Rahman2017], this is likely due to the meaning of calibration being unclear in a survival context [@VanHouwelingen2000].
However the meaning of calibration is better specified once specific metrics are introduced.
As with other measure classes, only measures that can generalise beyond Cox PH models are included here but note that several calibration measures for re-calibrating PH models have been discussed in the literature [@Demler2015; @VanHouwelingen2000].

Calibration measures can be grouped [@Andres2018] into those that evaluate distributions at a single time-point, '1-Calibration' or 'Point Calibration' measures, and those that evaluate distributions at all time-points 'distributional-calibration' or 'probabilistic calibration' measures.
A point-calibration measure will evaluate a function of the predicted distribution at a single time-point whereas a probabilistic measure evaluates the distribution over a range of time-points; in both cases the evaluated quantity is compared to the observed outcome, $(T^*, \Delta^*)$.

## Point Calibration {#sec-eval-distr-calib-point}

Point calibration measures can be further divided into metrics that evaluate calibration at a single time-point (by reduction) and measures that evaluate an entire distribution by only considering the event time.
The difference may sound subtle but it affects conclusions that can be drawn.
In the first case, a calibration measure can only draw conclusions at that one time-point, whereas the second case can draw conclusions about the calibration of the entire distribution.
This is the same caveat as using prediction error curves for scoring rules.

```{r echo=FALSE}
learn("sec-pecs", "prediction error curves")
```

### Calibration by Reduction

Point calibration measures are implicitly reduction methods as they use classification methods to evaluate a full distribution based on a single point only.
For example, given a predicted survival function $\hatS$, one could calculate the survival function at a single time point, $\hatS{\tau^*}$ and then use probabilistic classification calibration measures.
Using this approach one may employ common calibration methods such as the Hosmer–Lemeshow test [@Hosmer1980].
Measuring calibration in this way can have significant drawbacks as a model may be well-calibrated at one time-point but poorly calibrated at all others [@Haider2020].
To mitigate this, one could perform the Hosmer–Lemeshow test (or other applicable tests) multiple times with multiple testing correction at many (or all possible) time points, however this would be less efficient and more difficult to interpret than other measures discussed in this chapter.

```{r echo=FALSE}
learn("sec-car", "reduction")
```

### Houwelingen's $\alpha$

As opposed to evaluating distributions at one or more arbitrary time points, one could instead evaluate distribution predictions at meaningful times.
van Houwelingen proposed several measures [@VanHouwelingen2000] for calibration but only one generalises to all probabilistic survival models, termed here 'Houwelingen's $\alpha$'.
The statistic measures calibration by assessing if the model correctly predicts the true number of total events in the test data.

$$
\frac{\sum_i H_i(T)}{\sum_i \hatH_i(T)}
$$

This follows by noting the closely related nature of survival analysis and counting processes, and exploiting the fact that the sum of the cumulative hazard function is an estimate for the number of events in a given time-period [@dataapplied].
As this result is often surprising result to readers, below is a short experiment using R that demonstrates how the sum of the cumulative hazard estimated by a Kaplan-Meier estimator is identical to the number of randomly simulated deaths in a dataset:
```{r}
#| echo: true
set.seed(42)
library(survival)

event = rbinom(100, 1, 0.7)
times = runif(100)
H = survfit(Surv(times, event) ~ 1)$cumhaz
sum(event) / sum(H)
```

Houwelingen's $\alpha$ is therefore defined by substituting $\sum_i H_i(T)$ for $\sum_i \Delta_i$:

$$
H_\alpha(\Delta, \hatH, T) = \frac{\sum_i \Delta}{\sum_i \hatH(T_i)}
$$

with standard error $SE(H_\alpha) = \exp(1/\sqrt{(\sum_i \Delta)})$.
A model is well-calibrated with respect to $H_\alpha$ if $H_\alpha = 1$.


This method can infer results about the calibration of an entire model and not just at a single point because the measure is calculated at a meaningful time (the event time) and utilises known results from counting processes to verify if the expected number of deaths equals the observed number of deaths.

However, as with the reduction method, the statistic is derived from a single point (the observed event time) for each individual and thus it is possible that the model is well-calibrated only for making predictions at the event time, but not over the full $\PReals$ range.

## Probabilistic Calibration {#sec-eval-distr-calib-prob}

Unlike other areas of evaluation, graphical methods are favoured in calibration and possibly more so than numerical ones. Graphical methods compare the average predicted distribution to the expected distribution. As the expected distribution is itself unknown, this is often estimated with the Kaplan-Meier curve.

### Kaplan-Meier Comparison

The simplest graphical comparison compares the average predicted survival curve to the Kaplan-Meier curve estimated on the testing data. Formally, let \\ $\zeta_1.S,...,\zeta_m.S$ be predicted survival functions, then the average predicted survival function is a mixture of these distributions, $\mean[m]{\zeta_i.S(\tau)}$. Plotting this mixture and the Kaplan-Meier on $\tau$ vs $S(\tau)$ allows a visual comparison of how closely these curves align. An example is given in @fig-eval-calib-km, the Cox model (CPH) is well-calibrated as it almost perfectly overlaps the Kaplan-Meier estimator, whereas predictions from the poorly-calibrated support vector machine (SVM) are far from this line.

![Assessing the calibration of a Cox PH (CPH) and SVM (with distribution composition by PH form and Kaplan-Meier (@sec-car)) by comparing the average survival prediction to a Kaplan-Meier (KM) estimate on the testing dataset. x-axis is time and y-axis is the predicted survival functions evaluated over time. The CPH (red line) is said to be well-calibrated as it almost perfectly overlaps the Kaplan-Meier (green line), whereas the SVM (blue line) is far from this line. Models trained and tested on randomly simulated data from the `r pkg("simsurv")` [@pkgsimsurv] package in $\proba$ [@pkgmlr3proba].](Figures/evaluation/calib_km.png){#fig-eval-calib-km fig-alt="TODO"}

This approach is both simple and interpretable. In the example above one can conclude: on average, the trained Cox PH predicts a distribution just as well as (or very close to) an unconditional estimator using the real test data. A major caveat is that conclusions are at an average *population* level with no individual-level measurement.

In order to capture finer information on a level closer to inidivduals, calibration can be applied to the predicted relative risks or linear predictor. One such approach is to bin the predictions to create different 'risk groups' from low-to-high risk [@Royston2013]. These groups are then plotted against a stratified Kaplan-Meier estimator. This allows for a more nuanced approach to calibration and can simultaneously visualise a model's discrimination. However this method is far less transparent as it adds even more subjectivity around how many risk groups to create and how to create them [@Royston2013].

### D-Calibration

D-Calibration [@Andres2018; @Haider2020] is a very recent method that aims to evaluate a model's calibration at all time-points in a predicted survival distribution. The D-calibration measure is identical to the $\chi^2$ test-statistic, which is usually written as follows

$$
\chi^2 := \sum_{i=1}^n \frac{(O_i - E_i)^2}{E_i}
$$
where $O_1,...,O_n$ is the observed number of events in $n$ groups and $E_1,...,E_n$ is the expected number of events. The statistic is utilised to determine if the underlying distribution of the observed events follows a theoretical/expected distribution.

The D-Calibration measure tests if predictions (observations) from the  survival functions of predicted distributions, $\zeta_1.S,...,\zeta_m.S$, follow the uniform distribution as expected. The following lemma motivates this test.

::: {#lem-uniform-surv}
Let $\zeta$ be a continuous probability distribution and let $X \sim \zeta$ be a random variable. Let $S_X$ be the survival function of $X$. Then $S_X(X)$ follows the standard Uniform distribution: $S_X(X) \sim \calU(0, 1)$.
:::

In order to utilise the $\chi^2$ test (for categorical variables), the $[0,1]$ codomain of $\zeta_i.S$ is cut into $B$ disjoint contiguous intervals ('bins') over the full range $[0,1]$. Let $m$ be the total number of observations in the test data. Then assuming a discrete uniform distribution as the theoretical distribution, the expected number of events is $m/B$.

The observed number of events in bin $i$, $O_i$, is defined as follows: Define $b_i$ as the set of observations that die in the $i$th bin, formally defined by $b_i := \{j \in 1,...,m : \lceil \zeta_j.S(T^*_j)B \rceil = i\}$, where $j = 1,...,m$ are the indices of the test observations and $\zeta = (\zeta_1,...,\zeta_m)$ are predicted distributions.^[This is a slightly simplified procedure which omits handling of censoring, but this is easily extended in the full algorithm, see Algorithm 2 of Haider $\etal$ (2020) [@Haider2020].] Then, $O_i = |b_i|, \forall i \in 1,...,B$.

The D-Calibration measure, or $\chi^2$ statistic, is now defined by,


$$
D_{\chi^2}(\zeta, T^*) :=  \frac{\sum^B_{i = 1} (O_i - \frac{m}{B})^2}{m/B}
$$

This measure has several useful properties. Firstly, a $p$-value can be derived from $\chi^2_{B-1}$ to hypothesis test if a single model is 'D-calibrated'. Secondly, as a model is increasingly well-calibrated it holds that $D_{\chi^2} \rightarrow 0$ (as the number of observed events approach expected events), which motivates utilising the test for model comparison. Thirdly, the theory lends itself very nicely to an intuitive graphical calibration method:

If a model is D-calibrated, i.e. predicted distributions from the model result in a low D-calibration, then one expects,

$$
p = \frac{\sum_i \II(T^*_i \leq \zeta_i.F^{-1}(p))}{|T^*|}
$$ {#eq-eval-dcalib}
where $p \in [0,1]$ and $\zeta_i.F^{-1}$ is the inverse cumulative distribution function of the $i$th predicted distribution. In words, if a model is D-calibrated then the number of deaths occurring at or before each quantile should be equal to the quantile itself, for example 50% of deaths should occur before their predicted median survival time. Therefore one can graphically test for D-calibration by plotting $p$ on the x-axis and the right hand side of @eq-eval-dcalib on the y-axis. A D-calibrated model should result in a straight line on $x = y$. This is visualised in @fig-eval-dcalib for the same models as in @fig-eval-calib-km. Again the SVM is terribly-calibrated but the CPH is better calibrated. In this case it is clearer that the D-calibration of the CPH is not perfect, especially at higher quantiles. Comparison to $\chi^2_9$ indicates the CPH is D-calibrated whereas the SVM is not.

![Assessing the D-calibration of the Cox PH (CPH) and SVM from the same data as @fig-eval-calib-km: models trained and tested on randomly simulated data from the `r pkg("simsurv")` [@pkgsimsurv] package in $\proba$ [@pkgmlr3proba]. x-axis are quantiles in $[0,1]$ and y-axis are predicted quantiles from the models. The dashed line is $y = x$. Again the SVM is terribly calibrated and the CPH is better calibrated as it is closer to $y = x$.](Figures/evaluation/dcalib.png){#fig-eval-dcalib fig-alt="TODO"}

### Transparency and Accessibility

It has already been stated that performance cannot be considered for calibration measures however it is unclear if any of these measures are even accessible or transparent as they often require expert interpretation to prevent erroneous conclusions. This is demonstrated by example using the same data and models as in  @fig-eval-dcalib. The predictions from these models are evaluated with Harrell's C (@sec-eval-crank-disc-conc), the Integrated Graf Score (@sec-eval-distr-commonsurv), D-Calibration, and Houwelingen's $\alpha$ (@tbl-eval-calib). All measures agree that the SVM performs poorly. In contrast, whilst the Cox PH (CPH) is well-calibrated according to both measures, its concordance is quite bad (barely above baseline). Haider $\etal$ [@Haider2020] claimed that if a model is D-Calibrated then a 'patient should believe the prediction from the survival curve', these results clearly demonstrate otherwise. Measures of calibration alone are clearly not sufficient to determine if a survival curve prediction should be 'believed' and should therefore be computed alongside measures of discrimination or scoring rules, discussed next.

| Model | KM | CPH | SVM |
| ---- | -- | --- | -- |
| $C_H^1$ | 0.5 | 0.52 | 0.45 |
| $L_{IGS}^2$ | 0.18 | 0.18 | 0.52  |
| $H_\alpha^3$ |  0.99 | 1.00 | 15.42 |
| $D_{\chi^2}^4$ | 2.23$^*$ | 7.03$^*$ | $1.02\times10^{10}$  |

: Comparison of numerical calibration metrics. Same models and data as in @fig-eval-calib-km: models trained and tested on randomly simulated data from the `r pkg("simsurv")` [@pkgsimsurv] package in $\proba$. {#tbl-eval-calib}

<sup>
1. Harrell's C (@sec-eval-crank-disc-conc).
2. Integrated Graf Score (@sec-eval-distr-commonsurv).
3. Houwelingen's $\alpha$ (@sec-eval-distr-calib-point).
4. D-Calibration statistic. A $'*'$ indicates the model is D-Calibrated according to a $\chi^2_9$ test.
</sup>
