{{< include _setup.qmd >}}

# Introduction {#sec-intro}

Writing in the middle of a global pandemic, applications of survival analysis are more relevant than ever. Predicting the time from onset of COVID-19 symptoms to hospitalisation, or the time from hospitalisation to intubation, or intubation to death, are all time-to-event predictions that are at the centre of survival analysis. As well as morbid applications, survival analysis predictions may be concerned with predicting the time until a customer cancels their gym membership, or the lifetime of a lightbulb; any event that is guaranteed (or at least very likely) to occur can be modelled by a survival analysis prediction. As these predictions can be so sensitive, for example a model predicting when a child should be taken off breathing support [@Turing2020], the best possible predictions, evaluated to the highest standard, are a necessity. In other fields of predictive modelling, machine learning has made incredible breakthroughs (such as AlphaFold), therefore applying machine learning to survival analysis is a natural step in the evolution of an important field.

Survival analysis is the field of Statistics focusing on modelling the distribution of an event, which may mean the time until the event takes place, the risk of the event happening, the probability of the event occurring at a single time, or the event's underlying probability distribution. Survival analysis ('survival') is a unique field of study in Statistics as it includes the added difficulty of 'censoring'. Censoring is best described through example: a study is conducted to determine the mortality rate of a group of patients after diagnoses with a particular disease. If a patient dies during this study then their outcome is 'death' and their time of death can be recorded. However if a patient drops-out of the study before they die, then their time of death (though guaranteed to occur) is unknown and the only available information is the time at which they left the study. This patient is now said to be *censored* at the time they drop out. The censoring mechanism allows as much outcome information (time and event) to be captured as possible for all patients (observations).

Machine learning (ML) is the field of Statistics primarily concerned with building models to either predict outputs from inputs or to learn relationships from data [@Hastie2001;@Hastie2013]. This book is limited to the former case, or more specifically supervised learning, as this is the field in which the vast majority of survival problems live. Relative to other areas of supervised learning, development in survival analysis has been slow -- the majority of developments in machine learning for survival analysis have only been in the past decade (see chapters (@sec-review)-(@sec-eval)). This appears to have resulted in less interest in the development of machine learning survival models (@sec-review), less rigour in the evaluation of such models (@sec-eval), and fewer off-shelf/open-source implementations [@pkgmlr3proba]. This book seeks to set the foundations for clear workflows, good practice, and precise results for 'machine learning survival analysis'.

@sec-intro-motobj will elaborate further on the motivation and objectives behind this PhD; research objectives and contributions are then presented in @sec-intro-structure.

## Motivations and Objectives {#sec-intro-motobj}

Experiments throughout the literature demonstrate that machine learning survival models often perform worse (or at least no better) than classical statistical models [@Goli2016a;@KATTAN2003;@Ohno-Machado1997;@Puddu2012].^[The distinction between a 'classical' and 'machine learning' model used in this book is provided in @sec-sec-review.] This book sets out to explore why this is the case and how this has potential to be improved. The following questions, based on observations of the field, motivated this book:

### Why are regression and classification more popular than survival analysis in machine learning?

There is no doubt that this is the case, for example the 'bibles of machine learning' [@Bishop2006;@Hastie2001;@Hastie2013] discuss classification and regression in detail but survival analysis is never discussed. Survival analysis has important applications in healthcare, finance, engineering and more, all fields that directly impact upon individual lives on a day-to-day basis, and should perhaps be considered as important as classification and regression. The result of this gap in interest, is the erroneous assumption that one field can be directly applied to another. For example there is evidence of researchers treating censoring as a nuisance to be ignored and using regression models instead [@Schwarzer2000]. Censoring is indeed a challenge and may contribute to making survival analysis less accessible than other fields, but this need not be the case; a clear unification of terminology and presentation of methods may help make 'machine learning survival analysis' more accessible. Added accessibility could lead to more academics (and non-academics) engaging with the field and promoting good standards of practice, as well as developing more novel models and measures.

**Why are probabilistic survival predictions important?**

Development of survival models appears to be skewed towards 'ranking models', which predict the relative risk of an event occurring (@sec-surv-set-types). In many applications these predictions are sufficient, for example in randomised control trials if assessing the increased/decreased risk of an event after treatment. However, there are many use-cases where predicting an individual's survival probability distribution is required. Take, for example, an engineer calculating the lifetime of a plane's engine.^[In this engineering context, survival analysis is usually referred to as reliability analysis.] There are three important reasons to replace a jet engine at the optimal time:

* financial: jet engines are very expensive and replacing one sooner than required is a waste of money;
* environmental: an engine being replaced too early is a waste of potential usage;
* safety: if the engine is replaced too late then there is a risk to passengers.

Now consider examples for the three possible 'prediction types' the engineer can make:

i. A 'relative risk prediction': This engine is twice as likely to fail as another.
i. A 'survival time prediction': The engine is expected to fail in 30 days.
i. A 'survival distribution prediction': The lifetime of the engine is distributed according to the probability distribution $\zeta$.

The first prediction type is not useful as the underlying relative risk may be unknown and the engineer is concerned with the individual lifetime. The second prediction type provides a useful quantity for the engineer to work with however there is no uncertainty captured in this prediction. The third prediction type can capture the uncertainty of failure over the entirety of the positive Reals (though usually only a small subset is possible and useful). With this final prediction type, the engineer can create safe decisions: 'replace the engine at time $\tau$, where $\tau$ is the time when the predicted probability of survival drops below 60%, $S(\tau) = 0.6$'. There are ethical, economic, and environmental reasons for a good survival distribution prediction and this book considers a distribution prediction to be the most important prediction type.

**How are survival models evaluated?**

Evaluating predictions from survival models is of the utmost importance. This is especially important as survival models are often deployed in the public domain, particularly in healthcare. Physical products in healthcare, such as new vaccines, undergo rigorous testing and research in randomised control trials before being publically deployed; the same level of rigour should be expected for the evaluation of survival models that are used in life-and-death situations. Evaluation measures for regression and classification are well-understood with important properties, however survival measures have not undergone the same treatment. For example many survival models are still being evaluated solely with concordance indices that have been repeatedly crticised [@Gonen2005;@Rahman2017;@Schmid2012]. This paper argues for the use of scoring rules (@sec-eval-distr), which simultaneously assess predictions of distribution and relative risk.

Motivated by these questions, this book attempts to unify the two fields of machine learning and survival analysis to make the intersection of the two ('machine learning survival analysis') more concise and accessible. This aim is guided by three key themes: Accessibility, Transparency, and Performance. These are now briefly described to explain why they have been identified as key principles for this book.

### Accessibility, Transparency, and Predictive Performance {#sec-intro-motobj-tap}

In all critical analyses there must be a metric with which to judge the surveyed objects. For example, machine learning models may be judged by predictive performance, i.e. does one model outperform another? Or estimators may be judged according to bias and consistency properties. As this book compares multiple different types of objects, a more universal criteria is applied for the reviews, surveys, and comparisons. These are: Accessibility, Transparency, and (predictive) Performance. A model that satisfies all three criteria may be considered APT (accessible, transparent, performant). These key themes are now briefly described and then further discussion is given to why all must be satisfied for this book to consider a model or measure to be 'good'. These are primarily explained in terms of a 'model', though all extend naturally to other objects.

A model is termed *accessible* if there either exists an open-source implementation of the model, or sufficient infrastructure and published mathematics for the model to be implementable.^[The term 'accessible' is slightly more general than terms such as 'off-shelf' as accessibility is defined to include objects that are not off-shelf but that can be implemented given information provided in the literature.] For example, a novel neural network without an open-source implementation can still be accessible if the model's architecture is clearly described and can therefore be implemented with neural network packages such as TensorFlow [@pkgtensorflow].

A model is called *transparent* if its properties are well-understood, its use and manipulation of data is clear, and its predictions have a precise interpretation. The word 'transparent' does not refer to the inner workings of the model and therefore a transparent model could still be a 'black-box'.^[Therefore the term 'transparent' here does not refer to the concept of a 'glass-box' model, which is the opposite of a black-box model.] For example, random forests (@sec-surv-ml-models-ranfor) are built of hundreds or thousands of individual predictive models, thus making it impossible to fully identify how the final prediction is created. However the model is considered transparent as it is mathematically clear and intuitive how it utilises the individual components to produce its prediction.

A model has good predictive *performance* if its predictions are notably improved over some baseline model [@Gressmann2018]. Unlike transparency and accessibility, it is possible to quantify performance and compare this between models (@sec-eval). Whilst there is often a trade-off between predictive performance and model interpretability (e.g. compare neural networks and linear regression), this is not the case for predictive performance and transparency. When considering non-predictive objects, such as measures, then performance instead refers to verifying other established performance properties, for example consistency, unbiasedness, and robustness. An object with good performance is called 'performant'.

Performance is traditionally the primary metric by which models (and measures) are judged, but this book only considers a model to be 'good' (or APT) if all three of these themes are satisfied. In fact, it can be demonstrated that if even one of these conditions is not satisfied a model can be dishonest or inefficient.

By example, take the model that always predicts the height of a person as 42cm. This model is very accessible and transparent but has terrible predictive performance, the model is therefore useless. Now consider a patented model without open-source implementation that not only makes perfect predictions but is also clearly described. In this case as no accessible implementation exists, the model cannot be used and tested by the community and more importantly cannot be externally validated, leading to ethical questions about commercial implementation and even whether the results can be trusted. Finally, in the case of an accessible model with strong predictive performance but without clear description in a paper or reader-friendly code/documentation, there can only be limited trust in the model's performance, especially with respect to future performance.

### Code and Reproducibility

This book includes simulations and figures generated in $\Rstats$, the code for any figures or experiments in this book are freely available at [https://github.com/RaphaelS1/MLSA](https://github.com/RaphaelS1/MLSA) under an MIT licence and all content on this website is available under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/).
We use `r pkg("renv")` to manage package versions, you can find our lockfile at `r link("https://github.com/RaphaelS1/MLSA/blob/main/book/renv.lock")`.