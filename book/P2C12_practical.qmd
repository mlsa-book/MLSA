---
abstract: TODO (150-200 WORDS)
---

::: {.content-visible when-format="html"}
{{< include _macros.tex >}}
:::

# Practical Considerations {#sec-eval-choose}

{{< include _wip_minor.qmd >}}

After reading this part of the book, evaluating survival analysis models may appear more daunting than regression and classification settings, which, in contrast, have fewer (common) measures to choose from.
In regression problems, the RMSE and MAE are common choices for evaluating how far predictions are from the truth.
In classification, the Brier score or logloss may be used to evaluate probabilistic predictions and the accuracy score or TPR/TNR/FPR/FNR are common for deterministic predictions.
In contrast, there are many more measures in survival analysis which are necessarily more complex, due to the need to handle censoring.
Moreover, this Part of the book has only considered the single-event setting thus far.

Therefore, this final chapter aims to provide some simple practical guidelines for selecting measures for different types of experiments and for extending measures to competing risk settings.

## Competing risks {#sec-meas-cr}

Following the same measure grouping as in the rest of this Part of the book, we now examine how measures can be extended to the competing risks setting.
Discrimination measures and scoring rules are grouped together as their competing risk extensions are the same.

### Discrimination measures and scoring rules

Discrimination measures and scoring rules are usually extended to the competing risk setting by evaluating cause-specific probabilities individually and then potentially summing or averaging over cause-specific measures [@vanGeloven2022; @Lee2018a; @Bender2021].

To recap, given $k$ possible events then the cause-specific hazard for an event is defined as $h_{e}$:

$$
h_{e}(\tau) = \lim_{\Delta \tau \to 0} \frac{P(\tau \leq Y \leq \tau + \Delta \tau, E = e\ |\ Y \geq \tau)}{\Delta \tau}, \; e = 1, \dots, k.
$$

where $Y$ is the random variable representing the time-to-event and $E\in \{1,\ldots,k\}$ is the random variable with realizations $e$, which denotes one of $k$ competing events that can occur at event time $Y$.
Given the cause-specific hazard $h_e$, one can then derive the cause-specific survival, $S_e$, density, $f_e$, and cumulative distribution function, $F_e$; from which all above measures can be computed.
For example, the right-censored log-loss for event $e$ is defined as

$$
L^e_{RCLL;i}(\hatS_{i;e}, t_i, \delta_i) = -\log[\delta_i\hatf_{i;e}(t_i) + (1-\delta_i)\hatS_{i;e}(t_i)]
$$

A measure of overall predictive performance can then be defined as

$$
L^A_{RCLL} = \sum^k_{e=1} \frac{1}{n} \sum^n_{i=1} L^e_{RCLL;i}(\hatS_{i;e}, t_i, \delta_i)
$$ {#eq-surv-rcll-cr}

This 'all-cause' loss could then be minimized in an automated procedure and/or used for model comparison.
This assumes that better/worse performance should be treated the same for all causes.
However, if this is not the case then either a weighted sum needs to be applied, or multi-objective optimization methods must be used [@Morales-Hernandez2023].
Note that dividing @eq-surv-rcll-cr by $k$ to find the average performance is not useful in practice.
As already stated, the numerical value of a loss rule does not have any inherent meaning beyond comparison to another model's performance within the same experiment, hence dividing all losses by the same constant (i.e., $k$) does not change the interpretation.

In a discrimination context, analogous results have been proposed.
In a single-event setting, a pair of observations, $(i,j)$, with outcomes $\{(t_i,\delta_i),(t_j,\delta_j)\}$, and predicted risk predictions, $r_i,r_j \in \Reals$, are called comparable if $t_i < t_j$ and $\delta_i = 1$; and concordant if also $r_i > r_j$.
In a competing risks setting, for an event of interest $e$, the observations are [@vanGeloven2022]:

* *Comparable* if $t_i < t_j$ and $\delta_{ie} = 1$; and
* *Concordant* if $r_i > r_j$

Where $\delta_{ie} = 1$ is equivalent to $\II(Y_i \leq C_i \wedge E_i = e)$.

The usual definition of concordance measures then follow given this additional conditioning on the event of interest $e$.
In practice, given that competing risk models estimate cause-specific hazard functions, it is Antolini's time-dependent concordance measure that suits the competing risks setting best.
Hence for an event $e$, the measure is given by:

$$
C(\hat{\mathbf{h}}_e, \tt, \bsdelta|\tau) = \frac{\sum_{i\neq j} W(t_i)\II(t_i < t_j, \hat{h}_{e_i}(t_i) > \hat{h}_{e_j}(t_i), t_i < \tau)\delta_{ie}}{\sum_{i\neq j}W(t_i)\II(t_i < t_j, t_i < \tau)\delta_{ie}}
$$

where $\hat{\hh}_e = (\hath_{e_1} \ \hath_{e_2} \cdots \hath_{e_m})^\trans$ are cause specific hazards for individual observations risk of event $e$.

In contrast to scoring rules, the average performance over all causes when using concordance measures could be more meaningful than just the sum.

### Calibration measures

Numerical methods have been proposed for calibration in a competing risk setting, including using pseudo-measures [@Schoop2011] and overall calibration ratios [@vanGeloven2022].
However, these are complex to implement and interpret and therefore graphical methods are more often used in practice [@Monterrubio2024].

Recall the single-event setting where predictions of the survival function are plotted against the 'true' survival distribution, i.e., the Kaplan-Meier estimator fit on the testing data.
In the competing risks setting, an analogous process is followed but in this case by plotting the predicted vs 'true' cumulative incidence function [@Austin2022; @Wolbers2009].

As discussed in @sec-eval-distr-calib, interpreting and using calibration measures is complex enough in the single event setting.
In the competing risks setting, it is even harder to understand exactly what is being calibrated and how by looking at multiple plots.
Moreover, considering that these plots are often computed at multiple time-points and then for multiple events of interest, it is easy to imagine having to decipher 10s or even 100s of plots.
Using scoring rules to capture calibration performance in the competing risks setting is likely to be more straightforward.

### Survival time predictions

In a competing risks setting, there is no obvious metric to evaluate survival time predictions, primarily because there is no meaningful interpretation for an 'all-cause survival time' or a 'cause-specific survival time'.
If an observation could realistically experience one of multiple, mutually-exclusive events, then predicting the time to one particular event has no inherent meaning without first attaching a probability of the event taking place (i.e., the CIF), hence evaluating this probability is a more sensible approach for evaluating a competing risks model's predictive performance.

## Selecting measures

Deciding what measure to use depends on the experiment at hand.
Experiments may be performed to make predictions for new data, compare the performance of multiple models ('benchmark experiments'), investigate patterns in observed data, or some combination of these.
Each experiment requires different choices of measures, with different levels of strictness applied to measure assumptions.

### Predictive experiments

In the real world, predictive experiments are most common.
These are now daily occurrences as machine learning models are routinely deployed on servers to make ongoing predictions.
In these cases, the exact task must be precisely stated before any model is deployed and evaluated.
Common survival problems to solve include:

1. Identifying low and high risk groups in new data (for resource allocation);
2. Predicting the survival distribution for an individual over time; and
3. Predicting the survival probability for an individual at a specific time.

The first of these is a discrimination problem and it is therefore most important that the model optimises corresponding measures and that measure assumptions are justified.
However, even this task may be more complex than it initially seems.
For example, while some papers have shown flaws in Harrell's C [@Gonen2005; @Rahman2017; @Schmid2012; @Uno2007], others have demonstrated that common alternatives yield very similar results [@Rahman2017; @Therneau2020] and moreover some prominent alternatives may be harder to interpret due to high variance [@Rahman2017; @Schmid2012].
In predictive experiment that may require more level of automation, it is important to be careful of C-hacking (@sec-eval-crank-choose) and to avoid overoptimistic results.
Hence one should not compute a range of concordance indices and report the maximum but instead calculate a single discrimination measure and then establish a pre-defined threshold to determine if the deployed model is optimal, a natural threshold would be 0.5 as anything above this is better than a baseline model.
Given Harrell's C to be increasingly over-optimistic with additional censoring [@Rahman2017], it is advisable to use Uno's C instead.

If the task of interest is to predict survival distributions *over time*, then the choice of measure is more limited and only the RCLL and the proper Graf score are recommended.
Both these measures can only be interpreted with respect to a baseline so use of the ERV representation is strongly recommended.
As with the previous task, establishing a threshold for performance is essential prior to deployment and for ongoing evaluation.
It is less clear in these cases what this threshold might be, but the simplest starting point would be to ensure that the model continues to outperform the baseline or a simpler gold-standard model (e.g., the Cox PH).

The final task of interest differs from the previous by only making predictions at a specific time.
In this case, prediction error curves, and single-time point calibration measures can be used, as well as scoring rules with shorter cut-offs (i.e., the upper limit of the integral).
It is imperative that model performance is never extrapolated outside of the pre-specified time.

### Benchmark experiments

When conducting benchmark experiments, it is advisable to use a spread of measures so that results can be compared across various properties.
In this case, models should be tested against discrimination, calibration, and overall predictive ability (i.e., with scoring rules).
As models make different types of predictions, results from these experiments should be limited to metrics that are directly comparable, in other words, two models should only be compared based on the *same* metric.
In benchmark experiments, models are compared across the same data and same resampling strategy, hence measure assumptions become less important as they are equally valid or flawed for all models.
For example, if one dataset has particularly high amounts of censoring leading to an artificially higher concordance index, then this bias would affect all models equally and the overall experiment would not be affected.
Hence, in these experiments it suffices to pick one or two measures for concordance, discrimination, and predictive ability, without having to be overly concerned with the individual metric.

This book recommends using Harrell's C and Uno's C for concordance as these are simplest to compute and including both enables more confidence in model comparison, i.e., if a model outperforms another with respect to both these measures then there can be higher confidence in drawing statements about the model's discriminatory power.
For calibration, D-calibration is recommended as it can be meaningfully compared between models, and the RCLL is recommended for a scoring rule (which is proper for outcome-independent censoring).
No distance measure is recommended as these do not apply to the vast majority of models.
All these measures can be used for automated tuning, in the case of discrimination tuning to Harrell's C alone should suffice (without also tuning to Uno's C).

### Investigation

Investigating patterns in observed data is increasingly common as model interpretability methods have become more accessible [@Molnar2019].
Before data can be investigated, any model that is trained on the data must first be demonstrated to be a good fit to the data.
A model's fit to data can also be evaluated by resampling the data (@sec-ml) and evaluating the predictions.
In this case, it is important to choose measures that are interpretable and have justified assumptions.
Calibration measures are particularly useful for evaluating if a model is well fit to data, and any of the methods described in @sec-eval-distr-calib are recommended for this purpose.
Discrimination measures *may* be useful, however, given how susceptible they are to censoring, they can be difficult to interpret on their own, and the same is true for scoring rules.
One method to resolve ambiguity is to perform a benchmark experiment of multiple models on the same data (ideally with some automated tuning) and then select the best model from this experiment and refit it on the full data [@Becker2024] -- this is a robust, empirical method that demonstrates a clear trail to selecting a model that outperforms other potential candidates.
When investigating a dataset, one may also consider using different measures to assess algorithmic fairness [@Sonabend2022a], any measure that can be optimised (i.e., where the lowest or highest value is the best) may be used in this case.
Finally, there are survival adaptations to the well-known AIC [@Liang2008] and BIC [@VolinskyRaftery2000] however as these are generally only applicable to 'classical ' models (@sec-models-classical), they are out of scope for this book and hence have not been discussed.
<!-- FIXME: UPDATE 'classical' to whatever term we use for that chapter -->

## Conclusions

This part of the book focused on survival measures.
Measures may be used to evaluate model predictions, to tune a model, or to train a model (e.g., in boosting or neural networks).
Unlike other settings, there are many different choices of survival measures and it can be hard to determine which to use and when.
In practice, like many areas of Statistics, the most important factor is to clearly define any experiment upfront and to be clear about which measures will be used and why.
As a rule of thumb, good choices for measures are Harrell's C for evaluating discrimination, with Uno's C supporting findings, D-calibration for calibration, and the RCLL for evaluating overall predictive ability from distribution predictions.
Finally, if you are restricted to a single measure choice (e.g., for automated tuning or continuous evaluation of deployed models), then we recommended selecting a scoring rule such as RCLL which captures information about calibration and discrimination simultaneously.
